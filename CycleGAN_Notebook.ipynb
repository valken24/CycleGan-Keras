{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from keras_contrib.layers import GroupNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Add\n",
    "from keras.layers import BatchNormalization, Activation, MaxPooling2D, Conv2DTranspose\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"checkpoint/\"\n",
    "SAVED_IMAGES_PATH = \"saved_images/\"\n",
    "LOG_PATH = \"checkpoint/log.txt\"\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Saves Model in every N minutes\n",
    "TIME_INTERVALS = 2 \n",
    "\n",
    "# Show Summary of Models\n",
    "SHOW_SUMMARY = True\n",
    "\n",
    "# Learning rate of Generator and Discriminator\n",
    "DISCRIMINATOR_LR_RATE = 0.0002\n",
    "GENERATOR_LR_RATE = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Image Dimensions\n",
    "IMAGE_ROWS = 256\n",
    "IMAGE_COLS = 256\n",
    "IMAGE_CHANNELS = 3\n",
    "IMAGE_SHAPE = (IMAGE_ROWS, IMAGE_COLS, IMAGE_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import Data\n",
    "data = Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISCRIMINIATOR (PATHGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Valken\\Anaconda3\\envs\\CondaPy\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "d_layers = 3 # discriminator size\n",
    "d_filter_size = 64 # filter size of first layer\n",
    "d_dropout = 0.4\n",
    "d_loss = 'mse'\n",
    "d_optimizer = Adam(DISCRIMINATOR_LR_RATE, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch shape:  (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(IMAGE_ROWS / 2**(d_layers))\n",
    "patch = (patch, patch, 1)\n",
    "print(\"Patch shape: \", patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_d_layer(layer_input, filters, f_size=4, strides =2, normalization=True, dropout_rate=d_dropout):\n",
    "    \n",
    "    d = Conv2D(filters, kernel_size=f_size, strides=strides, padding='same')(layer_input)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    if dropout_rate:\n",
    "        d = Dropout(dropout_rate)(d)\n",
    "    if normalization:\n",
    "        d = InstanceNormalization()(d)\n",
    "    return d\n",
    "    \n",
    "    \n",
    "def build_discriminator():\n",
    "    image = Input(shape=IMAGE_SHAPE)\n",
    "    layers  = []\n",
    "    \n",
    "    # Making  N (d_layers) Layers \n",
    "    for i in range(d_layers):\n",
    "        filter_size = d_filter_size * (2 ** i)\n",
    "        if not i:\n",
    "            layer = build_d_layer(image, filter_size, normalization=False)\n",
    "        else:\n",
    "            layer = build_d_layer(layers[-1], filter_size)\n",
    "            \n",
    "        layers.append(layer)\n",
    "    layers.append(layer)    \n",
    "    \n",
    "    confidence = Conv2D(1, kernel_size=4, strides=1,activation='sigmoid', padding='same')(layers[-1])\n",
    "    \n",
    "    # Model(input, output)\n",
    "    return Model(image, confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Valken\\Anaconda3\\envs\\CondaPy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Summary DCRM_A:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_1 (In (None, 64, 64, 128)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_2 (In (None, 32, 32, 256)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 1)         4097      \n",
      "=================================================================\n",
      "Total params: 662,981\n",
      "Trainable params: 662,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Summary DCRM_B:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 128, 64)      3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_3 (In (None, 64, 64, 128)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_4 (In (None, 32, 32, 256)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 1)         4097      \n",
      "=================================================================\n",
      "Total params: 662,981\n",
      "Trainable params: 662,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Discriminator Initialization\n",
    "DCRM_A = build_discriminator()\n",
    "DCRM_B = build_discriminator()\n",
    "\n",
    "# Compile the Discriminator Model\n",
    "DCRM_A.compile(loss=d_loss, optimizer=d_optimizer, metrics=['accuracy'])\n",
    "DCRM_B.compile(loss=d_loss, optimizer=d_optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Show Summary\n",
    "if SHOW_SUMMARY:\n",
    "    print('Summary DCRM_A:')\n",
    "    DCRM_A.summary()\n",
    "    print('Summary DCRM_B:')\n",
    "    DCRM_B.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATOR (U-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_filter_size = 64 # filter size of first layer\n",
    "g_dropout = 0.2\n",
    "dropout = 0.25\n",
    "g_optimizer = Adam(GENERATOR_LR_RATE, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv2d(layer_input, filters, strides=2, f_size=4):\n",
    "    g = Conv2D(filters, kernel_size=f_size, strides=strides, padding='same')(layer_input)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    return g\n",
    "\n",
    "\n",
    "def build_deconv2d(layer_input, skip_input, filters, strides=2, f_size=4, dropout_rate=g_dropout):\n",
    "    g = Conv2DTranspose(filters, kernel_size=f_size, strides=strides, padding='same')(layer_input)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    if dropout_rate:\n",
    "        g = Dropout(dropout_rate)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Concatenate()([g, skip_input])\n",
    "    return g\n",
    "\n",
    "\n",
    "def build_generator():\n",
    "    image = Input(shape=IMAGE_SHAPE)\n",
    "    layers = []\n",
    "    \n",
    "    #  DownSampling the layers\n",
    "    conv1 = build_conv2d(image, g_filter_size)\n",
    "    conv2 = build_conv2d(conv1, g_filter_size*2)\n",
    "    conv3 = build_conv2d(conv2, g_filter_size*4)\n",
    "    conv4 = build_conv2d(conv3, g_filter_size*8)\n",
    "    conv5 = build_conv2d(conv4, g_filter_size*16)\n",
    "    \n",
    "    # UpSampling the Layers\n",
    "    deconv1 = build_deconv2d(conv5, conv4, g_filter_size*8)\n",
    "    deconv2 = build_deconv2d(deconv1, conv3, g_filter_size*4)\n",
    "    deconv3 = build_deconv2d(deconv2, conv2, g_filter_size*2)\n",
    "    deconv4 = build_deconv2d(deconv3, conv1, g_filter_size)\n",
    "    deconv5 = UpSampling2D(size=2)(deconv4)\n",
    "    \n",
    "    generated_image = Conv2D(IMAGE_CHANNELS, kernel_size=4, strides=1, padding='same', activation='tanh')(deconv5)\n",
    "    \n",
    "    return Model(image, generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEN_AB\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 128, 128, 64) 3136        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 128, 128, 64) 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_5 (Insta (None, 128, 128, 64) 2           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 64, 128)  131200      instance_normalization_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 64, 64, 128)  0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_6 (Insta (None, 64, 64, 128)  2           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 256)  524544      instance_normalization_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 32, 32, 256)  0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_7 (Insta (None, 32, 32, 256)  2           leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 512)  2097664     instance_normalization_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_8 (Insta (None, 16, 16, 512)  2           leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 1024)   8389632     instance_normalization_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 8, 8, 1024)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_9 (Insta (None, 8, 8, 1024)   2           leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 16, 16, 512)  8389120     instance_normalization_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 16, 16, 512)  0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_10 (Inst (None, 16, 16, 512)  2           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 1024) 0           instance_normalization_10[0][0]  \n",
      "                                                                 instance_normalization_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 32, 32, 256)  4194560     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 32, 256)  0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_11 (Inst (None, 32, 32, 256)  2           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 512)  0           instance_normalization_11[0][0]  \n",
      "                                                                 instance_normalization_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 64, 64, 128)  1048704     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 64, 64, 128)  0           conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 64, 64, 128)  0           leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_12 (Inst (None, 64, 64, 128)  2           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 256)  0           instance_normalization_12[0][0]  \n",
      "                                                                 instance_normalization_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 128, 128, 64) 262208      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 128, 128, 64) 0           conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128, 128, 64) 0           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_13 (Inst (None, 128, 128, 64) 2           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128, 128, 128 0           instance_normalization_13[0][0]  \n",
      "                                                                 instance_normalization_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 256, 256, 128 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 256, 256, 3)  6147        up_sampling2d_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 25,046,933\n",
      "Trainable params: 25,046,933\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "GEN_BA\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 64) 3136        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 128, 128, 64) 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_14 (Inst (None, 128, 128, 64) 2           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  131200      instance_normalization_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 64, 64, 128)  0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_15 (Inst (None, 64, 64, 128)  2           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 256)  524544      instance_normalization_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_16 (Inst (None, 32, 32, 256)  2           leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 512)  2097664     instance_normalization_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_17 (Inst (None, 16, 16, 512)  2           leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 1024)   8389632     instance_normalization_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 8, 8, 1024)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_18 (Inst (None, 8, 8, 1024)   2           leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 16, 16, 512)  8389120     instance_normalization_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16, 16, 512)  0           leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_19 (Inst (None, 16, 16, 512)  2           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 16, 1024) 0           instance_normalization_19[0][0]  \n",
      "                                                                 instance_normalization_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 32, 32, 256)  4194560     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 256)  0           leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_20 (Inst (None, 32, 32, 256)  2           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 512)  0           instance_normalization_20[0][0]  \n",
      "                                                                 instance_normalization_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 64, 64, 128)  1048704     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 64, 64, 128)  0           conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 64, 64, 128)  0           leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_21 (Inst (None, 64, 64, 128)  2           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64, 64, 256)  0           instance_normalization_21[0][0]  \n",
      "                                                                 instance_normalization_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 128, 128, 64) 262208      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 128, 128, 64) 0           conv2d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 128, 128, 64) 0           leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_22 (Inst (None, 128, 128, 64) 2           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128, 128, 128 0           instance_normalization_22[0][0]  \n",
      "                                                                 instance_normalization_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 128 0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 3)  6147        up_sampling2d_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 25,046,933\n",
      "Trainable params: 25,046,933\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Generator Initialization\n",
    "GEN_AB = build_generator()\n",
    "GEN_BA = build_generator()\n",
    "\n",
    "# Generator input PlaceHolder\n",
    "IMG_A = Input(shape=IMAGE_SHAPE)\n",
    "IMG_B = Input(shape=IMAGE_SHAPE)\n",
    "\n",
    "# Generator Summary\n",
    "if SHOW_SUMMARY:\n",
    "    print('GEN_AB')\n",
    "    GEN_AB.summary()\n",
    "    print('GEN_BA')\n",
    "    GEN_BA.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMBINE NETWORK - CYCLEGAN (ADVERSARIAL NET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare fake images from Generator\n",
    "FAKE_IMG_A = GEN_BA(IMG_B)\n",
    "FAKE_IMG_B = GEN_AB(IMG_A)\n",
    "\n",
    "# Reconstruct fake images back to original\n",
    "RECONSTRUCT_A = GEN_BA(FAKE_IMG_B)\n",
    "RECONSTRUCT_B = GEN_AB(FAKE_IMG_A)\n",
    "\n",
    "# Original Idendity of the Image\n",
    "ID_A = GEN_BA(IMG_A)\n",
    "ID_B = GEN_AB(IMG_B)\n",
    "\n",
    "# Discriminator Model shouldn't be affected during Adverserial(Combined Model) Optimization\n",
    "# So the discriminator model is frozen \n",
    "DCRM_A.trainable = False\n",
    "DCRM_B.trainable = False\n",
    "\n",
    "# Discriminator Confidence of Fake images\n",
    "CONF_FAKE_IMG_A = DCRM_A(FAKE_IMG_A)\n",
    "CONF_FAKE_IMG_B = DCRM_B(FAKE_IMG_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZE ADVERSIAL NET - COMBINED NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The Combined model will calculate all this loss:\n",
    "1) Discriminator loss of generated image\n",
    "2) Reconstruction loss from generated image back to the original image\n",
    "3) Identity loss from original image to generated image\n",
    "\n",
    "This loss will be used to optimize the Generator Model\n",
    "'''\n",
    "COMBINED = Model([IMG_A, IMG_B],\n",
    "                [CONF_FAKE_IMG_A, CONF_FAKE_IMG_B,\n",
    "                 RECONSTRUCT_A, RECONSTRUCT_B,\n",
    "                 ID_A, ID_B])\n",
    "\n",
    "COMBINED.compile(loss= ['mse', 'mse',\n",
    "                        'mae', 'mae',\n",
    "                        'mae', 'mae'], \n",
    "                 loss_weights = [1, 1,\n",
    "                                 10.0, 10.0,\n",
    "                                 1.0, 1.0],\n",
    "                 optimizer = g_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "    - Saving Model\n",
    "    - Loading Model\n",
    "    - Saving Logs\n",
    "    - Save Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Discriminator and Generator Models at Checkpoint Path\n",
    "def save_model():\n",
    "    global DCRM_A, DCRM_B, GEN_AB, GEN_BA\n",
    "    models = [DCRM_A, DCRM_B, GEN_AB, GEN_BA]\n",
    "    model_names = ['DCRM_A', 'DCRM_B', 'GEN_AB', 'GEN_BA']\n",
    "\n",
    "    for model, model_name in zip(models, model_names):\n",
    "        model_path =  CHECKPOINT + \"%s.json\" % model_name\n",
    "        weights_path = CHECKPOINT + \"/%s.hdf5\" % model_name\n",
    "        options = {\"file_arch\": model_path, \n",
    "                    \"file_weight\": weights_path}\n",
    "        json_string = model.to_json()\n",
    "        open(options['file_arch'], 'w').write(json_string)\n",
    "        model.save_weights(options['file_weight'])\n",
    "    print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Discriminator and Generator Models from CHECKPOINT Path\n",
    "def load_model():\n",
    "    # Checking if all the model exists\n",
    "    model_names = ['DCRM_A', 'DCRM_B', 'GEN_AB', 'GEN_BA']\n",
    "    files = os.listdir(CHECKPOINT)\n",
    "    for model_name in model_names:\n",
    "        if model_name+\".json\" not in files or\\\n",
    "           model_name+\".hdf5\" not in files:\n",
    "            print(\"Models not Found\")\n",
    "            return\n",
    "    \n",
    "    global DCRM_A, DCRM_B, GEN_AB, GEN_BA, d_optimizer, d_loss\n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "    \n",
    "    # load DCRM_A Model\n",
    "    model_path = CHECKPOINT + \"%s.json\" % 'DCRM_A'\n",
    "    weight_path = CHECKPOINT + \"%s.hdf5\" % 'DCRM_A'\n",
    "    with open(model_path, 'r') as f:\n",
    "        DCRM_A = model_from_json(f.read(), {'InstanceNormalization': InstanceNormalization})\n",
    "    DCRM_A.load_weights(weight_path)\n",
    "    DCRM_A.compile(loss=d_loss, optimizer=d_optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # load DCRM_B Model\n",
    "    model_path = CHECKPOINT + \"%s.json\" % 'DCRM_B'\n",
    "    weight_path = CHECKPOINT + \"%s.hdf5\" % 'DCRM_B'\n",
    "    with open(model_path, 'r') as f:\n",
    "        DCRM_B = model_from_json(f.read(), {'InstanceNormalization': InstanceNormalization})\n",
    "    DCRM_B.load_weights(weight_path)\n",
    "    DCRM_B.compile(loss=d_loss, optimizer=d_optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # # load GEN_AB Model\n",
    "    model_path = CHECKPOINT + \"%s.json\" % 'GEN_AB'\n",
    "    weight_path = CHECKPOINT + \"%s.hdf5\" % 'GEN_AB' \n",
    "    with open(model_path, 'r') as f:\n",
    "        GEN_AB = model_from_json(f.read(), {'InstanceNormalization': InstanceNormalization})\n",
    "    GEN_AB.load_weights(weight_path)\n",
    "\n",
    "    #load GEN_BA Model\n",
    "    model_path = CHECKPOINT + \"%s.json\" % 'GEN_BA'\n",
    "    weight_path = CHECKPOINT + \"%s.hdf5\" % 'GEN_BA'\n",
    "    with open(model_path, 'r') as f:\n",
    "         GEN_BA = model_from_json(f.read(), {'InstanceNormalization': InstanceNormalization})\n",
    "    GEN_BA.load_weights(weight_path)\n",
    "        \n",
    "    print(\"Loaded Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving log in LOG_PATH\n",
    "def save_log(log):\n",
    "    with open(LOG_PATH, 'a') as f:\n",
    "        f.write(\"%s\\n\" %log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save images to SAVE_IMAGE directory\n",
    "#          #################################\n",
    "# img =    #IMG_A, FAKE_A, RECONSTRUCTED_A#\n",
    "#          #IMG_B, FAKE_B, RECONSTRUCTED_B#\n",
    "           ################################\n",
    "\n",
    "def save_image(epoch, steps):\n",
    "    TEST_DATA_A, TEST_DATA_B = data.get_data(1)\n",
    "    # If TEST_DATA is None, data exhausted\n",
    "    # Reinitialise TEST_DATA\n",
    "    if TEST_DATA_A is None or TEST_DATA_B is None:\n",
    "        TEST_DATA_A, TEST_DATA_B = data.get_data(1)\n",
    "    \n",
    "    for i in range(TEST_DATA_A.shape[0]):\n",
    "        original_A = TEST_DATA_A[i]\n",
    "        original_B = TEST_DATA_B[i]\n",
    "        real_A = original_A / 127.5 - 1\n",
    "        real_B = original_B / 127.5 - 1\n",
    "        real_A = np.reshape(real_A, [1, real_A.shape[0], real_A.shape[1], real_A.shape[2]])\n",
    "        real_B = np.reshape(real_B, [1, real_B.shape[0], real_B.shape[1], real_B.shape[2]])\n",
    "        \n",
    "        fake_A = GEN_BA.predict(real_B)\n",
    "        fake_A = (fake_A + 1) * 127.5\n",
    "        fake_A = fake_A.astype(np.uint8)\n",
    "        fake_B = GEN_AB.predict(real_A)\n",
    "        fake_B = (fake_B + 1) * 127.5\n",
    "        fake_B = fake_B.astype(np.uint8)\n",
    "        \n",
    "        recon_A = GEN_BA.predict(fake_B)\n",
    "        recon_A = (recon_A + 1) * 127.5\n",
    "        recon_A = recon_A.astype(np.uint8)\n",
    "        recon_B = GEN_AB.predict(fake_A)\n",
    "        recon_B = (recon_B + 1) * 127.5\n",
    "        recon_B = recon_B.astype(np.uint8)\n",
    "        \n",
    "        image_1 = np.concatenate((original_A, fake_B[0], recon_A[0]), axis=1)\n",
    "        image_2 = np.concatenate((original_B, fake_A[0], recon_B[0]), axis=1)\n",
    "        image = np.concatenate((image_1, image_2), axis=0)\n",
    "        image_path = SAVED_IMAGES_PATH + \"_\".join([str(epoch), str(steps)]) + \".jpg\"\n",
    "        cv2.imwrite(image_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    start_time = datetime.now()\n",
    "    saved_time = start_time\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        steps = 1\n",
    "        while True:\n",
    "            \n",
    "            imgs_A, imgs_B = data.get_data(BATCH_SIZE)\n",
    "            \n",
    "            # If value is None it means it is out of data,\n",
    "            # it auto resets the data loader and breaks out of 'while' loop and starts as next batch\n",
    "            if imgs_A is None or imgs_B is None:\n",
    "                break\n",
    "                        \n",
    "            # assign local batch_size \n",
    "            batch_size = imgs_A.shape[0]\n",
    "            \n",
    "            # Rescale the image value from 255 => -1 to 1\n",
    "            imgs_A = imgs_A / 127.5 - 1\n",
    "            imgs_B = imgs_B / 127.5 - 1\n",
    "            \n",
    "            # Discriminator Ground Truth\n",
    "            real = np.ones((batch_size,) + patch)\n",
    "            fake = np.zeros((batch_size,) + patch)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            fake_A = GEN_BA.predict(imgs_B)\n",
    "            fake_B = GEN_AB.predict(imgs_A)\n",
    "            \n",
    "            dA_loss_real = DCRM_A.train_on_batch(imgs_A, real)\n",
    "            dA_loss_fake = DCRM_A.train_on_batch(fake_A, fake)\n",
    "            dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "            \n",
    "            dB_loss_real = DCRM_B.train_on_batch(imgs_B, real)\n",
    "            dB_loss_fake = DCRM_B.train_on_batch(fake_B, fake)\n",
    "            dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "            \n",
    "            d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "            \n",
    "            \n",
    "            # Train Generator\n",
    "            # Training the Generator twice as to catch up with the discriminator\n",
    "            for i in range(2):\n",
    "                g_loss = COMBINED.train_on_batch([imgs_A, imgs_B],\n",
    "                                                 [real, real,\n",
    "                                                  imgs_A, imgs_B,\n",
    "                                                  imgs_A, imgs_B])\n",
    "\n",
    "            \n",
    "            # Save Model\n",
    "            current_time = datetime.now()\n",
    "            difference_time = current_time - saved_time\n",
    "            if difference_time.seconds >= (TIME_INTERVALS * 60):\n",
    "                save_model()\n",
    "                save_image(epoch, steps)\n",
    "                saved_time = current_time\n",
    "                \n",
    "            # Print and Save Log\n",
    "            log = \"Ep: %d, steps: %d, D loss: %f, acc: %3d%%, G loss: %f\" %(epoch,\n",
    "                                                                                steps, d_loss[0], 100*d_loss[1],\n",
    "                                                                                g_loss[0])\n",
    "            print(log)\n",
    "            save_log(log)\n",
    "            steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models not Found\n"
     ]
    }
   ],
   "source": [
    "## Loads model if exist in Checkpoint Path\n",
    "load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Valken\\Anaconda3\\envs\\CondaPy\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Valken\\Anaconda3\\envs\\CondaPy\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0, steps: 1, D loss: 0.434775, acc:  37%, G loss: 11.257547\n",
      "Ep: 0, steps: 2, D loss: 0.384252, acc:  48%, G loss: 10.026544\n",
      "Ep: 0, steps: 3, D loss: 0.344495, acc:  52%, G loss: 9.773030\n",
      "Ep: 0, steps: 4, D loss: 0.388114, acc:  44%, G loss: 8.691401\n",
      "Ep: 0, steps: 5, D loss: 0.487126, acc:  29%, G loss: 7.631575\n",
      "Ep: 0, steps: 6, D loss: 0.378985, acc:  41%, G loss: 9.061798\n",
      "Ep: 0, steps: 7, D loss: 0.091126, acc:  86%, G loss: 8.535134\n",
      "Ep: 0, steps: 8, D loss: 0.209564, acc:  71%, G loss: 7.837389\n",
      "Ep: 0, steps: 9, D loss: 0.437955, acc:  46%, G loss: 7.083963\n",
      "Ep: 0, steps: 10, D loss: 0.449825, acc:  44%, G loss: 7.062690\n",
      "Ep: 0, steps: 11, D loss: 0.466687, acc:  37%, G loss: 6.377492\n",
      "Ep: 0, steps: 12, D loss: 0.275514, acc:  63%, G loss: 6.731289\n",
      "Ep: 0, steps: 13, D loss: 0.226670, acc:  69%, G loss: 6.317957\n",
      "Ep: 0, steps: 14, D loss: 0.218672, acc:  69%, G loss: 5.964538\n",
      "Ep: 0, steps: 15, D loss: 0.339007, acc:  42%, G loss: 4.622829\n",
      "Ep: 0, steps: 16, D loss: 0.338979, acc:  51%, G loss: 6.162909\n",
      "Ep: 0, steps: 17, D loss: 0.322759, acc:  47%, G loss: 6.023901\n",
      "Ep: 0, steps: 18, D loss: 0.278364, acc:  56%, G loss: 6.729021\n",
      "Ep: 0, steps: 19, D loss: 0.411622, acc:  29%, G loss: 5.487564\n",
      "Ep: 0, steps: 20, D loss: 0.292856, acc:  53%, G loss: 5.849968\n",
      "Ep: 0, steps: 21, D loss: 0.188434, acc:  72%, G loss: 5.815682\n",
      "Ep: 0, steps: 22, D loss: 0.288626, acc:  53%, G loss: 5.840234\n",
      "Ep: 0, steps: 23, D loss: 0.381774, acc:  41%, G loss: 5.508727\n",
      "Ep: 0, steps: 24, D loss: 0.281294, acc:  55%, G loss: 4.526323\n",
      "Ep: 0, steps: 25, D loss: 0.300551, acc:  53%, G loss: 5.748865\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 1, steps: 1, D loss: 0.390961, acc:  33%, G loss: 5.927891\n",
      "Ep: 1, steps: 2, D loss: 0.308566, acc:  44%, G loss: 4.925711\n",
      "Ep: 1, steps: 3, D loss: 0.306654, acc:  49%, G loss: 6.683447\n",
      "Ep: 1, steps: 4, D loss: 0.308745, acc:  44%, G loss: 4.794363\n",
      "Ep: 1, steps: 5, D loss: 0.308129, acc:  50%, G loss: 4.832583\n",
      "Ep: 1, steps: 6, D loss: 0.260623, acc:  53%, G loss: 5.607155\n",
      "Ep: 1, steps: 7, D loss: 0.120409, acc:  86%, G loss: 5.685846\n",
      "Saved Model\n",
      "Ep: 1, steps: 8, D loss: 0.211747, acc:  70%, G loss: 5.818254\n",
      "Ep: 1, steps: 9, D loss: 0.377582, acc:  40%, G loss: 5.343752\n",
      "Ep: 1, steps: 10, D loss: 0.382487, acc:  31%, G loss: 4.895451\n",
      "Ep: 1, steps: 11, D loss: 0.261651, acc:  56%, G loss: 4.912810\n",
      "Ep: 1, steps: 12, D loss: 0.231841, acc:  63%, G loss: 4.361739\n",
      "Ep: 1, steps: 13, D loss: 0.220366, acc:  67%, G loss: 4.641507\n",
      "Ep: 1, steps: 14, D loss: 0.306285, acc:  46%, G loss: 3.802926\n",
      "Ep: 1, steps: 15, D loss: 0.319321, acc:  49%, G loss: 4.418321\n",
      "Ep: 1, steps: 16, D loss: 0.294916, acc:  48%, G loss: 4.499424\n",
      "Ep: 1, steps: 17, D loss: 0.247225, acc:  60%, G loss: 4.975251\n",
      "Ep: 1, steps: 18, D loss: 0.363623, acc:  31%, G loss: 4.084754\n",
      "Ep: 1, steps: 19, D loss: 0.264724, acc:  58%, G loss: 4.902182\n",
      "Ep: 1, steps: 20, D loss: 0.213481, acc:  68%, G loss: 4.558417\n",
      "Ep: 1, steps: 21, D loss: 0.255122, acc:  53%, G loss: 5.319345\n",
      "Ep: 1, steps: 22, D loss: 0.360755, acc:  39%, G loss: 4.737571\n",
      "Ep: 1, steps: 23, D loss: 0.265392, acc:  54%, G loss: 4.014437\n",
      "Ep: 1, steps: 24, D loss: 0.283592, acc:  51%, G loss: 4.355534\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 2, steps: 1, D loss: 0.363976, acc:  31%, G loss: 4.730627\n",
      "Ep: 2, steps: 2, D loss: 0.292987, acc:  46%, G loss: 3.903399\n",
      "Ep: 2, steps: 3, D loss: 0.265205, acc:  54%, G loss: 5.085639\n",
      "Ep: 2, steps: 4, D loss: 0.294065, acc:  43%, G loss: 4.051426\n",
      "Ep: 2, steps: 5, D loss: 0.299868, acc:  49%, G loss: 3.933272\n",
      "Ep: 2, steps: 6, D loss: 0.261437, acc:  51%, G loss: 4.932014\n",
      "Ep: 2, steps: 7, D loss: 0.145436, acc:  82%, G loss: 4.886026\n",
      "Ep: 2, steps: 8, D loss: 0.202559, acc:  72%, G loss: 4.402967\n",
      "Ep: 2, steps: 9, D loss: 0.346382, acc:  44%, G loss: 4.069736\n",
      "Ep: 2, steps: 10, D loss: 0.332559, acc:  45%, G loss: 3.968967\n",
      "Ep: 2, steps: 11, D loss: 0.336277, acc:  36%, G loss: 4.108706\n",
      "Ep: 2, steps: 12, D loss: 0.252449, acc:  56%, G loss: 4.074487\n",
      "Ep: 2, steps: 13, D loss: 0.224312, acc:  65%, G loss: 3.444457\n",
      "Ep: 2, steps: 14, D loss: 0.216865, acc:  67%, G loss: 3.434601\n",
      "Ep: 2, steps: 15, D loss: 0.289578, acc:  46%, G loss: 3.336065\n",
      "Ep: 2, steps: 16, D loss: 0.300063, acc:  51%, G loss: 3.561821\n",
      "Ep: 2, steps: 17, D loss: 0.286363, acc:  50%, G loss: 4.000369\n",
      "Ep: 2, steps: 18, D loss: 0.239158, acc:  61%, G loss: 4.346633\n",
      "Ep: 2, steps: 19, D loss: 0.354498, acc:  30%, G loss: 3.518279\n",
      "Ep: 2, steps: 20, D loss: 0.251800, acc:  62%, G loss: 3.917359\n",
      "Ep: 2, steps: 21, D loss: 0.188003, acc:  75%, G loss: 7.138321\n",
      "Ep: 2, steps: 22, D loss: 0.260607, acc:  52%, G loss: 4.165912\n",
      "Ep: 2, steps: 23, D loss: 0.389497, acc:  30%, G loss: 4.331741\n",
      "Ep: 2, steps: 24, D loss: 0.267181, acc:  51%, G loss: 3.417173\n",
      "Ep: 2, steps: 25, D loss: 0.270312, acc:  52%, G loss: 3.681400\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 3, steps: 1, D loss: 0.340918, acc:  33%, G loss: 4.505533\n",
      "Ep: 3, steps: 2, D loss: 0.288581, acc:  46%, G loss: 3.518827\n",
      "Ep: 3, steps: 3, D loss: 0.252926, acc:  58%, G loss: 4.395462\n",
      "Ep: 3, steps: 4, D loss: 0.287078, acc:  44%, G loss: 3.558515\n",
      "Ep: 3, steps: 5, D loss: 0.286551, acc:  51%, G loss: 3.719537\n",
      "Ep: 3, steps: 6, D loss: 0.263874, acc:  52%, G loss: 4.087177\n",
      "Ep: 3, steps: 7, D loss: 0.146942, acc:  82%, G loss: 3.944586\n",
      "Ep: 3, steps: 8, D loss: 0.202757, acc:  72%, G loss: 4.151924\n",
      "Ep: 3, steps: 9, D loss: 0.327908, acc:  43%, G loss: 3.443300\n",
      "Ep: 3, steps: 10, D loss: 0.316721, acc:  44%, G loss: 3.725399\n",
      "Ep: 3, steps: 11, D loss: 0.327891, acc:  34%, G loss: 3.765512\n",
      "Ep: 3, steps: 12, D loss: 0.250105, acc:  56%, G loss: 3.673948\n",
      "Ep: 3, steps: 13, D loss: 0.226794, acc:  63%, G loss: 3.100020\n",
      "Ep: 3, steps: 14, D loss: 0.217589, acc:  67%, G loss: 3.397193\n",
      "Ep: 3, steps: 15, D loss: 0.279209, acc:  49%, G loss: 2.987975\n",
      "Ep: 3, steps: 16, D loss: 0.298460, acc:  52%, G loss: 3.451937\n",
      "Ep: 3, steps: 17, D loss: 0.272744, acc:  51%, G loss: 3.785884\n",
      "Ep: 3, steps: 18, D loss: 0.237727, acc:  63%, G loss: 4.126298\n",
      "Ep: 3, steps: 19, D loss: 0.334369, acc:  34%, G loss: 3.447135\n",
      "Ep: 3, steps: 20, D loss: 0.238876, acc:  66%, G loss: 3.480095\n",
      "Ep: 3, steps: 21, D loss: 0.190310, acc:  76%, G loss: 3.583381\n",
      "Ep: 3, steps: 22, D loss: 0.232190, acc:  58%, G loss: 3.932580\n",
      "Ep: 3, steps: 23, D loss: 0.375473, acc:  30%, G loss: 4.099569\n",
      "Saved Model\n",
      "Ep: 3, steps: 24, D loss: 0.268062, acc:  51%, G loss: 3.217696\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 4, steps: 1, D loss: 0.350981, acc:  30%, G loss: 3.902063\n",
      "Ep: 4, steps: 2, D loss: 0.287065, acc:  47%, G loss: 3.400423\n",
      "Ep: 4, steps: 3, D loss: 0.258053, acc:  53%, G loss: 4.282763\n",
      "Ep: 4, steps: 4, D loss: 0.281262, acc:  44%, G loss: 3.469107\n",
      "Ep: 4, steps: 5, D loss: 0.290628, acc:  47%, G loss: 3.390017\n",
      "Ep: 4, steps: 6, D loss: 0.263277, acc:  51%, G loss: 3.980347\n",
      "Ep: 4, steps: 7, D loss: 0.168504, acc:  76%, G loss: 3.837933\n",
      "Ep: 4, steps: 8, D loss: 0.208082, acc:  71%, G loss: 4.436270\n",
      "Ep: 4, steps: 9, D loss: 0.316930, acc:  43%, G loss: 3.724092\n",
      "Ep: 4, steps: 10, D loss: 0.298740, acc:  45%, G loss: 3.628540\n",
      "Ep: 4, steps: 11, D loss: 0.329836, acc:  32%, G loss: 3.683820\n",
      "Ep: 4, steps: 12, D loss: 0.253876, acc:  54%, G loss: 3.491527\n",
      "Ep: 4, steps: 13, D loss: 0.236648, acc:  58%, G loss: 3.144426\n",
      "Ep: 4, steps: 14, D loss: 0.224519, acc:  65%, G loss: 3.206848\n",
      "Ep: 4, steps: 15, D loss: 0.270240, acc:  48%, G loss: 2.911503\n",
      "Ep: 4, steps: 16, D loss: 0.294652, acc:  52%, G loss: 3.337895\n",
      "Ep: 4, steps: 17, D loss: 0.263552, acc:  52%, G loss: 3.455840\n",
      "Ep: 4, steps: 18, D loss: 0.254368, acc:  57%, G loss: 3.636642\n",
      "Ep: 4, steps: 19, D loss: 0.334440, acc:  32%, G loss: 3.196754\n",
      "Ep: 4, steps: 20, D loss: 0.240893, acc:  66%, G loss: 3.409390\n",
      "Ep: 4, steps: 21, D loss: 0.203822, acc:  72%, G loss: 3.021749\n",
      "Ep: 4, steps: 22, D loss: 0.222537, acc:  60%, G loss: 3.252508\n",
      "Ep: 4, steps: 23, D loss: 0.350114, acc:  35%, G loss: 3.505556\n",
      "Ep: 4, steps: 24, D loss: 0.250444, acc:  55%, G loss: 3.048220\n",
      "Ep: 4, steps: 25, D loss: 0.251722, acc:  55%, G loss: 2.931924\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 5, steps: 1, D loss: 0.339648, acc:  32%, G loss: 3.832357\n",
      "Ep: 5, steps: 2, D loss: 0.281768, acc:  48%, G loss: 3.018128\n",
      "Ep: 5, steps: 3, D loss: 0.242781, acc:  60%, G loss: 3.900041\n",
      "Ep: 5, steps: 4, D loss: 0.284775, acc:  42%, G loss: 3.106502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 5, steps: 5, D loss: 0.282232, acc:  51%, G loss: 3.244949\n",
      "Ep: 5, steps: 6, D loss: 0.253156, acc:  55%, G loss: 3.490133\n",
      "Ep: 5, steps: 7, D loss: 0.133808, acc:  85%, G loss: 3.402808\n",
      "Ep: 5, steps: 8, D loss: 0.202050, acc:  72%, G loss: 3.742985\n",
      "Ep: 5, steps: 9, D loss: 0.323136, acc:  41%, G loss: 3.022624\n",
      "Ep: 5, steps: 10, D loss: 0.289987, acc:  46%, G loss: 3.272429\n",
      "Ep: 5, steps: 11, D loss: 0.326551, acc:  33%, G loss: 3.438382\n",
      "Ep: 5, steps: 12, D loss: 0.249186, acc:  55%, G loss: 3.261439\n",
      "Ep: 5, steps: 13, D loss: 0.238997, acc:  56%, G loss: 3.061539\n",
      "Ep: 5, steps: 14, D loss: 0.224387, acc:  64%, G loss: 2.956791\n",
      "Ep: 5, steps: 15, D loss: 0.268010, acc:  49%, G loss: 2.820446\n",
      "Ep: 5, steps: 16, D loss: 0.292066, acc:  51%, G loss: 3.175693\n",
      "Ep: 5, steps: 17, D loss: 0.254453, acc:  55%, G loss: 3.271388\n",
      "Ep: 5, steps: 18, D loss: 0.259094, acc:  55%, G loss: 3.402173\n",
      "Ep: 5, steps: 19, D loss: 0.329985, acc:  34%, G loss: 2.989421\n",
      "Ep: 5, steps: 20, D loss: 0.237872, acc:  65%, G loss: 3.199334\n",
      "Ep: 5, steps: 21, D loss: 0.207449, acc:  69%, G loss: 2.778945\n",
      "Ep: 5, steps: 22, D loss: 0.221298, acc:  62%, G loss: 3.053919\n",
      "Ep: 5, steps: 23, D loss: 0.347550, acc:  35%, G loss: 3.343769\n",
      "Ep: 5, steps: 24, D loss: 0.243527, acc:  57%, G loss: 2.897804\n",
      "Ep: 5, steps: 25, D loss: 0.245123, acc:  57%, G loss: 2.834512\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 6, steps: 1, D loss: 0.342436, acc:  31%, G loss: 3.580027\n",
      "Ep: 6, steps: 2, D loss: 0.283605, acc:  47%, G loss: 2.871593\n",
      "Ep: 6, steps: 3, D loss: 0.239004, acc:  61%, G loss: 3.676198\n",
      "Ep: 6, steps: 4, D loss: 0.282876, acc:  43%, G loss: 2.877288\n",
      "Ep: 6, steps: 5, D loss: 0.278733, acc:  51%, G loss: 4.094709\n",
      "Ep: 6, steps: 6, D loss: 0.261895, acc:  52%, G loss: 3.839963\n",
      "Ep: 6, steps: 7, D loss: 0.139071, acc:  83%, G loss: 4.072310\n",
      "Ep: 6, steps: 8, D loss: 0.209945, acc:  70%, G loss: 3.899063\n",
      "Ep: 6, steps: 9, D loss: 0.326966, acc:  37%, G loss: 3.321476\n",
      "Ep: 6, steps: 10, D loss: 0.285182, acc:  47%, G loss: 3.408923\n",
      "Ep: 6, steps: 11, D loss: 0.319062, acc:  30%, G loss: 3.643445\n",
      "Ep: 6, steps: 12, D loss: 0.244035, acc:  56%, G loss: 3.283925\n",
      "Ep: 6, steps: 13, D loss: 0.231713, acc:  58%, G loss: 3.061353\n",
      "Ep: 6, steps: 14, D loss: 0.223710, acc:  64%, G loss: 3.008450\n",
      "Ep: 6, steps: 15, D loss: 0.261901, acc:  50%, G loss: 2.835148\n",
      "Ep: 6, steps: 16, D loss: 0.283263, acc:  52%, G loss: 3.266177\n",
      "Ep: 6, steps: 17, D loss: 0.247659, acc:  56%, G loss: 3.253805\n",
      "Ep: 6, steps: 18, D loss: 0.257876, acc:  56%, G loss: 3.580036\n",
      "Ep: 6, steps: 19, D loss: 0.326659, acc:  33%, G loss: 3.035034\n",
      "Ep: 6, steps: 20, D loss: 0.233711, acc:  66%, G loss: 3.239072\n",
      "Ep: 6, steps: 21, D loss: 0.206011, acc:  68%, G loss: 2.782934\n",
      "Saved Model\n",
      "Ep: 6, steps: 22, D loss: 0.227021, acc:  59%, G loss: 3.011842\n",
      "Ep: 6, steps: 23, D loss: 0.244621, acc:  59%, G loss: 3.015435\n",
      "Ep: 6, steps: 24, D loss: 0.248322, acc:  58%, G loss: 3.076639\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 7, steps: 1, D loss: 0.354780, acc:  30%, G loss: 3.357593\n",
      "Ep: 7, steps: 2, D loss: 0.280930, acc:  47%, G loss: 2.869944\n",
      "Ep: 7, steps: 3, D loss: 0.236587, acc:  62%, G loss: 3.537772\n",
      "Ep: 7, steps: 4, D loss: 0.281556, acc:  43%, G loss: 2.948299\n",
      "Ep: 7, steps: 5, D loss: 0.274093, acc:  51%, G loss: 3.162733\n",
      "Ep: 7, steps: 6, D loss: 0.255636, acc:  55%, G loss: 3.016865\n",
      "Ep: 7, steps: 7, D loss: 0.115351, acc:  88%, G loss: 3.406693\n",
      "Ep: 7, steps: 8, D loss: 0.195389, acc:  73%, G loss: 3.822652\n",
      "Ep: 7, steps: 9, D loss: 0.323914, acc:  37%, G loss: 2.948432\n",
      "Ep: 7, steps: 10, D loss: 0.273730, acc:  51%, G loss: 3.040318\n",
      "Ep: 7, steps: 11, D loss: 0.321251, acc:  31%, G loss: 3.233250\n",
      "Ep: 7, steps: 12, D loss: 0.239550, acc:  57%, G loss: 3.047919\n",
      "Ep: 7, steps: 13, D loss: 0.227443, acc:  60%, G loss: 2.787508\n",
      "Ep: 7, steps: 14, D loss: 0.214408, acc:  67%, G loss: 2.836733\n",
      "Ep: 7, steps: 15, D loss: 0.256147, acc:  53%, G loss: 2.853321\n",
      "Ep: 7, steps: 16, D loss: 0.288468, acc:  51%, G loss: 3.128267\n",
      "Ep: 7, steps: 17, D loss: 0.243123, acc:  59%, G loss: 3.138919\n",
      "Ep: 7, steps: 18, D loss: 0.264754, acc:  53%, G loss: 3.362757\n",
      "Ep: 7, steps: 19, D loss: 0.332020, acc:  33%, G loss: 2.865065\n",
      "Ep: 7, steps: 20, D loss: 0.230353, acc:  66%, G loss: 3.115113\n",
      "Ep: 7, steps: 21, D loss: 0.202757, acc:  69%, G loss: 2.859458\n",
      "Ep: 7, steps: 22, D loss: 0.215942, acc:  64%, G loss: 3.403309\n",
      "Ep: 7, steps: 23, D loss: 0.317228, acc:  40%, G loss: 3.275254\n",
      "Ep: 7, steps: 24, D loss: 0.229118, acc:  62%, G loss: 2.904352\n",
      "Ep: 7, steps: 25, D loss: 0.226910, acc:  63%, G loss: 2.818949\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 8, steps: 1, D loss: 0.341646, acc:  30%, G loss: 3.220085\n",
      "Ep: 8, steps: 2, D loss: 0.284642, acc:  45%, G loss: 2.760743\n",
      "Ep: 8, steps: 3, D loss: 0.224436, acc:  65%, G loss: 3.430752\n",
      "Ep: 8, steps: 4, D loss: 0.280089, acc:  44%, G loss: 2.893626\n",
      "Ep: 8, steps: 5, D loss: 0.272558, acc:  49%, G loss: 3.093204\n",
      "Ep: 8, steps: 6, D loss: 0.252743, acc:  55%, G loss: 2.998329\n",
      "Ep: 8, steps: 7, D loss: 0.112773, acc:  87%, G loss: 3.425841\n",
      "Ep: 8, steps: 8, D loss: 0.185874, acc:  75%, G loss: 3.753417\n",
      "Ep: 8, steps: 9, D loss: 0.323802, acc:  35%, G loss: 2.943593\n",
      "Ep: 8, steps: 10, D loss: 0.273033, acc:  51%, G loss: 3.029692\n",
      "Ep: 8, steps: 11, D loss: 0.331974, acc:  30%, G loss: 3.232617\n",
      "Ep: 8, steps: 12, D loss: 0.235225, acc:  59%, G loss: 2.972761\n",
      "Ep: 8, steps: 13, D loss: 0.223430, acc:  63%, G loss: 2.793623\n",
      "Ep: 8, steps: 14, D loss: 0.208596, acc:  69%, G loss: 2.856703\n",
      "Ep: 8, steps: 15, D loss: 0.257782, acc:  53%, G loss: 2.792258\n",
      "Ep: 8, steps: 16, D loss: 0.286787, acc:  51%, G loss: 3.080222\n",
      "Ep: 8, steps: 17, D loss: 0.234836, acc:  62%, G loss: 3.071696\n",
      "Ep: 8, steps: 18, D loss: 0.268629, acc:  51%, G loss: 3.181086\n",
      "Ep: 8, steps: 19, D loss: 0.344938, acc:  30%, G loss: 2.854209\n",
      "Ep: 8, steps: 20, D loss: 0.229494, acc:  65%, G loss: 3.070022\n",
      "Ep: 8, steps: 21, D loss: 0.202940, acc:  69%, G loss: 2.523417\n",
      "Ep: 8, steps: 22, D loss: 0.222031, acc:  63%, G loss: 2.782342\n",
      "Ep: 8, steps: 23, D loss: 0.334167, acc:  35%, G loss: 3.268519\n",
      "Ep: 8, steps: 24, D loss: 0.231417, acc:  61%, G loss: 2.825262\n",
      "Ep: 8, steps: 25, D loss: 0.230568, acc:  61%, G loss: 2.695339\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 9, steps: 1, D loss: 0.339419, acc:  28%, G loss: 3.388298\n",
      "Ep: 9, steps: 2, D loss: 0.281696, acc:  46%, G loss: 2.673041\n",
      "Ep: 9, steps: 3, D loss: 0.227241, acc:  66%, G loss: 3.182423\n",
      "Ep: 9, steps: 4, D loss: 0.277396, acc:  45%, G loss: 2.620586\n",
      "Ep: 9, steps: 5, D loss: 0.266181, acc:  49%, G loss: 2.972660\n",
      "Ep: 9, steps: 6, D loss: 0.254945, acc:  55%, G loss: 2.873350\n",
      "Ep: 9, steps: 7, D loss: 0.113183, acc:  88%, G loss: 3.165552\n",
      "Ep: 9, steps: 8, D loss: 0.192858, acc:  75%, G loss: 3.320834\n",
      "Ep: 9, steps: 9, D loss: 0.325008, acc:  33%, G loss: 2.691118\n",
      "Ep: 9, steps: 10, D loss: 0.272565, acc:  50%, G loss: 2.991929\n",
      "Ep: 9, steps: 11, D loss: 0.320088, acc:  32%, G loss: 3.100770\n",
      "Ep: 9, steps: 12, D loss: 0.232420, acc:  61%, G loss: 2.904660\n",
      "Ep: 9, steps: 13, D loss: 0.221284, acc:  62%, G loss: 2.749492\n",
      "Ep: 9, steps: 14, D loss: 0.204966, acc:  71%, G loss: 2.842267\n",
      "Ep: 9, steps: 15, D loss: 0.255203, acc:  56%, G loss: 2.745800\n",
      "Ep: 9, steps: 16, D loss: 0.285315, acc:  51%, G loss: 2.966969\n",
      "Ep: 9, steps: 17, D loss: 0.228910, acc:  63%, G loss: 3.021965\n",
      "Ep: 9, steps: 18, D loss: 0.268607, acc:  50%, G loss: 3.141114\n",
      "Ep: 9, steps: 19, D loss: 0.336650, acc:  31%, G loss: 2.752756\n",
      "Saved Model\n",
      "Ep: 9, steps: 20, D loss: 0.223463, acc:  69%, G loss: 2.911412\n",
      "Ep: 9, steps: 21, D loss: 0.226175, acc:  67%, G loss: 3.219716\n",
      "Ep: 9, steps: 22, D loss: 0.303505, acc:  38%, G loss: 2.886942\n",
      "Ep: 9, steps: 23, D loss: 0.227114, acc:  63%, G loss: 2.776047\n",
      "Ep: 9, steps: 24, D loss: 0.228042, acc:  63%, G loss: 2.575386\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 10, steps: 1, D loss: 0.333248, acc:  28%, G loss: 2.975096\n",
      "Ep: 10, steps: 2, D loss: 0.279648, acc:  46%, G loss: 2.635287\n",
      "Ep: 10, steps: 3, D loss: 0.226762, acc:  66%, G loss: 3.194854\n",
      "Ep: 10, steps: 4, D loss: 0.271508, acc:  49%, G loss: 2.833679\n",
      "Ep: 10, steps: 5, D loss: 0.266786, acc:  47%, G loss: 2.886572\n",
      "Ep: 10, steps: 6, D loss: 0.251084, acc:  57%, G loss: 2.898143\n",
      "Ep: 10, steps: 7, D loss: 0.116387, acc:  87%, G loss: 3.298085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 10, steps: 8, D loss: 0.193926, acc:  73%, G loss: 3.635544\n",
      "Ep: 10, steps: 9, D loss: 0.318287, acc:  35%, G loss: 2.799087\n",
      "Ep: 10, steps: 10, D loss: 0.268581, acc:  53%, G loss: 2.846242\n",
      "Ep: 10, steps: 11, D loss: 0.319192, acc:  32%, G loss: 2.994699\n",
      "Ep: 10, steps: 12, D loss: 0.229746, acc:  63%, G loss: 2.797205\n",
      "Ep: 10, steps: 13, D loss: 0.221946, acc:  64%, G loss: 2.657612\n",
      "Ep: 10, steps: 14, D loss: 0.203584, acc:  71%, G loss: 2.772214\n",
      "Ep: 10, steps: 15, D loss: 0.248767, acc:  58%, G loss: 2.728554\n",
      "Ep: 10, steps: 16, D loss: 0.285210, acc:  50%, G loss: 2.925973\n",
      "Ep: 10, steps: 17, D loss: 0.225134, acc:  65%, G loss: 3.005263\n",
      "Ep: 10, steps: 18, D loss: 0.269339, acc:  50%, G loss: 3.124548\n",
      "Ep: 10, steps: 19, D loss: 0.332341, acc:  31%, G loss: 2.705921\n",
      "Ep: 10, steps: 20, D loss: 0.222611, acc:  69%, G loss: 2.867001\n",
      "Ep: 10, steps: 21, D loss: 0.204522, acc:  70%, G loss: 2.455363\n",
      "Ep: 10, steps: 22, D loss: 0.221306, acc:  64%, G loss: 2.707001\n",
      "Ep: 10, steps: 23, D loss: 0.330372, acc:  34%, G loss: 2.990289\n",
      "Ep: 10, steps: 24, D loss: 0.226812, acc:  63%, G loss: 2.629469\n",
      "Ep: 10, steps: 25, D loss: 0.220462, acc:  63%, G loss: 2.495889\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 11, steps: 1, D loss: 0.335995, acc:  26%, G loss: 3.091134\n",
      "Ep: 11, steps: 2, D loss: 0.284068, acc:  45%, G loss: 2.583730\n",
      "Ep: 11, steps: 3, D loss: 0.227158, acc:  66%, G loss: 2.978217\n",
      "Ep: 11, steps: 4, D loss: 0.270059, acc:  50%, G loss: 2.744328\n",
      "Ep: 11, steps: 5, D loss: 0.264677, acc:  48%, G loss: 2.806267\n",
      "Ep: 11, steps: 6, D loss: 0.259202, acc:  53%, G loss: 2.668487\n",
      "Ep: 11, steps: 7, D loss: 0.114956, acc:  87%, G loss: 3.227031\n",
      "Ep: 11, steps: 8, D loss: 0.195849, acc:  74%, G loss: 3.609704\n",
      "Ep: 11, steps: 9, D loss: 0.312502, acc:  36%, G loss: 2.781149\n",
      "Ep: 11, steps: 10, D loss: 0.260954, acc:  55%, G loss: 2.746150\n",
      "Ep: 11, steps: 11, D loss: 0.319643, acc:  32%, G loss: 2.907309\n",
      "Ep: 11, steps: 12, D loss: 0.227116, acc:  63%, G loss: 2.712158\n",
      "Ep: 11, steps: 13, D loss: 0.220982, acc:  63%, G loss: 2.699453\n",
      "Ep: 11, steps: 14, D loss: 0.203490, acc:  70%, G loss: 2.696136\n",
      "Ep: 11, steps: 15, D loss: 0.246541, acc:  59%, G loss: 2.734670\n",
      "Ep: 11, steps: 16, D loss: 0.289866, acc:  49%, G loss: 2.883348\n",
      "Ep: 11, steps: 17, D loss: 0.224731, acc:  66%, G loss: 2.972765\n",
      "Ep: 11, steps: 18, D loss: 0.270955, acc:  49%, G loss: 3.074848\n",
      "Ep: 11, steps: 19, D loss: 0.328075, acc:  32%, G loss: 2.661584\n",
      "Ep: 11, steps: 20, D loss: 0.220926, acc:  70%, G loss: 2.744879\n",
      "Ep: 11, steps: 21, D loss: 0.203726, acc:  70%, G loss: 2.420282\n",
      "Ep: 11, steps: 22, D loss: 0.222747, acc:  63%, G loss: 2.620460\n",
      "Ep: 11, steps: 23, D loss: 0.324302, acc:  34%, G loss: 2.914026\n",
      "Ep: 11, steps: 24, D loss: 0.222929, acc:  65%, G loss: 2.635793\n",
      "Ep: 11, steps: 25, D loss: 0.217643, acc:  63%, G loss: 2.389892\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 12, steps: 1, D loss: 0.334685, acc:  26%, G loss: 3.092102\n",
      "Ep: 12, steps: 2, D loss: 0.278754, acc:  48%, G loss: 2.655719\n",
      "Ep: 12, steps: 3, D loss: 0.218429, acc:  67%, G loss: 2.968804\n",
      "Ep: 12, steps: 4, D loss: 0.264998, acc:  50%, G loss: 2.663454\n",
      "Ep: 12, steps: 5, D loss: 0.266668, acc:  47%, G loss: 2.769651\n",
      "Ep: 12, steps: 6, D loss: 0.262728, acc:  52%, G loss: 2.600904\n",
      "Ep: 12, steps: 7, D loss: 0.112449, acc:  87%, G loss: 3.442038\n",
      "Ep: 12, steps: 8, D loss: 0.197672, acc:  74%, G loss: 3.392707\n",
      "Ep: 12, steps: 9, D loss: 0.314421, acc:  35%, G loss: 2.669774\n",
      "Ep: 12, steps: 10, D loss: 0.259566, acc:  54%, G loss: 2.645033\n",
      "Ep: 12, steps: 11, D loss: 0.320758, acc:  31%, G loss: 2.973666\n",
      "Ep: 12, steps: 12, D loss: 0.230352, acc:  62%, G loss: 2.671842\n",
      "Ep: 12, steps: 13, D loss: 0.222510, acc:  63%, G loss: 2.516309\n",
      "Ep: 12, steps: 14, D loss: 0.202046, acc:  71%, G loss: 2.523316\n",
      "Ep: 12, steps: 15, D loss: 0.247697, acc:  59%, G loss: 2.812628\n",
      "Ep: 12, steps: 16, D loss: 0.283689, acc:  50%, G loss: 2.842730\n",
      "Ep: 12, steps: 17, D loss: 0.221015, acc:  67%, G loss: 2.965433\n",
      "Saved Model\n",
      "Ep: 12, steps: 18, D loss: 0.268910, acc:  50%, G loss: 3.090158\n",
      "Ep: 12, steps: 19, D loss: 0.229065, acc:  68%, G loss: 2.966358\n",
      "Ep: 12, steps: 20, D loss: 0.191714, acc:  74%, G loss: 2.859234\n",
      "Ep: 12, steps: 21, D loss: 0.212653, acc:  58%, G loss: 2.795046\n",
      "Ep: 12, steps: 22, D loss: 0.335323, acc:  28%, G loss: 3.208532\n",
      "Ep: 12, steps: 23, D loss: 0.220553, acc:  63%, G loss: 2.527861\n",
      "Ep: 12, steps: 24, D loss: 0.216532, acc:  60%, G loss: 2.535773\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 13, steps: 1, D loss: 0.337642, acc:  24%, G loss: 3.093901\n",
      "Ep: 13, steps: 2, D loss: 0.282715, acc:  44%, G loss: 2.726212\n",
      "Ep: 13, steps: 3, D loss: 0.226071, acc:  65%, G loss: 3.151169\n",
      "Ep: 13, steps: 4, D loss: 0.271305, acc:  47%, G loss: 2.532024\n",
      "Ep: 13, steps: 5, D loss: 0.264495, acc:  48%, G loss: 2.718387\n",
      "Ep: 13, steps: 6, D loss: 0.270023, acc:  49%, G loss: 2.507657\n",
      "Ep: 13, steps: 7, D loss: 0.116606, acc:  87%, G loss: 3.304026\n",
      "Ep: 13, steps: 8, D loss: 0.199581, acc:  73%, G loss: 3.408147\n",
      "Ep: 13, steps: 9, D loss: 0.308526, acc:  36%, G loss: 2.627895\n",
      "Ep: 13, steps: 10, D loss: 0.257831, acc:  55%, G loss: 2.590356\n",
      "Ep: 13, steps: 11, D loss: 0.322325, acc:  31%, G loss: 2.816167\n",
      "Ep: 13, steps: 12, D loss: 0.225720, acc:  63%, G loss: 2.590403\n",
      "Ep: 13, steps: 13, D loss: 0.220041, acc:  65%, G loss: 2.558063\n",
      "Ep: 13, steps: 14, D loss: 0.200048, acc:  72%, G loss: 2.507805\n",
      "Ep: 13, steps: 15, D loss: 0.245733, acc:  59%, G loss: 2.726663\n",
      "Ep: 13, steps: 16, D loss: 0.280514, acc:  50%, G loss: 2.818592\n",
      "Ep: 13, steps: 17, D loss: 0.219567, acc:  68%, G loss: 2.901674\n",
      "Ep: 13, steps: 18, D loss: 0.268658, acc:  49%, G loss: 3.065533\n",
      "Ep: 13, steps: 19, D loss: 0.323925, acc:  30%, G loss: 2.622081\n",
      "Ep: 13, steps: 20, D loss: 0.217706, acc:  72%, G loss: 2.619226\n",
      "Ep: 13, steps: 21, D loss: 0.203907, acc:  72%, G loss: 2.298854\n",
      "Ep: 13, steps: 22, D loss: 0.219359, acc:  63%, G loss: 2.643933\n",
      "Ep: 13, steps: 23, D loss: 0.317224, acc:  34%, G loss: 3.074065\n",
      "Ep: 13, steps: 24, D loss: 0.217912, acc:  67%, G loss: 2.548504\n",
      "Ep: 13, steps: 25, D loss: 0.208859, acc:  66%, G loss: 2.367982\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 14, steps: 1, D loss: 0.333337, acc:  24%, G loss: 2.810303\n",
      "Ep: 14, steps: 2, D loss: 0.296105, acc:  41%, G loss: 2.585134\n",
      "Ep: 14, steps: 3, D loss: 0.222505, acc:  67%, G loss: 2.868113\n",
      "Ep: 14, steps: 4, D loss: 0.266493, acc:  50%, G loss: 2.582052\n",
      "Ep: 14, steps: 5, D loss: 0.258074, acc:  51%, G loss: 2.704072\n",
      "Ep: 14, steps: 6, D loss: 0.262904, acc:  51%, G loss: 2.439875\n",
      "Ep: 14, steps: 7, D loss: 0.112628, acc:  88%, G loss: 3.074331\n",
      "Ep: 14, steps: 8, D loss: 0.198385, acc:  73%, G loss: 3.374848\n",
      "Ep: 14, steps: 9, D loss: 0.306761, acc:  34%, G loss: 2.594779\n",
      "Ep: 14, steps: 10, D loss: 0.259296, acc:  54%, G loss: 2.583603\n",
      "Ep: 14, steps: 11, D loss: 0.314818, acc:  31%, G loss: 2.778987\n",
      "Ep: 14, steps: 12, D loss: 0.225610, acc:  64%, G loss: 2.539894\n",
      "Ep: 14, steps: 13, D loss: 0.217144, acc:  66%, G loss: 2.413919\n",
      "Ep: 14, steps: 14, D loss: 0.195604, acc:  76%, G loss: 2.521086\n",
      "Ep: 14, steps: 15, D loss: 0.247900, acc:  59%, G loss: 2.733101\n",
      "Ep: 14, steps: 16, D loss: 0.283319, acc:  50%, G loss: 2.781327\n",
      "Ep: 14, steps: 17, D loss: 0.217515, acc:  69%, G loss: 2.859765\n",
      "Ep: 14, steps: 18, D loss: 0.270543, acc:  47%, G loss: 3.025864\n",
      "Ep: 14, steps: 19, D loss: 0.323114, acc:  31%, G loss: 2.584494\n",
      "Ep: 14, steps: 20, D loss: 0.218362, acc:  72%, G loss: 2.572955\n",
      "Ep: 14, steps: 21, D loss: 0.204564, acc:  71%, G loss: 2.236276\n",
      "Ep: 14, steps: 22, D loss: 0.217111, acc:  64%, G loss: 2.670673\n",
      "Ep: 14, steps: 23, D loss: 0.319040, acc:  31%, G loss: 3.030043\n",
      "Ep: 14, steps: 24, D loss: 0.217442, acc:  67%, G loss: 2.526189\n",
      "Ep: 14, steps: 25, D loss: 0.210583, acc:  66%, G loss: 2.375071\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 15, steps: 1, D loss: 0.335360, acc:  24%, G loss: 2.636176\n",
      "Ep: 15, steps: 2, D loss: 0.293317, acc:  42%, G loss: 2.558375\n",
      "Ep: 15, steps: 3, D loss: 0.222358, acc:  66%, G loss: 2.931761\n",
      "Ep: 15, steps: 4, D loss: 0.260450, acc:  52%, G loss: 2.604061\n",
      "Ep: 15, steps: 5, D loss: 0.258113, acc:  51%, G loss: 2.641324\n",
      "Ep: 15, steps: 6, D loss: 0.260221, acc:  53%, G loss: 2.444747\n",
      "Ep: 15, steps: 7, D loss: 0.117616, acc:  88%, G loss: 3.073864\n",
      "Ep: 15, steps: 8, D loss: 0.199272, acc:  73%, G loss: 3.365077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 15, steps: 9, D loss: 0.302494, acc:  37%, G loss: 2.578854\n",
      "Ep: 15, steps: 10, D loss: 0.257165, acc:  55%, G loss: 2.529783\n",
      "Ep: 15, steps: 11, D loss: 0.311838, acc:  31%, G loss: 2.714713\n",
      "Ep: 15, steps: 12, D loss: 0.223643, acc:  63%, G loss: 2.480108\n",
      "Ep: 15, steps: 13, D loss: 0.217621, acc:  65%, G loss: 2.362325\n",
      "Ep: 15, steps: 14, D loss: 0.197982, acc:  76%, G loss: 2.375927\n",
      "Ep: 15, steps: 15, D loss: 0.248940, acc:  59%, G loss: 2.721513\n",
      "Saved Model\n",
      "Ep: 15, steps: 16, D loss: 0.285760, acc:  49%, G loss: 2.746515\n",
      "Ep: 15, steps: 17, D loss: 0.259780, acc:  52%, G loss: 3.122894\n",
      "Ep: 15, steps: 18, D loss: 0.298406, acc:  36%, G loss: 2.653968\n",
      "Ep: 15, steps: 19, D loss: 0.214503, acc:  74%, G loss: 2.633447\n",
      "Ep: 15, steps: 20, D loss: 0.199260, acc:  72%, G loss: 2.466890\n",
      "Ep: 15, steps: 21, D loss: 0.211617, acc:  63%, G loss: 2.545481\n",
      "Ep: 15, steps: 22, D loss: 0.321819, acc:  28%, G loss: 3.167242\n",
      "Ep: 15, steps: 23, D loss: 0.219996, acc:  67%, G loss: 2.274210\n",
      "Ep: 15, steps: 24, D loss: 0.217230, acc:  62%, G loss: 3.058436\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 16, steps: 1, D loss: 0.320859, acc:  26%, G loss: 3.280785\n",
      "Ep: 16, steps: 2, D loss: 0.283600, acc:  42%, G loss: 2.571831\n",
      "Ep: 16, steps: 3, D loss: 0.221398, acc:  64%, G loss: 2.972273\n",
      "Ep: 16, steps: 4, D loss: 0.262078, acc:  50%, G loss: 2.595262\n",
      "Ep: 16, steps: 5, D loss: 0.259131, acc:  51%, G loss: 2.704260\n",
      "Ep: 16, steps: 6, D loss: 0.265159, acc:  48%, G loss: 2.528965\n",
      "Ep: 16, steps: 7, D loss: 0.129563, acc:  88%, G loss: 2.774952\n",
      "Ep: 16, steps: 8, D loss: 0.193684, acc:  76%, G loss: 3.043859\n",
      "Ep: 16, steps: 9, D loss: 0.304825, acc:  35%, G loss: 2.518984\n",
      "Ep: 16, steps: 10, D loss: 0.252904, acc:  56%, G loss: 2.525454\n",
      "Ep: 16, steps: 11, D loss: 0.311706, acc:  31%, G loss: 2.759472\n",
      "Ep: 16, steps: 12, D loss: 0.227941, acc:  62%, G loss: 2.517403\n",
      "Ep: 16, steps: 13, D loss: 0.220780, acc:  65%, G loss: 2.375677\n",
      "Ep: 16, steps: 14, D loss: 0.196653, acc:  76%, G loss: 2.485696\n",
      "Ep: 16, steps: 15, D loss: 0.247663, acc:  58%, G loss: 2.674646\n",
      "Ep: 16, steps: 16, D loss: 0.283094, acc:  49%, G loss: 2.744870\n",
      "Ep: 16, steps: 17, D loss: 0.215274, acc:  70%, G loss: 2.827258\n",
      "Ep: 16, steps: 18, D loss: 0.267960, acc:  49%, G loss: 3.003460\n",
      "Ep: 16, steps: 19, D loss: 0.313552, acc:  31%, G loss: 2.598895\n",
      "Ep: 16, steps: 20, D loss: 0.213325, acc:  75%, G loss: 2.473382\n",
      "Ep: 16, steps: 21, D loss: 0.202652, acc:  72%, G loss: 2.176505\n",
      "Ep: 16, steps: 22, D loss: 0.217202, acc:  64%, G loss: 2.657496\n",
      "Ep: 16, steps: 23, D loss: 0.313540, acc:  32%, G loss: 2.996543\n",
      "Ep: 16, steps: 24, D loss: 0.214984, acc:  70%, G loss: 2.435422\n",
      "Ep: 16, steps: 25, D loss: 0.207506, acc:  67%, G loss: 2.343872\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 17, steps: 1, D loss: 0.331490, acc:  24%, G loss: 2.652927\n",
      "Ep: 17, steps: 2, D loss: 0.279071, acc:  45%, G loss: 2.610483\n",
      "Ep: 17, steps: 3, D loss: 0.218201, acc:  66%, G loss: 2.960792\n",
      "Ep: 17, steps: 4, D loss: 0.258511, acc:  52%, G loss: 2.481768\n",
      "Ep: 17, steps: 5, D loss: 0.261727, acc:  50%, G loss: 2.629726\n",
      "Ep: 17, steps: 6, D loss: 0.262751, acc:  50%, G loss: 2.466573\n",
      "Ep: 17, steps: 7, D loss: 0.117584, acc:  87%, G loss: 3.121044\n",
      "Ep: 17, steps: 8, D loss: 0.199425, acc:  72%, G loss: 4.433414\n",
      "Ep: 17, steps: 9, D loss: 0.286375, acc:  42%, G loss: 3.125478\n",
      "Ep: 17, steps: 10, D loss: 0.250239, acc:  56%, G loss: 3.296209\n",
      "Ep: 17, steps: 11, D loss: 0.300386, acc:  36%, G loss: 2.995109\n",
      "Ep: 17, steps: 12, D loss: 0.220592, acc:  68%, G loss: 2.807463\n",
      "Ep: 17, steps: 13, D loss: 0.198607, acc:  73%, G loss: 2.733163\n",
      "Ep: 17, steps: 14, D loss: 0.173061, acc:  88%, G loss: 2.760755\n",
      "Ep: 17, steps: 15, D loss: 0.258332, acc:  54%, G loss: 2.711938\n",
      "Ep: 17, steps: 16, D loss: 0.279804, acc:  50%, G loss: 2.841815\n",
      "Ep: 17, steps: 17, D loss: 0.212757, acc:  72%, G loss: 2.925809\n",
      "Ep: 17, steps: 18, D loss: 0.263601, acc:  49%, G loss: 3.120543\n",
      "Ep: 17, steps: 19, D loss: 0.332190, acc:  25%, G loss: 2.760511\n",
      "Ep: 17, steps: 20, D loss: 0.209505, acc:  76%, G loss: 2.605308\n",
      "Ep: 17, steps: 21, D loss: 0.185546, acc:  79%, G loss: 2.635978\n",
      "Ep: 17, steps: 22, D loss: 0.198150, acc:  63%, G loss: 2.897004\n",
      "Ep: 17, steps: 23, D loss: 0.335954, acc:  25%, G loss: 2.924379\n",
      "Ep: 17, steps: 24, D loss: 0.216374, acc:  67%, G loss: 2.662941\n",
      "Ep: 17, steps: 25, D loss: 0.213650, acc:  61%, G loss: 2.530643\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 18, steps: 1, D loss: 0.334742, acc:  22%, G loss: 2.983099\n",
      "Ep: 18, steps: 2, D loss: 0.281403, acc:  42%, G loss: 2.672119\n",
      "Ep: 18, steps: 3, D loss: 0.222519, acc:  63%, G loss: 3.040128\n",
      "Ep: 18, steps: 4, D loss: 0.267631, acc:  45%, G loss: 2.489677\n",
      "Ep: 18, steps: 5, D loss: 0.256707, acc:  52%, G loss: 2.709661\n",
      "Ep: 18, steps: 6, D loss: 0.267487, acc:  48%, G loss: 2.458127\n",
      "Ep: 18, steps: 7, D loss: 0.114239, acc:  89%, G loss: 2.872632\n",
      "Ep: 18, steps: 8, D loss: 0.192812, acc:  77%, G loss: 3.223253\n",
      "Ep: 18, steps: 9, D loss: 0.307882, acc:  32%, G loss: 2.585395\n",
      "Ep: 18, steps: 10, D loss: 0.253515, acc:  54%, G loss: 2.551807\n",
      "Ep: 18, steps: 11, D loss: 0.319786, acc:  29%, G loss: 2.907168\n",
      "Ep: 18, steps: 12, D loss: 0.213913, acc:  69%, G loss: 2.793859\n",
      "Ep: 18, steps: 13, D loss: 0.207991, acc:  69%, G loss: 2.686230\n",
      "Saved Model\n",
      "Ep: 18, steps: 14, D loss: 0.193416, acc:  77%, G loss: 2.488558\n",
      "Ep: 18, steps: 15, D loss: 0.282303, acc:  50%, G loss: 2.647298\n",
      "Ep: 18, steps: 16, D loss: 0.213597, acc:  70%, G loss: 3.029216\n",
      "Ep: 18, steps: 17, D loss: 0.257304, acc:  52%, G loss: 3.361873\n",
      "Ep: 18, steps: 18, D loss: 0.325174, acc:  27%, G loss: 2.678641\n",
      "Ep: 18, steps: 19, D loss: 0.208968, acc:  76%, G loss: 2.492776\n",
      "Ep: 18, steps: 20, D loss: 0.202138, acc:  74%, G loss: 2.400600\n",
      "Ep: 18, steps: 21, D loss: 0.218048, acc:  65%, G loss: 2.587264\n",
      "Ep: 18, steps: 22, D loss: 0.315013, acc:  30%, G loss: 2.873665\n",
      "Ep: 18, steps: 23, D loss: 0.215618, acc:  70%, G loss: 2.476789\n",
      "Ep: 18, steps: 24, D loss: 0.203090, acc:  68%, G loss: 2.459874\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 19, steps: 1, D loss: 0.327388, acc:  24%, G loss: 2.834620\n",
      "Ep: 19, steps: 2, D loss: 0.278475, acc:  44%, G loss: 2.586479\n",
      "Ep: 19, steps: 3, D loss: 0.212585, acc:  65%, G loss: 2.932703\n",
      "Ep: 19, steps: 4, D loss: 0.257129, acc:  51%, G loss: 2.457060\n",
      "Ep: 19, steps: 5, D loss: 0.257822, acc:  52%, G loss: 2.674358\n",
      "Ep: 19, steps: 6, D loss: 0.266197, acc:  48%, G loss: 2.403258\n",
      "Ep: 19, steps: 7, D loss: 0.121228, acc:  87%, G loss: 3.047679\n",
      "Ep: 19, steps: 8, D loss: 0.199785, acc:  75%, G loss: 3.222973\n",
      "Ep: 19, steps: 9, D loss: 0.302390, acc:  33%, G loss: 2.560239\n",
      "Ep: 19, steps: 10, D loss: 0.253343, acc:  56%, G loss: 2.584447\n",
      "Ep: 19, steps: 11, D loss: 0.305310, acc:  31%, G loss: 2.734857\n",
      "Ep: 19, steps: 12, D loss: 0.221630, acc:  66%, G loss: 2.450434\n",
      "Ep: 19, steps: 13, D loss: 0.216904, acc:  66%, G loss: 2.403711\n",
      "Ep: 19, steps: 14, D loss: 0.193563, acc:  77%, G loss: 2.438764\n",
      "Ep: 19, steps: 15, D loss: 0.249415, acc:  56%, G loss: 2.673521\n",
      "Ep: 19, steps: 16, D loss: 0.276523, acc:  50%, G loss: 2.792539\n",
      "Ep: 19, steps: 17, D loss: 0.211556, acc:  70%, G loss: 2.848646\n",
      "Ep: 19, steps: 18, D loss: 0.264875, acc:  49%, G loss: 3.030282\n",
      "Ep: 19, steps: 19, D loss: 0.310328, acc:  30%, G loss: 2.607749\n",
      "Ep: 19, steps: 20, D loss: 0.208518, acc:  77%, G loss: 2.428656\n",
      "Ep: 19, steps: 21, D loss: 0.199670, acc:  75%, G loss: 2.254774\n",
      "Ep: 19, steps: 22, D loss: 0.214567, acc:  67%, G loss: 2.595882\n",
      "Ep: 19, steps: 23, D loss: 0.317135, acc:  29%, G loss: 2.819071\n",
      "Ep: 19, steps: 24, D loss: 0.213298, acc:  70%, G loss: 2.485260\n",
      "Ep: 19, steps: 25, D loss: 0.205596, acc:  67%, G loss: 2.377149\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 20, steps: 1, D loss: 0.328648, acc:  24%, G loss: 2.803364\n",
      "Ep: 20, steps: 2, D loss: 0.283689, acc:  42%, G loss: 2.558803\n",
      "Ep: 20, steps: 3, D loss: 0.215541, acc:  63%, G loss: 2.955609\n",
      "Ep: 20, steps: 4, D loss: 0.256266, acc:  51%, G loss: 2.356229\n",
      "Ep: 20, steps: 5, D loss: 0.257442, acc:  52%, G loss: 2.620264\n",
      "Ep: 20, steps: 6, D loss: 0.270851, acc:  45%, G loss: 2.393447\n",
      "Ep: 20, steps: 7, D loss: 0.123617, acc:  88%, G loss: 2.891619\n",
      "Ep: 20, steps: 8, D loss: 0.197282, acc:  75%, G loss: 3.230647\n",
      "Ep: 20, steps: 9, D loss: 0.303294, acc:  32%, G loss: 2.483567\n",
      "Ep: 20, steps: 10, D loss: 0.255663, acc:  54%, G loss: 2.423462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 20, steps: 11, D loss: 0.303720, acc:  32%, G loss: 2.637836\n",
      "Ep: 20, steps: 12, D loss: 0.225154, acc:  65%, G loss: 2.373044\n",
      "Ep: 20, steps: 13, D loss: 0.211943, acc:  68%, G loss: 2.390632\n",
      "Ep: 20, steps: 14, D loss: 0.191529, acc:  79%, G loss: 2.387742\n",
      "Ep: 20, steps: 15, D loss: 0.249458, acc:  55%, G loss: 2.676724\n",
      "Ep: 20, steps: 16, D loss: 0.277892, acc:  49%, G loss: 2.731159\n",
      "Ep: 20, steps: 17, D loss: 0.211473, acc:  71%, G loss: 2.803661\n",
      "Ep: 20, steps: 18, D loss: 0.268722, acc:  47%, G loss: 2.971024\n",
      "Ep: 20, steps: 19, D loss: 0.310843, acc:  28%, G loss: 2.492608\n",
      "Ep: 20, steps: 20, D loss: 0.207243, acc:  77%, G loss: 2.425121\n",
      "Ep: 20, steps: 21, D loss: 0.197170, acc:  76%, G loss: 2.224462\n",
      "Ep: 20, steps: 22, D loss: 0.212104, acc:  67%, G loss: 2.528681\n",
      "Ep: 20, steps: 23, D loss: 0.309640, acc:  29%, G loss: 2.781243\n",
      "Ep: 20, steps: 24, D loss: 0.209594, acc:  73%, G loss: 2.437046\n",
      "Ep: 20, steps: 25, D loss: 0.203382, acc:  67%, G loss: 2.358398\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 21, steps: 1, D loss: 0.328496, acc:  23%, G loss: 2.694705\n",
      "Ep: 21, steps: 2, D loss: 0.279137, acc:  43%, G loss: 2.547731\n",
      "Ep: 21, steps: 3, D loss: 0.215222, acc:  63%, G loss: 2.994446\n",
      "Ep: 21, steps: 4, D loss: 0.256402, acc:  50%, G loss: 2.374634\n",
      "Ep: 21, steps: 5, D loss: 0.259009, acc:  52%, G loss: 2.596565\n",
      "Ep: 21, steps: 6, D loss: 0.271094, acc:  45%, G loss: 2.415733\n",
      "Ep: 21, steps: 7, D loss: 0.127741, acc:  88%, G loss: 2.955862\n",
      "Ep: 21, steps: 8, D loss: 0.194929, acc:  76%, G loss: 3.238978\n",
      "Ep: 21, steps: 9, D loss: 0.297877, acc:  34%, G loss: 2.542407\n",
      "Ep: 21, steps: 10, D loss: 0.254056, acc:  54%, G loss: 2.482476\n",
      "Saved Model\n",
      "Ep: 21, steps: 11, D loss: 0.309666, acc:  30%, G loss: 2.654504\n",
      "Ep: 21, steps: 12, D loss: 0.210776, acc:  69%, G loss: 2.454161\n",
      "Ep: 21, steps: 13, D loss: 0.202742, acc:  74%, G loss: 2.344071\n",
      "Ep: 21, steps: 14, D loss: 0.240236, acc:  59%, G loss: 2.739713\n",
      "Ep: 21, steps: 15, D loss: 0.267674, acc:  52%, G loss: 2.596960\n",
      "Ep: 21, steps: 16, D loss: 0.213291, acc:  72%, G loss: 2.702096\n",
      "Ep: 21, steps: 17, D loss: 0.275104, acc:  45%, G loss: 2.897022\n",
      "Ep: 21, steps: 18, D loss: 0.303453, acc:  32%, G loss: 2.437010\n",
      "Ep: 21, steps: 19, D loss: 0.206525, acc:  77%, G loss: 2.437421\n",
      "Ep: 21, steps: 20, D loss: 0.200411, acc:  75%, G loss: 2.119632\n",
      "Ep: 21, steps: 21, D loss: 0.207677, acc:  67%, G loss: 2.481475\n",
      "Ep: 21, steps: 22, D loss: 0.308190, acc:  31%, G loss: 2.782651\n",
      "Ep: 21, steps: 23, D loss: 0.209244, acc:  73%, G loss: 2.460886\n",
      "Ep: 21, steps: 24, D loss: 0.203807, acc:  68%, G loss: 2.388052\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 22, steps: 1, D loss: 0.330541, acc:  23%, G loss: 2.667174\n",
      "Ep: 22, steps: 2, D loss: 0.287664, acc:  40%, G loss: 2.512890\n",
      "Ep: 22, steps: 3, D loss: 0.207192, acc:  65%, G loss: 2.860252\n",
      "Ep: 22, steps: 4, D loss: 0.254916, acc:  52%, G loss: 2.302873\n",
      "Ep: 22, steps: 5, D loss: 0.261362, acc:  50%, G loss: 2.575091\n",
      "Ep: 22, steps: 6, D loss: 0.276296, acc:  43%, G loss: 2.344116\n",
      "Ep: 22, steps: 7, D loss: 0.123845, acc:  88%, G loss: 2.865363\n",
      "Ep: 22, steps: 8, D loss: 0.196882, acc:  75%, G loss: 3.097239\n",
      "Ep: 22, steps: 9, D loss: 0.300003, acc:  34%, G loss: 2.462527\n",
      "Ep: 22, steps: 10, D loss: 0.257777, acc:  52%, G loss: 2.451067\n",
      "Ep: 22, steps: 11, D loss: 0.302990, acc:  31%, G loss: 2.608093\n",
      "Ep: 22, steps: 12, D loss: 0.225082, acc:  65%, G loss: 2.344857\n",
      "Ep: 22, steps: 13, D loss: 0.219737, acc:  66%, G loss: 2.272903\n",
      "Ep: 22, steps: 14, D loss: 0.197288, acc:  78%, G loss: 2.262438\n",
      "Ep: 22, steps: 15, D loss: 0.246143, acc:  55%, G loss: 2.690856\n",
      "Ep: 22, steps: 16, D loss: 0.270836, acc:  49%, G loss: 2.637937\n",
      "Ep: 22, steps: 17, D loss: 0.209420, acc:  72%, G loss: 2.768327\n",
      "Ep: 22, steps: 18, D loss: 0.270288, acc:  44%, G loss: 2.918881\n",
      "Ep: 22, steps: 19, D loss: 0.305851, acc:  30%, G loss: 2.388507\n",
      "Ep: 22, steps: 20, D loss: 0.208052, acc:  78%, G loss: 2.346774\n",
      "Ep: 22, steps: 21, D loss: 0.198563, acc:  76%, G loss: 2.120891\n",
      "Ep: 22, steps: 22, D loss: 0.207980, acc:  66%, G loss: 2.555745\n",
      "Ep: 22, steps: 23, D loss: 0.303038, acc:  32%, G loss: 2.763055\n",
      "Ep: 22, steps: 24, D loss: 0.208911, acc:  73%, G loss: 2.451594\n",
      "Ep: 22, steps: 25, D loss: 0.202259, acc:  68%, G loss: 2.358491\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 23, steps: 1, D loss: 0.325797, acc:  24%, G loss: 2.693988\n",
      "Ep: 23, steps: 2, D loss: 0.280247, acc:  42%, G loss: 2.485196\n",
      "Ep: 23, steps: 3, D loss: 0.212907, acc:  64%, G loss: 2.877460\n",
      "Ep: 23, steps: 4, D loss: 0.253164, acc:  52%, G loss: 2.257261\n",
      "Ep: 23, steps: 5, D loss: 0.256952, acc:  53%, G loss: 2.568499\n",
      "Ep: 23, steps: 6, D loss: 0.277376, acc:  43%, G loss: 2.343815\n",
      "Ep: 23, steps: 7, D loss: 0.127278, acc:  89%, G loss: 2.868368\n",
      "Ep: 23, steps: 8, D loss: 0.198522, acc:  74%, G loss: 3.089389\n",
      "Ep: 23, steps: 9, D loss: 0.302510, acc:  30%, G loss: 2.416732\n",
      "Ep: 23, steps: 10, D loss: 0.255877, acc:  53%, G loss: 2.375002\n",
      "Ep: 23, steps: 11, D loss: 0.301982, acc:  30%, G loss: 2.789141\n",
      "Ep: 23, steps: 12, D loss: 0.224365, acc:  66%, G loss: 2.390588\n",
      "Ep: 23, steps: 13, D loss: 0.217406, acc:  66%, G loss: 2.416299\n",
      "Ep: 23, steps: 14, D loss: 0.197824, acc:  77%, G loss: 2.502175\n",
      "Ep: 23, steps: 15, D loss: 0.243951, acc:  58%, G loss: 2.689598\n",
      "Ep: 23, steps: 16, D loss: 0.269657, acc:  49%, G loss: 2.636713\n",
      "Ep: 23, steps: 17, D loss: 0.213094, acc:  71%, G loss: 2.756342\n",
      "Ep: 23, steps: 18, D loss: 0.276925, acc:  42%, G loss: 2.921519\n",
      "Ep: 23, steps: 19, D loss: 0.303241, acc:  30%, G loss: 2.342943\n",
      "Ep: 23, steps: 20, D loss: 0.206029, acc:  77%, G loss: 2.336015\n",
      "Ep: 23, steps: 21, D loss: 0.198039, acc:  76%, G loss: 2.187620\n",
      "Ep: 23, steps: 22, D loss: 0.208032, acc:  68%, G loss: 2.532549\n",
      "Ep: 23, steps: 23, D loss: 0.305291, acc:  31%, G loss: 2.755603\n",
      "Ep: 23, steps: 24, D loss: 0.207226, acc:  74%, G loss: 2.470864\n",
      "Ep: 23, steps: 25, D loss: 0.198066, acc:  70%, G loss: 2.332086\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 24, steps: 1, D loss: 0.331436, acc:  22%, G loss: 2.591206\n",
      "Ep: 24, steps: 2, D loss: 0.277714, acc:  43%, G loss: 2.472955\n",
      "Ep: 24, steps: 3, D loss: 0.210194, acc:  64%, G loss: 2.809250\n",
      "Ep: 24, steps: 4, D loss: 0.253619, acc:  52%, G loss: 2.268077\n",
      "Ep: 24, steps: 5, D loss: 0.257462, acc:  52%, G loss: 2.589337\n",
      "Ep: 24, steps: 6, D loss: 0.279153, acc:  42%, G loss: 2.307170\n",
      "Ep: 24, steps: 7, D loss: 0.125911, acc:  89%, G loss: 2.825982\n",
      "Ep: 24, steps: 8, D loss: 0.195919, acc:  77%, G loss: 3.135286\n",
      "Saved Model\n",
      "Ep: 24, steps: 9, D loss: 0.311689, acc:  27%, G loss: 2.426574\n",
      "Ep: 24, steps: 10, D loss: 0.303137, acc:  29%, G loss: 2.587636\n",
      "Ep: 24, steps: 11, D loss: 0.222595, acc:  68%, G loss: 2.313723\n",
      "Ep: 24, steps: 12, D loss: 0.209822, acc:  71%, G loss: 2.314964\n",
      "Ep: 24, steps: 13, D loss: 0.191545, acc:  81%, G loss: 2.320590\n",
      "Ep: 24, steps: 14, D loss: 0.250903, acc:  53%, G loss: 2.677508\n",
      "Ep: 24, steps: 15, D loss: 0.261117, acc:  50%, G loss: 2.569767\n",
      "Ep: 24, steps: 16, D loss: 0.212778, acc:  69%, G loss: 2.736727\n",
      "Ep: 24, steps: 17, D loss: 0.259096, acc:  50%, G loss: 2.887328\n",
      "Ep: 24, steps: 18, D loss: 0.308591, acc:  27%, G loss: 2.383345\n",
      "Ep: 24, steps: 19, D loss: 0.203901, acc:  79%, G loss: 2.356104\n",
      "Ep: 24, steps: 20, D loss: 0.190478, acc:  80%, G loss: 2.154972\n",
      "Ep: 24, steps: 21, D loss: 0.203917, acc:  68%, G loss: 2.432642\n",
      "Ep: 24, steps: 22, D loss: 0.316455, acc:  26%, G loss: 2.700358\n",
      "Ep: 24, steps: 23, D loss: 0.205352, acc:  75%, G loss: 2.432706\n",
      "Ep: 24, steps: 24, D loss: 0.197788, acc:  68%, G loss: 2.318640\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 25, steps: 1, D loss: 0.339190, acc:  20%, G loss: 2.484401\n",
      "Ep: 25, steps: 2, D loss: 0.283036, acc:  41%, G loss: 2.497939\n",
      "Ep: 25, steps: 3, D loss: 0.209890, acc:  64%, G loss: 2.877773\n",
      "Ep: 25, steps: 4, D loss: 0.261628, acc:  49%, G loss: 2.234399\n",
      "Ep: 25, steps: 5, D loss: 0.255945, acc:  54%, G loss: 2.506218\n",
      "Ep: 25, steps: 6, D loss: 0.288392, acc:  38%, G loss: 2.248487\n",
      "Ep: 25, steps: 7, D loss: 0.124630, acc:  89%, G loss: 2.742331\n",
      "Ep: 25, steps: 8, D loss: 0.195042, acc:  78%, G loss: 3.072801\n",
      "Ep: 25, steps: 9, D loss: 0.306512, acc:  27%, G loss: 2.407774\n",
      "Ep: 25, steps: 10, D loss: 0.262469, acc:  49%, G loss: 2.360342\n",
      "Ep: 25, steps: 11, D loss: 0.302935, acc:  29%, G loss: 2.576783\n",
      "Ep: 25, steps: 12, D loss: 0.226162, acc:  65%, G loss: 2.340237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 25, steps: 13, D loss: 0.214885, acc:  67%, G loss: 2.218254\n",
      "Ep: 25, steps: 14, D loss: 0.196534, acc:  78%, G loss: 2.179489\n",
      "Ep: 25, steps: 15, D loss: 0.244268, acc:  56%, G loss: 2.694504\n",
      "Ep: 25, steps: 16, D loss: 0.264924, acc:  51%, G loss: 2.555130\n",
      "Ep: 25, steps: 17, D loss: 0.209994, acc:  73%, G loss: 2.751512\n",
      "Ep: 25, steps: 18, D loss: 0.266926, acc:  46%, G loss: 2.845048\n",
      "Ep: 25, steps: 19, D loss: 0.301355, acc:  31%, G loss: 2.277512\n",
      "Ep: 25, steps: 20, D loss: 0.204755, acc:  79%, G loss: 2.348618\n",
      "Ep: 25, steps: 21, D loss: 0.197966, acc:  76%, G loss: 2.339028\n",
      "Ep: 25, steps: 22, D loss: 0.206631, acc:  69%, G loss: 2.333263\n",
      "Ep: 25, steps: 23, D loss: 0.302573, acc:  30%, G loss: 2.636688\n",
      "Ep: 25, steps: 24, D loss: 0.207860, acc:  74%, G loss: 2.291025\n",
      "Ep: 25, steps: 25, D loss: 0.204811, acc:  66%, G loss: 2.194367\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 26, steps: 1, D loss: 0.340199, acc:  21%, G loss: 2.697337\n",
      "Ep: 26, steps: 2, D loss: 0.281451, acc:  42%, G loss: 2.511383\n",
      "Ep: 26, steps: 3, D loss: 0.211646, acc:  67%, G loss: 2.700758\n",
      "Ep: 26, steps: 4, D loss: 0.254657, acc:  51%, G loss: 2.252427\n",
      "Ep: 26, steps: 5, D loss: 0.254304, acc:  54%, G loss: 2.530954\n",
      "Ep: 26, steps: 6, D loss: 0.276992, acc:  42%, G loss: 2.345528\n",
      "Ep: 26, steps: 7, D loss: 0.127567, acc:  90%, G loss: 2.609761\n",
      "Ep: 26, steps: 8, D loss: 0.191284, acc:  79%, G loss: 2.904624\n",
      "Ep: 26, steps: 9, D loss: 0.306845, acc:  28%, G loss: 2.360612\n",
      "Ep: 26, steps: 10, D loss: 0.259496, acc:  51%, G loss: 2.343652\n",
      "Ep: 26, steps: 11, D loss: 0.302815, acc:  29%, G loss: 2.554532\n",
      "Ep: 26, steps: 12, D loss: 0.226786, acc:  65%, G loss: 2.326952\n",
      "Ep: 26, steps: 13, D loss: 0.218674, acc:  67%, G loss: 2.187879\n",
      "Ep: 26, steps: 14, D loss: 0.196625, acc:  78%, G loss: 2.247052\n",
      "Ep: 26, steps: 15, D loss: 0.241292, acc:  57%, G loss: 2.649303\n",
      "Ep: 26, steps: 16, D loss: 0.264935, acc:  49%, G loss: 2.511985\n",
      "Ep: 26, steps: 17, D loss: 0.209291, acc:  73%, G loss: 2.724743\n",
      "Ep: 26, steps: 18, D loss: 0.272249, acc:  45%, G loss: 2.804130\n",
      "Ep: 26, steps: 19, D loss: 0.299495, acc:  30%, G loss: 2.222533\n",
      "Ep: 26, steps: 20, D loss: 0.204999, acc:  78%, G loss: 2.344470\n",
      "Ep: 26, steps: 21, D loss: 0.193124, acc:  78%, G loss: 2.397719\n",
      "Ep: 26, steps: 22, D loss: 0.200459, acc:  70%, G loss: 2.347776\n",
      "Ep: 26, steps: 23, D loss: 0.305357, acc:  29%, G loss: 2.608323\n",
      "Ep: 26, steps: 24, D loss: 0.208566, acc:  74%, G loss: 2.324427\n",
      "Ep: 26, steps: 25, D loss: 0.202214, acc:  67%, G loss: 2.321669\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 27, steps: 1, D loss: 0.343399, acc:  21%, G loss: 2.687818\n",
      "Ep: 27, steps: 2, D loss: 0.278522, acc:  42%, G loss: 2.506293\n",
      "Ep: 27, steps: 3, D loss: 0.209438, acc:  66%, G loss: 2.714827\n",
      "Ep: 27, steps: 4, D loss: 0.253200, acc:  53%, G loss: 2.220066\n",
      "Ep: 27, steps: 5, D loss: 0.250080, acc:  56%, G loss: 2.571516\n",
      "Ep: 27, steps: 6, D loss: 0.278785, acc:  40%, G loss: 2.510885\n",
      "Saved Model\n",
      "Ep: 27, steps: 7, D loss: 0.123749, acc:  90%, G loss: 2.600111\n",
      "Ep: 27, steps: 8, D loss: 0.279770, acc:  40%, G loss: 2.213014\n",
      "Ep: 27, steps: 9, D loss: 0.263800, acc:  49%, G loss: 2.247225\n",
      "Ep: 27, steps: 10, D loss: 0.286820, acc:  37%, G loss: 2.584657\n",
      "Ep: 27, steps: 11, D loss: 0.223009, acc:  67%, G loss: 2.267203\n",
      "Ep: 27, steps: 12, D loss: 0.210975, acc:  72%, G loss: 2.228317\n",
      "Ep: 27, steps: 13, D loss: 0.196325, acc:  80%, G loss: 2.320982\n",
      "Ep: 27, steps: 14, D loss: 0.245197, acc:  57%, G loss: 2.642884\n",
      "Ep: 27, steps: 15, D loss: 0.255671, acc:  52%, G loss: 2.787714\n",
      "Ep: 27, steps: 16, D loss: 0.207338, acc:  74%, G loss: 2.800982\n",
      "Ep: 27, steps: 17, D loss: 0.256309, acc:  51%, G loss: 2.888660\n",
      "Ep: 27, steps: 18, D loss: 0.307568, acc:  29%, G loss: 2.336907\n",
      "Ep: 27, steps: 19, D loss: 0.202635, acc:  78%, G loss: 2.383053\n",
      "Ep: 27, steps: 20, D loss: 0.195025, acc:  78%, G loss: 2.586872\n",
      "Ep: 27, steps: 21, D loss: 0.209157, acc:  73%, G loss: 2.455490\n",
      "Ep: 27, steps: 22, D loss: 0.299712, acc:  33%, G loss: 2.740346\n",
      "Ep: 27, steps: 23, D loss: 0.204631, acc:  74%, G loss: 2.306412\n",
      "Ep: 27, steps: 24, D loss: 0.199735, acc:  67%, G loss: 2.471093\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 28, steps: 1, D loss: 0.333618, acc:  26%, G loss: 2.560122\n",
      "Ep: 28, steps: 2, D loss: 0.281619, acc:  41%, G loss: 2.458949\n",
      "Ep: 28, steps: 3, D loss: 0.204378, acc:  68%, G loss: 2.783189\n",
      "Ep: 28, steps: 4, D loss: 0.247088, acc:  57%, G loss: 2.238040\n",
      "Ep: 28, steps: 5, D loss: 0.257209, acc:  53%, G loss: 2.525136\n",
      "Ep: 28, steps: 6, D loss: 0.281139, acc:  40%, G loss: 2.202868\n",
      "Ep: 28, steps: 7, D loss: 0.129962, acc:  88%, G loss: 2.704467\n",
      "Ep: 28, steps: 8, D loss: 0.191687, acc:  80%, G loss: 3.069742\n",
      "Ep: 28, steps: 9, D loss: 0.311055, acc:  27%, G loss: 2.413969\n",
      "Ep: 28, steps: 10, D loss: 0.261070, acc:  48%, G loss: 2.350569\n",
      "Ep: 28, steps: 11, D loss: 0.300391, acc:  31%, G loss: 2.526688\n",
      "Ep: 28, steps: 12, D loss: 0.224074, acc:  67%, G loss: 2.285090\n",
      "Ep: 28, steps: 13, D loss: 0.218831, acc:  67%, G loss: 2.274081\n",
      "Ep: 28, steps: 14, D loss: 0.197590, acc:  77%, G loss: 2.294638\n",
      "Ep: 28, steps: 15, D loss: 0.241215, acc:  58%, G loss: 2.628285\n",
      "Ep: 28, steps: 16, D loss: 0.261480, acc:  50%, G loss: 2.553194\n",
      "Ep: 28, steps: 17, D loss: 0.207978, acc:  75%, G loss: 2.668430\n",
      "Ep: 28, steps: 18, D loss: 0.274852, acc:  44%, G loss: 2.774452\n",
      "Ep: 28, steps: 19, D loss: 0.298429, acc:  31%, G loss: 2.281954\n",
      "Ep: 28, steps: 20, D loss: 0.206345, acc:  77%, G loss: 2.325390\n",
      "Ep: 28, steps: 21, D loss: 0.198417, acc:  76%, G loss: 2.610250\n",
      "Ep: 28, steps: 22, D loss: 0.201267, acc:  72%, G loss: 2.391360\n",
      "Ep: 28, steps: 23, D loss: 0.299841, acc:  31%, G loss: 2.566367\n",
      "Ep: 28, steps: 24, D loss: 0.204958, acc:  75%, G loss: 2.363568\n",
      "Ep: 28, steps: 25, D loss: 0.199210, acc:  69%, G loss: 2.312650\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 29, steps: 1, D loss: 0.341451, acc:  22%, G loss: 2.582217\n",
      "Ep: 29, steps: 2, D loss: 0.284785, acc:  40%, G loss: 2.469270\n",
      "Ep: 29, steps: 3, D loss: 0.203997, acc:  68%, G loss: 2.776303\n",
      "Ep: 29, steps: 4, D loss: 0.253882, acc:  52%, G loss: 2.186059\n",
      "Ep: 29, steps: 5, D loss: 0.253728, acc:  55%, G loss: 2.542091\n",
      "Ep: 29, steps: 6, D loss: 0.283126, acc:  39%, G loss: 2.227064\n",
      "Ep: 29, steps: 7, D loss: 0.121331, acc:  89%, G loss: 2.606315\n",
      "Ep: 29, steps: 8, D loss: 0.190187, acc:  80%, G loss: 3.022924\n",
      "Ep: 29, steps: 9, D loss: 0.312147, acc:  25%, G loss: 2.348686\n",
      "Ep: 29, steps: 10, D loss: 0.261920, acc:  48%, G loss: 2.365615\n",
      "Ep: 29, steps: 11, D loss: 0.305237, acc:  29%, G loss: 2.508430\n",
      "Ep: 29, steps: 12, D loss: 0.227815, acc:  65%, G loss: 2.235569\n",
      "Ep: 29, steps: 13, D loss: 0.221963, acc:  65%, G loss: 2.198098\n",
      "Ep: 29, steps: 14, D loss: 0.197875, acc:  77%, G loss: 2.240893\n",
      "Ep: 29, steps: 15, D loss: 0.242077, acc:  57%, G loss: 2.639631\n",
      "Ep: 29, steps: 16, D loss: 0.261590, acc:  51%, G loss: 2.546641\n",
      "Ep: 29, steps: 17, D loss: 0.207918, acc:  74%, G loss: 2.678369\n",
      "Ep: 29, steps: 18, D loss: 0.273397, acc:  43%, G loss: 2.739011\n",
      "Ep: 29, steps: 19, D loss: 0.299092, acc:  31%, G loss: 2.213035\n",
      "Ep: 29, steps: 20, D loss: 0.201205, acc:  79%, G loss: 2.297330\n",
      "Ep: 29, steps: 21, D loss: 0.197815, acc:  75%, G loss: 2.257197\n",
      "Ep: 29, steps: 22, D loss: 0.202098, acc:  71%, G loss: 2.291229\n",
      "Ep: 29, steps: 23, D loss: 0.298331, acc:  31%, G loss: 2.549070\n",
      "Ep: 29, steps: 24, D loss: 0.204372, acc:  75%, G loss: 2.280468\n",
      "Ep: 29, steps: 25, D loss: 0.201040, acc:  67%, G loss: 2.223368\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 30, steps: 1, D loss: 0.330176, acc:  24%, G loss: 2.451521\n",
      "Ep: 30, steps: 2, D loss: 0.273859, acc:  44%, G loss: 2.391293\n",
      "Ep: 30, steps: 3, D loss: 0.205418, acc:  66%, G loss: 2.968083\n",
      "Ep: 30, steps: 4, D loss: 0.252424, acc:  55%, G loss: 2.232329\n",
      "Saved Model\n",
      "Ep: 30, steps: 5, D loss: 0.249271, acc:  55%, G loss: 2.479836\n",
      "Ep: 30, steps: 6, D loss: 0.126797, acc:  91%, G loss: 2.393193\n",
      "Ep: 30, steps: 7, D loss: 0.166834, acc:  85%, G loss: 2.648306\n",
      "Ep: 30, steps: 8, D loss: 0.308546, acc:  29%, G loss: 2.193716\n",
      "Ep: 30, steps: 9, D loss: 0.237185, acc:  61%, G loss: 2.338629\n",
      "Ep: 30, steps: 10, D loss: 0.314266, acc:  26%, G loss: 2.507070\n",
      "Ep: 30, steps: 11, D loss: 0.236503, acc:  62%, G loss: 2.151453\n",
      "Ep: 30, steps: 12, D loss: 0.226512, acc:  65%, G loss: 2.275508\n",
      "Ep: 30, steps: 13, D loss: 0.191960, acc:  80%, G loss: 2.233473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 30, steps: 14, D loss: 0.241952, acc:  56%, G loss: 2.692896\n",
      "Ep: 30, steps: 15, D loss: 0.272625, acc:  48%, G loss: 2.528876\n",
      "Ep: 30, steps: 16, D loss: 0.204069, acc:  76%, G loss: 2.673305\n",
      "Ep: 30, steps: 17, D loss: 0.280154, acc:  40%, G loss: 2.704587\n",
      "Ep: 30, steps: 18, D loss: 0.303395, acc:  29%, G loss: 2.200065\n",
      "Ep: 30, steps: 19, D loss: 0.199892, acc:  79%, G loss: 2.314353\n",
      "Ep: 30, steps: 20, D loss: 0.191026, acc:  79%, G loss: 2.362904\n",
      "Ep: 30, steps: 21, D loss: 0.194761, acc:  72%, G loss: 2.162772\n",
      "Ep: 30, steps: 22, D loss: 0.316395, acc:  27%, G loss: 2.582690\n",
      "Ep: 30, steps: 23, D loss: 0.206344, acc:  75%, G loss: 2.327732\n",
      "Ep: 30, steps: 24, D loss: 0.199056, acc:  70%, G loss: 2.640464\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 31, steps: 1, D loss: 0.330150, acc:  24%, G loss: 2.435744\n",
      "Ep: 31, steps: 2, D loss: 0.286045, acc:  39%, G loss: 2.338169\n",
      "Ep: 31, steps: 3, D loss: 0.199332, acc:  69%, G loss: 2.699233\n",
      "Ep: 31, steps: 4, D loss: 0.248364, acc:  57%, G loss: 2.294093\n",
      "Ep: 31, steps: 5, D loss: 0.247735, acc:  58%, G loss: 2.467444\n",
      "Ep: 31, steps: 6, D loss: 0.283766, acc:  38%, G loss: 2.244161\n",
      "Ep: 31, steps: 7, D loss: 0.125578, acc:  88%, G loss: 2.538669\n",
      "Ep: 31, steps: 8, D loss: 0.191012, acc:  79%, G loss: 3.076938\n",
      "Ep: 31, steps: 9, D loss: 0.310641, acc:  26%, G loss: 2.348967\n",
      "Ep: 31, steps: 10, D loss: 0.258564, acc:  51%, G loss: 2.274254\n",
      "Ep: 31, steps: 11, D loss: 0.305909, acc:  29%, G loss: 2.501195\n",
      "Ep: 31, steps: 12, D loss: 0.233639, acc:  63%, G loss: 2.268666\n",
      "Ep: 31, steps: 13, D loss: 0.221140, acc:  65%, G loss: 2.159185\n",
      "Ep: 31, steps: 14, D loss: 0.199589, acc:  75%, G loss: 2.164137\n",
      "Ep: 31, steps: 15, D loss: 0.242174, acc:  56%, G loss: 2.688331\n",
      "Ep: 31, steps: 16, D loss: 0.268728, acc:  50%, G loss: 2.505073\n",
      "Ep: 31, steps: 17, D loss: 0.205136, acc:  75%, G loss: 2.665207\n",
      "Ep: 31, steps: 18, D loss: 0.273590, acc:  44%, G loss: 2.671069\n",
      "Ep: 31, steps: 19, D loss: 0.291949, acc:  34%, G loss: 2.199497\n",
      "Ep: 31, steps: 20, D loss: 0.201422, acc:  78%, G loss: 2.316789\n",
      "Ep: 31, steps: 21, D loss: 0.199431, acc:  73%, G loss: 2.272285\n",
      "Ep: 31, steps: 22, D loss: 0.206719, acc:  70%, G loss: 2.297933\n",
      "Ep: 31, steps: 23, D loss: 0.291469, acc:  35%, G loss: 2.540375\n",
      "Ep: 31, steps: 24, D loss: 0.202875, acc:  76%, G loss: 2.321506\n",
      "Ep: 31, steps: 25, D loss: 0.193635, acc:  70%, G loss: 2.209879\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 32, steps: 1, D loss: 0.322553, acc:  26%, G loss: 2.393625\n",
      "Ep: 32, steps: 2, D loss: 0.276687, acc:  44%, G loss: 2.377319\n",
      "Ep: 32, steps: 3, D loss: 0.196852, acc:  70%, G loss: 2.812906\n",
      "Ep: 32, steps: 4, D loss: 0.238558, acc:  61%, G loss: 2.228328\n",
      "Ep: 32, steps: 5, D loss: 0.255187, acc:  53%, G loss: 2.433248\n",
      "Ep: 32, steps: 6, D loss: 0.284970, acc:  39%, G loss: 2.285543\n",
      "Ep: 32, steps: 7, D loss: 0.118492, acc:  89%, G loss: 2.557180\n",
      "Ep: 32, steps: 8, D loss: 0.188130, acc:  78%, G loss: 3.089211\n",
      "Ep: 32, steps: 9, D loss: 0.311616, acc:  25%, G loss: 2.356378\n",
      "Ep: 32, steps: 10, D loss: 0.257398, acc:  49%, G loss: 2.229536\n",
      "Ep: 32, steps: 11, D loss: 0.304830, acc:  31%, G loss: 2.424597\n",
      "Ep: 32, steps: 12, D loss: 0.235598, acc:  61%, G loss: 2.186408\n",
      "Ep: 32, steps: 13, D loss: 0.221974, acc:  65%, G loss: 2.145032\n",
      "Ep: 32, steps: 14, D loss: 0.196192, acc:  75%, G loss: 2.240500\n",
      "Ep: 32, steps: 15, D loss: 0.242755, acc:  56%, G loss: 2.683303\n",
      "Ep: 32, steps: 16, D loss: 0.262221, acc:  50%, G loss: 2.474348\n",
      "Ep: 32, steps: 17, D loss: 0.203391, acc:  75%, G loss: 2.654714\n",
      "Ep: 32, steps: 18, D loss: 0.273121, acc:  44%, G loss: 2.688496\n",
      "Ep: 32, steps: 19, D loss: 0.289503, acc:  35%, G loss: 2.221605\n",
      "Ep: 32, steps: 20, D loss: 0.196317, acc:  80%, G loss: 2.292198\n",
      "Ep: 32, steps: 21, D loss: 0.192997, acc:  78%, G loss: 2.358944\n",
      "Ep: 32, steps: 22, D loss: 0.198108, acc:  72%, G loss: 2.319742\n",
      "Ep: 32, steps: 23, D loss: 0.298959, acc:  31%, G loss: 2.522139\n",
      "Ep: 32, steps: 24, D loss: 0.205467, acc:  75%, G loss: 2.302096\n",
      "Ep: 32, steps: 25, D loss: 0.196686, acc:  69%, G loss: 2.248724\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 33, steps: 1, D loss: 0.331575, acc:  24%, G loss: 2.314177\n",
      "Saved Model\n",
      "Ep: 33, steps: 2, D loss: 0.298751, acc:  36%, G loss: 2.318685\n",
      "Ep: 33, steps: 3, D loss: 0.259978, acc:  49%, G loss: 2.322686\n",
      "Ep: 33, steps: 4, D loss: 0.246506, acc:  57%, G loss: 2.378572\n",
      "Ep: 33, steps: 5, D loss: 0.270814, acc:  41%, G loss: 2.252664\n",
      "Ep: 33, steps: 6, D loss: 0.122575, acc:  90%, G loss: 2.605848\n",
      "Ep: 33, steps: 7, D loss: 0.191250, acc:  79%, G loss: 3.013688\n",
      "Ep: 33, steps: 8, D loss: 0.304375, acc:  27%, G loss: 2.292351\n",
      "Ep: 33, steps: 9, D loss: 0.262940, acc:  46%, G loss: 2.249905\n",
      "Ep: 33, steps: 10, D loss: 0.299951, acc:  30%, G loss: 2.511384\n",
      "Ep: 33, steps: 11, D loss: 0.231371, acc:  63%, G loss: 2.207473\n",
      "Ep: 33, steps: 12, D loss: 0.219509, acc:  65%, G loss: 2.171790\n",
      "Ep: 33, steps: 13, D loss: 0.196945, acc:  77%, G loss: 2.145841\n",
      "Ep: 33, steps: 14, D loss: 0.241452, acc:  56%, G loss: 2.684347\n",
      "Ep: 33, steps: 15, D loss: 0.262211, acc:  51%, G loss: 2.445397\n",
      "Ep: 33, steps: 16, D loss: 0.204906, acc:  74%, G loss: 2.662254\n",
      "Ep: 33, steps: 17, D loss: 0.273145, acc:  45%, G loss: 2.664890\n",
      "Ep: 33, steps: 18, D loss: 0.291602, acc:  35%, G loss: 2.207666\n",
      "Ep: 33, steps: 19, D loss: 0.197427, acc:  80%, G loss: 2.223535\n",
      "Ep: 33, steps: 20, D loss: 0.194001, acc:  77%, G loss: 2.108951\n",
      "Ep: 33, steps: 21, D loss: 0.198592, acc:  72%, G loss: 2.297743\n",
      "Ep: 33, steps: 22, D loss: 0.292185, acc:  33%, G loss: 2.579518\n",
      "Ep: 33, steps: 23, D loss: 0.203999, acc:  74%, G loss: 2.307254\n",
      "Ep: 33, steps: 24, D loss: 0.195504, acc:  70%, G loss: 2.175299\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 34, steps: 1, D loss: 0.318351, acc:  27%, G loss: 2.508096\n",
      "Ep: 34, steps: 2, D loss: 0.281682, acc:  41%, G loss: 2.295644\n",
      "Ep: 34, steps: 3, D loss: 0.192262, acc:  70%, G loss: 3.000648\n",
      "Ep: 34, steps: 4, D loss: 0.243927, acc:  58%, G loss: 2.193039\n",
      "Ep: 34, steps: 5, D loss: 0.250660, acc:  55%, G loss: 2.469482\n",
      "Ep: 34, steps: 6, D loss: 0.287236, acc:  38%, G loss: 2.285029\n",
      "Ep: 34, steps: 7, D loss: 0.126826, acc:  88%, G loss: 2.535356\n",
      "Ep: 34, steps: 8, D loss: 0.191579, acc:  78%, G loss: 2.773031\n",
      "Ep: 34, steps: 9, D loss: 0.308845, acc:  27%, G loss: 2.284625\n",
      "Ep: 34, steps: 10, D loss: 0.257670, acc:  48%, G loss: 2.263829\n",
      "Ep: 34, steps: 11, D loss: 0.303116, acc:  31%, G loss: 2.484146\n",
      "Ep: 34, steps: 12, D loss: 0.235032, acc:  61%, G loss: 2.209173\n",
      "Ep: 34, steps: 13, D loss: 0.218004, acc:  68%, G loss: 2.163673\n",
      "Ep: 34, steps: 14, D loss: 0.199660, acc:  76%, G loss: 2.174775\n",
      "Ep: 34, steps: 15, D loss: 0.242991, acc:  55%, G loss: 2.647366\n",
      "Ep: 34, steps: 16, D loss: 0.257720, acc:  51%, G loss: 2.453536\n",
      "Ep: 34, steps: 17, D loss: 0.204003, acc:  75%, G loss: 2.629938\n",
      "Ep: 34, steps: 18, D loss: 0.265819, acc:  46%, G loss: 2.638786\n",
      "Ep: 34, steps: 19, D loss: 0.286577, acc:  37%, G loss: 2.211612\n",
      "Ep: 34, steps: 20, D loss: 0.195002, acc:  79%, G loss: 2.260415\n",
      "Ep: 34, steps: 21, D loss: 0.196459, acc:  76%, G loss: 2.524648\n",
      "Ep: 34, steps: 22, D loss: 0.197602, acc:  75%, G loss: 2.275468\n",
      "Ep: 34, steps: 23, D loss: 0.297351, acc:  34%, G loss: 2.533325\n",
      "Ep: 34, steps: 24, D loss: 0.201445, acc:  76%, G loss: 2.286247\n",
      "Ep: 34, steps: 25, D loss: 0.195565, acc:  70%, G loss: 2.322953\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 35, steps: 1, D loss: 0.334556, acc:  25%, G loss: 2.422773\n",
      "Ep: 35, steps: 2, D loss: 0.293087, acc:  37%, G loss: 2.309299\n",
      "Ep: 35, steps: 3, D loss: 0.199138, acc:  70%, G loss: 2.874776\n",
      "Ep: 35, steps: 4, D loss: 0.241819, acc:  60%, G loss: 2.149796\n",
      "Ep: 35, steps: 5, D loss: 0.255343, acc:  54%, G loss: 2.408223\n",
      "Ep: 35, steps: 6, D loss: 0.292191, acc:  38%, G loss: 2.294950\n",
      "Ep: 35, steps: 7, D loss: 0.129422, acc:  89%, G loss: 2.528323\n",
      "Ep: 35, steps: 8, D loss: 0.186901, acc:  79%, G loss: 3.016628\n",
      "Ep: 35, steps: 9, D loss: 0.307159, acc:  27%, G loss: 2.330364\n",
      "Ep: 35, steps: 10, D loss: 0.263865, acc:  46%, G loss: 2.242586\n",
      "Ep: 35, steps: 11, D loss: 0.294502, acc:  35%, G loss: 2.861048\n",
      "Ep: 35, steps: 12, D loss: 0.230808, acc:  62%, G loss: 2.567930\n",
      "Ep: 35, steps: 13, D loss: 0.216697, acc:  67%, G loss: 2.384913\n",
      "Ep: 35, steps: 14, D loss: 0.188692, acc:  81%, G loss: 2.278722\n",
      "Ep: 35, steps: 15, D loss: 0.237766, acc:  61%, G loss: 2.689368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 35, steps: 16, D loss: 0.254648, acc:  52%, G loss: 2.499030\n",
      "Ep: 35, steps: 17, D loss: 0.199695, acc:  76%, G loss: 2.775511\n",
      "Ep: 35, steps: 18, D loss: 0.260564, acc:  50%, G loss: 2.744779\n",
      "Ep: 35, steps: 19, D loss: 0.295293, acc:  33%, G loss: 2.489704\n",
      "Ep: 35, steps: 20, D loss: 0.187647, acc:  82%, G loss: 2.349492\n",
      "Ep: 35, steps: 21, D loss: 0.186822, acc:  80%, G loss: 2.294621\n",
      "Ep: 35, steps: 22, D loss: 0.183972, acc:  78%, G loss: 2.461713\n",
      "Ep: 35, steps: 23, D loss: 0.314416, acc:  27%, G loss: 2.808483\n",
      "Ep: 35, steps: 24, D loss: 0.201560, acc:  74%, G loss: 2.328123\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 35, steps: 25, D loss: 0.189386, acc:  72%, G loss: 2.198526\n",
      "Ep: 35, steps: 26, D loss: 0.284749, acc:  39%, G loss: 2.458032\n",
      "Ep: 35, steps: 27, D loss: 0.184361, acc:  71%, G loss: 2.789801\n",
      "Ep: 35, steps: 28, D loss: 0.251192, acc:  54%, G loss: 2.455661\n",
      "Ep: 35, steps: 29, D loss: 0.243698, acc:  56%, G loss: 2.482402\n",
      "Ep: 35, steps: 30, D loss: 0.281446, acc:  43%, G loss: 2.292315\n",
      "Ep: 35, steps: 31, D loss: 0.105457, acc:  90%, G loss: 2.840496\n",
      "Ep: 35, steps: 32, D loss: 0.183984, acc:  79%, G loss: 2.991459\n",
      "Ep: 35, steps: 33, D loss: 0.301380, acc:  29%, G loss: 2.443371\n",
      "Ep: 35, steps: 34, D loss: 0.236880, acc:  58%, G loss: 2.410918\n",
      "Ep: 35, steps: 35, D loss: 0.313877, acc:  30%, G loss: 2.681420\n",
      "Ep: 35, steps: 36, D loss: 0.227083, acc:  64%, G loss: 2.432495\n",
      "Ep: 35, steps: 37, D loss: 0.216180, acc:  66%, G loss: 2.361386\n",
      "Ep: 35, steps: 38, D loss: 0.181009, acc:  82%, G loss: 2.258382\n",
      "Ep: 35, steps: 39, D loss: 0.246893, acc:  56%, G loss: 2.779700\n",
      "Ep: 35, steps: 40, D loss: 0.264527, acc:  51%, G loss: 2.587337\n",
      "Ep: 35, steps: 41, D loss: 0.187177, acc:  79%, G loss: 2.764186\n",
      "Ep: 35, steps: 42, D loss: 0.297333, acc:  36%, G loss: 2.718580\n",
      "Ep: 35, steps: 43, D loss: 0.295926, acc:  35%, G loss: 2.519067\n",
      "Ep: 35, steps: 44, D loss: 0.189300, acc:  82%, G loss: 2.382312\n",
      "Ep: 35, steps: 45, D loss: 0.196694, acc:  76%, G loss: 2.296474\n",
      "Ep: 35, steps: 46, D loss: 0.198178, acc:  72%, G loss: 2.459298\n",
      "Ep: 35, steps: 47, D loss: 0.298236, acc:  34%, G loss: 2.776212\n",
      "Ep: 35, steps: 48, D loss: 0.195286, acc:  77%, G loss: 2.508561\n",
      "Ep: 35, steps: 49, D loss: 0.186837, acc:  75%, G loss: 2.379298\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 36, steps: 1, D loss: 0.331594, acc:  27%, G loss: 2.526370\n",
      "Ep: 36, steps: 2, D loss: 0.291220, acc:  38%, G loss: 2.321389\n",
      "Ep: 36, steps: 3, D loss: 0.181857, acc:  75%, G loss: 2.686292\n",
      "Ep: 36, steps: 4, D loss: 0.233053, acc:  63%, G loss: 2.377521\n",
      "Ep: 36, steps: 5, D loss: 0.260706, acc:  53%, G loss: 2.542888\n",
      "Ep: 36, steps: 6, D loss: 0.298494, acc:  37%, G loss: 2.198674\n",
      "Ep: 36, steps: 7, D loss: 0.142886, acc:  86%, G loss: 2.606631\n",
      "Ep: 36, steps: 8, D loss: 0.194115, acc:  77%, G loss: 2.798260\n",
      "Ep: 36, steps: 9, D loss: 0.302000, acc:  31%, G loss: 2.354308\n",
      "Ep: 36, steps: 10, D loss: 0.256462, acc:  50%, G loss: 2.347993\n",
      "Ep: 36, steps: 11, D loss: 0.306478, acc:  31%, G loss: 2.620349\n",
      "Ep: 36, steps: 12, D loss: 0.235295, acc:  61%, G loss: 2.374667\n",
      "Ep: 36, steps: 13, D loss: 0.217854, acc:  67%, G loss: 2.200872\n",
      "Ep: 36, steps: 14, D loss: 0.195250, acc:  77%, G loss: 2.168988\n",
      "Ep: 36, steps: 15, D loss: 0.244553, acc:  58%, G loss: 2.763917\n",
      "Ep: 36, steps: 16, D loss: 0.258977, acc:  52%, G loss: 2.499575\n",
      "Ep: 36, steps: 17, D loss: 0.192088, acc:  78%, G loss: 2.666267\n",
      "Ep: 36, steps: 18, D loss: 0.273469, acc:  45%, G loss: 2.748646\n",
      "Ep: 36, steps: 19, D loss: 0.288791, acc:  36%, G loss: 2.301751\n",
      "Ep: 36, steps: 20, D loss: 0.187108, acc:  82%, G loss: 2.302657\n",
      "Ep: 36, steps: 21, D loss: 0.195704, acc:  76%, G loss: 2.509105\n",
      "Ep: 36, steps: 22, D loss: 0.191500, acc:  77%, G loss: 2.374501\n",
      "Ep: 36, steps: 23, D loss: 0.293445, acc:  35%, G loss: 2.616500\n",
      "Ep: 36, steps: 24, D loss: 0.195294, acc:  78%, G loss: 2.370100\n",
      "Ep: 36, steps: 25, D loss: 0.195487, acc:  70%, G loss: 2.230367\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 37, steps: 1, D loss: 0.336102, acc:  27%, G loss: 2.461834\n",
      "Ep: 37, steps: 2, D loss: 0.290254, acc:  39%, G loss: 2.327853\n",
      "Ep: 37, steps: 3, D loss: 0.190782, acc:  70%, G loss: 2.749799\n",
      "Ep: 37, steps: 4, D loss: 0.231470, acc:  65%, G loss: 2.229533\n",
      "Ep: 37, steps: 5, D loss: 0.252632, acc:  55%, G loss: 2.442513\n",
      "Ep: 37, steps: 6, D loss: 0.302306, acc:  36%, G loss: 2.108410\n",
      "Ep: 37, steps: 7, D loss: 0.149041, acc:  84%, G loss: 2.526500\n",
      "Ep: 37, steps: 8, D loss: 0.187343, acc:  77%, G loss: 2.860244\n",
      "Ep: 37, steps: 9, D loss: 0.303583, acc:  29%, G loss: 2.349474\n",
      "Ep: 37, steps: 10, D loss: 0.254067, acc:  51%, G loss: 2.182081\n",
      "Ep: 37, steps: 11, D loss: 0.307989, acc:  32%, G loss: 2.497837\n",
      "Ep: 37, steps: 12, D loss: 0.238904, acc:  59%, G loss: 2.262906\n",
      "Ep: 37, steps: 13, D loss: 0.219730, acc:  67%, G loss: 2.151510\n",
      "Ep: 37, steps: 14, D loss: 0.195868, acc:  78%, G loss: 2.182489\n",
      "Ep: 37, steps: 15, D loss: 0.241899, acc:  58%, G loss: 2.754416\n",
      "Ep: 37, steps: 16, D loss: 0.257053, acc:  53%, G loss: 2.453865\n",
      "Ep: 37, steps: 17, D loss: 0.197011, acc:  76%, G loss: 2.637786\n",
      "Ep: 37, steps: 18, D loss: 0.271067, acc:  47%, G loss: 2.720901\n",
      "Ep: 37, steps: 19, D loss: 0.283911, acc:  42%, G loss: 2.242174\n",
      "Ep: 37, steps: 20, D loss: 0.189436, acc:  81%, G loss: 2.286227\n",
      "Ep: 37, steps: 21, D loss: 0.195707, acc:  76%, G loss: 2.686173\n",
      "Ep: 37, steps: 22, D loss: 0.189632, acc:  77%, G loss: 2.302037\n",
      "Saved Model\n",
      "Ep: 37, steps: 23, D loss: 0.287204, acc:  36%, G loss: 2.537961\n",
      "Ep: 37, steps: 24, D loss: 0.202915, acc:  68%, G loss: 2.228917\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 38, steps: 1, D loss: 0.303302, acc:  36%, G loss: 2.247756\n",
      "Ep: 38, steps: 2, D loss: 0.296302, acc:  35%, G loss: 2.259136\n",
      "Ep: 38, steps: 3, D loss: 0.191667, acc:  72%, G loss: 2.626660\n",
      "Ep: 38, steps: 4, D loss: 0.236900, acc:  61%, G loss: 2.262581\n",
      "Ep: 38, steps: 5, D loss: 0.244712, acc:  56%, G loss: 2.381800\n",
      "Ep: 38, steps: 6, D loss: 0.289287, acc:  42%, G loss: 2.104894\n",
      "Ep: 38, steps: 7, D loss: 0.140621, acc:  86%, G loss: 2.494010\n",
      "Ep: 38, steps: 8, D loss: 0.180004, acc:  79%, G loss: 2.768653\n",
      "Ep: 38, steps: 9, D loss: 0.305409, acc:  28%, G loss: 2.269120\n",
      "Ep: 38, steps: 10, D loss: 0.250359, acc:  52%, G loss: 2.236759\n",
      "Ep: 38, steps: 11, D loss: 0.301826, acc:  34%, G loss: 2.470161\n",
      "Ep: 38, steps: 12, D loss: 0.245297, acc:  57%, G loss: 2.160459\n",
      "Ep: 38, steps: 13, D loss: 0.227939, acc:  63%, G loss: 2.239002\n",
      "Ep: 38, steps: 14, D loss: 0.202675, acc:  76%, G loss: 2.110512\n",
      "Ep: 38, steps: 15, D loss: 0.241723, acc:  58%, G loss: 2.750875\n",
      "Ep: 38, steps: 16, D loss: 0.263209, acc:  51%, G loss: 2.482661\n",
      "Ep: 38, steps: 17, D loss: 0.196495, acc:  77%, G loss: 2.615760\n",
      "Ep: 38, steps: 18, D loss: 0.271425, acc:  45%, G loss: 2.670555\n",
      "Ep: 38, steps: 19, D loss: 0.282953, acc:  43%, G loss: 2.250305\n",
      "Ep: 38, steps: 20, D loss: 0.193074, acc:  80%, G loss: 2.259515\n",
      "Ep: 38, steps: 21, D loss: 0.194554, acc:  78%, G loss: 2.743288\n",
      "Ep: 38, steps: 22, D loss: 0.197084, acc:  75%, G loss: 2.256362\n",
      "Ep: 38, steps: 23, D loss: 0.281670, acc:  41%, G loss: 2.536342\n",
      "Ep: 38, steps: 24, D loss: 0.193015, acc:  78%, G loss: 2.344428\n",
      "Ep: 38, steps: 25, D loss: 0.189619, acc:  73%, G loss: 2.289703\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 39, steps: 1, D loss: 0.319900, acc:  31%, G loss: 2.465560\n",
      "Ep: 39, steps: 2, D loss: 0.293001, acc:  37%, G loss: 2.305353\n",
      "Ep: 39, steps: 3, D loss: 0.182622, acc:  74%, G loss: 2.770872\n",
      "Ep: 39, steps: 4, D loss: 0.236478, acc:  62%, G loss: 2.292003\n",
      "Ep: 39, steps: 5, D loss: 0.253291, acc:  55%, G loss: 2.510075\n",
      "Ep: 39, steps: 6, D loss: 0.307940, acc:  35%, G loss: 2.413722\n",
      "Ep: 39, steps: 7, D loss: 0.150085, acc:  85%, G loss: 2.507049\n",
      "Ep: 39, steps: 8, D loss: 0.188052, acc:  77%, G loss: 2.642677\n",
      "Ep: 39, steps: 9, D loss: 0.305444, acc:  31%, G loss: 2.376638\n",
      "Ep: 39, steps: 10, D loss: 0.254312, acc:  52%, G loss: 2.081936\n",
      "Ep: 39, steps: 11, D loss: 0.301231, acc:  35%, G loss: 2.433568\n",
      "Ep: 39, steps: 12, D loss: 0.241644, acc:  57%, G loss: 2.120613\n",
      "Ep: 39, steps: 13, D loss: 0.225100, acc:  65%, G loss: 2.273628\n",
      "Ep: 39, steps: 14, D loss: 0.201992, acc:  77%, G loss: 2.100702\n",
      "Ep: 39, steps: 15, D loss: 0.238576, acc:  57%, G loss: 2.721626\n",
      "Ep: 39, steps: 16, D loss: 0.253338, acc:  53%, G loss: 2.420771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 39, steps: 17, D loss: 0.197480, acc:  76%, G loss: 2.605134\n",
      "Ep: 39, steps: 18, D loss: 0.264002, acc:  48%, G loss: 2.693067\n",
      "Ep: 39, steps: 19, D loss: 0.282283, acc:  42%, G loss: 2.232106\n",
      "Ep: 39, steps: 20, D loss: 0.189296, acc:  80%, G loss: 2.284206\n",
      "Ep: 39, steps: 21, D loss: 0.193128, acc:  76%, G loss: 2.216235\n",
      "Ep: 39, steps: 22, D loss: 0.195506, acc:  74%, G loss: 2.256083\n",
      "Ep: 39, steps: 23, D loss: 0.281645, acc:  41%, G loss: 2.502249\n",
      "Ep: 39, steps: 24, D loss: 0.189507, acc:  79%, G loss: 2.315819\n",
      "Ep: 39, steps: 25, D loss: 0.191858, acc:  72%, G loss: 2.263323\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 40, steps: 1, D loss: 0.323304, acc:  30%, G loss: 2.431982\n",
      "Ep: 40, steps: 2, D loss: 0.283565, acc:  40%, G loss: 2.271990\n",
      "Ep: 40, steps: 3, D loss: 0.180404, acc:  75%, G loss: 2.771045\n",
      "Ep: 40, steps: 4, D loss: 0.231804, acc:  65%, G loss: 2.339915\n",
      "Ep: 40, steps: 5, D loss: 0.257463, acc:  54%, G loss: 2.341086\n",
      "Ep: 40, steps: 6, D loss: 0.317824, acc:  31%, G loss: 2.145799\n",
      "Ep: 40, steps: 7, D loss: 0.145965, acc:  86%, G loss: 2.364281\n",
      "Ep: 40, steps: 8, D loss: 0.187864, acc:  76%, G loss: 2.996619\n",
      "Ep: 40, steps: 9, D loss: 0.309890, acc:  29%, G loss: 2.357787\n",
      "Ep: 40, steps: 10, D loss: 0.253500, acc:  51%, G loss: 2.153681\n",
      "Ep: 40, steps: 11, D loss: 0.312681, acc:  32%, G loss: 2.399215\n",
      "Ep: 40, steps: 12, D loss: 0.249530, acc:  52%, G loss: 2.137027\n",
      "Ep: 40, steps: 13, D loss: 0.232504, acc:  60%, G loss: 2.080267\n",
      "Ep: 40, steps: 14, D loss: 0.204253, acc:  76%, G loss: 2.126108\n",
      "Ep: 40, steps: 15, D loss: 0.238618, acc:  60%, G loss: 2.671633\n",
      "Ep: 40, steps: 16, D loss: 0.258494, acc:  52%, G loss: 2.386096\n",
      "Ep: 40, steps: 17, D loss: 0.194712, acc:  78%, G loss: 2.574678\n",
      "Ep: 40, steps: 18, D loss: 0.270968, acc:  46%, G loss: 2.602188\n",
      "Ep: 40, steps: 19, D loss: 0.279882, acc:  45%, G loss: 2.226354\n",
      "Ep: 40, steps: 20, D loss: 0.187698, acc:  81%, G loss: 2.254920\n",
      "Saved Model\n",
      "Ep: 40, steps: 21, D loss: 0.195115, acc:  75%, G loss: 2.420720\n",
      "Ep: 40, steps: 22, D loss: 0.250081, acc:  54%, G loss: 2.738897\n",
      "Ep: 40, steps: 23, D loss: 0.185879, acc:  82%, G loss: 2.277447\n",
      "Ep: 40, steps: 24, D loss: 0.191015, acc:  74%, G loss: 2.426628\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 41, steps: 1, D loss: 0.310240, acc:  35%, G loss: 2.390050\n",
      "Ep: 41, steps: 2, D loss: 0.285537, acc:  42%, G loss: 2.279214\n",
      "Ep: 41, steps: 3, D loss: 0.173518, acc:  79%, G loss: 2.879030\n",
      "Ep: 41, steps: 4, D loss: 0.223306, acc:  69%, G loss: 2.219924\n",
      "Ep: 41, steps: 5, D loss: 0.250884, acc:  56%, G loss: 2.401973\n",
      "Ep: 41, steps: 6, D loss: 0.294366, acc:  38%, G loss: 2.238569\n",
      "Ep: 41, steps: 7, D loss: 0.144348, acc:  85%, G loss: 2.407604\n",
      "Ep: 41, steps: 8, D loss: 0.185908, acc:  76%, G loss: 2.784947\n",
      "Ep: 41, steps: 9, D loss: 0.314145, acc:  30%, G loss: 2.306935\n",
      "Ep: 41, steps: 10, D loss: 0.260266, acc:  48%, G loss: 2.233235\n",
      "Ep: 41, steps: 11, D loss: 0.298265, acc:  37%, G loss: 2.439632\n",
      "Ep: 41, steps: 12, D loss: 0.247715, acc:  52%, G loss: 2.066526\n",
      "Ep: 41, steps: 13, D loss: 0.226887, acc:  64%, G loss: 2.169871\n",
      "Ep: 41, steps: 14, D loss: 0.204162, acc:  73%, G loss: 2.123825\n",
      "Ep: 41, steps: 15, D loss: 0.239929, acc:  59%, G loss: 2.676050\n",
      "Ep: 41, steps: 16, D loss: 0.255317, acc:  53%, G loss: 2.400513\n",
      "Ep: 41, steps: 17, D loss: 0.194336, acc:  77%, G loss: 2.571513\n",
      "Ep: 41, steps: 18, D loss: 0.269670, acc:  47%, G loss: 2.616942\n",
      "Ep: 41, steps: 19, D loss: 0.273787, acc:  48%, G loss: 2.207983\n",
      "Ep: 41, steps: 20, D loss: 0.186177, acc:  82%, G loss: 2.288692\n",
      "Ep: 41, steps: 21, D loss: 0.199131, acc:  75%, G loss: 2.176241\n",
      "Ep: 41, steps: 22, D loss: 0.196598, acc:  77%, G loss: 2.156312\n",
      "Ep: 41, steps: 23, D loss: 0.277617, acc:  40%, G loss: 2.509422\n",
      "Ep: 41, steps: 24, D loss: 0.190902, acc:  76%, G loss: 2.414120\n",
      "Ep: 41, steps: 25, D loss: 0.187861, acc:  75%, G loss: 2.221283\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 42, steps: 1, D loss: 0.315337, acc:  31%, G loss: 2.245512\n",
      "Ep: 42, steps: 2, D loss: 0.279104, acc:  42%, G loss: 2.277794\n",
      "Ep: 42, steps: 3, D loss: 0.172464, acc:  77%, G loss: 2.933830\n",
      "Ep: 42, steps: 4, D loss: 0.226224, acc:  67%, G loss: 2.152344\n",
      "Ep: 42, steps: 5, D loss: 0.247780, acc:  55%, G loss: 2.436506\n",
      "Ep: 42, steps: 6, D loss: 0.295165, acc:  40%, G loss: 2.425849\n",
      "Ep: 42, steps: 7, D loss: 0.142596, acc:  85%, G loss: 2.536144\n",
      "Ep: 42, steps: 8, D loss: 0.183723, acc:  76%, G loss: 2.689928\n",
      "Ep: 42, steps: 9, D loss: 0.306058, acc:  33%, G loss: 2.357193\n",
      "Ep: 42, steps: 10, D loss: 0.248401, acc:  52%, G loss: 2.217016\n",
      "Ep: 42, steps: 11, D loss: 0.304957, acc:  35%, G loss: 2.582985\n",
      "Ep: 42, steps: 12, D loss: 0.251225, acc:  53%, G loss: 2.195289\n",
      "Ep: 42, steps: 13, D loss: 0.231230, acc:  62%, G loss: 2.188774\n",
      "Ep: 42, steps: 14, D loss: 0.206670, acc:  71%, G loss: 2.117295\n",
      "Ep: 42, steps: 15, D loss: 0.240356, acc:  60%, G loss: 2.704670\n",
      "Ep: 42, steps: 16, D loss: 0.256735, acc:  53%, G loss: 2.623214\n",
      "Ep: 42, steps: 17, D loss: 0.191338, acc:  78%, G loss: 2.648358\n",
      "Ep: 42, steps: 18, D loss: 0.273097, acc:  45%, G loss: 2.637613\n",
      "Ep: 42, steps: 19, D loss: 0.270204, acc:  49%, G loss: 2.271477\n",
      "Ep: 42, steps: 20, D loss: 0.186317, acc:  83%, G loss: 2.273734\n",
      "Ep: 42, steps: 21, D loss: 0.198637, acc:  75%, G loss: 2.570657\n",
      "Ep: 42, steps: 22, D loss: 0.195395, acc:  77%, G loss: 2.308461\n",
      "Ep: 42, steps: 23, D loss: 0.273062, acc:  45%, G loss: 2.543872\n",
      "Ep: 42, steps: 24, D loss: 0.186624, acc:  78%, G loss: 2.301647\n",
      "Ep: 42, steps: 25, D loss: 0.186844, acc:  75%, G loss: 2.386199\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 43, steps: 1, D loss: 0.310781, acc:  35%, G loss: 2.394129\n",
      "Ep: 43, steps: 2, D loss: 0.281017, acc:  44%, G loss: 2.260491\n",
      "Ep: 43, steps: 3, D loss: 0.164410, acc:  80%, G loss: 2.887583\n",
      "Ep: 43, steps: 4, D loss: 0.219809, acc:  68%, G loss: 2.353473\n",
      "Ep: 43, steps: 5, D loss: 0.252261, acc:  54%, G loss: 2.355261\n",
      "Ep: 43, steps: 6, D loss: 0.310390, acc:  36%, G loss: 2.248727\n",
      "Ep: 43, steps: 7, D loss: 0.143211, acc:  85%, G loss: 2.395167\n",
      "Ep: 43, steps: 8, D loss: 0.183536, acc:  76%, G loss: 2.937161\n",
      "Ep: 43, steps: 9, D loss: 0.323828, acc:  27%, G loss: 2.306152\n",
      "Ep: 43, steps: 10, D loss: 0.258408, acc:  49%, G loss: 2.232847\n",
      "Ep: 43, steps: 11, D loss: 0.303240, acc:  35%, G loss: 2.428325\n",
      "Ep: 43, steps: 12, D loss: 0.256782, acc:  49%, G loss: 2.082044\n",
      "Ep: 43, steps: 13, D loss: 0.233556, acc:  60%, G loss: 2.146925\n",
      "Ep: 43, steps: 14, D loss: 0.209577, acc:  73%, G loss: 2.118948\n",
      "Ep: 43, steps: 15, D loss: 0.235594, acc:  60%, G loss: 2.678506\n",
      "Ep: 43, steps: 16, D loss: 0.254112, acc:  55%, G loss: 2.376085\n",
      "Ep: 43, steps: 17, D loss: 0.193165, acc:  78%, G loss: 2.559360\n",
      "Saved Model\n",
      "Ep: 43, steps: 18, D loss: 0.269994, acc:  48%, G loss: 2.593479\n",
      "Ep: 43, steps: 19, D loss: 0.182856, acc:  80%, G loss: 2.637515\n",
      "Ep: 43, steps: 20, D loss: 0.193761, acc:  75%, G loss: 2.350970\n",
      "Ep: 43, steps: 21, D loss: 0.205103, acc:  73%, G loss: 2.172790\n",
      "Ep: 43, steps: 22, D loss: 0.269488, acc:  47%, G loss: 2.510331\n",
      "Ep: 43, steps: 23, D loss: 0.180655, acc:  81%, G loss: 2.327101\n",
      "Ep: 43, steps: 24, D loss: 0.188283, acc:  73%, G loss: 2.161266\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 44, steps: 1, D loss: 0.309639, acc:  33%, G loss: 2.284995\n",
      "Ep: 44, steps: 2, D loss: 0.281418, acc:  43%, G loss: 2.162939\n",
      "Ep: 44, steps: 3, D loss: 0.166696, acc:  84%, G loss: 2.916410\n",
      "Ep: 44, steps: 4, D loss: 0.228852, acc:  64%, G loss: 2.362347\n",
      "Ep: 44, steps: 5, D loss: 0.241786, acc:  57%, G loss: 2.376992\n",
      "Ep: 44, steps: 6, D loss: 0.317312, acc:  37%, G loss: 2.174506\n",
      "Ep: 44, steps: 7, D loss: 0.155696, acc:  83%, G loss: 2.426715\n",
      "Ep: 44, steps: 8, D loss: 0.177426, acc:  77%, G loss: 3.000101\n",
      "Ep: 44, steps: 9, D loss: 0.312146, acc:  30%, G loss: 2.280462\n",
      "Ep: 44, steps: 10, D loss: 0.239836, acc:  56%, G loss: 2.198995\n",
      "Ep: 44, steps: 11, D loss: 0.303847, acc:  34%, G loss: 2.399045\n",
      "Ep: 44, steps: 12, D loss: 0.258238, acc:  50%, G loss: 2.107106\n",
      "Ep: 44, steps: 13, D loss: 0.237145, acc:  59%, G loss: 2.195273\n",
      "Ep: 44, steps: 14, D loss: 0.207178, acc:  72%, G loss: 2.091474\n",
      "Ep: 44, steps: 15, D loss: 0.239736, acc:  60%, G loss: 2.730880\n",
      "Ep: 44, steps: 16, D loss: 0.259598, acc:  53%, G loss: 2.504657\n",
      "Ep: 44, steps: 17, D loss: 0.189432, acc:  79%, G loss: 2.591267\n",
      "Ep: 44, steps: 18, D loss: 0.274875, acc:  45%, G loss: 2.586789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 44, steps: 19, D loss: 0.271478, acc:  50%, G loss: 2.207575\n",
      "Ep: 44, steps: 20, D loss: 0.176363, acc:  84%, G loss: 2.267921\n",
      "Ep: 44, steps: 21, D loss: 0.196609, acc:  76%, G loss: 2.344778\n",
      "Ep: 44, steps: 22, D loss: 0.191576, acc:  78%, G loss: 2.303041\n",
      "Ep: 44, steps: 23, D loss: 0.269480, acc:  47%, G loss: 2.486507\n",
      "Ep: 44, steps: 24, D loss: 0.182939, acc:  79%, G loss: 2.436692\n",
      "Ep: 44, steps: 25, D loss: 0.182674, acc:  76%, G loss: 2.354830\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 45, steps: 1, D loss: 0.307397, acc:  37%, G loss: 2.258851\n",
      "Ep: 45, steps: 2, D loss: 0.282836, acc:  42%, G loss: 2.260422\n",
      "Ep: 45, steps: 3, D loss: 0.161459, acc:  81%, G loss: 2.906887\n",
      "Ep: 45, steps: 4, D loss: 0.216201, acc:  70%, G loss: 2.259923\n",
      "Ep: 45, steps: 5, D loss: 0.263644, acc:  51%, G loss: 2.393296\n",
      "Ep: 45, steps: 6, D loss: 0.313977, acc:  36%, G loss: 2.242592\n",
      "Ep: 45, steps: 7, D loss: 0.150814, acc:  84%, G loss: 2.423623\n",
      "Ep: 45, steps: 8, D loss: 0.191360, acc:  75%, G loss: 2.922675\n",
      "Ep: 45, steps: 9, D loss: 0.310963, acc:  30%, G loss: 2.261885\n",
      "Ep: 45, steps: 10, D loss: 0.254221, acc:  50%, G loss: 2.247222\n",
      "Ep: 45, steps: 11, D loss: 0.302774, acc:  37%, G loss: 2.391116\n",
      "Ep: 45, steps: 12, D loss: 0.263630, acc:  47%, G loss: 2.055927\n",
      "Ep: 45, steps: 13, D loss: 0.235425, acc:  59%, G loss: 2.222430\n",
      "Ep: 45, steps: 14, D loss: 0.210378, acc:  74%, G loss: 2.102148\n",
      "Ep: 45, steps: 15, D loss: 0.240671, acc:  57%, G loss: 2.686424\n",
      "Ep: 45, steps: 16, D loss: 0.256186, acc:  53%, G loss: 2.466798\n",
      "Ep: 45, steps: 17, D loss: 0.189927, acc:  78%, G loss: 2.555000\n",
      "Ep: 45, steps: 18, D loss: 0.271346, acc:  45%, G loss: 2.590791\n",
      "Ep: 45, steps: 19, D loss: 0.273372, acc:  50%, G loss: 2.167710\n",
      "Ep: 45, steps: 20, D loss: 0.178396, acc:  84%, G loss: 2.254973\n",
      "Ep: 45, steps: 21, D loss: 0.199859, acc:  74%, G loss: 2.417337\n",
      "Ep: 45, steps: 22, D loss: 0.192839, acc:  77%, G loss: 2.259637\n",
      "Ep: 45, steps: 23, D loss: 0.266497, acc:  49%, G loss: 2.494430\n",
      "Ep: 45, steps: 24, D loss: 0.185654, acc:  78%, G loss: 2.425447\n",
      "Ep: 45, steps: 25, D loss: 0.182425, acc:  77%, G loss: 2.354865\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 46, steps: 1, D loss: 0.302679, acc:  38%, G loss: 2.344147\n",
      "Ep: 46, steps: 2, D loss: 0.286001, acc:  41%, G loss: 2.213146\n",
      "Ep: 46, steps: 3, D loss: 0.160804, acc:  82%, G loss: 2.908220\n",
      "Ep: 46, steps: 4, D loss: 0.212141, acc:  72%, G loss: 2.279215\n",
      "Ep: 46, steps: 5, D loss: 0.252493, acc:  53%, G loss: 2.375440\n",
      "Ep: 46, steps: 6, D loss: 0.304308, acc:  40%, G loss: 2.196927\n",
      "Ep: 46, steps: 7, D loss: 0.160130, acc:  82%, G loss: 2.402322\n",
      "Ep: 46, steps: 8, D loss: 0.187950, acc:  76%, G loss: 2.866597\n",
      "Ep: 46, steps: 9, D loss: 0.315858, acc:  31%, G loss: 2.249304\n",
      "Ep: 46, steps: 10, D loss: 0.241341, acc:  55%, G loss: 2.245385\n",
      "Ep: 46, steps: 11, D loss: 0.299555, acc:  36%, G loss: 2.420198\n",
      "Ep: 46, steps: 12, D loss: 0.264831, acc:  47%, G loss: 2.012769\n",
      "Ep: 46, steps: 13, D loss: 0.239205, acc:  58%, G loss: 2.208519\n",
      "Ep: 46, steps: 14, D loss: 0.214052, acc:  68%, G loss: 2.071016\n",
      "Ep: 46, steps: 15, D loss: 0.235597, acc:  61%, G loss: 2.710557\n",
      "Saved Model\n",
      "Ep: 46, steps: 16, D loss: 0.252939, acc:  55%, G loss: 2.497800\n",
      "Ep: 46, steps: 17, D loss: 0.263582, acc:  50%, G loss: 2.564863\n",
      "Ep: 46, steps: 18, D loss: 0.266664, acc:  51%, G loss: 2.112932\n",
      "Ep: 46, steps: 19, D loss: 0.169757, acc:  90%, G loss: 2.307090\n",
      "Ep: 46, steps: 20, D loss: 0.192659, acc:  77%, G loss: 2.010222\n",
      "Ep: 46, steps: 21, D loss: 0.197490, acc:  75%, G loss: 2.272480\n",
      "Ep: 46, steps: 22, D loss: 0.269752, acc:  48%, G loss: 2.506569\n",
      "Ep: 46, steps: 23, D loss: 0.186964, acc:  77%, G loss: 2.312972\n",
      "Ep: 46, steps: 24, D loss: 0.185424, acc:  78%, G loss: 2.280468\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 47, steps: 1, D loss: 0.293270, acc:  39%, G loss: 2.246472\n",
      "Ep: 47, steps: 2, D loss: 0.275824, acc:  48%, G loss: 2.224916\n",
      "Ep: 47, steps: 3, D loss: 0.152836, acc:  85%, G loss: 3.054985\n",
      "Ep: 47, steps: 4, D loss: 0.214831, acc:  72%, G loss: 2.177220\n",
      "Ep: 47, steps: 5, D loss: 0.253225, acc:  53%, G loss: 2.339869\n",
      "Ep: 47, steps: 6, D loss: 0.307474, acc:  41%, G loss: 2.197974\n",
      "Ep: 47, steps: 7, D loss: 0.159724, acc:  82%, G loss: 2.275660\n",
      "Ep: 47, steps: 8, D loss: 0.191670, acc:  75%, G loss: 2.951481\n",
      "Ep: 47, steps: 9, D loss: 0.314779, acc:  31%, G loss: 2.318171\n",
      "Ep: 47, steps: 10, D loss: 0.231605, acc:  58%, G loss: 2.310063\n",
      "Ep: 47, steps: 11, D loss: 0.301999, acc:  35%, G loss: 2.441250\n",
      "Ep: 47, steps: 12, D loss: 0.267034, acc:  47%, G loss: 2.132721\n",
      "Ep: 47, steps: 13, D loss: 0.231231, acc:  61%, G loss: 2.087160\n",
      "Ep: 47, steps: 14, D loss: 0.216319, acc:  72%, G loss: 2.096984\n",
      "Ep: 47, steps: 15, D loss: 0.235073, acc:  59%, G loss: 2.565839\n",
      "Ep: 47, steps: 16, D loss: 0.252000, acc:  54%, G loss: 2.703320\n",
      "Ep: 47, steps: 17, D loss: 0.187122, acc:  79%, G loss: 2.628071\n",
      "Ep: 47, steps: 18, D loss: 0.269752, acc:  46%, G loss: 2.516858\n",
      "Ep: 47, steps: 19, D loss: 0.267838, acc:  52%, G loss: 2.165577\n",
      "Ep: 47, steps: 20, D loss: 0.167205, acc:  87%, G loss: 2.270401\n",
      "Ep: 47, steps: 21, D loss: 0.203789, acc:  71%, G loss: 2.767045\n",
      "Ep: 47, steps: 22, D loss: 0.197686, acc:  74%, G loss: 2.281297\n",
      "Ep: 47, steps: 23, D loss: 0.256044, acc:  53%, G loss: 2.555446\n",
      "Ep: 47, steps: 24, D loss: 0.177819, acc:  80%, G loss: 2.332449\n",
      "Ep: 47, steps: 25, D loss: 0.181500, acc:  77%, G loss: 2.240915\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 48, steps: 1, D loss: 0.299822, acc:  40%, G loss: 2.352659\n",
      "Ep: 48, steps: 2, D loss: 0.281389, acc:  47%, G loss: 2.261349\n",
      "Ep: 48, steps: 3, D loss: 0.146235, acc:  86%, G loss: 2.906185\n",
      "Ep: 48, steps: 4, D loss: 0.207172, acc:  73%, G loss: 2.331652\n",
      "Ep: 48, steps: 5, D loss: 0.266871, acc:  50%, G loss: 2.360394\n",
      "Ep: 48, steps: 6, D loss: 0.323266, acc:  35%, G loss: 2.161596\n",
      "Ep: 48, steps: 7, D loss: 0.176948, acc:  79%, G loss: 2.358551\n",
      "Ep: 48, steps: 8, D loss: 0.194125, acc:  74%, G loss: 2.883251\n",
      "Ep: 48, steps: 9, D loss: 0.303122, acc:  34%, G loss: 2.235661\n",
      "Ep: 48, steps: 10, D loss: 0.237198, acc:  56%, G loss: 2.275593\n",
      "Ep: 48, steps: 11, D loss: 0.301495, acc:  39%, G loss: 2.447688\n",
      "Ep: 48, steps: 12, D loss: 0.259818, acc:  50%, G loss: 2.049453\n",
      "Ep: 48, steps: 13, D loss: 0.236486, acc:  61%, G loss: 2.240178\n",
      "Ep: 48, steps: 14, D loss: 0.215511, acc:  65%, G loss: 2.068865\n",
      "Ep: 48, steps: 15, D loss: 0.232975, acc:  61%, G loss: 2.738829\n",
      "Ep: 48, steps: 16, D loss: 0.255943, acc:  52%, G loss: 2.493233\n",
      "Ep: 48, steps: 17, D loss: 0.181272, acc:  80%, G loss: 2.645511\n",
      "Ep: 48, steps: 18, D loss: 0.267039, acc:  48%, G loss: 2.560599\n",
      "Ep: 48, steps: 19, D loss: 0.270023, acc:  51%, G loss: 2.159000\n",
      "Ep: 48, steps: 20, D loss: 0.167706, acc:  87%, G loss: 2.303027\n",
      "Ep: 48, steps: 21, D loss: 0.199677, acc:  73%, G loss: 2.238879\n",
      "Ep: 48, steps: 22, D loss: 0.189198, acc:  78%, G loss: 2.307542\n",
      "Ep: 48, steps: 23, D loss: 0.265802, acc:  46%, G loss: 2.468196\n",
      "Ep: 48, steps: 24, D loss: 0.172641, acc:  79%, G loss: 2.414816\n",
      "Ep: 48, steps: 25, D loss: 0.177270, acc:  80%, G loss: 2.397610\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 49, steps: 1, D loss: 0.301060, acc:  38%, G loss: 2.264313\n",
      "Ep: 49, steps: 2, D loss: 0.287666, acc:  42%, G loss: 2.192193\n",
      "Ep: 49, steps: 3, D loss: 0.154031, acc:  81%, G loss: 2.942545\n",
      "Ep: 49, steps: 4, D loss: 0.210877, acc:  71%, G loss: 2.314914\n",
      "Ep: 49, steps: 5, D loss: 0.247438, acc:  55%, G loss: 2.323467\n",
      "Ep: 49, steps: 6, D loss: 0.329737, acc:  38%, G loss: 2.214747\n",
      "Ep: 49, steps: 7, D loss: 0.189940, acc:  75%, G loss: 2.207901\n",
      "Ep: 49, steps: 8, D loss: 0.188008, acc:  75%, G loss: 2.822241\n",
      "Ep: 49, steps: 9, D loss: 0.293893, acc:  38%, G loss: 2.200735\n",
      "Ep: 49, steps: 10, D loss: 0.221436, acc:  60%, G loss: 2.245942\n",
      "Ep: 49, steps: 11, D loss: 0.311214, acc:  34%, G loss: 2.393821\n",
      "Ep: 49, steps: 12, D loss: 0.275789, acc:  46%, G loss: 1.974120\n",
      "Ep: 49, steps: 13, D loss: 0.241944, acc:  59%, G loss: 2.212926\n",
      "Saved Model\n",
      "Ep: 49, steps: 14, D loss: 0.219073, acc:  64%, G loss: 2.063663\n",
      "Ep: 49, steps: 15, D loss: 0.259650, acc:  54%, G loss: 2.218076\n",
      "Ep: 49, steps: 16, D loss: 0.184681, acc:  79%, G loss: 2.674527\n",
      "Ep: 49, steps: 17, D loss: 0.257417, acc:  50%, G loss: 2.682471\n",
      "Ep: 49, steps: 18, D loss: 0.265362, acc:  54%, G loss: 2.212783\n",
      "Ep: 49, steps: 19, D loss: 0.165389, acc:  87%, G loss: 2.374488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 49, steps: 20, D loss: 0.197712, acc:  75%, G loss: 2.395650\n",
      "Ep: 49, steps: 21, D loss: 0.198455, acc:  75%, G loss: 2.286800\n",
      "Ep: 49, steps: 22, D loss: 0.270315, acc:  45%, G loss: 2.443058\n",
      "Ep: 49, steps: 23, D loss: 0.185915, acc:  77%, G loss: 2.419004\n",
      "Ep: 49, steps: 24, D loss: 0.180870, acc:  79%, G loss: 2.379951\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 50, steps: 1, D loss: 0.291047, acc:  41%, G loss: 2.472332\n",
      "Ep: 50, steps: 2, D loss: 0.279223, acc:  47%, G loss: 2.242753\n",
      "Ep: 50, steps: 3, D loss: 0.140245, acc:  86%, G loss: 2.990685\n",
      "Ep: 50, steps: 4, D loss: 0.204170, acc:  73%, G loss: 2.259535\n",
      "Ep: 50, steps: 5, D loss: 0.257598, acc:  51%, G loss: 2.393207\n",
      "Ep: 50, steps: 6, D loss: 0.332110, acc:  37%, G loss: 2.365076\n",
      "Ep: 50, steps: 7, D loss: 0.192804, acc:  75%, G loss: 2.210675\n",
      "Ep: 50, steps: 8, D loss: 0.195685, acc:  75%, G loss: 2.818008\n",
      "Ep: 50, steps: 9, D loss: 0.309799, acc:  33%, G loss: 2.196151\n",
      "Ep: 50, steps: 10, D loss: 0.231644, acc:  60%, G loss: 2.355049\n",
      "Ep: 50, steps: 11, D loss: 0.296983, acc:  38%, G loss: 2.471010\n",
      "Ep: 50, steps: 12, D loss: 0.266783, acc:  47%, G loss: 2.015044\n",
      "Ep: 50, steps: 13, D loss: 0.240947, acc:  57%, G loss: 2.206700\n",
      "Ep: 50, steps: 14, D loss: 0.214771, acc:  67%, G loss: 1.988602\n",
      "Ep: 50, steps: 15, D loss: 0.238386, acc:  60%, G loss: 2.713210\n",
      "Ep: 50, steps: 16, D loss: 0.254227, acc:  54%, G loss: 2.482236\n",
      "Ep: 50, steps: 17, D loss: 0.180355, acc:  80%, G loss: 2.552367\n",
      "Ep: 50, steps: 18, D loss: 0.264710, acc:  49%, G loss: 2.475323\n",
      "Ep: 50, steps: 19, D loss: 0.261735, acc:  53%, G loss: 2.105460\n",
      "Ep: 50, steps: 20, D loss: 0.157207, acc:  90%, G loss: 2.308182\n",
      "Ep: 50, steps: 21, D loss: 0.211711, acc:  70%, G loss: 2.470578\n",
      "Ep: 50, steps: 22, D loss: 0.187532, acc:  79%, G loss: 2.310943\n",
      "Ep: 50, steps: 23, D loss: 0.259051, acc:  50%, G loss: 2.499569\n",
      "Ep: 50, steps: 24, D loss: 0.171582, acc:  80%, G loss: 2.321181\n",
      "Ep: 50, steps: 25, D loss: 0.174895, acc:  79%, G loss: 2.315019\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 51, steps: 1, D loss: 0.296645, acc:  42%, G loss: 2.328708\n",
      "Ep: 51, steps: 2, D loss: 0.285081, acc:  46%, G loss: 2.227078\n",
      "Ep: 51, steps: 3, D loss: 0.151540, acc:  81%, G loss: 3.072721\n",
      "Ep: 51, steps: 4, D loss: 0.211750, acc:  70%, G loss: 2.354761\n",
      "Ep: 51, steps: 5, D loss: 0.261504, acc:  51%, G loss: 2.442854\n",
      "Ep: 51, steps: 6, D loss: 0.333851, acc:  38%, G loss: 2.329422\n",
      "Ep: 51, steps: 7, D loss: 0.217133, acc:  67%, G loss: 2.302799\n",
      "Ep: 51, steps: 8, D loss: 0.206760, acc:  70%, G loss: 2.889692\n",
      "Ep: 51, steps: 9, D loss: 0.303708, acc:  37%, G loss: 2.306496\n",
      "Ep: 51, steps: 10, D loss: 0.215770, acc:  67%, G loss: 2.414879\n",
      "Ep: 51, steps: 11, D loss: 0.288268, acc:  42%, G loss: 2.419951\n",
      "Ep: 51, steps: 12, D loss: 0.273800, acc:  42%, G loss: 1.964922\n",
      "Ep: 51, steps: 13, D loss: 0.244963, acc:  56%, G loss: 2.113068\n",
      "Ep: 51, steps: 14, D loss: 0.219188, acc:  63%, G loss: 2.073084\n",
      "Ep: 51, steps: 15, D loss: 0.235466, acc:  59%, G loss: 2.662634\n",
      "Ep: 51, steps: 16, D loss: 0.246582, acc:  57%, G loss: 2.429883\n",
      "Ep: 51, steps: 17, D loss: 0.177564, acc:  80%, G loss: 2.524813\n",
      "Ep: 51, steps: 18, D loss: 0.257504, acc:  51%, G loss: 2.655248\n",
      "Ep: 51, steps: 19, D loss: 0.267367, acc:  51%, G loss: 2.142989\n",
      "Ep: 51, steps: 20, D loss: 0.150743, acc:  91%, G loss: 2.315122\n",
      "Ep: 51, steps: 21, D loss: 0.208814, acc:  72%, G loss: 2.137376\n",
      "Ep: 51, steps: 22, D loss: 0.181747, acc:  81%, G loss: 2.281180\n",
      "Ep: 51, steps: 23, D loss: 0.264759, acc:  49%, G loss: 2.408441\n",
      "Ep: 51, steps: 24, D loss: 0.173902, acc:  77%, G loss: 2.402451\n",
      "Ep: 51, steps: 25, D loss: 0.182556, acc:  77%, G loss: 2.151000\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 52, steps: 1, D loss: 0.296703, acc:  39%, G loss: 2.388977\n",
      "Ep: 52, steps: 2, D loss: 0.262228, acc:  54%, G loss: 2.242375\n",
      "Ep: 52, steps: 3, D loss: 0.133136, acc:  88%, G loss: 2.980153\n",
      "Ep: 52, steps: 4, D loss: 0.200447, acc:  76%, G loss: 2.208365\n",
      "Ep: 52, steps: 5, D loss: 0.245358, acc:  55%, G loss: 2.266809\n",
      "Ep: 52, steps: 6, D loss: 0.311307, acc:  43%, G loss: 2.238459\n",
      "Ep: 52, steps: 7, D loss: 0.200526, acc:  72%, G loss: 2.350813\n",
      "Ep: 52, steps: 8, D loss: 0.187715, acc:  75%, G loss: 2.961519\n",
      "Ep: 52, steps: 9, D loss: 0.307772, acc:  35%, G loss: 2.205414\n",
      "Ep: 52, steps: 10, D loss: 0.219181, acc:  62%, G loss: 2.249078\n",
      "Ep: 52, steps: 11, D loss: 0.301066, acc:  38%, G loss: 2.389089\n",
      "Saved Model\n",
      "Ep: 52, steps: 12, D loss: 0.275287, acc:  46%, G loss: 1.975480\n",
      "Ep: 52, steps: 13, D loss: 0.224929, acc:  64%, G loss: 2.083536\n",
      "Ep: 52, steps: 14, D loss: 0.229524, acc:  62%, G loss: 2.738117\n",
      "Ep: 52, steps: 15, D loss: 0.252579, acc:  57%, G loss: 2.295535\n",
      "Ep: 52, steps: 16, D loss: 0.170663, acc:  82%, G loss: 2.502410\n",
      "Ep: 52, steps: 17, D loss: 0.277607, acc:  45%, G loss: 2.414470\n",
      "Ep: 52, steps: 18, D loss: 0.257053, acc:  55%, G loss: 2.124789\n",
      "Ep: 52, steps: 19, D loss: 0.153181, acc:  89%, G loss: 2.329909\n",
      "Ep: 52, steps: 20, D loss: 0.225751, acc:  65%, G loss: 2.289707\n",
      "Ep: 52, steps: 21, D loss: 0.183078, acc:  79%, G loss: 2.312996\n",
      "Ep: 52, steps: 22, D loss: 0.256084, acc:  53%, G loss: 2.532148\n",
      "Ep: 52, steps: 23, D loss: 0.169169, acc:  79%, G loss: 2.470825\n",
      "Ep: 52, steps: 24, D loss: 0.165521, acc:  82%, G loss: 2.404365\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 53, steps: 1, D loss: 0.292464, acc:  44%, G loss: 2.410566\n",
      "Ep: 53, steps: 2, D loss: 0.277304, acc:  49%, G loss: 2.189264\n",
      "Ep: 53, steps: 3, D loss: 0.131696, acc:  86%, G loss: 3.146789\n",
      "Ep: 53, steps: 4, D loss: 0.200935, acc:  74%, G loss: 2.298637\n",
      "Ep: 53, steps: 5, D loss: 0.287553, acc:  50%, G loss: 2.405202\n",
      "Ep: 53, steps: 6, D loss: 0.340494, acc:  30%, G loss: 2.257310\n",
      "Ep: 53, steps: 7, D loss: 0.198350, acc:  72%, G loss: 2.293762\n",
      "Ep: 53, steps: 8, D loss: 0.207808, acc:  72%, G loss: 2.864539\n",
      "Ep: 53, steps: 9, D loss: 0.296782, acc:  39%, G loss: 2.274098\n",
      "Ep: 53, steps: 10, D loss: 0.228538, acc:  64%, G loss: 2.305754\n",
      "Ep: 53, steps: 11, D loss: 0.290238, acc:  43%, G loss: 2.453629\n",
      "Ep: 53, steps: 12, D loss: 0.279240, acc:  40%, G loss: 1.978519\n",
      "Ep: 53, steps: 13, D loss: 0.248529, acc:  54%, G loss: 1.971073\n",
      "Ep: 53, steps: 14, D loss: 0.221591, acc:  62%, G loss: 2.059529\n",
      "Ep: 53, steps: 15, D loss: 0.238423, acc:  59%, G loss: 2.536980\n",
      "Ep: 53, steps: 16, D loss: 0.245937, acc:  55%, G loss: 2.406004\n",
      "Ep: 53, steps: 17, D loss: 0.188796, acc:  76%, G loss: 2.491623\n",
      "Ep: 53, steps: 18, D loss: 0.256024, acc:  53%, G loss: 2.455867\n",
      "Ep: 53, steps: 19, D loss: 0.269743, acc:  52%, G loss: 2.112554\n",
      "Ep: 53, steps: 20, D loss: 0.153279, acc:  91%, G loss: 2.295835\n",
      "Ep: 53, steps: 21, D loss: 0.209988, acc:  71%, G loss: 2.388562\n",
      "Ep: 53, steps: 22, D loss: 0.182206, acc:  83%, G loss: 2.288610\n",
      "Ep: 53, steps: 23, D loss: 0.248106, acc:  57%, G loss: 2.539658\n",
      "Ep: 53, steps: 24, D loss: 0.169273, acc:  79%, G loss: 2.294039\n",
      "Ep: 53, steps: 25, D loss: 0.176769, acc:  81%, G loss: 2.222033\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 54, steps: 1, D loss: 0.294846, acc:  41%, G loss: 2.192420\n",
      "Ep: 54, steps: 2, D loss: 0.261035, acc:  55%, G loss: 2.242891\n",
      "Ep: 54, steps: 3, D loss: 0.134093, acc:  88%, G loss: 3.063342\n",
      "Ep: 54, steps: 4, D loss: 0.193942, acc:  76%, G loss: 2.332139\n",
      "Ep: 54, steps: 5, D loss: 0.239357, acc:  58%, G loss: 2.299994\n",
      "Ep: 54, steps: 6, D loss: 0.302613, acc:  45%, G loss: 2.221161\n",
      "Ep: 54, steps: 7, D loss: 0.193722, acc:  72%, G loss: 2.215895\n",
      "Ep: 54, steps: 8, D loss: 0.173550, acc:  78%, G loss: 2.945322\n",
      "Ep: 54, steps: 9, D loss: 0.311783, acc:  36%, G loss: 2.141259\n",
      "Ep: 54, steps: 10, D loss: 0.218457, acc:  61%, G loss: 2.299164\n",
      "Ep: 54, steps: 11, D loss: 0.304586, acc:  38%, G loss: 2.420729\n",
      "Ep: 54, steps: 12, D loss: 0.288913, acc:  41%, G loss: 1.952807\n",
      "Ep: 54, steps: 13, D loss: 0.242768, acc:  57%, G loss: 2.118617\n",
      "Ep: 54, steps: 14, D loss: 0.218596, acc:  69%, G loss: 2.010052\n",
      "Ep: 54, steps: 15, D loss: 0.240115, acc:  58%, G loss: 2.579173\n",
      "Ep: 54, steps: 16, D loss: 0.250859, acc:  55%, G loss: 2.435775\n",
      "Ep: 54, steps: 17, D loss: 0.180238, acc:  80%, G loss: 2.518653\n",
      "Ep: 54, steps: 18, D loss: 0.259473, acc:  52%, G loss: 2.683766\n",
      "Ep: 54, steps: 19, D loss: 0.261670, acc:  53%, G loss: 2.168439\n",
      "Ep: 54, steps: 20, D loss: 0.145146, acc:  91%, G loss: 2.384820\n",
      "Ep: 54, steps: 21, D loss: 0.215656, acc:  68%, G loss: 2.080246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 54, steps: 22, D loss: 0.180978, acc:  82%, G loss: 2.395293\n",
      "Ep: 54, steps: 23, D loss: 0.239331, acc:  61%, G loss: 2.891215\n",
      "Ep: 54, steps: 24, D loss: 0.159997, acc:  80%, G loss: 2.639898\n",
      "Ep: 54, steps: 25, D loss: 0.161564, acc:  83%, G loss: 2.549896\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 55, steps: 1, D loss: 0.297637, acc:  44%, G loss: 2.465906\n",
      "Ep: 55, steps: 2, D loss: 0.269162, acc:  51%, G loss: 2.350059\n",
      "Ep: 55, steps: 3, D loss: 0.114022, acc:  91%, G loss: 2.844375\n",
      "Ep: 55, steps: 4, D loss: 0.191352, acc:  76%, G loss: 2.517300\n",
      "Ep: 55, steps: 5, D loss: 0.259982, acc:  52%, G loss: 2.398049\n",
      "Ep: 55, steps: 6, D loss: 0.314533, acc:  45%, G loss: 2.221393\n",
      "Ep: 55, steps: 7, D loss: 0.209758, acc:  69%, G loss: 2.352102\n",
      "Ep: 55, steps: 8, D loss: 0.184068, acc:  75%, G loss: 3.022894\n",
      "Ep: 55, steps: 9, D loss: 0.299206, acc:  40%, G loss: 2.266860\n",
      "Saved Model\n",
      "Ep: 55, steps: 10, D loss: 0.218725, acc:  64%, G loss: 2.242571\n",
      "Ep: 55, steps: 11, D loss: 0.292099, acc:  39%, G loss: 2.048003\n",
      "Ep: 55, steps: 12, D loss: 0.236837, acc:  58%, G loss: 2.207416\n",
      "Ep: 55, steps: 13, D loss: 0.202472, acc:  70%, G loss: 2.157645\n",
      "Ep: 55, steps: 14, D loss: 0.255881, acc:  54%, G loss: 2.692245\n",
      "Ep: 55, steps: 15, D loss: 0.252777, acc:  55%, G loss: 2.489461\n",
      "Ep: 55, steps: 16, D loss: 0.168426, acc:  82%, G loss: 2.573691\n",
      "Ep: 55, steps: 17, D loss: 0.280726, acc:  45%, G loss: 2.484466\n",
      "Ep: 55, steps: 18, D loss: 0.279648, acc:  50%, G loss: 2.132776\n",
      "Ep: 55, steps: 19, D loss: 0.141163, acc:  93%, G loss: 2.407266\n",
      "Ep: 55, steps: 20, D loss: 0.206511, acc:  71%, G loss: 2.474128\n",
      "Ep: 55, steps: 21, D loss: 0.161177, acc:  85%, G loss: 2.220219\n",
      "Ep: 55, steps: 22, D loss: 0.263222, acc:  51%, G loss: 2.516544\n",
      "Ep: 55, steps: 23, D loss: 0.151072, acc:  82%, G loss: 2.479515\n",
      "Ep: 55, steps: 24, D loss: 0.166133, acc:  82%, G loss: 2.514587\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 56, steps: 1, D loss: 0.313403, acc:  39%, G loss: 2.219617\n",
      "Ep: 56, steps: 2, D loss: 0.268998, acc:  52%, G loss: 2.180197\n",
      "Ep: 56, steps: 3, D loss: 0.122726, acc:  88%, G loss: 3.125014\n",
      "Ep: 56, steps: 4, D loss: 0.211179, acc:  69%, G loss: 2.281536\n",
      "Ep: 56, steps: 5, D loss: 0.240074, acc:  56%, G loss: 2.299106\n",
      "Ep: 56, steps: 6, D loss: 0.317528, acc:  45%, G loss: 2.363347\n",
      "Ep: 56, steps: 7, D loss: 0.234464, acc:  62%, G loss: 2.199930\n",
      "Ep: 56, steps: 8, D loss: 0.168329, acc:  78%, G loss: 3.138292\n",
      "Ep: 56, steps: 9, D loss: 0.312952, acc:  38%, G loss: 2.242674\n",
      "Ep: 56, steps: 10, D loss: 0.209243, acc:  69%, G loss: 2.389428\n",
      "Ep: 56, steps: 11, D loss: 0.300094, acc:  42%, G loss: 2.440548\n",
      "Ep: 56, steps: 12, D loss: 0.296726, acc:  37%, G loss: 1.947229\n",
      "Ep: 56, steps: 13, D loss: 0.251781, acc:  53%, G loss: 2.178053\n",
      "Ep: 56, steps: 14, D loss: 0.226645, acc:  61%, G loss: 2.029744\n",
      "Ep: 56, steps: 15, D loss: 0.233685, acc:  62%, G loss: 2.483436\n",
      "Ep: 56, steps: 16, D loss: 0.252160, acc:  55%, G loss: 2.425580\n",
      "Ep: 56, steps: 17, D loss: 0.171645, acc:  81%, G loss: 2.485774\n",
      "Ep: 56, steps: 18, D loss: 0.264041, acc:  51%, G loss: 2.459919\n",
      "Ep: 56, steps: 19, D loss: 0.267712, acc:  54%, G loss: 2.068797\n",
      "Ep: 56, steps: 20, D loss: 0.140758, acc:  91%, G loss: 2.393893\n",
      "Ep: 56, steps: 21, D loss: 0.220037, acc:  67%, G loss: 2.340538\n",
      "Ep: 56, steps: 22, D loss: 0.160238, acc:  88%, G loss: 2.364940\n",
      "Ep: 56, steps: 23, D loss: 0.251931, acc:  54%, G loss: 2.601980\n",
      "Ep: 56, steps: 24, D loss: 0.165992, acc:  78%, G loss: 2.420169\n",
      "Ep: 56, steps: 25, D loss: 0.168124, acc:  82%, G loss: 2.447739\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 57, steps: 1, D loss: 0.286104, acc:  43%, G loss: 2.396117\n",
      "Ep: 57, steps: 2, D loss: 0.259568, acc:  54%, G loss: 2.284103\n",
      "Ep: 57, steps: 3, D loss: 0.114153, acc:  91%, G loss: 3.085658\n",
      "Ep: 57, steps: 4, D loss: 0.208240, acc:  69%, G loss: 2.434337\n",
      "Ep: 57, steps: 5, D loss: 0.247941, acc:  55%, G loss: 2.326190\n",
      "Ep: 57, steps: 6, D loss: 0.308966, acc:  47%, G loss: 2.390743\n",
      "Ep: 57, steps: 7, D loss: 0.255067, acc:  55%, G loss: 2.190509\n",
      "Ep: 57, steps: 8, D loss: 0.185614, acc:  75%, G loss: 3.143612\n",
      "Ep: 57, steps: 9, D loss: 0.292617, acc:  42%, G loss: 2.220925\n",
      "Ep: 57, steps: 10, D loss: 0.211965, acc:  67%, G loss: 2.354231\n",
      "Ep: 57, steps: 11, D loss: 0.294681, acc:  44%, G loss: 2.443130\n",
      "Ep: 57, steps: 12, D loss: 0.290250, acc:  38%, G loss: 1.949785\n",
      "Ep: 57, steps: 13, D loss: 0.252055, acc:  53%, G loss: 2.138857\n",
      "Ep: 57, steps: 14, D loss: 0.216743, acc:  64%, G loss: 2.057556\n",
      "Ep: 57, steps: 15, D loss: 0.230997, acc:  60%, G loss: 2.676389\n",
      "Ep: 57, steps: 16, D loss: 0.253106, acc:  55%, G loss: 2.377176\n",
      "Ep: 57, steps: 17, D loss: 0.173772, acc:  79%, G loss: 2.509907\n",
      "Ep: 57, steps: 18, D loss: 0.262385, acc:  52%, G loss: 2.501389\n",
      "Ep: 57, steps: 19, D loss: 0.274948, acc:  51%, G loss: 2.118921\n",
      "Ep: 57, steps: 20, D loss: 0.132944, acc:  95%, G loss: 2.383696\n",
      "Ep: 57, steps: 21, D loss: 0.212448, acc:  69%, G loss: 2.253551\n",
      "Ep: 57, steps: 22, D loss: 0.164776, acc:  85%, G loss: 2.246016\n",
      "Ep: 57, steps: 23, D loss: 0.237472, acc:  60%, G loss: 2.484124\n",
      "Ep: 57, steps: 24, D loss: 0.159836, acc:  78%, G loss: 2.431084\n",
      "Ep: 57, steps: 25, D loss: 0.173771, acc:  80%, G loss: 2.485611\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 58, steps: 1, D loss: 0.298374, acc:  43%, G loss: 2.348819\n",
      "Ep: 58, steps: 2, D loss: 0.257004, acc:  56%, G loss: 2.210999\n",
      "Ep: 58, steps: 3, D loss: 0.119376, acc:  90%, G loss: 3.106374\n",
      "Ep: 58, steps: 4, D loss: 0.205450, acc:  70%, G loss: 2.234320\n",
      "Ep: 58, steps: 5, D loss: 0.243887, acc:  56%, G loss: 2.331703\n",
      "Ep: 58, steps: 6, D loss: 0.319222, acc:  45%, G loss: 2.206521\n",
      "Ep: 58, steps: 7, D loss: 0.240350, acc:  59%, G loss: 2.113224\n",
      "Saved Model\n",
      "Ep: 58, steps: 8, D loss: 0.165827, acc:  79%, G loss: 3.182293\n",
      "Ep: 58, steps: 9, D loss: 0.247961, acc:  51%, G loss: 2.563616\n",
      "Ep: 58, steps: 10, D loss: 0.385300, acc:  34%, G loss: 2.438008\n",
      "Ep: 58, steps: 11, D loss: 0.297783, acc:  39%, G loss: 1.964184\n",
      "Ep: 58, steps: 12, D loss: 0.254079, acc:  52%, G loss: 2.056080\n",
      "Ep: 58, steps: 13, D loss: 0.223987, acc:  62%, G loss: 2.152494\n",
      "Ep: 58, steps: 14, D loss: 0.240772, acc:  59%, G loss: 2.499567\n",
      "Ep: 58, steps: 15, D loss: 0.260993, acc:  53%, G loss: 2.366167\n",
      "Ep: 58, steps: 16, D loss: 0.176473, acc:  78%, G loss: 2.469730\n",
      "Ep: 58, steps: 17, D loss: 0.267336, acc:  50%, G loss: 2.384388\n",
      "Ep: 58, steps: 18, D loss: 0.272029, acc:  52%, G loss: 2.059862\n",
      "Ep: 58, steps: 19, D loss: 0.138709, acc:  93%, G loss: 2.390533\n",
      "Ep: 58, steps: 20, D loss: 0.215655, acc:  68%, G loss: 2.128533\n",
      "Ep: 58, steps: 21, D loss: 0.179250, acc:  84%, G loss: 2.152550\n",
      "Ep: 58, steps: 22, D loss: 0.227998, acc:  63%, G loss: 2.471412\n",
      "Ep: 58, steps: 23, D loss: 0.162569, acc:  79%, G loss: 2.499092\n",
      "Ep: 58, steps: 24, D loss: 0.174296, acc:  79%, G loss: 2.307750\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 59, steps: 1, D loss: 0.302985, acc:  42%, G loss: 2.289346\n",
      "Ep: 59, steps: 2, D loss: 0.250114, acc:  57%, G loss: 2.127762\n",
      "Ep: 59, steps: 3, D loss: 0.131683, acc:  88%, G loss: 2.917427\n",
      "Ep: 59, steps: 4, D loss: 0.212169, acc:  69%, G loss: 2.366088\n",
      "Ep: 59, steps: 5, D loss: 0.228828, acc:  60%, G loss: 2.342226\n",
      "Ep: 59, steps: 6, D loss: 0.315382, acc:  45%, G loss: 2.211020\n",
      "Ep: 59, steps: 7, D loss: 0.235525, acc:  61%, G loss: 2.054379\n",
      "Ep: 59, steps: 8, D loss: 0.154253, acc:  82%, G loss: 3.053896\n",
      "Ep: 59, steps: 9, D loss: 0.288078, acc:  43%, G loss: 2.171214\n",
      "Ep: 59, steps: 10, D loss: 0.198829, acc:  69%, G loss: 2.304390\n",
      "Ep: 59, steps: 11, D loss: 0.318606, acc:  37%, G loss: 2.489444\n",
      "Ep: 59, steps: 12, D loss: 0.304462, acc:  35%, G loss: 2.004223\n",
      "Ep: 59, steps: 13, D loss: 0.255441, acc:  51%, G loss: 2.097859\n",
      "Ep: 59, steps: 14, D loss: 0.219370, acc:  68%, G loss: 2.094805\n",
      "Ep: 59, steps: 15, D loss: 0.233783, acc:  60%, G loss: 2.558568\n",
      "Ep: 59, steps: 16, D loss: 0.255272, acc:  53%, G loss: 2.332332\n",
      "Ep: 59, steps: 17, D loss: 0.160798, acc:  84%, G loss: 2.421450\n",
      "Ep: 59, steps: 18, D loss: 0.262205, acc:  52%, G loss: 2.395354\n",
      "Ep: 59, steps: 19, D loss: 0.265360, acc:  53%, G loss: 2.034294\n",
      "Ep: 59, steps: 20, D loss: 0.125784, acc:  92%, G loss: 2.612318\n",
      "Ep: 59, steps: 21, D loss: 0.239138, acc:  62%, G loss: 2.069348\n",
      "Ep: 59, steps: 22, D loss: 0.170137, acc:  82%, G loss: 2.245939\n",
      "Ep: 59, steps: 23, D loss: 0.239599, acc:  58%, G loss: 2.459578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 59, steps: 24, D loss: 0.161468, acc:  81%, G loss: 2.397213\n",
      "Ep: 59, steps: 25, D loss: 0.171233, acc:  81%, G loss: 2.207518\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 60, steps: 1, D loss: 0.286115, acc:  43%, G loss: 2.198321\n",
      "Ep: 60, steps: 2, D loss: 0.260722, acc:  53%, G loss: 2.132516\n",
      "Ep: 60, steps: 3, D loss: 0.116689, acc:  91%, G loss: 2.935829\n",
      "Ep: 60, steps: 4, D loss: 0.225766, acc:  63%, G loss: 2.424026\n",
      "Ep: 60, steps: 5, D loss: 0.239696, acc:  59%, G loss: 2.359862\n",
      "Ep: 60, steps: 6, D loss: 0.302289, acc:  49%, G loss: 2.462302\n",
      "Ep: 60, steps: 7, D loss: 0.258166, acc:  56%, G loss: 2.178013\n",
      "Ep: 60, steps: 8, D loss: 0.191297, acc:  73%, G loss: 2.958781\n",
      "Ep: 60, steps: 9, D loss: 0.268443, acc:  46%, G loss: 2.168532\n",
      "Ep: 60, steps: 10, D loss: 0.181231, acc:  78%, G loss: 2.409273\n",
      "Ep: 60, steps: 11, D loss: 0.301170, acc:  37%, G loss: 2.471095\n",
      "Ep: 60, steps: 12, D loss: 0.289066, acc:  41%, G loss: 2.010303\n",
      "Ep: 60, steps: 13, D loss: 0.252732, acc:  53%, G loss: 2.172967\n",
      "Ep: 60, steps: 14, D loss: 0.218091, acc:  63%, G loss: 2.090684\n",
      "Ep: 60, steps: 15, D loss: 0.233065, acc:  60%, G loss: 2.539762\n",
      "Ep: 60, steps: 16, D loss: 0.266293, acc:  49%, G loss: 2.346943\n",
      "Ep: 60, steps: 17, D loss: 0.160990, acc:  83%, G loss: 2.430366\n",
      "Ep: 60, steps: 18, D loss: 0.256458, acc:  53%, G loss: 2.460078\n",
      "Ep: 60, steps: 19, D loss: 0.270214, acc:  52%, G loss: 2.020179\n",
      "Ep: 60, steps: 20, D loss: 0.114457, acc:  95%, G loss: 2.662703\n",
      "Ep: 60, steps: 21, D loss: 0.225438, acc:  67%, G loss: 2.069021\n",
      "Ep: 60, steps: 22, D loss: 0.165827, acc:  82%, G loss: 2.106925\n",
      "Ep: 60, steps: 23, D loss: 0.245784, acc:  56%, G loss: 2.412962\n",
      "Ep: 60, steps: 24, D loss: 0.165059, acc:  77%, G loss: 2.366068\n",
      "Ep: 60, steps: 25, D loss: 0.171113, acc:  80%, G loss: 2.215927\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 61, steps: 1, D loss: 0.293698, acc:  42%, G loss: 2.175283\n",
      "Ep: 61, steps: 2, D loss: 0.256231, acc:  55%, G loss: 2.096486\n",
      "Ep: 61, steps: 3, D loss: 0.113549, acc:  91%, G loss: 3.020719\n",
      "Ep: 61, steps: 4, D loss: 0.208062, acc:  69%, G loss: 2.384878\n",
      "Saved Model\n",
      "Ep: 61, steps: 5, D loss: 0.245514, acc:  57%, G loss: 2.428020\n",
      "Ep: 61, steps: 6, D loss: 0.272330, acc:  50%, G loss: 2.022587\n",
      "Ep: 61, steps: 7, D loss: 0.149383, acc:  84%, G loss: 3.024395\n",
      "Ep: 61, steps: 8, D loss: 0.349855, acc:  39%, G loss: 2.176861\n",
      "Ep: 61, steps: 9, D loss: 0.176281, acc:  78%, G loss: 2.478656\n",
      "Ep: 61, steps: 10, D loss: 0.316506, acc:  38%, G loss: 2.452121\n",
      "Ep: 61, steps: 11, D loss: 0.305543, acc:  37%, G loss: 1.941337\n",
      "Ep: 61, steps: 12, D loss: 0.258019, acc:  50%, G loss: 2.083001\n",
      "Ep: 61, steps: 13, D loss: 0.216705, acc:  63%, G loss: 2.115862\n",
      "Ep: 61, steps: 14, D loss: 0.233314, acc:  60%, G loss: 2.430263\n",
      "Ep: 61, steps: 15, D loss: 0.268004, acc:  50%, G loss: 2.407429\n",
      "Ep: 61, steps: 16, D loss: 0.161393, acc:  82%, G loss: 2.463241\n",
      "Ep: 61, steps: 17, D loss: 0.256894, acc:  54%, G loss: 2.701770\n",
      "Ep: 61, steps: 18, D loss: 0.275688, acc:  50%, G loss: 2.088215\n",
      "Ep: 61, steps: 19, D loss: 0.122683, acc:  93%, G loss: 2.757767\n",
      "Ep: 61, steps: 20, D loss: 0.226335, acc:  67%, G loss: 2.153565\n",
      "Ep: 61, steps: 21, D loss: 0.165125, acc:  84%, G loss: 2.285228\n",
      "Ep: 61, steps: 22, D loss: 0.239930, acc:  56%, G loss: 2.505985\n",
      "Ep: 61, steps: 23, D loss: 0.167859, acc:  77%, G loss: 2.489851\n",
      "Ep: 61, steps: 24, D loss: 0.179131, acc:  78%, G loss: 2.265982\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 62, steps: 1, D loss: 0.285979, acc:  44%, G loss: 2.271442\n",
      "Ep: 62, steps: 2, D loss: 0.256563, acc:  55%, G loss: 2.193501\n",
      "Ep: 62, steps: 3, D loss: 0.116911, acc:  90%, G loss: 2.978737\n",
      "Ep: 62, steps: 4, D loss: 0.194298, acc:  74%, G loss: 2.247833\n",
      "Ep: 62, steps: 5, D loss: 0.237276, acc:  56%, G loss: 2.426431\n",
      "Ep: 62, steps: 6, D loss: 0.322054, acc:  45%, G loss: 2.309439\n",
      "Ep: 62, steps: 7, D loss: 0.229557, acc:  63%, G loss: 2.086204\n",
      "Ep: 62, steps: 8, D loss: 0.183288, acc:  76%, G loss: 3.091184\n",
      "Ep: 62, steps: 9, D loss: 0.298633, acc:  39%, G loss: 2.123170\n",
      "Ep: 62, steps: 10, D loss: 0.191625, acc:  72%, G loss: 2.337218\n",
      "Ep: 62, steps: 11, D loss: 0.311477, acc:  38%, G loss: 2.400715\n",
      "Ep: 62, steps: 12, D loss: 0.294422, acc:  40%, G loss: 2.040325\n",
      "Ep: 62, steps: 13, D loss: 0.245042, acc:  54%, G loss: 2.083508\n",
      "Ep: 62, steps: 14, D loss: 0.226693, acc:  67%, G loss: 2.093302\n",
      "Ep: 62, steps: 15, D loss: 0.237271, acc:  58%, G loss: 2.565980\n",
      "Ep: 62, steps: 16, D loss: 0.263808, acc:  52%, G loss: 2.299366\n",
      "Ep: 62, steps: 17, D loss: 0.165494, acc:  83%, G loss: 2.435056\n",
      "Ep: 62, steps: 18, D loss: 0.270608, acc:  48%, G loss: 2.338878\n",
      "Ep: 62, steps: 19, D loss: 0.271891, acc:  52%, G loss: 2.051130\n",
      "Ep: 62, steps: 20, D loss: 0.124051, acc:  90%, G loss: 2.663175\n",
      "Ep: 62, steps: 21, D loss: 0.227043, acc:  63%, G loss: 2.127959\n",
      "Ep: 62, steps: 22, D loss: 0.175291, acc:  81%, G loss: 2.208670\n",
      "Ep: 62, steps: 23, D loss: 0.244098, acc:  61%, G loss: 2.479165\n",
      "Ep: 62, steps: 24, D loss: 0.149650, acc:  81%, G loss: 2.475642\n",
      "Ep: 62, steps: 25, D loss: 0.165120, acc:  82%, G loss: 2.248324\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 63, steps: 1, D loss: 0.297382, acc:  44%, G loss: 2.153260\n",
      "Ep: 63, steps: 2, D loss: 0.258806, acc:  55%, G loss: 2.149077\n",
      "Ep: 63, steps: 3, D loss: 0.123733, acc:  88%, G loss: 2.904152\n",
      "Ep: 63, steps: 4, D loss: 0.187475, acc:  74%, G loss: 2.537881\n",
      "Ep: 63, steps: 5, D loss: 0.252889, acc:  55%, G loss: 2.493547\n",
      "Ep: 63, steps: 6, D loss: 0.312178, acc:  43%, G loss: 2.201735\n",
      "Ep: 63, steps: 7, D loss: 0.251577, acc:  56%, G loss: 2.034499\n",
      "Ep: 63, steps: 8, D loss: 0.200203, acc:  68%, G loss: 2.955545\n",
      "Ep: 63, steps: 9, D loss: 0.263213, acc:  53%, G loss: 2.181462\n",
      "Ep: 63, steps: 10, D loss: 0.182667, acc:  77%, G loss: 2.299596\n",
      "Ep: 63, steps: 11, D loss: 0.284159, acc:  46%, G loss: 2.477525\n",
      "Ep: 63, steps: 12, D loss: 0.303643, acc:  38%, G loss: 2.086399\n",
      "Ep: 63, steps: 13, D loss: 0.250159, acc:  54%, G loss: 2.081149\n",
      "Ep: 63, steps: 14, D loss: 0.216900, acc:  63%, G loss: 2.186869\n",
      "Ep: 63, steps: 15, D loss: 0.230334, acc:  61%, G loss: 2.590799\n",
      "Ep: 63, steps: 16, D loss: 0.262863, acc:  51%, G loss: 2.311962\n",
      "Ep: 63, steps: 17, D loss: 0.158621, acc:  83%, G loss: 2.910838\n",
      "Ep: 63, steps: 18, D loss: 0.248641, acc:  54%, G loss: 3.304852\n",
      "Ep: 63, steps: 19, D loss: 0.254480, acc:  55%, G loss: 2.427968\n",
      "Ep: 63, steps: 20, D loss: 0.105591, acc:  95%, G loss: 2.950118\n",
      "Ep: 63, steps: 21, D loss: 0.236459, acc:  65%, G loss: 2.133964\n",
      "Ep: 63, steps: 22, D loss: 0.163973, acc:  84%, G loss: 2.122011\n",
      "Ep: 63, steps: 23, D loss: 0.221597, acc:  63%, G loss: 2.679688\n",
      "Ep: 63, steps: 24, D loss: 0.143589, acc:  86%, G loss: 2.242379\n",
      "Ep: 63, steps: 25, D loss: 0.158700, acc:  83%, G loss: 2.454719\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 64, steps: 1, D loss: 0.266386, acc:  52%, G loss: 2.408072\n",
      "Ep: 64, steps: 2, D loss: 0.250098, acc:  56%, G loss: 2.356581\n",
      "Saved Model\n",
      "Ep: 64, steps: 3, D loss: 0.111332, acc:  92%, G loss: 2.894121\n",
      "Ep: 64, steps: 4, D loss: 0.214630, acc:  64%, G loss: 2.484128\n",
      "Ep: 64, steps: 5, D loss: 0.321107, acc:  45%, G loss: 2.409863\n",
      "Ep: 64, steps: 6, D loss: 0.199313, acc:  72%, G loss: 2.444076\n",
      "Ep: 64, steps: 7, D loss: 0.155489, acc:  81%, G loss: 2.947710\n",
      "Ep: 64, steps: 8, D loss: 0.290626, acc:  44%, G loss: 2.208937\n",
      "Ep: 64, steps: 9, D loss: 0.191691, acc:  74%, G loss: 2.359688\n",
      "Ep: 64, steps: 10, D loss: 0.299464, acc:  42%, G loss: 2.549249\n",
      "Ep: 64, steps: 11, D loss: 0.281841, acc:  45%, G loss: 2.102037\n",
      "Ep: 64, steps: 12, D loss: 0.245173, acc:  55%, G loss: 2.191399\n",
      "Ep: 64, steps: 13, D loss: 0.226730, acc:  62%, G loss: 2.158628\n",
      "Ep: 64, steps: 14, D loss: 0.232838, acc:  61%, G loss: 2.663608\n",
      "Ep: 64, steps: 15, D loss: 0.259591, acc:  52%, G loss: 2.430034\n",
      "Ep: 64, steps: 16, D loss: 0.161632, acc:  82%, G loss: 2.532018\n",
      "Ep: 64, steps: 17, D loss: 0.257018, acc:  54%, G loss: 2.552327\n",
      "Ep: 64, steps: 18, D loss: 0.262778, acc:  55%, G loss: 2.114533\n",
      "Ep: 64, steps: 19, D loss: 0.109587, acc:  94%, G loss: 2.653926\n",
      "Ep: 64, steps: 20, D loss: 0.232456, acc:  65%, G loss: 2.114787\n",
      "Ep: 64, steps: 21, D loss: 0.155750, acc:  85%, G loss: 2.509701\n",
      "Ep: 64, steps: 22, D loss: 0.244032, acc:  58%, G loss: 2.578866\n",
      "Ep: 64, steps: 23, D loss: 0.143262, acc:  82%, G loss: 2.524415\n",
      "Ep: 64, steps: 24, D loss: 0.152979, acc:  84%, G loss: 2.342811\n",
      "Data exhausted, Re Initialize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 65, steps: 1, D loss: 0.290287, acc:  47%, G loss: 2.276363\n",
      "Ep: 65, steps: 2, D loss: 0.271536, acc:  55%, G loss: 2.291536\n",
      "Ep: 65, steps: 3, D loss: 0.129874, acc:  85%, G loss: 2.920815\n",
      "Ep: 65, steps: 4, D loss: 0.215993, acc:  66%, G loss: 2.559836\n",
      "Ep: 65, steps: 5, D loss: 0.241058, acc:  57%, G loss: 2.549458\n",
      "Ep: 65, steps: 6, D loss: 0.317179, acc:  48%, G loss: 2.501405\n",
      "Ep: 65, steps: 7, D loss: 0.308083, acc:  43%, G loss: 2.083435\n",
      "Ep: 65, steps: 8, D loss: 0.220731, acc:  59%, G loss: 2.920931\n",
      "Ep: 65, steps: 9, D loss: 0.267972, acc:  49%, G loss: 2.170645\n",
      "Ep: 65, steps: 10, D loss: 0.177739, acc:  78%, G loss: 2.497870\n",
      "Ep: 65, steps: 11, D loss: 0.288394, acc:  47%, G loss: 2.490805\n",
      "Ep: 65, steps: 12, D loss: 0.305515, acc:  38%, G loss: 2.054324\n",
      "Ep: 65, steps: 13, D loss: 0.258684, acc:  51%, G loss: 2.138368\n",
      "Ep: 65, steps: 14, D loss: 0.207200, acc:  67%, G loss: 2.240998\n",
      "Ep: 65, steps: 15, D loss: 0.240431, acc:  56%, G loss: 2.549549\n",
      "Ep: 65, steps: 16, D loss: 0.264394, acc:  51%, G loss: 2.380499\n",
      "Ep: 65, steps: 17, D loss: 0.160692, acc:  82%, G loss: 2.469476\n",
      "Ep: 65, steps: 18, D loss: 0.262830, acc:  51%, G loss: 2.573070\n",
      "Ep: 65, steps: 19, D loss: 0.278309, acc:  52%, G loss: 2.047942\n",
      "Ep: 65, steps: 20, D loss: 0.104940, acc:  96%, G loss: 2.731886\n",
      "Ep: 65, steps: 21, D loss: 0.222226, acc:  67%, G loss: 2.210563\n",
      "Ep: 65, steps: 22, D loss: 0.156983, acc:  83%, G loss: 2.332579\n",
      "Ep: 65, steps: 23, D loss: 0.239687, acc:  61%, G loss: 2.457803\n",
      "Ep: 65, steps: 24, D loss: 0.162408, acc:  78%, G loss: 2.365508\n",
      "Ep: 65, steps: 25, D loss: 0.161037, acc:  82%, G loss: 2.264091\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 66, steps: 1, D loss: 0.290298, acc:  46%, G loss: 2.300930\n",
      "Ep: 66, steps: 2, D loss: 0.249744, acc:  58%, G loss: 2.184881\n",
      "Ep: 66, steps: 3, D loss: 0.109028, acc:  91%, G loss: 2.993194\n",
      "Ep: 66, steps: 4, D loss: 0.191970, acc:  74%, G loss: 2.314074\n",
      "Ep: 66, steps: 5, D loss: 0.260728, acc:  55%, G loss: 2.554983\n",
      "Ep: 66, steps: 6, D loss: 0.311130, acc:  48%, G loss: 2.342027\n",
      "Ep: 66, steps: 7, D loss: 0.257092, acc:  55%, G loss: 2.175815\n",
      "Ep: 66, steps: 8, D loss: 0.183249, acc:  75%, G loss: 2.998908\n",
      "Ep: 66, steps: 9, D loss: 0.266453, acc:  51%, G loss: 2.221065\n",
      "Ep: 66, steps: 10, D loss: 0.188357, acc:  73%, G loss: 2.393854\n",
      "Ep: 66, steps: 11, D loss: 0.277076, acc:  48%, G loss: 2.474023\n",
      "Ep: 66, steps: 12, D loss: 0.297781, acc:  38%, G loss: 2.018772\n",
      "Ep: 66, steps: 13, D loss: 0.246922, acc:  55%, G loss: 2.311403\n",
      "Ep: 66, steps: 14, D loss: 0.217962, acc:  63%, G loss: 2.212504\n",
      "Ep: 66, steps: 15, D loss: 0.234708, acc:  59%, G loss: 2.514680\n",
      "Ep: 66, steps: 16, D loss: 0.264126, acc:  50%, G loss: 2.374247\n",
      "Ep: 66, steps: 17, D loss: 0.155128, acc:  84%, G loss: 2.462808\n",
      "Ep: 66, steps: 18, D loss: 0.257567, acc:  53%, G loss: 2.560393\n",
      "Ep: 66, steps: 19, D loss: 0.270865, acc:  53%, G loss: 2.058123\n",
      "Ep: 66, steps: 20, D loss: 0.106048, acc:  94%, G loss: 2.712154\n",
      "Ep: 66, steps: 21, D loss: 0.237363, acc:  64%, G loss: 2.096024\n",
      "Ep: 66, steps: 22, D loss: 0.160102, acc:  84%, G loss: 2.309892\n",
      "Ep: 66, steps: 23, D loss: 0.238435, acc:  59%, G loss: 2.523044\n",
      "Ep: 66, steps: 24, D loss: 0.141100, acc:  84%, G loss: 2.571870\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 66, steps: 25, D loss: 0.152718, acc:  84%, G loss: 2.318565\n",
      "Ep: 66, steps: 26, D loss: 0.288536, acc:  46%, G loss: 2.259189\n",
      "Ep: 66, steps: 27, D loss: 0.125103, acc:  88%, G loss: 3.056984\n",
      "Ep: 66, steps: 28, D loss: 0.245844, acc:  57%, G loss: 2.399780\n",
      "Ep: 66, steps: 29, D loss: 0.233840, acc:  60%, G loss: 2.510175\n",
      "Ep: 66, steps: 30, D loss: 0.306361, acc:  47%, G loss: 2.426354\n",
      "Ep: 66, steps: 31, D loss: 0.225723, acc:  63%, G loss: 2.149995\n",
      "Ep: 66, steps: 32, D loss: 0.211207, acc:  66%, G loss: 2.952015\n",
      "Ep: 66, steps: 33, D loss: 0.281197, acc:  42%, G loss: 2.133538\n",
      "Ep: 66, steps: 34, D loss: 0.185159, acc:  79%, G loss: 2.447058\n",
      "Ep: 66, steps: 35, D loss: 0.288108, acc:  45%, G loss: 2.514574\n",
      "Ep: 66, steps: 36, D loss: 0.294707, acc:  41%, G loss: 2.000335\n",
      "Ep: 66, steps: 37, D loss: 0.252963, acc:  53%, G loss: 2.147842\n",
      "Ep: 66, steps: 38, D loss: 0.209365, acc:  65%, G loss: 2.233394\n",
      "Ep: 66, steps: 39, D loss: 0.248745, acc:  55%, G loss: 2.552743\n",
      "Ep: 66, steps: 40, D loss: 0.260463, acc:  52%, G loss: 2.365036\n",
      "Ep: 66, steps: 41, D loss: 0.159718, acc:  82%, G loss: 2.445827\n",
      "Ep: 66, steps: 42, D loss: 0.267131, acc:  50%, G loss: 2.523882\n",
      "Ep: 66, steps: 43, D loss: 0.279676, acc:  52%, G loss: 2.056951\n",
      "Ep: 66, steps: 44, D loss: 0.105533, acc:  95%, G loss: 2.680562\n",
      "Ep: 66, steps: 45, D loss: 0.223973, acc:  67%, G loss: 2.205901\n",
      "Ep: 66, steps: 46, D loss: 0.157297, acc:  85%, G loss: 2.307652\n",
      "Ep: 66, steps: 47, D loss: 0.240810, acc:  61%, G loss: 2.458303\n",
      "Ep: 66, steps: 48, D loss: 0.148941, acc:  80%, G loss: 2.396326\n",
      "Ep: 66, steps: 49, D loss: 0.160333, acc:  81%, G loss: 2.434171\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 67, steps: 1, D loss: 0.316898, acc:  43%, G loss: 2.340109\n",
      "Ep: 67, steps: 2, D loss: 0.261897, acc:  56%, G loss: 2.230489\n",
      "Ep: 67, steps: 3, D loss: 0.127761, acc:  86%, G loss: 2.956000\n",
      "Ep: 67, steps: 4, D loss: 0.205495, acc:  69%, G loss: 2.270101\n",
      "Ep: 67, steps: 5, D loss: 0.247828, acc:  57%, G loss: 2.511697\n",
      "Ep: 67, steps: 6, D loss: 0.313324, acc:  46%, G loss: 2.281170\n",
      "Ep: 67, steps: 7, D loss: 0.260126, acc:  51%, G loss: 1.933173\n",
      "Ep: 67, steps: 8, D loss: 0.153495, acc:  83%, G loss: 3.013396\n",
      "Ep: 67, steps: 9, D loss: 0.265177, acc:  51%, G loss: 2.196167\n",
      "Ep: 67, steps: 10, D loss: 0.177481, acc:  75%, G loss: 2.393888\n",
      "Ep: 67, steps: 11, D loss: 0.288939, acc:  44%, G loss: 2.465410\n",
      "Ep: 67, steps: 12, D loss: 0.306839, acc:  37%, G loss: 1.943754\n",
      "Ep: 67, steps: 13, D loss: 0.242966, acc:  57%, G loss: 2.260545\n",
      "Ep: 67, steps: 14, D loss: 0.235071, acc:  56%, G loss: 2.117935\n",
      "Ep: 67, steps: 15, D loss: 0.220451, acc:  63%, G loss: 2.544971\n",
      "Ep: 67, steps: 16, D loss: 0.267601, acc:  50%, G loss: 2.304088\n",
      "Ep: 67, steps: 17, D loss: 0.149064, acc:  85%, G loss: 2.504009\n",
      "Ep: 67, steps: 18, D loss: 0.257835, acc:  54%, G loss: 2.588751\n",
      "Ep: 67, steps: 19, D loss: 0.255342, acc:  57%, G loss: 2.129573\n",
      "Ep: 67, steps: 20, D loss: 0.102334, acc:  92%, G loss: 2.797894\n",
      "Ep: 67, steps: 21, D loss: 0.256550, acc:  61%, G loss: 2.048884\n",
      "Ep: 67, steps: 22, D loss: 0.169408, acc:  84%, G loss: 2.134881\n",
      "Ep: 67, steps: 23, D loss: 0.229708, acc:  61%, G loss: 2.402660\n",
      "Ep: 67, steps: 24, D loss: 0.160210, acc:  81%, G loss: 2.347202\n",
      "Ep: 67, steps: 25, D loss: 0.170108, acc:  80%, G loss: 2.216350\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 68, steps: 1, D loss: 0.268032, acc:  49%, G loss: 2.242180\n",
      "Ep: 68, steps: 2, D loss: 0.246773, acc:  59%, G loss: 2.211830\n",
      "Ep: 68, steps: 3, D loss: 0.123332, acc:  88%, G loss: 2.961869\n",
      "Ep: 68, steps: 4, D loss: 0.205328, acc:  68%, G loss: 2.307981\n",
      "Ep: 68, steps: 5, D loss: 0.252939, acc:  57%, G loss: 2.360651\n",
      "Ep: 68, steps: 6, D loss: 0.299381, acc:  50%, G loss: 2.403927\n",
      "Ep: 68, steps: 7, D loss: 0.269793, acc:  53%, G loss: 2.009474\n",
      "Ep: 68, steps: 8, D loss: 0.211535, acc:  66%, G loss: 2.964283\n",
      "Ep: 68, steps: 9, D loss: 0.258966, acc:  50%, G loss: 2.162220\n",
      "Ep: 68, steps: 10, D loss: 0.181819, acc:  79%, G loss: 2.293967\n",
      "Ep: 68, steps: 11, D loss: 0.287713, acc:  45%, G loss: 2.431077\n",
      "Ep: 68, steps: 12, D loss: 0.294414, acc:  39%, G loss: 2.005548\n",
      "Ep: 68, steps: 13, D loss: 0.249704, acc:  52%, G loss: 2.160303\n",
      "Ep: 68, steps: 14, D loss: 0.211770, acc:  65%, G loss: 2.188736\n",
      "Ep: 68, steps: 15, D loss: 0.233922, acc:  59%, G loss: 2.529650\n",
      "Ep: 68, steps: 16, D loss: 0.259352, acc:  51%, G loss: 2.283384\n",
      "Ep: 68, steps: 17, D loss: 0.157209, acc:  82%, G loss: 2.417627\n",
      "Ep: 68, steps: 18, D loss: 0.252398, acc:  54%, G loss: 2.472299\n",
      "Ep: 68, steps: 19, D loss: 0.264045, acc:  55%, G loss: 2.032847\n",
      "Ep: 68, steps: 20, D loss: 0.103157, acc:  93%, G loss: 2.636486\n",
      "Ep: 68, steps: 21, D loss: 0.234993, acc:  65%, G loss: 2.108857\n",
      "Ep: 68, steps: 22, D loss: 0.160325, acc:  83%, G loss: 2.260923\n",
      "Saved Model\n",
      "Ep: 68, steps: 23, D loss: 0.233314, acc:  62%, G loss: 2.545928\n",
      "Ep: 68, steps: 24, D loss: 0.186795, acc:  75%, G loss: 2.247772\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 69, steps: 1, D loss: 0.267307, acc:  53%, G loss: 2.429410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 69, steps: 2, D loss: 0.252182, acc:  58%, G loss: 2.259986\n",
      "Ep: 69, steps: 3, D loss: 0.117449, acc:  90%, G loss: 2.966614\n",
      "Ep: 69, steps: 4, D loss: 0.191356, acc:  72%, G loss: 2.238368\n",
      "Ep: 69, steps: 5, D loss: 0.232258, acc:  61%, G loss: 2.429257\n",
      "Ep: 69, steps: 6, D loss: 0.300535, acc:  46%, G loss: 2.341312\n",
      "Ep: 69, steps: 7, D loss: 0.263288, acc:  52%, G loss: 1.972907\n",
      "Ep: 69, steps: 8, D loss: 0.161717, acc:  80%, G loss: 2.949008\n",
      "Ep: 69, steps: 9, D loss: 0.244766, acc:  57%, G loss: 2.206336\n",
      "Ep: 69, steps: 10, D loss: 0.167156, acc:  80%, G loss: 2.375900\n",
      "Ep: 69, steps: 11, D loss: 0.291671, acc:  44%, G loss: 2.497781\n",
      "Ep: 69, steps: 12, D loss: 0.300431, acc:  39%, G loss: 2.116686\n",
      "Ep: 69, steps: 13, D loss: 0.249350, acc:  53%, G loss: 2.209879\n",
      "Ep: 69, steps: 14, D loss: 0.240718, acc:  56%, G loss: 2.159063\n",
      "Ep: 69, steps: 15, D loss: 0.221019, acc:  63%, G loss: 2.499153\n",
      "Ep: 69, steps: 16, D loss: 0.264397, acc:  49%, G loss: 2.307019\n",
      "Ep: 69, steps: 17, D loss: 0.152052, acc:  84%, G loss: 2.517697\n",
      "Ep: 69, steps: 18, D loss: 0.265086, acc:  51%, G loss: 2.340052\n",
      "Ep: 69, steps: 19, D loss: 0.263421, acc:  56%, G loss: 2.014969\n",
      "Ep: 69, steps: 20, D loss: 0.107537, acc:  92%, G loss: 2.684461\n",
      "Ep: 69, steps: 21, D loss: 0.233683, acc:  64%, G loss: 2.193341\n",
      "Ep: 69, steps: 22, D loss: 0.170577, acc:  81%, G loss: 2.139907\n",
      "Ep: 69, steps: 23, D loss: 0.231636, acc:  62%, G loss: 2.449606\n",
      "Ep: 69, steps: 24, D loss: 0.154963, acc:  80%, G loss: 2.377121\n",
      "Ep: 69, steps: 25, D loss: 0.179970, acc:  75%, G loss: 2.200392\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 70, steps: 1, D loss: 0.298617, acc:  44%, G loss: 2.322427\n",
      "Ep: 70, steps: 2, D loss: 0.260024, acc:  55%, G loss: 2.260580\n",
      "Ep: 70, steps: 3, D loss: 0.141205, acc:  84%, G loss: 2.944120\n",
      "Ep: 70, steps: 4, D loss: 0.228496, acc:  64%, G loss: 2.496914\n",
      "Ep: 70, steps: 5, D loss: 0.227573, acc:  60%, G loss: 2.462831\n",
      "Ep: 70, steps: 6, D loss: 0.317251, acc:  45%, G loss: 2.320606\n",
      "Ep: 70, steps: 7, D loss: 0.285442, acc:  45%, G loss: 2.072933\n",
      "Ep: 70, steps: 8, D loss: 0.166556, acc:  79%, G loss: 2.945736\n",
      "Ep: 70, steps: 9, D loss: 0.248776, acc:  59%, G loss: 2.204148\n",
      "Ep: 70, steps: 10, D loss: 0.167403, acc:  77%, G loss: 2.399563\n",
      "Ep: 70, steps: 11, D loss: 0.287347, acc:  45%, G loss: 2.481441\n",
      "Ep: 70, steps: 12, D loss: 0.302294, acc:  40%, G loss: 1.991312\n",
      "Ep: 70, steps: 13, D loss: 0.247609, acc:  54%, G loss: 2.308528\n",
      "Ep: 70, steps: 14, D loss: 0.208771, acc:  66%, G loss: 2.246678\n",
      "Ep: 70, steps: 15, D loss: 0.224281, acc:  61%, G loss: 2.536929\n",
      "Ep: 70, steps: 16, D loss: 0.268887, acc:  48%, G loss: 2.320885\n",
      "Ep: 70, steps: 17, D loss: 0.136763, acc:  88%, G loss: 2.495373\n",
      "Ep: 70, steps: 18, D loss: 0.262534, acc:  52%, G loss: 2.297269\n",
      "Ep: 70, steps: 19, D loss: 0.259148, acc:  56%, G loss: 2.049656\n",
      "Ep: 70, steps: 20, D loss: 0.102152, acc:  93%, G loss: 2.714248\n",
      "Ep: 70, steps: 21, D loss: 0.246683, acc:  64%, G loss: 2.041208\n",
      "Ep: 70, steps: 22, D loss: 0.155008, acc:  84%, G loss: 2.167216\n",
      "Ep: 70, steps: 23, D loss: 0.221445, acc:  63%, G loss: 2.363551\n",
      "Ep: 70, steps: 24, D loss: 0.147651, acc:  82%, G loss: 2.319703\n",
      "Ep: 70, steps: 25, D loss: 0.172303, acc:  79%, G loss: 2.174825\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 71, steps: 1, D loss: 0.265208, acc:  52%, G loss: 2.274983\n",
      "Ep: 71, steps: 2, D loss: 0.262757, acc:  55%, G loss: 2.254022\n",
      "Ep: 71, steps: 3, D loss: 0.122186, acc:  87%, G loss: 3.017170\n",
      "Ep: 71, steps: 4, D loss: 0.213627, acc:  66%, G loss: 2.298375\n",
      "Ep: 71, steps: 5, D loss: 0.228140, acc:  62%, G loss: 2.510106\n",
      "Ep: 71, steps: 6, D loss: 0.305025, acc:  47%, G loss: 2.478991\n",
      "Ep: 71, steps: 7, D loss: 0.279185, acc:  48%, G loss: 1.955009\n",
      "Ep: 71, steps: 8, D loss: 0.208341, acc:  69%, G loss: 2.913159\n",
      "Ep: 71, steps: 9, D loss: 0.245833, acc:  58%, G loss: 2.196344\n",
      "Ep: 71, steps: 10, D loss: 0.164563, acc:  81%, G loss: 2.451729\n",
      "Ep: 71, steps: 11, D loss: 0.285692, acc:  47%, G loss: 2.484144\n",
      "Ep: 71, steps: 12, D loss: 0.303307, acc:  40%, G loss: 2.015897\n",
      "Ep: 71, steps: 13, D loss: 0.253481, acc:  52%, G loss: 2.246845\n",
      "Ep: 71, steps: 14, D loss: 0.208525, acc:  66%, G loss: 2.209448\n",
      "Ep: 71, steps: 15, D loss: 0.242897, acc:  54%, G loss: 2.521407\n",
      "Ep: 71, steps: 16, D loss: 0.260484, acc:  51%, G loss: 2.360298\n",
      "Ep: 71, steps: 17, D loss: 0.154915, acc:  83%, G loss: 2.412143\n",
      "Ep: 71, steps: 18, D loss: 0.265201, acc:  49%, G loss: 2.385018\n",
      "Ep: 71, steps: 19, D loss: 0.272094, acc:  53%, G loss: 2.039056\n",
      "Ep: 71, steps: 20, D loss: 0.103087, acc:  95%, G loss: 2.627683\n",
      "Saved Model\n",
      "Ep: 71, steps: 21, D loss: 0.239625, acc:  64%, G loss: 2.098249\n",
      "Ep: 71, steps: 22, D loss: 0.213512, acc:  66%, G loss: 2.539779\n",
      "Ep: 71, steps: 23, D loss: 0.161167, acc:  80%, G loss: 2.269479\n",
      "Ep: 71, steps: 24, D loss: 0.176959, acc:  78%, G loss: 2.220502\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 72, steps: 1, D loss: 0.288291, acc:  47%, G loss: 2.222687\n",
      "Ep: 72, steps: 2, D loss: 0.267783, acc:  54%, G loss: 2.260532\n",
      "Ep: 72, steps: 3, D loss: 0.128312, acc:  88%, G loss: 2.868801\n",
      "Ep: 72, steps: 4, D loss: 0.207647, acc:  66%, G loss: 2.188137\n",
      "Ep: 72, steps: 5, D loss: 0.247460, acc:  58%, G loss: 2.493648\n",
      "Ep: 72, steps: 6, D loss: 0.312984, acc:  45%, G loss: 2.392285\n",
      "Ep: 72, steps: 7, D loss: 0.278948, acc:  47%, G loss: 2.043209\n",
      "Ep: 72, steps: 8, D loss: 0.177924, acc:  76%, G loss: 2.849881\n",
      "Ep: 72, steps: 9, D loss: 0.241649, acc:  59%, G loss: 2.172550\n",
      "Ep: 72, steps: 10, D loss: 0.171831, acc:  78%, G loss: 2.453403\n",
      "Ep: 72, steps: 11, D loss: 0.275296, acc:  47%, G loss: 2.546422\n",
      "Ep: 72, steps: 12, D loss: 0.304278, acc:  40%, G loss: 2.086695\n",
      "Ep: 72, steps: 13, D loss: 0.254932, acc:  51%, G loss: 2.158945\n",
      "Ep: 72, steps: 14, D loss: 0.235305, acc:  57%, G loss: 2.166008\n",
      "Ep: 72, steps: 15, D loss: 0.224787, acc:  64%, G loss: 2.575046\n",
      "Ep: 72, steps: 16, D loss: 0.274335, acc:  47%, G loss: 2.337875\n",
      "Ep: 72, steps: 17, D loss: 0.151320, acc:  84%, G loss: 2.419223\n",
      "Ep: 72, steps: 18, D loss: 0.272069, acc:  48%, G loss: 2.318413\n",
      "Ep: 72, steps: 19, D loss: 0.239868, acc:  62%, G loss: 2.072168\n",
      "Ep: 72, steps: 20, D loss: 0.104534, acc:  91%, G loss: 2.743958\n",
      "Ep: 72, steps: 21, D loss: 0.261327, acc:  61%, G loss: 2.176889\n",
      "Ep: 72, steps: 22, D loss: 0.174054, acc:  82%, G loss: 2.047255\n",
      "Ep: 72, steps: 23, D loss: 0.218915, acc:  66%, G loss: 2.283745\n",
      "Ep: 72, steps: 24, D loss: 0.165167, acc:  79%, G loss: 2.196671\n",
      "Ep: 72, steps: 25, D loss: 0.186752, acc:  76%, G loss: 2.161365\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 73, steps: 1, D loss: 0.259015, acc:  53%, G loss: 2.223136\n",
      "Ep: 73, steps: 2, D loss: 0.257755, acc:  55%, G loss: 2.263342\n",
      "Ep: 73, steps: 3, D loss: 0.152917, acc:  77%, G loss: 2.833060\n",
      "Ep: 73, steps: 4, D loss: 0.290515, acc:  58%, G loss: 2.460285\n",
      "Ep: 73, steps: 5, D loss: 0.230186, acc:  61%, G loss: 2.521059\n",
      "Ep: 73, steps: 6, D loss: 0.292839, acc:  48%, G loss: 2.467991\n",
      "Ep: 73, steps: 7, D loss: 0.318353, acc:  39%, G loss: 1.974172\n",
      "Ep: 73, steps: 8, D loss: 0.184877, acc:  73%, G loss: 2.931048\n",
      "Ep: 73, steps: 9, D loss: 0.247472, acc:  59%, G loss: 2.207495\n",
      "Ep: 73, steps: 10, D loss: 0.172449, acc:  80%, G loss: 2.362243\n",
      "Ep: 73, steps: 11, D loss: 0.267734, acc:  50%, G loss: 2.554993\n",
      "Ep: 73, steps: 12, D loss: 0.302098, acc:  37%, G loss: 2.013919\n",
      "Ep: 73, steps: 13, D loss: 0.253496, acc:  50%, G loss: 2.183321\n",
      "Ep: 73, steps: 14, D loss: 0.236724, acc:  58%, G loss: 2.198109\n",
      "Ep: 73, steps: 15, D loss: 0.223866, acc:  65%, G loss: 2.522644\n",
      "Ep: 73, steps: 16, D loss: 0.266233, acc:  51%, G loss: 2.295801\n",
      "Ep: 73, steps: 17, D loss: 0.154704, acc:  83%, G loss: 2.369048\n",
      "Ep: 73, steps: 18, D loss: 0.289890, acc:  40%, G loss: 2.318436\n",
      "Ep: 73, steps: 19, D loss: 0.265678, acc:  56%, G loss: 1.986540\n",
      "Ep: 73, steps: 20, D loss: 0.111254, acc:  95%, G loss: 2.611030\n",
      "Ep: 73, steps: 21, D loss: 0.239097, acc:  63%, G loss: 2.105554\n",
      "Ep: 73, steps: 22, D loss: 0.172198, acc:  79%, G loss: 2.262577\n",
      "Ep: 73, steps: 23, D loss: 0.207735, acc:  70%, G loss: 2.446468\n",
      "Ep: 73, steps: 24, D loss: 0.152052, acc:  82%, G loss: 2.357591\n",
      "Ep: 73, steps: 25, D loss: 0.247748, acc:  57%, G loss: 2.220564\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 74, steps: 1, D loss: 0.295274, acc:  46%, G loss: 2.254817\n",
      "Ep: 74, steps: 2, D loss: 0.260503, acc:  54%, G loss: 2.176412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 74, steps: 3, D loss: 0.160133, acc:  78%, G loss: 2.868057\n",
      "Ep: 74, steps: 4, D loss: 0.205299, acc:  71%, G loss: 2.151114\n",
      "Ep: 74, steps: 5, D loss: 0.241123, acc:  57%, G loss: 2.390763\n",
      "Ep: 74, steps: 6, D loss: 0.304516, acc:  45%, G loss: 2.169942\n",
      "Ep: 74, steps: 7, D loss: 0.319978, acc:  37%, G loss: 1.839016\n",
      "Ep: 74, steps: 8, D loss: 0.159000, acc:  84%, G loss: 2.722893\n",
      "Ep: 74, steps: 9, D loss: 0.248264, acc:  57%, G loss: 2.281103\n",
      "Ep: 74, steps: 10, D loss: 0.176003, acc:  80%, G loss: 2.349558\n",
      "Ep: 74, steps: 11, D loss: 0.281839, acc:  44%, G loss: 2.474859\n",
      "Ep: 74, steps: 12, D loss: 0.298376, acc:  38%, G loss: 1.921062\n",
      "Ep: 74, steps: 13, D loss: 0.241103, acc:  55%, G loss: 2.222339\n",
      "Ep: 74, steps: 14, D loss: 0.229977, acc:  57%, G loss: 2.098542\n",
      "Ep: 74, steps: 15, D loss: 0.217842, acc:  66%, G loss: 2.476445\n",
      "Ep: 74, steps: 16, D loss: 0.258890, acc:  51%, G loss: 2.298027\n",
      "Ep: 74, steps: 17, D loss: 0.150521, acc:  84%, G loss: 2.463140\n",
      "Ep: 74, steps: 18, D loss: 0.258226, acc:  51%, G loss: 2.336430\n",
      "Saved Model\n",
      "Ep: 74, steps: 19, D loss: 0.258396, acc:  57%, G loss: 2.030168\n",
      "Ep: 74, steps: 20, D loss: 0.226025, acc:  65%, G loss: 2.185745\n",
      "Ep: 74, steps: 21, D loss: 0.169814, acc:  82%, G loss: 2.186567\n",
      "Ep: 74, steps: 22, D loss: 0.217689, acc:  68%, G loss: 2.517529\n",
      "Ep: 74, steps: 23, D loss: 0.145189, acc:  82%, G loss: 2.392068\n",
      "Ep: 74, steps: 24, D loss: 0.180091, acc:  79%, G loss: 2.182396\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 75, steps: 1, D loss: 0.258406, acc:  58%, G loss: 2.181923\n",
      "Ep: 75, steps: 2, D loss: 0.245748, acc:  59%, G loss: 2.210697\n",
      "Ep: 75, steps: 3, D loss: 0.153888, acc:  77%, G loss: 2.667706\n",
      "Ep: 75, steps: 4, D loss: 0.163216, acc:  83%, G loss: 2.868319\n",
      "Ep: 75, steps: 5, D loss: 0.191209, acc:  67%, G loss: 2.811188\n",
      "Ep: 75, steps: 6, D loss: 0.288625, acc:  47%, G loss: 2.419055\n",
      "Ep: 75, steps: 7, D loss: 0.246363, acc:  57%, G loss: 2.199284\n",
      "Ep: 75, steps: 8, D loss: 0.159932, acc:  79%, G loss: 2.979274\n",
      "Ep: 75, steps: 9, D loss: 0.226270, acc:  65%, G loss: 2.378649\n",
      "Ep: 75, steps: 10, D loss: 0.158812, acc:  81%, G loss: 2.430924\n",
      "Ep: 75, steps: 11, D loss: 0.262487, acc:  52%, G loss: 2.686843\n",
      "Ep: 75, steps: 12, D loss: 0.288586, acc:  41%, G loss: 2.217479\n",
      "Ep: 75, steps: 13, D loss: 0.237289, acc:  56%, G loss: 2.320923\n",
      "Ep: 75, steps: 14, D loss: 0.224999, acc:  61%, G loss: 2.217284\n",
      "Ep: 75, steps: 15, D loss: 0.207833, acc:  68%, G loss: 2.897670\n",
      "Ep: 75, steps: 16, D loss: 0.254139, acc:  52%, G loss: 2.348131\n",
      "Ep: 75, steps: 17, D loss: 0.141673, acc:  84%, G loss: 2.546304\n",
      "Ep: 75, steps: 18, D loss: 0.288280, acc:  43%, G loss: 2.339387\n",
      "Ep: 75, steps: 19, D loss: 0.242604, acc:  60%, G loss: 2.134024\n",
      "Ep: 75, steps: 20, D loss: 0.108655, acc:  91%, G loss: 2.712275\n",
      "Ep: 75, steps: 21, D loss: 0.247791, acc:  60%, G loss: 2.174639\n",
      "Ep: 75, steps: 22, D loss: 0.175835, acc:  79%, G loss: 2.172719\n",
      "Ep: 75, steps: 23, D loss: 0.216165, acc:  64%, G loss: 2.490125\n",
      "Ep: 75, steps: 24, D loss: 0.146968, acc:  81%, G loss: 2.445317\n",
      "Ep: 75, steps: 25, D loss: 0.164320, acc:  81%, G loss: 2.236978\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 76, steps: 1, D loss: 0.276988, acc:  49%, G loss: 2.358319\n",
      "Ep: 76, steps: 2, D loss: 0.273237, acc:  51%, G loss: 2.292364\n",
      "Ep: 76, steps: 3, D loss: 0.215826, acc:  66%, G loss: 3.501299\n",
      "Ep: 76, steps: 4, D loss: 0.197497, acc:  70%, G loss: 2.361987\n",
      "Ep: 76, steps: 5, D loss: 0.231774, acc:  61%, G loss: 2.549191\n",
      "Ep: 76, steps: 6, D loss: 0.271879, acc:  51%, G loss: 2.399431\n",
      "Ep: 76, steps: 7, D loss: 0.300886, acc:  45%, G loss: 1.870629\n",
      "Ep: 76, steps: 8, D loss: 0.174101, acc:  78%, G loss: 2.804160\n",
      "Ep: 76, steps: 9, D loss: 0.231593, acc:  64%, G loss: 2.299965\n",
      "Ep: 76, steps: 10, D loss: 0.152855, acc:  82%, G loss: 2.515219\n",
      "Ep: 76, steps: 11, D loss: 0.272323, acc:  51%, G loss: 2.560839\n",
      "Ep: 76, steps: 12, D loss: 0.297207, acc:  41%, G loss: 2.034204\n",
      "Ep: 76, steps: 13, D loss: 0.245065, acc:  52%, G loss: 2.267635\n",
      "Ep: 76, steps: 14, D loss: 0.259364, acc:  51%, G loss: 2.145432\n",
      "Ep: 76, steps: 15, D loss: 0.216987, acc:  66%, G loss: 2.621299\n",
      "Ep: 76, steps: 16, D loss: 0.277364, acc:  47%, G loss: 2.334986\n",
      "Ep: 76, steps: 17, D loss: 0.148975, acc:  83%, G loss: 2.358780\n",
      "Ep: 76, steps: 18, D loss: 0.272467, acc:  48%, G loss: 2.539044\n",
      "Ep: 76, steps: 19, D loss: 0.243219, acc:  60%, G loss: 2.113101\n",
      "Ep: 76, steps: 20, D loss: 0.106545, acc:  92%, G loss: 2.764666\n",
      "Ep: 76, steps: 21, D loss: 0.257190, acc:  61%, G loss: 2.152224\n",
      "Ep: 76, steps: 22, D loss: 0.167376, acc:  82%, G loss: 2.320913\n",
      "Ep: 76, steps: 23, D loss: 0.221559, acc:  66%, G loss: 2.485933\n",
      "Ep: 76, steps: 24, D loss: 0.154190, acc:  81%, G loss: 2.390471\n",
      "Ep: 76, steps: 25, D loss: 0.194667, acc:  72%, G loss: 2.106317\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 77, steps: 1, D loss: 0.267675, acc:  52%, G loss: 2.198961\n",
      "Ep: 77, steps: 2, D loss: 0.263016, acc:  54%, G loss: 2.182661\n",
      "Ep: 77, steps: 3, D loss: 0.137306, acc:  86%, G loss: 2.419359\n",
      "Ep: 77, steps: 4, D loss: 0.304162, acc:  53%, G loss: 2.064849\n",
      "Ep: 77, steps: 5, D loss: 0.240642, acc:  54%, G loss: 2.546471\n",
      "Ep: 77, steps: 6, D loss: 0.318315, acc:  40%, G loss: 2.375606\n",
      "Ep: 77, steps: 7, D loss: 0.309561, acc:  38%, G loss: 2.017895\n",
      "Ep: 77, steps: 8, D loss: 0.190345, acc:  70%, G loss: 2.916966\n",
      "Ep: 77, steps: 9, D loss: 0.243943, acc:  58%, G loss: 2.277939\n",
      "Ep: 77, steps: 10, D loss: 0.157602, acc:  82%, G loss: 2.483717\n",
      "Ep: 77, steps: 11, D loss: 0.260978, acc:  54%, G loss: 2.542765\n",
      "Ep: 77, steps: 12, D loss: 0.309524, acc:  36%, G loss: 2.179631\n",
      "Ep: 77, steps: 13, D loss: 0.259740, acc:  48%, G loss: 2.237304\n",
      "Ep: 77, steps: 14, D loss: 0.230513, acc:  60%, G loss: 2.198153\n",
      "Ep: 77, steps: 15, D loss: 0.216647, acc:  65%, G loss: 2.732155\n",
      "Ep: 77, steps: 16, D loss: 0.257081, acc:  54%, G loss: 2.228463\n",
      "Saved Model\n",
      "Ep: 77, steps: 17, D loss: 0.128703, acc:  88%, G loss: 2.463434\n",
      "Ep: 77, steps: 18, D loss: 0.245584, acc:  60%, G loss: 2.114980\n",
      "Ep: 77, steps: 19, D loss: 0.112050, acc:  90%, G loss: 2.665518\n",
      "Ep: 77, steps: 20, D loss: 0.305838, acc:  54%, G loss: 2.267756\n",
      "Ep: 77, steps: 21, D loss: 0.222921, acc:  68%, G loss: 2.108873\n",
      "Ep: 77, steps: 22, D loss: 0.200356, acc:  72%, G loss: 2.558713\n",
      "Ep: 77, steps: 23, D loss: 0.155691, acc:  86%, G loss: 2.323008\n",
      "Ep: 77, steps: 24, D loss: 0.181154, acc:  76%, G loss: 2.151997\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 78, steps: 1, D loss: 0.273451, acc:  49%, G loss: 2.254067\n",
      "Ep: 78, steps: 2, D loss: 0.254437, acc:  55%, G loss: 2.124794\n",
      "Ep: 78, steps: 3, D loss: 0.136500, acc:  88%, G loss: 2.823617\n",
      "Ep: 78, steps: 4, D loss: 0.195475, acc:  72%, G loss: 2.130764\n",
      "Ep: 78, steps: 5, D loss: 0.212824, acc:  64%, G loss: 2.597427\n",
      "Ep: 78, steps: 6, D loss: 0.271121, acc:  50%, G loss: 2.165832\n",
      "Ep: 78, steps: 7, D loss: 0.205888, acc:  66%, G loss: 2.067493\n",
      "Ep: 78, steps: 8, D loss: 0.151928, acc:  78%, G loss: 3.150062\n",
      "Ep: 78, steps: 9, D loss: 0.273612, acc:  52%, G loss: 2.186282\n",
      "Ep: 78, steps: 10, D loss: 0.178353, acc:  74%, G loss: 2.205875\n",
      "Ep: 78, steps: 11, D loss: 0.281806, acc:  45%, G loss: 2.458478\n",
      "Ep: 78, steps: 12, D loss: 0.290583, acc:  42%, G loss: 2.153007\n",
      "Ep: 78, steps: 13, D loss: 0.237371, acc:  55%, G loss: 2.346195\n",
      "Ep: 78, steps: 14, D loss: 0.233063, acc:  62%, G loss: 2.163572\n",
      "Ep: 78, steps: 15, D loss: 0.237715, acc:  57%, G loss: 2.502198\n",
      "Ep: 78, steps: 16, D loss: 0.268844, acc:  49%, G loss: 2.237026\n",
      "Ep: 78, steps: 17, D loss: 0.149522, acc:  84%, G loss: 2.312827\n",
      "Ep: 78, steps: 18, D loss: 0.285742, acc:  43%, G loss: 2.510505\n",
      "Ep: 78, steps: 19, D loss: 0.257552, acc:  57%, G loss: 2.061969\n",
      "Ep: 78, steps: 20, D loss: 0.111122, acc:  91%, G loss: 2.783384\n",
      "Ep: 78, steps: 21, D loss: 0.235343, acc:  62%, G loss: 2.111751\n",
      "Ep: 78, steps: 22, D loss: 0.180954, acc:  78%, G loss: 2.249192\n",
      "Ep: 78, steps: 23, D loss: 0.232069, acc:  63%, G loss: 2.526722\n",
      "Ep: 78, steps: 24, D loss: 0.139389, acc:  86%, G loss: 2.431164\n",
      "Ep: 78, steps: 25, D loss: 0.190700, acc:  74%, G loss: 2.161481\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 79, steps: 1, D loss: 0.278890, acc:  47%, G loss: 2.196248\n",
      "Ep: 79, steps: 2, D loss: 0.274277, acc:  50%, G loss: 2.220782\n",
      "Ep: 79, steps: 3, D loss: 0.159786, acc:  79%, G loss: 2.901300\n",
      "Ep: 79, steps: 4, D loss: 0.199739, acc:  72%, G loss: 2.124045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 79, steps: 5, D loss: 0.226461, acc:  62%, G loss: 2.543180\n",
      "Ep: 79, steps: 6, D loss: 0.262461, acc:  51%, G loss: 2.250119\n",
      "Ep: 79, steps: 7, D loss: 0.235002, acc:  58%, G loss: 1.986294\n",
      "Ep: 79, steps: 8, D loss: 0.161785, acc:  79%, G loss: 2.940423\n",
      "Ep: 79, steps: 9, D loss: 0.235112, acc:  64%, G loss: 2.315462\n",
      "Ep: 79, steps: 10, D loss: 0.161570, acc:  76%, G loss: 2.423077\n",
      "Ep: 79, steps: 11, D loss: 0.269203, acc:  51%, G loss: 2.500882\n",
      "Ep: 79, steps: 12, D loss: 0.288780, acc:  42%, G loss: 2.077375\n",
      "Ep: 79, steps: 13, D loss: 0.242788, acc:  55%, G loss: 2.159212\n",
      "Ep: 79, steps: 14, D loss: 0.248172, acc:  55%, G loss: 2.190884\n",
      "Ep: 79, steps: 15, D loss: 0.224341, acc:  63%, G loss: 2.547589\n",
      "Ep: 79, steps: 16, D loss: 0.276897, acc:  48%, G loss: 2.319394\n",
      "Ep: 79, steps: 17, D loss: 0.153421, acc:  83%, G loss: 2.301777\n",
      "Ep: 79, steps: 18, D loss: 0.280143, acc:  45%, G loss: 2.498286\n",
      "Ep: 79, steps: 19, D loss: 0.260992, acc:  56%, G loss: 2.056725\n",
      "Ep: 79, steps: 20, D loss: 0.114869, acc:  90%, G loss: 2.670215\n",
      "Ep: 79, steps: 21, D loss: 0.233088, acc:  64%, G loss: 2.134960\n",
      "Ep: 79, steps: 22, D loss: 0.174856, acc:  79%, G loss: 2.234100\n",
      "Ep: 79, steps: 23, D loss: 0.254279, acc:  58%, G loss: 2.434594\n",
      "Ep: 79, steps: 24, D loss: 0.156339, acc:  82%, G loss: 2.367939\n",
      "Ep: 79, steps: 25, D loss: 0.313330, acc:  48%, G loss: 2.338624\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 80, steps: 1, D loss: 0.274229, acc:  50%, G loss: 2.230263\n",
      "Ep: 80, steps: 2, D loss: 0.254109, acc:  55%, G loss: 2.191602\n",
      "Ep: 80, steps: 3, D loss: 0.126845, acc:  92%, G loss: 2.870343\n",
      "Ep: 80, steps: 4, D loss: 0.210409, acc:  65%, G loss: 2.487918\n",
      "Ep: 80, steps: 5, D loss: 0.229333, acc:  60%, G loss: 2.363482\n",
      "Ep: 80, steps: 6, D loss: 0.290669, acc:  49%, G loss: 2.226507\n",
      "Ep: 80, steps: 7, D loss: 0.237064, acc:  57%, G loss: 1.976275\n",
      "Ep: 80, steps: 8, D loss: 0.156888, acc:  84%, G loss: 2.973840\n",
      "Ep: 80, steps: 9, D loss: 0.228015, acc:  65%, G loss: 2.281358\n",
      "Ep: 80, steps: 10, D loss: 0.157847, acc:  80%, G loss: 2.503024\n",
      "Ep: 80, steps: 11, D loss: 0.269332, acc:  50%, G loss: 2.531871\n",
      "Ep: 80, steps: 12, D loss: 0.294073, acc:  42%, G loss: 2.062860\n",
      "Ep: 80, steps: 13, D loss: 0.242156, acc:  55%, G loss: 2.312378\n",
      "Ep: 80, steps: 14, D loss: 0.232344, acc:  59%, G loss: 2.190517\n",
      "Saved Model\n",
      "Ep: 80, steps: 15, D loss: 0.226324, acc:  63%, G loss: 2.466900\n",
      "Ep: 80, steps: 16, D loss: 0.139742, acc:  85%, G loss: 2.500159\n",
      "Ep: 80, steps: 17, D loss: 0.319016, acc:  35%, G loss: 2.205973\n",
      "Ep: 80, steps: 18, D loss: 0.257950, acc:  58%, G loss: 2.074219\n",
      "Ep: 80, steps: 19, D loss: 0.103288, acc:  93%, G loss: 2.848773\n",
      "Ep: 80, steps: 20, D loss: 0.247189, acc:  60%, G loss: 2.116133\n",
      "Ep: 80, steps: 21, D loss: 0.168960, acc:  85%, G loss: 2.115151\n",
      "Ep: 80, steps: 22, D loss: 0.194903, acc:  74%, G loss: 2.470074\n",
      "Ep: 80, steps: 23, D loss: 0.136249, acc:  86%, G loss: 2.374725\n",
      "Ep: 80, steps: 24, D loss: 0.181373, acc:  78%, G loss: 2.230228\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 81, steps: 1, D loss: 0.267400, acc:  52%, G loss: 2.163483\n",
      "Ep: 81, steps: 2, D loss: 0.259641, acc:  54%, G loss: 2.156860\n",
      "Ep: 81, steps: 3, D loss: 0.142563, acc:  83%, G loss: 2.662253\n",
      "Ep: 81, steps: 4, D loss: 0.220797, acc:  62%, G loss: 2.347116\n",
      "Ep: 81, steps: 5, D loss: 0.235978, acc:  60%, G loss: 2.650986\n",
      "Ep: 81, steps: 6, D loss: 0.290260, acc:  49%, G loss: 2.321208\n",
      "Ep: 81, steps: 7, D loss: 0.274121, acc:  49%, G loss: 1.924762\n",
      "Ep: 81, steps: 8, D loss: 0.173573, acc:  78%, G loss: 3.050397\n",
      "Ep: 81, steps: 9, D loss: 0.229886, acc:  64%, G loss: 2.267394\n",
      "Ep: 81, steps: 10, D loss: 0.164086, acc:  78%, G loss: 2.461097\n",
      "Ep: 81, steps: 11, D loss: 0.263169, acc:  53%, G loss: 2.626240\n",
      "Ep: 81, steps: 12, D loss: 0.298012, acc:  40%, G loss: 2.113008\n",
      "Ep: 81, steps: 13, D loss: 0.254044, acc:  52%, G loss: 2.401244\n",
      "Ep: 81, steps: 14, D loss: 0.233020, acc:  56%, G loss: 2.314231\n",
      "Ep: 81, steps: 15, D loss: 0.246924, acc:  52%, G loss: 2.474423\n",
      "Ep: 81, steps: 16, D loss: 0.258854, acc:  52%, G loss: 2.285710\n",
      "Ep: 81, steps: 17, D loss: 0.142975, acc:  85%, G loss: 2.372993\n",
      "Ep: 81, steps: 18, D loss: 0.270787, acc:  49%, G loss: 2.343227\n",
      "Ep: 81, steps: 19, D loss: 0.261062, acc:  54%, G loss: 2.059553\n",
      "Ep: 81, steps: 20, D loss: 0.102821, acc:  95%, G loss: 2.778701\n",
      "Ep: 81, steps: 21, D loss: 0.231926, acc:  64%, G loss: 2.173968\n",
      "Ep: 81, steps: 22, D loss: 0.155962, acc:  84%, G loss: 2.154044\n",
      "Ep: 81, steps: 23, D loss: 0.228161, acc:  63%, G loss: 2.432225\n",
      "Ep: 81, steps: 24, D loss: 0.153083, acc:  82%, G loss: 2.309490\n",
      "Ep: 81, steps: 25, D loss: 0.194722, acc:  72%, G loss: 2.134726\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 82, steps: 1, D loss: 0.291179, acc:  47%, G loss: 2.216002\n",
      "Ep: 82, steps: 2, D loss: 0.269799, acc:  53%, G loss: 2.175196\n",
      "Ep: 82, steps: 3, D loss: 0.132672, acc:  83%, G loss: 2.717162\n",
      "Ep: 82, steps: 4, D loss: 0.234421, acc:  58%, G loss: 2.082830\n",
      "Ep: 82, steps: 5, D loss: 0.251823, acc:  51%, G loss: 2.626054\n",
      "Ep: 82, steps: 6, D loss: 0.323905, acc:  44%, G loss: 2.283128\n",
      "Ep: 82, steps: 7, D loss: 0.266513, acc:  47%, G loss: 1.928034\n",
      "Ep: 82, steps: 8, D loss: 0.148516, acc:  84%, G loss: 2.721400\n",
      "Ep: 82, steps: 9, D loss: 0.259980, acc:  54%, G loss: 2.204582\n",
      "Ep: 82, steps: 10, D loss: 0.178785, acc:  74%, G loss: 2.465770\n",
      "Ep: 82, steps: 11, D loss: 0.272236, acc:  47%, G loss: 2.525501\n",
      "Ep: 82, steps: 12, D loss: 0.283421, acc:  42%, G loss: 2.064500\n",
      "Ep: 82, steps: 13, D loss: 0.245489, acc:  52%, G loss: 2.200672\n",
      "Ep: 82, steps: 14, D loss: 0.227976, acc:  65%, G loss: 2.070187\n",
      "Ep: 82, steps: 15, D loss: 0.228152, acc:  60%, G loss: 2.548871\n",
      "Ep: 82, steps: 16, D loss: 0.281086, acc:  45%, G loss: 2.279111\n",
      "Ep: 82, steps: 17, D loss: 0.135395, acc:  87%, G loss: 2.472879\n",
      "Ep: 82, steps: 18, D loss: 0.293194, acc:  41%, G loss: 2.176372\n",
      "Ep: 82, steps: 19, D loss: 0.240240, acc:  59%, G loss: 2.109064\n",
      "Ep: 82, steps: 20, D loss: 0.115032, acc:  90%, G loss: 2.631441\n",
      "Ep: 82, steps: 21, D loss: 0.264616, acc:  58%, G loss: 2.095062\n",
      "Ep: 82, steps: 22, D loss: 0.183787, acc:  78%, G loss: 2.190854\n",
      "Ep: 82, steps: 23, D loss: 0.211281, acc:  67%, G loss: 2.563398\n",
      "Ep: 82, steps: 24, D loss: 0.149472, acc:  82%, G loss: 2.387975\n",
      "Ep: 82, steps: 25, D loss: 0.179665, acc:  79%, G loss: 2.204309\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 83, steps: 1, D loss: 0.261396, acc:  54%, G loss: 2.152520\n",
      "Ep: 83, steps: 2, D loss: 0.249857, acc:  57%, G loss: 2.212524\n",
      "Ep: 83, steps: 3, D loss: 0.121264, acc:  89%, G loss: 2.479422\n",
      "Ep: 83, steps: 4, D loss: 0.229639, acc:  60%, G loss: 2.343899\n",
      "Ep: 83, steps: 5, D loss: 0.228647, acc:  60%, G loss: 2.480373\n",
      "Ep: 83, steps: 6, D loss: 0.283931, acc:  51%, G loss: 2.474712\n",
      "Ep: 83, steps: 7, D loss: 0.282825, acc:  49%, G loss: 1.883579\n",
      "Ep: 83, steps: 8, D loss: 0.185016, acc:  74%, G loss: 2.751814\n",
      "Ep: 83, steps: 9, D loss: 0.235512, acc:  62%, G loss: 2.212770\n",
      "Ep: 83, steps: 10, D loss: 0.159295, acc:  81%, G loss: 2.369743\n",
      "Ep: 83, steps: 11, D loss: 0.281814, acc:  46%, G loss: 2.502674\n",
      "Ep: 83, steps: 12, D loss: 0.301354, acc:  38%, G loss: 2.016490\n",
      "Saved Model\n",
      "Ep: 83, steps: 13, D loss: 0.247321, acc:  54%, G loss: 2.273658\n",
      "Ep: 83, steps: 14, D loss: 0.223576, acc:  62%, G loss: 2.452752\n",
      "Ep: 83, steps: 15, D loss: 0.251237, acc:  56%, G loss: 2.397261\n",
      "Ep: 83, steps: 16, D loss: 0.161100, acc:  81%, G loss: 2.297785\n",
      "Ep: 83, steps: 17, D loss: 0.244264, acc:  56%, G loss: 2.330068\n",
      "Ep: 83, steps: 18, D loss: 0.230590, acc:  64%, G loss: 2.059636\n",
      "Ep: 83, steps: 19, D loss: 0.100214, acc:  92%, G loss: 2.787725\n",
      "Ep: 83, steps: 20, D loss: 0.244354, acc:  63%, G loss: 2.071099\n",
      "Ep: 83, steps: 21, D loss: 0.169934, acc:  81%, G loss: 2.098732\n",
      "Ep: 83, steps: 22, D loss: 0.203638, acc:  71%, G loss: 2.511922\n",
      "Ep: 83, steps: 23, D loss: 0.145799, acc:  83%, G loss: 2.309291\n",
      "Ep: 83, steps: 24, D loss: 0.184036, acc:  77%, G loss: 2.187707\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 84, steps: 1, D loss: 0.272082, acc:  51%, G loss: 2.256084\n",
      "Ep: 84, steps: 2, D loss: 0.262558, acc:  54%, G loss: 2.469352\n",
      "Ep: 84, steps: 3, D loss: 0.120431, acc:  90%, G loss: 2.715158\n",
      "Ep: 84, steps: 4, D loss: 0.180324, acc:  73%, G loss: 2.129851\n",
      "Ep: 84, steps: 5, D loss: 0.222789, acc:  63%, G loss: 2.444430\n",
      "Ep: 84, steps: 6, D loss: 0.271597, acc:  49%, G loss: 2.161610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 84, steps: 7, D loss: 0.241479, acc:  58%, G loss: 1.912293\n",
      "Ep: 84, steps: 8, D loss: 0.161312, acc:  79%, G loss: 2.926019\n",
      "Ep: 84, steps: 9, D loss: 0.255036, acc:  58%, G loss: 2.235355\n",
      "Ep: 84, steps: 10, D loss: 0.162399, acc:  77%, G loss: 2.428479\n",
      "Ep: 84, steps: 11, D loss: 0.270531, acc:  50%, G loss: 2.640434\n",
      "Ep: 84, steps: 12, D loss: 0.303620, acc:  39%, G loss: 2.158698\n",
      "Ep: 84, steps: 13, D loss: 0.252707, acc:  52%, G loss: 2.302880\n",
      "Ep: 84, steps: 14, D loss: 0.244678, acc:  54%, G loss: 2.224295\n",
      "Ep: 84, steps: 15, D loss: 0.227553, acc:  60%, G loss: 2.409642\n",
      "Ep: 84, steps: 16, D loss: 0.265080, acc:  48%, G loss: 2.306656\n",
      "Ep: 84, steps: 17, D loss: 0.132477, acc:  87%, G loss: 2.454000\n",
      "Ep: 84, steps: 18, D loss: 0.300100, acc:  40%, G loss: 2.332258\n",
      "Ep: 84, steps: 19, D loss: 0.250719, acc:  58%, G loss: 2.064817\n",
      "Ep: 84, steps: 20, D loss: 0.115271, acc:  90%, G loss: 2.798068\n",
      "Ep: 84, steps: 21, D loss: 0.245340, acc:  60%, G loss: 2.073834\n",
      "Ep: 84, steps: 22, D loss: 0.169062, acc:  80%, G loss: 2.173536\n",
      "Ep: 84, steps: 23, D loss: 0.200125, acc:  70%, G loss: 2.655357\n",
      "Ep: 84, steps: 24, D loss: 0.150509, acc:  81%, G loss: 2.344508\n",
      "Ep: 84, steps: 25, D loss: 0.221385, acc:  64%, G loss: 2.293645\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 85, steps: 1, D loss: 0.285863, acc:  49%, G loss: 2.221441\n",
      "Ep: 85, steps: 2, D loss: 0.292759, acc:  47%, G loss: 2.239565\n",
      "Ep: 85, steps: 3, D loss: 0.131561, acc:  90%, G loss: 2.742924\n",
      "Ep: 85, steps: 4, D loss: 0.239914, acc:  59%, G loss: 2.204422\n",
      "Ep: 85, steps: 5, D loss: 0.226202, acc:  59%, G loss: 2.451335\n",
      "Ep: 85, steps: 6, D loss: 0.308362, acc:  47%, G loss: 2.187808\n",
      "Ep: 85, steps: 7, D loss: 0.289947, acc:  42%, G loss: 1.773483\n",
      "Ep: 85, steps: 8, D loss: 0.161883, acc:  83%, G loss: 2.620755\n",
      "Ep: 85, steps: 9, D loss: 0.226208, acc:  65%, G loss: 2.259568\n",
      "Ep: 85, steps: 10, D loss: 0.147820, acc:  83%, G loss: 2.297257\n",
      "Ep: 85, steps: 11, D loss: 0.272422, acc:  48%, G loss: 2.486872\n",
      "Ep: 85, steps: 12, D loss: 0.294185, acc:  44%, G loss: 2.012306\n",
      "Ep: 85, steps: 13, D loss: 0.252051, acc:  52%, G loss: 2.099161\n",
      "Ep: 85, steps: 14, D loss: 0.249290, acc:  52%, G loss: 2.221476\n",
      "Ep: 85, steps: 15, D loss: 0.225026, acc:  64%, G loss: 2.415557\n",
      "Ep: 85, steps: 16, D loss: 0.266830, acc:  49%, G loss: 2.310769\n",
      "Ep: 85, steps: 17, D loss: 0.149132, acc:  84%, G loss: 2.338565\n",
      "Ep: 85, steps: 18, D loss: 0.283948, acc:  46%, G loss: 2.497222\n",
      "Ep: 85, steps: 19, D loss: 0.241407, acc:  60%, G loss: 2.079270\n",
      "Ep: 85, steps: 20, D loss: 0.116708, acc:  90%, G loss: 2.638412\n",
      "Ep: 85, steps: 21, D loss: 0.252080, acc:  60%, G loss: 2.060358\n",
      "Ep: 85, steps: 22, D loss: 0.172474, acc:  80%, G loss: 2.245620\n",
      "Ep: 85, steps: 23, D loss: 0.236380, acc:  60%, G loss: 2.466538\n",
      "Ep: 85, steps: 24, D loss: 0.156095, acc:  79%, G loss: 2.307203\n",
      "Ep: 85, steps: 25, D loss: 0.296409, acc:  52%, G loss: 2.174603\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 86, steps: 1, D loss: 0.331598, acc:  35%, G loss: 2.239812\n",
      "Ep: 86, steps: 2, D loss: 0.256194, acc:  54%, G loss: 2.190210\n",
      "Ep: 86, steps: 3, D loss: 0.165479, acc:  79%, G loss: 2.832022\n",
      "Ep: 86, steps: 4, D loss: 0.217349, acc:  65%, G loss: 2.096358\n",
      "Ep: 86, steps: 5, D loss: 0.224453, acc:  60%, G loss: 2.409135\n",
      "Ep: 86, steps: 6, D loss: 0.287888, acc:  48%, G loss: 2.117947\n",
      "Ep: 86, steps: 7, D loss: 0.296846, acc:  44%, G loss: 1.961008\n",
      "Ep: 86, steps: 8, D loss: 0.166668, acc:  82%, G loss: 2.866823\n",
      "Ep: 86, steps: 9, D loss: 0.233891, acc:  63%, G loss: 2.338264\n",
      "Ep: 86, steps: 10, D loss: 0.154451, acc:  78%, G loss: 2.618346\n",
      "Saved Model\n",
      "Ep: 86, steps: 11, D loss: 0.283522, acc:  45%, G loss: 2.621542\n",
      "Ep: 86, steps: 12, D loss: 0.274170, acc:  44%, G loss: 2.072332\n",
      "Ep: 86, steps: 13, D loss: 0.258097, acc:  50%, G loss: 2.158071\n",
      "Ep: 86, steps: 14, D loss: 0.216545, acc:  68%, G loss: 2.422379\n",
      "Ep: 86, steps: 15, D loss: 0.278491, acc:  46%, G loss: 2.322953\n",
      "Ep: 86, steps: 16, D loss: 0.143601, acc:  84%, G loss: 2.388004\n",
      "Ep: 86, steps: 17, D loss: 0.297854, acc:  40%, G loss: 2.409293\n",
      "Ep: 86, steps: 18, D loss: 0.248099, acc:  59%, G loss: 2.124765\n",
      "Ep: 86, steps: 19, D loss: 0.109595, acc:  93%, G loss: 2.788476\n",
      "Ep: 86, steps: 20, D loss: 0.262775, acc:  60%, G loss: 2.027824\n",
      "Ep: 86, steps: 21, D loss: 0.163789, acc:  81%, G loss: 2.096068\n",
      "Ep: 86, steps: 22, D loss: 0.215906, acc:  66%, G loss: 2.386111\n",
      "Ep: 86, steps: 23, D loss: 0.170493, acc:  79%, G loss: 2.164727\n",
      "Ep: 86, steps: 24, D loss: 0.200146, acc:  71%, G loss: 2.203026\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 87, steps: 1, D loss: 0.274627, acc:  52%, G loss: 2.187366\n",
      "Ep: 87, steps: 2, D loss: 0.246372, acc:  59%, G loss: 2.170629\n",
      "Ep: 87, steps: 3, D loss: 0.114054, acc:  94%, G loss: 2.631150\n",
      "Ep: 87, steps: 4, D loss: 0.199795, acc:  66%, G loss: 2.150532\n",
      "Ep: 87, steps: 5, D loss: 0.208674, acc:  64%, G loss: 2.584273\n",
      "Ep: 87, steps: 6, D loss: 0.262859, acc:  51%, G loss: 2.348595\n",
      "Ep: 87, steps: 7, D loss: 0.291119, acc:  41%, G loss: 2.023122\n",
      "Ep: 87, steps: 8, D loss: 0.150620, acc:  81%, G loss: 2.802490\n",
      "Ep: 87, steps: 9, D loss: 0.233017, acc:  63%, G loss: 2.270178\n",
      "Ep: 87, steps: 10, D loss: 0.159926, acc:  78%, G loss: 2.422416\n",
      "Ep: 87, steps: 11, D loss: 0.268114, acc:  53%, G loss: 2.565219\n",
      "Ep: 87, steps: 12, D loss: 0.295754, acc:  44%, G loss: 2.038906\n",
      "Ep: 87, steps: 13, D loss: 0.257637, acc:  52%, G loss: 2.278739\n",
      "Ep: 87, steps: 14, D loss: 0.257763, acc:  51%, G loss: 2.181760\n",
      "Ep: 87, steps: 15, D loss: 0.234647, acc:  61%, G loss: 2.467484\n",
      "Ep: 87, steps: 16, D loss: 0.260735, acc:  51%, G loss: 2.339514\n",
      "Ep: 87, steps: 17, D loss: 0.149039, acc:  83%, G loss: 2.300081\n",
      "Ep: 87, steps: 18, D loss: 0.290377, acc:  42%, G loss: 2.356202\n",
      "Ep: 87, steps: 19, D loss: 0.244291, acc:  60%, G loss: 2.045958\n",
      "Ep: 87, steps: 20, D loss: 0.129466, acc:  89%, G loss: 2.733016\n",
      "Ep: 87, steps: 21, D loss: 0.243013, acc:  62%, G loss: 2.064967\n",
      "Ep: 87, steps: 22, D loss: 0.166559, acc:  83%, G loss: 2.097933\n",
      "Ep: 87, steps: 23, D loss: 0.216885, acc:  68%, G loss: 2.460092\n",
      "Ep: 87, steps: 24, D loss: 0.166967, acc:  80%, G loss: 2.276070\n",
      "Ep: 87, steps: 25, D loss: 0.214304, acc:  67%, G loss: 2.226755\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 88, steps: 1, D loss: 0.268997, acc:  55%, G loss: 2.206387\n",
      "Ep: 88, steps: 2, D loss: 0.238186, acc:  60%, G loss: 2.303990\n",
      "Ep: 88, steps: 3, D loss: 0.145397, acc:  80%, G loss: 2.736445\n",
      "Ep: 88, steps: 4, D loss: 0.208310, acc:  67%, G loss: 2.232214\n",
      "Ep: 88, steps: 5, D loss: 0.194222, acc:  68%, G loss: 2.477143\n",
      "Ep: 88, steps: 6, D loss: 0.295422, acc:  49%, G loss: 2.232144\n",
      "Ep: 88, steps: 7, D loss: 0.322015, acc:  39%, G loss: 1.819809\n",
      "Ep: 88, steps: 8, D loss: 0.178752, acc:  77%, G loss: 2.906410\n",
      "Ep: 88, steps: 9, D loss: 0.233335, acc:  63%, G loss: 2.187596\n",
      "Ep: 88, steps: 10, D loss: 0.141911, acc:  84%, G loss: 2.405374\n",
      "Ep: 88, steps: 11, D loss: 0.285278, acc:  46%, G loss: 2.433740\n",
      "Ep: 88, steps: 12, D loss: 0.309275, acc:  38%, G loss: 1.878602\n",
      "Ep: 88, steps: 13, D loss: 0.268396, acc:  47%, G loss: 2.150021\n",
      "Ep: 88, steps: 14, D loss: 0.229408, acc:  59%, G loss: 2.183289\n",
      "Ep: 88, steps: 15, D loss: 0.255439, acc:  52%, G loss: 2.325634\n",
      "Ep: 88, steps: 16, D loss: 0.270746, acc:  48%, G loss: 2.296773\n",
      "Ep: 88, steps: 17, D loss: 0.155560, acc:  83%, G loss: 2.275708\n",
      "Ep: 88, steps: 18, D loss: 0.292053, acc:  41%, G loss: 2.228446\n",
      "Ep: 88, steps: 19, D loss: 0.261689, acc:  53%, G loss: 1.972598\n",
      "Ep: 88, steps: 20, D loss: 0.121354, acc:  92%, G loss: 2.727212\n",
      "Ep: 88, steps: 21, D loss: 0.246320, acc:  61%, G loss: 2.015276\n",
      "Ep: 88, steps: 22, D loss: 0.146603, acc:  86%, G loss: 2.183329\n",
      "Ep: 88, steps: 23, D loss: 0.240184, acc:  57%, G loss: 2.568107\n",
      "Ep: 88, steps: 24, D loss: 0.175087, acc:  76%, G loss: 2.343966\n",
      "Ep: 88, steps: 25, D loss: 0.190602, acc:  75%, G loss: 2.234630\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 89, steps: 1, D loss: 0.266834, acc:  54%, G loss: 2.152478\n",
      "Ep: 89, steps: 2, D loss: 0.244646, acc:  59%, G loss: 2.189096\n",
      "Ep: 89, steps: 3, D loss: 0.137731, acc:  82%, G loss: 2.641585\n",
      "Ep: 89, steps: 4, D loss: 0.218895, acc:  65%, G loss: 2.239136\n",
      "Ep: 89, steps: 5, D loss: 0.201069, acc:  65%, G loss: 2.464090\n",
      "Ep: 89, steps: 6, D loss: 0.306755, acc:  48%, G loss: 2.184115\n",
      "Ep: 89, steps: 7, D loss: 0.351298, acc:  31%, G loss: 1.704875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 89, steps: 8, D loss: 0.175931, acc:  79%, G loss: 2.948531\n",
      "Saved Model\n",
      "Ep: 89, steps: 9, D loss: 0.210276, acc:  72%, G loss: 2.224399\n",
      "Ep: 89, steps: 10, D loss: 0.282925, acc:  44%, G loss: 2.368204\n",
      "Ep: 89, steps: 11, D loss: 0.296339, acc:  39%, G loss: 1.901874\n",
      "Ep: 89, steps: 12, D loss: 0.256150, acc:  49%, G loss: 2.140023\n",
      "Ep: 89, steps: 13, D loss: 0.237662, acc:  56%, G loss: 2.215246\n",
      "Ep: 89, steps: 14, D loss: 0.250959, acc:  52%, G loss: 2.453475\n",
      "Ep: 89, steps: 15, D loss: 0.258834, acc:  52%, G loss: 2.333461\n",
      "Ep: 89, steps: 16, D loss: 0.147437, acc:  85%, G loss: 2.218221\n",
      "Ep: 89, steps: 17, D loss: 0.295591, acc:  39%, G loss: 2.518585\n",
      "Ep: 89, steps: 18, D loss: 0.257135, acc:  52%, G loss: 1.986150\n",
      "Ep: 89, steps: 19, D loss: 0.119626, acc:  93%, G loss: 2.560502\n",
      "Ep: 89, steps: 20, D loss: 0.246436, acc:  62%, G loss: 2.140546\n",
      "Ep: 89, steps: 21, D loss: 0.149874, acc:  87%, G loss: 2.007014\n",
      "Ep: 89, steps: 22, D loss: 0.197261, acc:  72%, G loss: 2.509017\n",
      "Ep: 89, steps: 23, D loss: 0.164719, acc:  78%, G loss: 2.216213\n",
      "Ep: 89, steps: 24, D loss: 0.207464, acc:  67%, G loss: 2.294456\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 90, steps: 1, D loss: 0.287078, acc:  49%, G loss: 2.196075\n",
      "Ep: 90, steps: 2, D loss: 0.245759, acc:  58%, G loss: 2.278651\n",
      "Ep: 90, steps: 3, D loss: 0.145136, acc:  81%, G loss: 2.748614\n",
      "Ep: 90, steps: 4, D loss: 0.197458, acc:  69%, G loss: 2.121151\n",
      "Ep: 90, steps: 5, D loss: 0.208677, acc:  64%, G loss: 2.464629\n",
      "Ep: 90, steps: 6, D loss: 0.296358, acc:  47%, G loss: 2.027019\n",
      "Ep: 90, steps: 7, D loss: 0.282040, acc:  46%, G loss: 1.807940\n",
      "Ep: 90, steps: 8, D loss: 0.166150, acc:  80%, G loss: 2.895066\n",
      "Ep: 90, steps: 9, D loss: 0.211868, acc:  70%, G loss: 2.181056\n",
      "Ep: 90, steps: 10, D loss: 0.157450, acc:  79%, G loss: 2.451728\n",
      "Ep: 90, steps: 11, D loss: 0.277286, acc:  50%, G loss: 2.522415\n",
      "Ep: 90, steps: 12, D loss: 0.324846, acc:  36%, G loss: 1.879180\n",
      "Ep: 90, steps: 13, D loss: 0.278908, acc:  45%, G loss: 2.218816\n",
      "Ep: 90, steps: 14, D loss: 0.249986, acc:  52%, G loss: 2.115679\n",
      "Ep: 90, steps: 15, D loss: 0.246086, acc:  58%, G loss: 2.317165\n",
      "Ep: 90, steps: 16, D loss: 0.266206, acc:  48%, G loss: 2.318511\n",
      "Ep: 90, steps: 17, D loss: 0.156173, acc:  84%, G loss: 2.244720\n",
      "Ep: 90, steps: 18, D loss: 0.309410, acc:  38%, G loss: 2.404247\n",
      "Ep: 90, steps: 19, D loss: 0.237615, acc:  62%, G loss: 2.017267\n",
      "Ep: 90, steps: 20, D loss: 0.120867, acc:  92%, G loss: 2.664349\n",
      "Ep: 90, steps: 21, D loss: 0.257640, acc:  58%, G loss: 2.147099\n",
      "Ep: 90, steps: 22, D loss: 0.169239, acc:  81%, G loss: 2.109715\n",
      "Ep: 90, steps: 23, D loss: 0.221447, acc:  63%, G loss: 2.492427\n",
      "Ep: 90, steps: 24, D loss: 0.199628, acc:  68%, G loss: 2.558370\n",
      "Ep: 90, steps: 25, D loss: 0.187854, acc:  77%, G loss: 2.376333\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 91, steps: 1, D loss: 0.232170, acc:  63%, G loss: 2.325871\n",
      "Ep: 91, steps: 2, D loss: 0.231654, acc:  62%, G loss: 2.322125\n",
      "Ep: 91, steps: 3, D loss: 0.131574, acc:  83%, G loss: 2.585195\n",
      "Ep: 91, steps: 4, D loss: 0.214238, acc:  67%, G loss: 2.253784\n",
      "Ep: 91, steps: 5, D loss: 0.204009, acc:  68%, G loss: 2.478326\n",
      "Ep: 91, steps: 6, D loss: 0.298837, acc:  50%, G loss: 2.295892\n",
      "Ep: 91, steps: 7, D loss: 0.323058, acc:  40%, G loss: 1.722066\n",
      "Ep: 91, steps: 8, D loss: 0.194295, acc:  70%, G loss: 3.281893\n",
      "Ep: 91, steps: 9, D loss: 0.196546, acc:  73%, G loss: 2.260146\n",
      "Ep: 91, steps: 10, D loss: 0.148339, acc:  86%, G loss: 2.385600\n",
      "Ep: 91, steps: 11, D loss: 0.262064, acc:  53%, G loss: 2.486309\n",
      "Ep: 91, steps: 12, D loss: 0.318052, acc:  33%, G loss: 1.915523\n",
      "Ep: 91, steps: 13, D loss: 0.282110, acc:  44%, G loss: 2.209660\n",
      "Ep: 91, steps: 14, D loss: 0.219171, acc:  62%, G loss: 2.274445\n",
      "Ep: 91, steps: 15, D loss: 0.264799, acc:  48%, G loss: 2.273358\n",
      "Ep: 91, steps: 16, D loss: 0.247105, acc:  55%, G loss: 2.285634\n",
      "Ep: 91, steps: 17, D loss: 0.161217, acc:  82%, G loss: 2.220982\n",
      "Ep: 91, steps: 18, D loss: 0.293743, acc:  40%, G loss: 2.387516\n",
      "Ep: 91, steps: 19, D loss: 0.261591, acc:  50%, G loss: 1.946892\n",
      "Ep: 91, steps: 20, D loss: 0.117990, acc:  93%, G loss: 2.717158\n",
      "Ep: 91, steps: 21, D loss: 0.235749, acc:  62%, G loss: 2.208925\n",
      "Ep: 91, steps: 22, D loss: 0.153255, acc:  87%, G loss: 2.102143\n",
      "Ep: 91, steps: 23, D loss: 0.206810, acc:  70%, G loss: 2.534484\n",
      "Ep: 91, steps: 24, D loss: 0.138079, acc:  85%, G loss: 2.343478\n",
      "Ep: 91, steps: 25, D loss: 0.194131, acc:  72%, G loss: 2.374248\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 92, steps: 1, D loss: 0.273890, acc:  53%, G loss: 2.266770\n",
      "Ep: 92, steps: 2, D loss: 0.246939, acc:  58%, G loss: 2.192486\n",
      "Ep: 92, steps: 3, D loss: 0.144572, acc:  80%, G loss: 2.677331\n",
      "Ep: 92, steps: 4, D loss: 0.208525, acc:  64%, G loss: 2.060628\n",
      "Ep: 92, steps: 5, D loss: 0.195745, acc:  69%, G loss: 2.460183\n",
      "Ep: 92, steps: 6, D loss: 0.276241, acc:  50%, G loss: 2.246605\n",
      "Saved Model\n",
      "Ep: 92, steps: 7, D loss: 0.299752, acc:  42%, G loss: 1.675499\n",
      "Ep: 92, steps: 8, D loss: 0.192042, acc:  73%, G loss: 2.200441\n",
      "Ep: 92, steps: 9, D loss: 0.162937, acc:  79%, G loss: 2.681625\n",
      "Ep: 92, steps: 10, D loss: 0.259317, acc:  54%, G loss: 2.578986\n",
      "Ep: 92, steps: 11, D loss: 0.316093, acc:  37%, G loss: 1.920177\n",
      "Ep: 92, steps: 12, D loss: 0.270953, acc:  48%, G loss: 2.139229\n",
      "Ep: 92, steps: 13, D loss: 0.250583, acc:  49%, G loss: 2.167245\n",
      "Ep: 92, steps: 14, D loss: 0.253745, acc:  52%, G loss: 2.385004\n",
      "Ep: 92, steps: 15, D loss: 0.237800, acc:  58%, G loss: 2.280720\n",
      "Ep: 92, steps: 16, D loss: 0.150633, acc:  83%, G loss: 2.200145\n",
      "Ep: 92, steps: 17, D loss: 0.294274, acc:  44%, G loss: 2.343844\n",
      "Ep: 92, steps: 18, D loss: 0.227762, acc:  61%, G loss: 2.016077\n",
      "Ep: 92, steps: 19, D loss: 0.114303, acc:  91%, G loss: 2.739309\n",
      "Ep: 92, steps: 20, D loss: 0.257173, acc:  59%, G loss: 2.141853\n",
      "Ep: 92, steps: 21, D loss: 0.167939, acc:  79%, G loss: 2.126913\n",
      "Ep: 92, steps: 22, D loss: 0.232036, acc:  60%, G loss: 2.722640\n",
      "Ep: 92, steps: 23, D loss: 0.143909, acc:  83%, G loss: 2.613529\n",
      "Ep: 92, steps: 24, D loss: 0.188147, acc:  75%, G loss: 2.336610\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 93, steps: 1, D loss: 0.251312, acc:  59%, G loss: 2.120147\n",
      "Ep: 93, steps: 2, D loss: 0.242750, acc:  60%, G loss: 2.204645\n",
      "Ep: 93, steps: 3, D loss: 0.128259, acc:  84%, G loss: 2.518681\n",
      "Ep: 93, steps: 4, D loss: 0.213781, acc:  66%, G loss: 2.260229\n",
      "Ep: 93, steps: 5, D loss: 0.196690, acc:  68%, G loss: 2.393937\n",
      "Ep: 93, steps: 6, D loss: 0.286947, acc:  49%, G loss: 2.123348\n",
      "Ep: 93, steps: 7, D loss: 0.338814, acc:  33%, G loss: 1.809478\n",
      "Ep: 93, steps: 8, D loss: 0.178654, acc:  76%, G loss: 3.127357\n",
      "Ep: 93, steps: 9, D loss: 0.203877, acc:  72%, G loss: 2.263852\n",
      "Ep: 93, steps: 10, D loss: 0.158753, acc:  80%, G loss: 2.338542\n",
      "Ep: 93, steps: 11, D loss: 0.275465, acc:  49%, G loss: 2.510333\n",
      "Ep: 93, steps: 12, D loss: 0.312050, acc:  37%, G loss: 2.000857\n",
      "Ep: 93, steps: 13, D loss: 0.264637, acc:  48%, G loss: 2.101449\n",
      "Ep: 93, steps: 14, D loss: 0.249275, acc:  52%, G loss: 2.228610\n",
      "Ep: 93, steps: 15, D loss: 0.245085, acc:  56%, G loss: 2.316068\n",
      "Ep: 93, steps: 16, D loss: 0.263609, acc:  49%, G loss: 2.214478\n",
      "Ep: 93, steps: 17, D loss: 0.152951, acc:  83%, G loss: 2.206459\n",
      "Ep: 93, steps: 18, D loss: 0.302527, acc:  41%, G loss: 2.633622\n",
      "Ep: 93, steps: 19, D loss: 0.229483, acc:  62%, G loss: 2.011885\n",
      "Ep: 93, steps: 20, D loss: 0.122254, acc:  90%, G loss: 2.691723\n",
      "Ep: 93, steps: 21, D loss: 0.238775, acc:  60%, G loss: 2.152622\n",
      "Ep: 93, steps: 22, D loss: 0.167834, acc:  81%, G loss: 2.186149\n",
      "Ep: 93, steps: 23, D loss: 0.176196, acc:  77%, G loss: 2.589999\n",
      "Ep: 93, steps: 24, D loss: 0.139569, acc:  84%, G loss: 2.339791\n",
      "Ep: 93, steps: 25, D loss: 0.215091, acc:  66%, G loss: 2.238294\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 94, steps: 1, D loss: 0.265294, acc:  54%, G loss: 2.497443\n",
      "Ep: 94, steps: 2, D loss: 0.242462, acc:  60%, G loss: 2.110684\n",
      "Ep: 94, steps: 3, D loss: 0.144699, acc:  80%, G loss: 2.678591\n",
      "Ep: 94, steps: 4, D loss: 0.198353, acc:  67%, G loss: 2.212320\n",
      "Ep: 94, steps: 5, D loss: 0.197793, acc:  67%, G loss: 2.472279\n",
      "Ep: 94, steps: 6, D loss: 0.287092, acc:  49%, G loss: 2.191588\n",
      "Ep: 94, steps: 7, D loss: 0.325522, acc:  36%, G loss: 1.885388\n",
      "Ep: 94, steps: 8, D loss: 0.163332, acc:  81%, G loss: 2.804061\n",
      "Ep: 94, steps: 9, D loss: 0.201359, acc:  72%, G loss: 2.235975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 94, steps: 10, D loss: 0.155883, acc:  78%, G loss: 2.455741\n",
      "Ep: 94, steps: 11, D loss: 0.249331, acc:  58%, G loss: 2.594383\n",
      "Ep: 94, steps: 12, D loss: 0.327740, acc:  33%, G loss: 1.876362\n",
      "Ep: 94, steps: 13, D loss: 0.274781, acc:  45%, G loss: 2.225988\n",
      "Ep: 94, steps: 14, D loss: 0.247254, acc:  52%, G loss: 2.174587\n",
      "Ep: 94, steps: 15, D loss: 0.235089, acc:  59%, G loss: 2.266581\n",
      "Ep: 94, steps: 16, D loss: 0.271875, acc:  47%, G loss: 2.312086\n",
      "Ep: 94, steps: 17, D loss: 0.180959, acc:  77%, G loss: 2.303315\n",
      "Ep: 94, steps: 18, D loss: 0.286374, acc:  43%, G loss: 2.298715\n",
      "Ep: 94, steps: 19, D loss: 0.232488, acc:  62%, G loss: 2.045280\n",
      "Ep: 94, steps: 20, D loss: 0.100137, acc:  95%, G loss: 2.714483\n",
      "Ep: 94, steps: 21, D loss: 0.241928, acc:  59%, G loss: 2.158676\n",
      "Ep: 94, steps: 22, D loss: 0.171997, acc:  81%, G loss: 2.110701\n",
      "Ep: 94, steps: 23, D loss: 0.196156, acc:  72%, G loss: 2.467432\n",
      "Ep: 94, steps: 24, D loss: 0.143133, acc:  85%, G loss: 2.203344\n",
      "Ep: 94, steps: 25, D loss: 0.205801, acc:  71%, G loss: 2.160874\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 95, steps: 1, D loss: 0.265295, acc:  53%, G loss: 2.149731\n",
      "Ep: 95, steps: 2, D loss: 0.252681, acc:  56%, G loss: 2.081539\n",
      "Ep: 95, steps: 3, D loss: 0.161551, acc:  72%, G loss: 2.572813\n",
      "Saved Model\n",
      "Ep: 95, steps: 4, D loss: 0.232265, acc:  65%, G loss: 2.318573\n",
      "Ep: 95, steps: 5, D loss: 0.267374, acc:  50%, G loss: 2.130895\n",
      "Ep: 95, steps: 6, D loss: 0.410448, acc:  22%, G loss: 1.651472\n",
      "Ep: 95, steps: 7, D loss: 0.191923, acc:  71%, G loss: 2.875509\n",
      "Ep: 95, steps: 8, D loss: 0.194010, acc:  76%, G loss: 2.287976\n",
      "Ep: 95, steps: 9, D loss: 0.150300, acc:  81%, G loss: 2.409820\n",
      "Ep: 95, steps: 10, D loss: 0.249308, acc:  57%, G loss: 2.569172\n",
      "Ep: 95, steps: 11, D loss: 0.323380, acc:  33%, G loss: 1.874658\n",
      "Ep: 95, steps: 12, D loss: 0.292697, acc:  39%, G loss: 2.098329\n",
      "Ep: 95, steps: 13, D loss: 0.261037, acc:  49%, G loss: 2.152783\n",
      "Ep: 95, steps: 14, D loss: 0.242004, acc:  57%, G loss: 2.230089\n",
      "Ep: 95, steps: 15, D loss: 0.251277, acc:  54%, G loss: 2.261737\n",
      "Ep: 95, steps: 16, D loss: 0.162148, acc:  80%, G loss: 2.168554\n",
      "Ep: 95, steps: 17, D loss: 0.310347, acc:  38%, G loss: 2.641994\n",
      "Ep: 95, steps: 18, D loss: 0.230675, acc:  62%, G loss: 2.043350\n",
      "Ep: 95, steps: 19, D loss: 0.114434, acc:  94%, G loss: 2.809547\n",
      "Ep: 95, steps: 20, D loss: 0.252743, acc:  58%, G loss: 2.082986\n",
      "Ep: 95, steps: 21, D loss: 0.164342, acc:  79%, G loss: 2.163961\n",
      "Ep: 95, steps: 22, D loss: 0.195959, acc:  73%, G loss: 2.565314\n",
      "Ep: 95, steps: 23, D loss: 0.142200, acc:  85%, G loss: 2.567087\n",
      "Ep: 95, steps: 24, D loss: 0.211585, acc:  69%, G loss: 2.358644\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 96, steps: 1, D loss: 0.263039, acc:  54%, G loss: 2.159141\n",
      "Ep: 96, steps: 2, D loss: 0.249354, acc:  56%, G loss: 2.139970\n",
      "Ep: 96, steps: 3, D loss: 0.139539, acc:  83%, G loss: 2.630652\n",
      "Ep: 96, steps: 4, D loss: 0.214835, acc:  65%, G loss: 2.146861\n",
      "Ep: 96, steps: 5, D loss: 0.189139, acc:  70%, G loss: 2.484622\n",
      "Ep: 96, steps: 6, D loss: 0.273412, acc:  51%, G loss: 2.097729\n",
      "Ep: 96, steps: 7, D loss: 0.344376, acc:  32%, G loss: 1.751623\n",
      "Ep: 96, steps: 8, D loss: 0.182558, acc:  74%, G loss: 2.657506\n",
      "Ep: 96, steps: 9, D loss: 0.207803, acc:  69%, G loss: 2.239907\n",
      "Ep: 96, steps: 10, D loss: 0.169057, acc:  80%, G loss: 2.375963\n",
      "Ep: 96, steps: 11, D loss: 0.242407, acc:  59%, G loss: 2.504995\n",
      "Ep: 96, steps: 12, D loss: 0.327818, acc:  33%, G loss: 1.828363\n",
      "Ep: 96, steps: 13, D loss: 0.273220, acc:  44%, G loss: 2.099711\n",
      "Ep: 96, steps: 14, D loss: 0.256938, acc:  49%, G loss: 2.127871\n",
      "Ep: 96, steps: 15, D loss: 0.234899, acc:  59%, G loss: 2.161682\n",
      "Ep: 96, steps: 16, D loss: 0.263387, acc:  51%, G loss: 2.223133\n",
      "Ep: 96, steps: 17, D loss: 0.195357, acc:  73%, G loss: 2.325803\n",
      "Ep: 96, steps: 18, D loss: 0.293042, acc:  41%, G loss: 2.281638\n",
      "Ep: 96, steps: 19, D loss: 0.233754, acc:  62%, G loss: 2.029275\n",
      "Ep: 96, steps: 20, D loss: 0.106301, acc:  94%, G loss: 2.722587\n",
      "Ep: 96, steps: 21, D loss: 0.249165, acc:  60%, G loss: 2.181041\n",
      "Ep: 96, steps: 22, D loss: 0.155784, acc:  83%, G loss: 2.164097\n",
      "Ep: 96, steps: 23, D loss: 0.180635, acc:  77%, G loss: 2.555031\n",
      "Ep: 96, steps: 24, D loss: 0.154473, acc:  82%, G loss: 2.385587\n",
      "Ep: 96, steps: 25, D loss: 0.202761, acc:  72%, G loss: 2.198969\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 97, steps: 1, D loss: 0.259063, acc:  57%, G loss: 2.275240\n",
      "Ep: 97, steps: 2, D loss: 0.249878, acc:  59%, G loss: 2.079272\n",
      "Ep: 97, steps: 3, D loss: 0.137118, acc:  84%, G loss: 2.510046\n",
      "Ep: 97, steps: 4, D loss: 0.212315, acc:  64%, G loss: 2.176772\n",
      "Ep: 97, steps: 5, D loss: 0.195343, acc:  70%, G loss: 2.563037\n",
      "Ep: 97, steps: 6, D loss: 0.278104, acc:  49%, G loss: 2.089326\n",
      "Ep: 97, steps: 7, D loss: 0.321815, acc:  36%, G loss: 1.739687\n",
      "Ep: 97, steps: 8, D loss: 0.177056, acc:  76%, G loss: 3.000180\n",
      "Ep: 97, steps: 9, D loss: 0.211292, acc:  70%, G loss: 2.194660\n",
      "Ep: 97, steps: 10, D loss: 0.147441, acc:  84%, G loss: 2.274993\n",
      "Ep: 97, steps: 11, D loss: 0.257725, acc:  54%, G loss: 2.514180\n",
      "Ep: 97, steps: 12, D loss: 0.332790, acc:  31%, G loss: 1.882949\n",
      "Ep: 97, steps: 13, D loss: 0.284428, acc:  42%, G loss: 2.013622\n",
      "Ep: 97, steps: 14, D loss: 0.244300, acc:  54%, G loss: 2.141152\n",
      "Ep: 97, steps: 15, D loss: 0.238112, acc:  58%, G loss: 2.186313\n",
      "Ep: 97, steps: 16, D loss: 0.244202, acc:  54%, G loss: 2.262375\n",
      "Ep: 97, steps: 17, D loss: 0.148674, acc:  84%, G loss: 2.213145\n",
      "Ep: 97, steps: 18, D loss: 0.301826, acc:  43%, G loss: 2.339907\n",
      "Ep: 97, steps: 19, D loss: 0.237200, acc:  60%, G loss: 1.988217\n",
      "Ep: 97, steps: 20, D loss: 0.117686, acc:  91%, G loss: 2.677508\n",
      "Ep: 97, steps: 21, D loss: 0.249859, acc:  59%, G loss: 2.256896\n",
      "Ep: 97, steps: 22, D loss: 0.156126, acc:  82%, G loss: 2.107757\n",
      "Ep: 97, steps: 23, D loss: 0.204661, acc:  70%, G loss: 2.522258\n",
      "Ep: 97, steps: 24, D loss: 0.136717, acc:  86%, G loss: 2.433188\n",
      "Ep: 97, steps: 25, D loss: 0.195879, acc:  74%, G loss: 2.308675\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 98, steps: 1, D loss: 0.256757, acc:  59%, G loss: 2.469332\n",
      "Saved Model\n",
      "Ep: 98, steps: 2, D loss: 0.249723, acc:  56%, G loss: 2.105131\n",
      "Ep: 98, steps: 3, D loss: 0.213882, acc:  63%, G loss: 2.377465\n",
      "Ep: 98, steps: 4, D loss: 0.198071, acc:  67%, G loss: 2.490625\n",
      "Ep: 98, steps: 5, D loss: 0.270422, acc:  50%, G loss: 2.266567\n",
      "Ep: 98, steps: 6, D loss: 0.279477, acc:  42%, G loss: 1.978935\n",
      "Ep: 98, steps: 7, D loss: 0.170124, acc:  78%, G loss: 2.847785\n",
      "Ep: 98, steps: 8, D loss: 0.225258, acc:  66%, G loss: 2.286155\n",
      "Ep: 98, steps: 9, D loss: 0.157729, acc:  80%, G loss: 2.322238\n",
      "Ep: 98, steps: 10, D loss: 0.251848, acc:  53%, G loss: 2.576750\n",
      "Ep: 98, steps: 11, D loss: 0.316242, acc:  35%, G loss: 1.858174\n",
      "Ep: 98, steps: 12, D loss: 0.276577, acc:  44%, G loss: 2.159048\n",
      "Ep: 98, steps: 13, D loss: 0.259730, acc:  49%, G loss: 2.156796\n",
      "Ep: 98, steps: 14, D loss: 0.241101, acc:  57%, G loss: 2.179906\n",
      "Ep: 98, steps: 15, D loss: 0.244243, acc:  55%, G loss: 2.203149\n",
      "Ep: 98, steps: 16, D loss: 0.175166, acc:  79%, G loss: 2.136824\n",
      "Ep: 98, steps: 17, D loss: 0.293207, acc:  43%, G loss: 2.381634\n",
      "Ep: 98, steps: 18, D loss: 0.245915, acc:  56%, G loss: 2.021472\n",
      "Ep: 98, steps: 19, D loss: 0.111619, acc:  95%, G loss: 2.600405\n",
      "Ep: 98, steps: 20, D loss: 0.267804, acc:  58%, G loss: 2.110193\n",
      "Ep: 98, steps: 21, D loss: 0.160261, acc:  81%, G loss: 1.975674\n",
      "Ep: 98, steps: 22, D loss: 0.207204, acc:  69%, G loss: 2.431450\n",
      "Ep: 98, steps: 23, D loss: 0.154342, acc:  82%, G loss: 2.402354\n",
      "Ep: 98, steps: 24, D loss: 0.220428, acc:  65%, G loss: 2.285667\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 99, steps: 1, D loss: 0.254008, acc:  59%, G loss: 2.167882\n",
      "Ep: 99, steps: 2, D loss: 0.240576, acc:  59%, G loss: 2.167988\n",
      "Ep: 99, steps: 3, D loss: 0.158736, acc:  75%, G loss: 2.450812\n",
      "Ep: 99, steps: 4, D loss: 0.231464, acc:  64%, G loss: 2.110350\n",
      "Ep: 99, steps: 5, D loss: 0.193026, acc:  71%, G loss: 2.455204\n",
      "Ep: 99, steps: 6, D loss: 0.291302, acc:  48%, G loss: 2.151037\n",
      "Ep: 99, steps: 7, D loss: 0.344772, acc:  31%, G loss: 1.839612\n",
      "Ep: 99, steps: 8, D loss: 0.183165, acc:  76%, G loss: 2.917712\n",
      "Ep: 99, steps: 9, D loss: 0.200033, acc:  74%, G loss: 2.211377\n",
      "Ep: 99, steps: 10, D loss: 0.144448, acc:  81%, G loss: 2.317708\n",
      "Ep: 99, steps: 11, D loss: 0.254327, acc:  55%, G loss: 2.459472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 99, steps: 12, D loss: 0.332895, acc:  30%, G loss: 1.789086\n",
      "Ep: 99, steps: 13, D loss: 0.279604, acc:  44%, G loss: 1.958233\n",
      "Ep: 99, steps: 14, D loss: 0.244922, acc:  53%, G loss: 2.152780\n",
      "Ep: 99, steps: 15, D loss: 0.244703, acc:  57%, G loss: 2.324281\n",
      "Ep: 99, steps: 16, D loss: 0.236933, acc:  58%, G loss: 2.186853\n",
      "Ep: 99, steps: 17, D loss: 0.142548, acc:  86%, G loss: 2.110327\n",
      "Ep: 99, steps: 18, D loss: 0.302162, acc:  40%, G loss: 2.482629\n",
      "Ep: 99, steps: 19, D loss: 0.241703, acc:  58%, G loss: 2.044180\n",
      "Ep: 99, steps: 20, D loss: 0.115627, acc:  93%, G loss: 2.620597\n",
      "Ep: 99, steps: 21, D loss: 0.257780, acc:  57%, G loss: 2.122044\n",
      "Ep: 99, steps: 22, D loss: 0.158809, acc:  83%, G loss: 2.275685\n",
      "Ep: 99, steps: 23, D loss: 0.200204, acc:  71%, G loss: 2.518693\n",
      "Ep: 99, steps: 24, D loss: 0.156428, acc:  84%, G loss: 2.238938\n",
      "Ep: 99, steps: 25, D loss: 0.212339, acc:  68%, G loss: 2.239301\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 100, steps: 1, D loss: 0.238329, acc:  64%, G loss: 2.127387\n",
      "Ep: 100, steps: 2, D loss: 0.228867, acc:  63%, G loss: 2.198143\n",
      "Ep: 100, steps: 3, D loss: 0.159658, acc:  74%, G loss: 2.369005\n",
      "Ep: 100, steps: 4, D loss: 0.234394, acc:  66%, G loss: 2.127861\n",
      "Ep: 100, steps: 5, D loss: 0.206352, acc:  66%, G loss: 2.446190\n",
      "Ep: 100, steps: 6, D loss: 0.277893, acc:  50%, G loss: 2.094765\n",
      "Ep: 100, steps: 7, D loss: 0.340168, acc:  34%, G loss: 1.676726\n",
      "Ep: 100, steps: 8, D loss: 0.207503, acc:  65%, G loss: 2.849073\n",
      "Ep: 100, steps: 9, D loss: 0.211876, acc:  68%, G loss: 2.213873\n",
      "Ep: 100, steps: 10, D loss: 0.173362, acc:  80%, G loss: 2.306930\n",
      "Ep: 100, steps: 11, D loss: 0.272304, acc:  52%, G loss: 2.490112\n",
      "Ep: 100, steps: 12, D loss: 0.322314, acc:  31%, G loss: 1.814330\n",
      "Ep: 100, steps: 13, D loss: 0.281632, acc:  43%, G loss: 2.085581\n",
      "Ep: 100, steps: 14, D loss: 0.247941, acc:  51%, G loss: 2.184381\n",
      "Ep: 100, steps: 15, D loss: 0.238508, acc:  59%, G loss: 2.290001\n",
      "Ep: 100, steps: 16, D loss: 0.241367, acc:  58%, G loss: 2.218317\n",
      "Ep: 100, steps: 17, D loss: 0.235483, acc:  66%, G loss: 2.208577\n",
      "Ep: 100, steps: 18, D loss: 0.304016, acc:  37%, G loss: 2.365583\n",
      "Ep: 100, steps: 19, D loss: 0.232906, acc:  61%, G loss: 2.079143\n",
      "Ep: 100, steps: 20, D loss: 0.118168, acc:  94%, G loss: 2.617361\n",
      "Ep: 100, steps: 21, D loss: 0.246682, acc:  56%, G loss: 2.227548\n",
      "Ep: 100, steps: 22, D loss: 0.157133, acc:  83%, G loss: 2.163980\n",
      "Ep: 100, steps: 23, D loss: 0.175488, acc:  78%, G loss: 2.608898\n",
      "Saved Model\n",
      "Ep: 100, steps: 24, D loss: 0.146195, acc:  84%, G loss: 2.453447\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 101, steps: 1, D loss: 0.259510, acc:  60%, G loss: 2.623461\n",
      "Ep: 101, steps: 2, D loss: 0.213718, acc:  69%, G loss: 2.256924\n",
      "Ep: 101, steps: 3, D loss: 0.194114, acc:  65%, G loss: 2.787789\n",
      "Ep: 101, steps: 4, D loss: 0.186257, acc:  72%, G loss: 2.182779\n",
      "Ep: 101, steps: 5, D loss: 0.195634, acc:  70%, G loss: 2.437973\n",
      "Ep: 101, steps: 6, D loss: 0.280040, acc:  50%, G loss: 2.016655\n",
      "Ep: 101, steps: 7, D loss: 0.352002, acc:  31%, G loss: 1.767005\n",
      "Ep: 101, steps: 8, D loss: 0.194485, acc:  72%, G loss: 2.743883\n",
      "Ep: 101, steps: 9, D loss: 0.206816, acc:  71%, G loss: 2.149788\n",
      "Ep: 101, steps: 10, D loss: 0.149136, acc:  84%, G loss: 2.163314\n",
      "Ep: 101, steps: 11, D loss: 0.246968, acc:  57%, G loss: 2.495186\n",
      "Ep: 101, steps: 12, D loss: 0.340397, acc:  25%, G loss: 1.749360\n",
      "Ep: 101, steps: 13, D loss: 0.288755, acc:  39%, G loss: 1.981280\n",
      "Ep: 101, steps: 14, D loss: 0.243587, acc:  53%, G loss: 2.156240\n",
      "Ep: 101, steps: 15, D loss: 0.249257, acc:  51%, G loss: 2.178083\n",
      "Ep: 101, steps: 16, D loss: 0.234551, acc:  59%, G loss: 2.179748\n",
      "Ep: 101, steps: 17, D loss: 0.141336, acc:  87%, G loss: 2.233508\n",
      "Ep: 101, steps: 18, D loss: 0.307437, acc:  38%, G loss: 2.186117\n",
      "Ep: 101, steps: 19, D loss: 0.252765, acc:  52%, G loss: 2.007984\n",
      "Ep: 101, steps: 20, D loss: 0.127193, acc:  91%, G loss: 2.492355\n",
      "Ep: 101, steps: 21, D loss: 0.247817, acc:  58%, G loss: 2.336716\n",
      "Ep: 101, steps: 22, D loss: 0.162030, acc:  81%, G loss: 2.078654\n",
      "Ep: 101, steps: 23, D loss: 0.216158, acc:  66%, G loss: 3.455558\n",
      "Ep: 101, steps: 24, D loss: 0.145365, acc:  85%, G loss: 2.789171\n",
      "Ep: 101, steps: 25, D loss: 0.197700, acc:  71%, G loss: 2.491371\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 102, steps: 1, D loss: 0.225888, acc:  66%, G loss: 2.694487\n",
      "Ep: 102, steps: 2, D loss: 0.239607, acc:  59%, G loss: 2.449832\n",
      "Ep: 102, steps: 3, D loss: 0.127208, acc:  88%, G loss: 2.834762\n",
      "Ep: 102, steps: 4, D loss: 0.160242, acc:  82%, G loss: 2.364200\n",
      "Ep: 102, steps: 5, D loss: 0.169746, acc:  77%, G loss: 2.539930\n",
      "Ep: 102, steps: 6, D loss: 0.256364, acc:  51%, G loss: 2.201703\n",
      "Ep: 102, steps: 7, D loss: 0.278774, acc:  50%, G loss: 1.803130\n",
      "Ep: 102, steps: 8, D loss: 0.165092, acc:  78%, G loss: 2.886919\n",
      "Ep: 102, steps: 9, D loss: 0.216335, acc:  68%, G loss: 2.190751\n",
      "Ep: 102, steps: 10, D loss: 0.147864, acc:  82%, G loss: 2.390682\n",
      "Ep: 102, steps: 11, D loss: 0.242435, acc:  58%, G loss: 2.580143\n",
      "Ep: 102, steps: 12, D loss: 0.311421, acc:  36%, G loss: 1.987274\n",
      "Ep: 102, steps: 13, D loss: 0.273436, acc:  46%, G loss: 2.146275\n",
      "Ep: 102, steps: 14, D loss: 0.234060, acc:  54%, G loss: 2.252470\n",
      "Ep: 102, steps: 15, D loss: 0.245497, acc:  56%, G loss: 2.325897\n",
      "Ep: 102, steps: 16, D loss: 0.221663, acc:  63%, G loss: 2.333413\n",
      "Ep: 102, steps: 17, D loss: 0.126653, acc:  88%, G loss: 2.306202\n",
      "Ep: 102, steps: 18, D loss: 0.387984, acc:  32%, G loss: 2.218711\n",
      "Ep: 102, steps: 19, D loss: 0.252207, acc:  56%, G loss: 2.058261\n",
      "Ep: 102, steps: 20, D loss: 0.101758, acc:  96%, G loss: 2.610048\n",
      "Ep: 102, steps: 21, D loss: 0.239990, acc:  57%, G loss: 2.310204\n",
      "Ep: 102, steps: 22, D loss: 0.156679, acc:  81%, G loss: 2.075345\n",
      "Ep: 102, steps: 23, D loss: 0.182503, acc:  76%, G loss: 2.561115\n",
      "Ep: 102, steps: 24, D loss: 0.147529, acc:  85%, G loss: 2.391548\n",
      "Ep: 102, steps: 25, D loss: 0.177777, acc:  78%, G loss: 2.301962\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 103, steps: 1, D loss: 0.277644, acc:  51%, G loss: 2.360046\n",
      "Ep: 103, steps: 2, D loss: 0.229035, acc:  62%, G loss: 2.156404\n",
      "Ep: 103, steps: 3, D loss: 0.143920, acc:  79%, G loss: 2.687576\n",
      "Ep: 103, steps: 4, D loss: 0.205236, acc:  66%, G loss: 2.163876\n",
      "Ep: 103, steps: 5, D loss: 0.176370, acc:  74%, G loss: 2.548160\n",
      "Ep: 103, steps: 6, D loss: 0.265463, acc:  50%, G loss: 2.287493\n",
      "Ep: 103, steps: 7, D loss: 0.311133, acc:  42%, G loss: 1.849216\n",
      "Ep: 103, steps: 8, D loss: 0.177834, acc:  77%, G loss: 2.682015\n",
      "Ep: 103, steps: 9, D loss: 0.239588, acc:  61%, G loss: 2.159924\n",
      "Ep: 103, steps: 10, D loss: 0.159876, acc:  80%, G loss: 2.285630\n",
      "Ep: 103, steps: 11, D loss: 0.263994, acc:  51%, G loss: 2.553768\n",
      "Ep: 103, steps: 12, D loss: 0.327834, acc:  35%, G loss: 1.946266\n",
      "Ep: 103, steps: 13, D loss: 0.280782, acc:  44%, G loss: 2.045796\n",
      "Ep: 103, steps: 14, D loss: 0.238607, acc:  54%, G loss: 2.191355\n",
      "Ep: 103, steps: 15, D loss: 0.241248, acc:  56%, G loss: 2.253814\n",
      "Ep: 103, steps: 16, D loss: 0.232855, acc:  60%, G loss: 2.151342\n",
      "Ep: 103, steps: 17, D loss: 0.148276, acc:  85%, G loss: 2.063659\n",
      "Ep: 103, steps: 18, D loss: 0.312410, acc:  36%, G loss: 2.225619\n",
      "Ep: 103, steps: 19, D loss: 0.264041, acc:  49%, G loss: 2.028278\n",
      "Ep: 103, steps: 20, D loss: 0.115462, acc:  94%, G loss: 2.594723\n",
      "Ep: 103, steps: 21, D loss: 0.236426, acc:  60%, G loss: 2.135113\n",
      "Saved Model\n",
      "Ep: 103, steps: 22, D loss: 0.161598, acc:  82%, G loss: 2.217970\n",
      "Ep: 103, steps: 23, D loss: 0.170556, acc:  78%, G loss: 2.128921\n",
      "Ep: 103, steps: 24, D loss: 0.223752, acc:  65%, G loss: 2.225036\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 104, steps: 1, D loss: 0.279300, acc:  50%, G loss: 2.202901\n",
      "Ep: 104, steps: 2, D loss: 0.231314, acc:  62%, G loss: 2.115626\n",
      "Ep: 104, steps: 3, D loss: 0.151188, acc:  77%, G loss: 2.539877\n",
      "Ep: 104, steps: 4, D loss: 0.215802, acc:  62%, G loss: 2.039769\n",
      "Ep: 104, steps: 5, D loss: 0.183027, acc:  74%, G loss: 2.427352\n",
      "Ep: 104, steps: 6, D loss: 0.270252, acc:  49%, G loss: 2.220130\n",
      "Ep: 104, steps: 7, D loss: 0.346833, acc:  36%, G loss: 1.729797\n",
      "Ep: 104, steps: 8, D loss: 0.203306, acc:  69%, G loss: 2.717515\n",
      "Ep: 104, steps: 9, D loss: 0.222100, acc:  66%, G loss: 2.154529\n",
      "Ep: 104, steps: 10, D loss: 0.169359, acc:  76%, G loss: 2.250892\n",
      "Ep: 104, steps: 11, D loss: 0.263614, acc:  53%, G loss: 2.431040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 104, steps: 12, D loss: 0.324382, acc:  33%, G loss: 1.853230\n",
      "Ep: 104, steps: 13, D loss: 0.273493, acc:  45%, G loss: 2.054553\n",
      "Ep: 104, steps: 14, D loss: 0.238470, acc:  55%, G loss: 2.136642\n",
      "Ep: 104, steps: 15, D loss: 0.243856, acc:  54%, G loss: 2.169986\n",
      "Ep: 104, steps: 16, D loss: 0.253069, acc:  53%, G loss: 2.193805\n",
      "Ep: 104, steps: 17, D loss: 0.180681, acc:  73%, G loss: 1.994342\n",
      "Ep: 104, steps: 18, D loss: 0.295463, acc:  41%, G loss: 2.176058\n",
      "Ep: 104, steps: 19, D loss: 0.256111, acc:  51%, G loss: 2.033628\n",
      "Ep: 104, steps: 20, D loss: 0.120672, acc:  93%, G loss: 2.543108\n",
      "Ep: 104, steps: 21, D loss: 0.246288, acc:  59%, G loss: 2.125150\n",
      "Ep: 104, steps: 22, D loss: 0.157595, acc:  84%, G loss: 2.082629\n",
      "Ep: 104, steps: 23, D loss: 0.193085, acc:  73%, G loss: 2.518937\n",
      "Ep: 104, steps: 24, D loss: 0.156941, acc:  83%, G loss: 2.216752\n",
      "Ep: 104, steps: 25, D loss: 0.204012, acc:  70%, G loss: 2.198579\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 105, steps: 1, D loss: 0.257437, acc:  59%, G loss: 2.316898\n",
      "Ep: 105, steps: 2, D loss: 0.240172, acc:  60%, G loss: 2.118728\n",
      "Ep: 105, steps: 3, D loss: 0.170118, acc:  72%, G loss: 2.476178\n",
      "Ep: 105, steps: 4, D loss: 0.210397, acc:  66%, G loss: 2.176390\n",
      "Ep: 105, steps: 5, D loss: 0.189171, acc:  69%, G loss: 2.452491\n",
      "Ep: 105, steps: 6, D loss: 0.272662, acc:  51%, G loss: 2.117285\n",
      "Ep: 105, steps: 7, D loss: 0.408931, acc:  21%, G loss: 2.699358\n",
      "Ep: 105, steps: 8, D loss: 0.209590, acc:  64%, G loss: 2.805820\n",
      "Ep: 105, steps: 9, D loss: 0.219080, acc:  67%, G loss: 2.221761\n",
      "Ep: 105, steps: 10, D loss: 0.151074, acc:  86%, G loss: 2.308151\n",
      "Ep: 105, steps: 11, D loss: 0.256683, acc:  55%, G loss: 2.523427\n",
      "Ep: 105, steps: 12, D loss: 0.317856, acc:  33%, G loss: 1.893209\n",
      "Ep: 105, steps: 13, D loss: 0.264727, acc:  47%, G loss: 2.117971\n",
      "Ep: 105, steps: 14, D loss: 0.225027, acc:  59%, G loss: 2.242265\n",
      "Ep: 105, steps: 15, D loss: 0.232793, acc:  59%, G loss: 2.251576\n",
      "Ep: 105, steps: 16, D loss: 0.229925, acc:  60%, G loss: 2.222565\n",
      "Ep: 105, steps: 17, D loss: 0.115271, acc:  90%, G loss: 2.274953\n",
      "Ep: 105, steps: 18, D loss: 0.319741, acc:  37%, G loss: 2.373248\n",
      "Ep: 105, steps: 19, D loss: 0.262728, acc:  51%, G loss: 2.099924\n",
      "Ep: 105, steps: 20, D loss: 0.106587, acc:  95%, G loss: 2.711891\n",
      "Ep: 105, steps: 21, D loss: 0.235489, acc:  57%, G loss: 2.166825\n",
      "Ep: 105, steps: 22, D loss: 0.152178, acc:  83%, G loss: 2.295137\n",
      "Ep: 105, steps: 23, D loss: 0.185526, acc:  75%, G loss: 2.571987\n",
      "Ep: 105, steps: 24, D loss: 0.158519, acc:  83%, G loss: 2.285481\n",
      "Ep: 105, steps: 25, D loss: 0.187640, acc:  76%, G loss: 2.288715\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 106, steps: 1, D loss: 0.267796, acc:  54%, G loss: 2.420544\n",
      "Ep: 106, steps: 2, D loss: 0.247339, acc:  58%, G loss: 2.112249\n",
      "Ep: 106, steps: 3, D loss: 0.157969, acc:  77%, G loss: 2.587073\n",
      "Ep: 106, steps: 4, D loss: 0.199257, acc:  69%, G loss: 2.163680\n",
      "Ep: 106, steps: 5, D loss: 0.204759, acc:  67%, G loss: 2.409247\n",
      "Ep: 106, steps: 6, D loss: 0.281551, acc:  49%, G loss: 2.221518\n",
      "Ep: 106, steps: 7, D loss: 0.331278, acc:  34%, G loss: 1.723962\n",
      "Ep: 106, steps: 8, D loss: 0.176136, acc:  78%, G loss: 2.895807\n",
      "Ep: 106, steps: 9, D loss: 0.209801, acc:  69%, G loss: 2.126248\n",
      "Ep: 106, steps: 10, D loss: 0.148056, acc:  83%, G loss: 2.222381\n",
      "Ep: 106, steps: 11, D loss: 0.253300, acc:  53%, G loss: 2.454149\n",
      "Ep: 106, steps: 12, D loss: 0.324945, acc:  29%, G loss: 1.797250\n",
      "Ep: 106, steps: 13, D loss: 0.283963, acc:  43%, G loss: 2.017861\n",
      "Ep: 106, steps: 14, D loss: 0.244509, acc:  54%, G loss: 2.156558\n",
      "Ep: 106, steps: 15, D loss: 0.242173, acc:  57%, G loss: 2.225930\n",
      "Ep: 106, steps: 16, D loss: 0.236624, acc:  57%, G loss: 2.200658\n",
      "Ep: 106, steps: 17, D loss: 0.147361, acc:  84%, G loss: 2.155721\n",
      "Ep: 106, steps: 18, D loss: 0.287915, acc:  43%, G loss: 2.345359\n",
      "Ep: 106, steps: 19, D loss: 0.247129, acc:  57%, G loss: 2.055211\n",
      "Saved Model\n",
      "Ep: 106, steps: 20, D loss: 0.124018, acc:  93%, G loss: 2.664350\n",
      "Ep: 106, steps: 21, D loss: 0.160240, acc:  82%, G loss: 2.297488\n",
      "Ep: 106, steps: 22, D loss: 0.161473, acc:  81%, G loss: 2.476974\n",
      "Ep: 106, steps: 23, D loss: 0.148763, acc:  80%, G loss: 2.321353\n",
      "Ep: 106, steps: 24, D loss: 0.230489, acc:  63%, G loss: 2.214800\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 107, steps: 1, D loss: 0.201915, acc:  72%, G loss: 2.395525\n",
      "Ep: 107, steps: 2, D loss: 0.232859, acc:  62%, G loss: 2.148942\n",
      "Ep: 107, steps: 3, D loss: 0.187351, acc:  73%, G loss: 2.655375\n",
      "Ep: 107, steps: 4, D loss: 0.200743, acc:  71%, G loss: 2.382087\n",
      "Ep: 107, steps: 5, D loss: 0.207446, acc:  64%, G loss: 2.335804\n",
      "Ep: 107, steps: 6, D loss: 0.294998, acc:  49%, G loss: 2.092654\n",
      "Ep: 107, steps: 7, D loss: 0.433852, acc:  22%, G loss: 1.704348\n",
      "Ep: 107, steps: 8, D loss: 0.243135, acc:  59%, G loss: 2.662121\n",
      "Ep: 107, steps: 9, D loss: 0.209441, acc:  69%, G loss: 2.161262\n",
      "Ep: 107, steps: 10, D loss: 0.141055, acc:  87%, G loss: 2.302840\n",
      "Ep: 107, steps: 11, D loss: 0.252855, acc:  57%, G loss: 2.477029\n",
      "Ep: 107, steps: 12, D loss: 0.344535, acc:  24%, G loss: 1.788325\n",
      "Ep: 107, steps: 13, D loss: 0.295809, acc:  41%, G loss: 2.015464\n",
      "Ep: 107, steps: 14, D loss: 0.240421, acc:  53%, G loss: 2.214415\n",
      "Ep: 107, steps: 15, D loss: 0.239691, acc:  58%, G loss: 2.200689\n",
      "Ep: 107, steps: 16, D loss: 0.250148, acc:  55%, G loss: 2.210365\n",
      "Ep: 107, steps: 17, D loss: 0.159230, acc:  83%, G loss: 2.004122\n",
      "Ep: 107, steps: 18, D loss: 0.295112, acc:  42%, G loss: 2.174606\n",
      "Ep: 107, steps: 19, D loss: 0.224536, acc:  63%, G loss: 1.999583\n",
      "Ep: 107, steps: 20, D loss: 0.118351, acc:  93%, G loss: 2.521285\n",
      "Ep: 107, steps: 21, D loss: 0.283821, acc:  50%, G loss: 2.219176\n",
      "Ep: 107, steps: 22, D loss: 0.158862, acc:  82%, G loss: 2.061259\n",
      "Ep: 107, steps: 23, D loss: 0.190420, acc:  74%, G loss: 2.479754\n",
      "Ep: 107, steps: 24, D loss: 0.155599, acc:  81%, G loss: 2.248875\n",
      "Ep: 107, steps: 25, D loss: 0.207020, acc:  69%, G loss: 2.166736\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 108, steps: 1, D loss: 0.267572, acc:  53%, G loss: 2.141000\n",
      "Ep: 108, steps: 2, D loss: 0.236567, acc:  61%, G loss: 2.069030\n",
      "Ep: 108, steps: 3, D loss: 0.149077, acc:  78%, G loss: 2.382718\n",
      "Ep: 108, steps: 4, D loss: 0.204637, acc:  69%, G loss: 2.106539\n",
      "Ep: 108, steps: 5, D loss: 0.194134, acc:  70%, G loss: 2.429212\n",
      "Ep: 108, steps: 6, D loss: 0.277275, acc:  50%, G loss: 2.099714\n",
      "Ep: 108, steps: 7, D loss: 0.356843, acc:  31%, G loss: 1.711393\n",
      "Ep: 108, steps: 8, D loss: 0.185973, acc:  75%, G loss: 2.631752\n",
      "Ep: 108, steps: 9, D loss: 0.231112, acc:  63%, G loss: 2.162699\n",
      "Ep: 108, steps: 10, D loss: 0.153688, acc:  79%, G loss: 2.346736\n",
      "Ep: 108, steps: 11, D loss: 0.247875, acc:  57%, G loss: 2.510512\n",
      "Ep: 108, steps: 12, D loss: 0.337691, acc:  27%, G loss: 1.731471\n",
      "Ep: 108, steps: 13, D loss: 0.294423, acc:  38%, G loss: 1.996310\n",
      "Ep: 108, steps: 14, D loss: 0.244873, acc:  52%, G loss: 2.103142\n",
      "Ep: 108, steps: 15, D loss: 0.241356, acc:  57%, G loss: 2.147022\n",
      "Ep: 108, steps: 16, D loss: 0.234472, acc:  59%, G loss: 2.202490\n",
      "Ep: 108, steps: 17, D loss: 0.183531, acc:  72%, G loss: 2.299869\n",
      "Ep: 108, steps: 18, D loss: 0.303704, acc:  39%, G loss: 2.446791\n",
      "Ep: 108, steps: 19, D loss: 0.235103, acc:  60%, G loss: 2.047517\n",
      "Ep: 108, steps: 20, D loss: 0.113444, acc:  93%, G loss: 2.573730\n",
      "Ep: 108, steps: 21, D loss: 0.272730, acc:  55%, G loss: 2.206313\n",
      "Ep: 108, steps: 22, D loss: 0.168037, acc:  81%, G loss: 2.092638\n",
      "Ep: 108, steps: 23, D loss: 0.187358, acc:  76%, G loss: 2.414327\n",
      "Ep: 108, steps: 24, D loss: 0.173334, acc:  78%, G loss: 2.180325\n",
      "Ep: 108, steps: 25, D loss: 0.195093, acc:  75%, G loss: 2.120518\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 109, steps: 1, D loss: 0.235633, acc:  63%, G loss: 2.267782\n",
      "Ep: 109, steps: 2, D loss: 0.234844, acc:  62%, G loss: 2.041299\n",
      "Ep: 109, steps: 3, D loss: 0.165817, acc:  74%, G loss: 2.418495\n",
      "Ep: 109, steps: 4, D loss: 0.204357, acc:  68%, G loss: 2.359582\n",
      "Ep: 109, steps: 5, D loss: 0.195854, acc:  71%, G loss: 2.510598\n",
      "Ep: 109, steps: 6, D loss: 0.266702, acc:  51%, G loss: 2.202105\n",
      "Ep: 109, steps: 7, D loss: 0.377382, acc:  29%, G loss: 1.742110\n",
      "Ep: 109, steps: 8, D loss: 0.226442, acc:  65%, G loss: 3.006267\n",
      "Ep: 109, steps: 9, D loss: 0.225024, acc:  64%, G loss: 2.170769\n",
      "Ep: 109, steps: 10, D loss: 0.154237, acc:  82%, G loss: 2.221983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 109, steps: 11, D loss: 0.241083, acc:  61%, G loss: 2.495727\n",
      "Ep: 109, steps: 12, D loss: 0.353484, acc:  24%, G loss: 1.743457\n",
      "Ep: 109, steps: 13, D loss: 0.307636, acc:  35%, G loss: 1.968174\n",
      "Ep: 109, steps: 14, D loss: 0.232175, acc:  55%, G loss: 2.183737\n",
      "Ep: 109, steps: 15, D loss: 0.242703, acc:  55%, G loss: 2.190603\n",
      "Ep: 109, steps: 16, D loss: 0.227409, acc:  61%, G loss: 2.161564\n",
      "Ep: 109, steps: 17, D loss: 0.162950, acc:  78%, G loss: 2.047695\n",
      "Saved Model\n",
      "Ep: 109, steps: 18, D loss: 0.297719, acc:  39%, G loss: 2.429543\n",
      "Ep: 109, steps: 19, D loss: 0.136317, acc:  90%, G loss: 2.658683\n",
      "Ep: 109, steps: 20, D loss: 0.254534, acc:  59%, G loss: 2.596807\n",
      "Ep: 109, steps: 21, D loss: 0.161850, acc:  78%, G loss: 2.077342\n",
      "Ep: 109, steps: 22, D loss: 0.223053, acc:  65%, G loss: 2.416491\n",
      "Ep: 109, steps: 23, D loss: 0.151638, acc:  87%, G loss: 2.186565\n",
      "Ep: 109, steps: 24, D loss: 0.179796, acc:  79%, G loss: 2.156476\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 110, steps: 1, D loss: 0.284572, acc:  45%, G loss: 2.388315\n",
      "Ep: 110, steps: 2, D loss: 0.248045, acc:  59%, G loss: 1.994339\n",
      "Ep: 110, steps: 3, D loss: 0.171523, acc:  73%, G loss: 2.416082\n",
      "Ep: 110, steps: 4, D loss: 0.235084, acc:  58%, G loss: 2.057349\n",
      "Ep: 110, steps: 5, D loss: 0.212442, acc:  65%, G loss: 2.358380\n",
      "Ep: 110, steps: 6, D loss: 0.257944, acc:  53%, G loss: 2.074964\n",
      "Ep: 110, steps: 7, D loss: 0.335215, acc:  34%, G loss: 1.672177\n",
      "Ep: 110, steps: 8, D loss: 0.195624, acc:  71%, G loss: 2.968623\n",
      "Ep: 110, steps: 9, D loss: 0.202477, acc:  72%, G loss: 2.149512\n",
      "Ep: 110, steps: 10, D loss: 0.159697, acc:  78%, G loss: 2.279390\n",
      "Ep: 110, steps: 11, D loss: 0.264942, acc:  52%, G loss: 2.514755\n",
      "Ep: 110, steps: 12, D loss: 0.332264, acc:  30%, G loss: 1.825424\n",
      "Ep: 110, steps: 13, D loss: 0.296001, acc:  37%, G loss: 1.910338\n",
      "Ep: 110, steps: 14, D loss: 0.246694, acc:  52%, G loss: 2.098538\n",
      "Ep: 110, steps: 15, D loss: 0.249687, acc:  54%, G loss: 2.205436\n",
      "Ep: 110, steps: 16, D loss: 0.233840, acc:  59%, G loss: 2.192901\n",
      "Ep: 110, steps: 17, D loss: 0.124014, acc:  90%, G loss: 2.064636\n",
      "Ep: 110, steps: 18, D loss: 0.299075, acc:  38%, G loss: 2.262975\n",
      "Ep: 110, steps: 19, D loss: 0.257294, acc:  51%, G loss: 1.985396\n",
      "Ep: 110, steps: 20, D loss: 0.126321, acc:  92%, G loss: 2.529621\n",
      "Ep: 110, steps: 21, D loss: 0.251796, acc:  54%, G loss: 2.163358\n",
      "Ep: 110, steps: 22, D loss: 0.161225, acc:  80%, G loss: 2.080112\n",
      "Ep: 110, steps: 23, D loss: 0.188180, acc:  75%, G loss: 2.559156\n",
      "Ep: 110, steps: 24, D loss: 0.151822, acc:  84%, G loss: 2.355649\n",
      "Ep: 110, steps: 25, D loss: 0.189390, acc:  76%, G loss: 2.166492\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 111, steps: 1, D loss: 0.254956, acc:  57%, G loss: 2.164158\n",
      "Ep: 111, steps: 2, D loss: 0.245439, acc:  58%, G loss: 2.029948\n",
      "Ep: 111, steps: 3, D loss: 0.165002, acc:  77%, G loss: 2.498083\n",
      "Ep: 111, steps: 4, D loss: 0.184595, acc:  76%, G loss: 2.161720\n",
      "Ep: 111, steps: 5, D loss: 0.205552, acc:  66%, G loss: 2.357088\n",
      "Ep: 111, steps: 6, D loss: 0.274945, acc:  51%, G loss: 2.020089\n",
      "Ep: 111, steps: 7, D loss: 0.376069, acc:  25%, G loss: 1.665856\n",
      "Ep: 111, steps: 8, D loss: 0.206127, acc:  69%, G loss: 2.632316\n",
      "Ep: 111, steps: 9, D loss: 0.197699, acc:  72%, G loss: 2.139634\n",
      "Ep: 111, steps: 10, D loss: 0.145347, acc:  84%, G loss: 2.352819\n",
      "Ep: 111, steps: 11, D loss: 0.269669, acc:  51%, G loss: 2.496876\n",
      "Ep: 111, steps: 12, D loss: 0.340952, acc:  28%, G loss: 1.769294\n",
      "Ep: 111, steps: 13, D loss: 0.299180, acc:  39%, G loss: 1.930553\n",
      "Ep: 111, steps: 14, D loss: 0.242825, acc:  53%, G loss: 2.133631\n",
      "Ep: 111, steps: 15, D loss: 0.239415, acc:  57%, G loss: 2.169731\n",
      "Ep: 111, steps: 16, D loss: 0.234593, acc:  60%, G loss: 2.207315\n",
      "Ep: 111, steps: 17, D loss: 0.200231, acc:  70%, G loss: 2.312315\n",
      "Ep: 111, steps: 18, D loss: 0.298311, acc:  40%, G loss: 2.213511\n",
      "Ep: 111, steps: 19, D loss: 0.235294, acc:  60%, G loss: 1.984270\n",
      "Ep: 111, steps: 20, D loss: 0.126825, acc:  92%, G loss: 2.583660\n",
      "Ep: 111, steps: 21, D loss: 0.253743, acc:  58%, G loss: 2.181774\n",
      "Ep: 111, steps: 22, D loss: 0.172641, acc:  81%, G loss: 1.998009\n",
      "Ep: 111, steps: 23, D loss: 0.189288, acc:  75%, G loss: 2.496212\n",
      "Ep: 111, steps: 24, D loss: 0.155178, acc:  84%, G loss: 2.263317\n",
      "Ep: 111, steps: 25, D loss: 0.195149, acc:  72%, G loss: 2.206920\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 112, steps: 1, D loss: 0.270916, acc:  52%, G loss: 2.140853\n",
      "Ep: 112, steps: 2, D loss: 0.257739, acc:  54%, G loss: 2.033528\n",
      "Ep: 112, steps: 3, D loss: 0.159768, acc:  77%, G loss: 2.420586\n",
      "Ep: 112, steps: 4, D loss: 0.204230, acc:  70%, G loss: 2.089063\n",
      "Ep: 112, steps: 5, D loss: 0.208004, acc:  65%, G loss: 2.265542\n",
      "Ep: 112, steps: 6, D loss: 0.267969, acc:  51%, G loss: 2.000826\n",
      "Ep: 112, steps: 7, D loss: 0.369162, acc:  24%, G loss: 1.617479\n",
      "Ep: 112, steps: 8, D loss: 0.187634, acc:  73%, G loss: 2.765382\n",
      "Ep: 112, steps: 9, D loss: 0.188403, acc:  77%, G loss: 2.187521\n",
      "Ep: 112, steps: 10, D loss: 0.143561, acc:  85%, G loss: 2.267060\n",
      "Ep: 112, steps: 11, D loss: 0.263017, acc:  51%, G loss: 2.462425\n",
      "Ep: 112, steps: 12, D loss: 0.334460, acc:  28%, G loss: 1.744555\n",
      "Ep: 112, steps: 13, D loss: 0.293083, acc:  40%, G loss: 1.908592\n",
      "Ep: 112, steps: 14, D loss: 0.260777, acc:  46%, G loss: 2.091418\n",
      "Saved Model\n",
      "Ep: 112, steps: 15, D loss: 0.248561, acc:  55%, G loss: 2.166720\n",
      "Ep: 112, steps: 16, D loss: 0.137906, acc:  86%, G loss: 2.155272\n",
      "Ep: 112, steps: 17, D loss: 0.295563, acc:  37%, G loss: 2.155376\n",
      "Ep: 112, steps: 18, D loss: 0.218449, acc:  63%, G loss: 2.062958\n",
      "Ep: 112, steps: 19, D loss: 0.119511, acc:  92%, G loss: 2.608103\n",
      "Ep: 112, steps: 20, D loss: 0.303164, acc:  48%, G loss: 2.154554\n",
      "Ep: 112, steps: 21, D loss: 0.176349, acc:  81%, G loss: 1.992566\n",
      "Ep: 112, steps: 22, D loss: 0.189281, acc:  76%, G loss: 2.400682\n",
      "Ep: 112, steps: 23, D loss: 0.164240, acc:  80%, G loss: 2.231225\n",
      "Ep: 112, steps: 24, D loss: 0.204482, acc:  68%, G loss: 2.114924\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 113, steps: 1, D loss: 0.227872, acc:  66%, G loss: 2.129406\n",
      "Ep: 113, steps: 2, D loss: 0.223926, acc:  64%, G loss: 2.029971\n",
      "Ep: 113, steps: 3, D loss: 0.168085, acc:  75%, G loss: 2.385685\n",
      "Ep: 113, steps: 4, D loss: 0.223587, acc:  59%, G loss: 2.071655\n",
      "Ep: 113, steps: 5, D loss: 0.226198, acc:  58%, G loss: 2.323367\n",
      "Ep: 113, steps: 6, D loss: 0.274130, acc:  51%, G loss: 2.051861\n",
      "Ep: 113, steps: 7, D loss: 0.367462, acc:  25%, G loss: 1.695671\n",
      "Ep: 113, steps: 8, D loss: 0.198328, acc:  68%, G loss: 2.612972\n",
      "Ep: 113, steps: 9, D loss: 0.215051, acc:  68%, G loss: 2.128707\n",
      "Ep: 113, steps: 10, D loss: 0.148481, acc:  82%, G loss: 2.176507\n",
      "Ep: 113, steps: 11, D loss: 0.264495, acc:  53%, G loss: 2.455735\n",
      "Ep: 113, steps: 12, D loss: 0.331528, acc:  28%, G loss: 2.102609\n",
      "Ep: 113, steps: 13, D loss: 0.293532, acc:  41%, G loss: 2.094763\n",
      "Ep: 113, steps: 14, D loss: 0.244246, acc:  52%, G loss: 2.112872\n",
      "Ep: 113, steps: 15, D loss: 0.225602, acc:  64%, G loss: 2.283580\n",
      "Ep: 113, steps: 16, D loss: 0.229672, acc:  61%, G loss: 2.173316\n",
      "Ep: 113, steps: 17, D loss: 0.106890, acc:  92%, G loss: 2.371239\n",
      "Ep: 113, steps: 18, D loss: 0.315916, acc:  33%, G loss: 2.064921\n",
      "Ep: 113, steps: 19, D loss: 0.199913, acc:  65%, G loss: 1.978570\n",
      "Ep: 113, steps: 20, D loss: 0.116864, acc:  91%, G loss: 2.484930\n",
      "Ep: 113, steps: 21, D loss: 0.270207, acc:  52%, G loss: 2.316545\n",
      "Ep: 113, steps: 22, D loss: 0.162602, acc:  82%, G loss: 1.975511\n",
      "Ep: 113, steps: 23, D loss: 0.152009, acc:  84%, G loss: 2.587204\n",
      "Ep: 113, steps: 24, D loss: 0.142914, acc:  85%, G loss: 2.288564\n",
      "Ep: 113, steps: 25, D loss: 0.201751, acc:  71%, G loss: 2.453660\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 114, steps: 1, D loss: 0.222275, acc:  67%, G loss: 2.261129\n",
      "Ep: 114, steps: 2, D loss: 0.234285, acc:  61%, G loss: 2.244215\n",
      "Ep: 114, steps: 3, D loss: 0.110483, acc:  92%, G loss: 2.541883\n",
      "Ep: 114, steps: 4, D loss: 0.192992, acc:  72%, G loss: 2.289764\n",
      "Ep: 114, steps: 5, D loss: 0.199216, acc:  67%, G loss: 2.211260\n",
      "Ep: 114, steps: 6, D loss: 0.297237, acc:  50%, G loss: 2.197423\n",
      "Ep: 114, steps: 7, D loss: 0.441668, acc:  24%, G loss: 1.612342\n",
      "Ep: 114, steps: 8, D loss: 0.198266, acc:  71%, G loss: 2.500979\n",
      "Ep: 114, steps: 9, D loss: 0.205075, acc:  71%, G loss: 2.118691\n",
      "Ep: 114, steps: 10, D loss: 0.137278, acc:  86%, G loss: 2.255227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 114, steps: 11, D loss: 0.259806, acc:  55%, G loss: 2.414831\n",
      "Ep: 114, steps: 12, D loss: 0.324381, acc:  33%, G loss: 1.770190\n",
      "Ep: 114, steps: 13, D loss: 0.314594, acc:  32%, G loss: 1.902594\n",
      "Ep: 114, steps: 14, D loss: 0.250356, acc:  50%, G loss: 2.084098\n",
      "Ep: 114, steps: 15, D loss: 0.246649, acc:  54%, G loss: 2.134539\n",
      "Ep: 114, steps: 16, D loss: 0.256723, acc:  54%, G loss: 2.370533\n",
      "Ep: 114, steps: 17, D loss: 0.149677, acc:  87%, G loss: 1.991419\n",
      "Ep: 114, steps: 18, D loss: 0.302972, acc:  35%, G loss: 2.116117\n",
      "Ep: 114, steps: 19, D loss: 0.225322, acc:  62%, G loss: 2.033434\n",
      "Ep: 114, steps: 20, D loss: 0.133806, acc:  92%, G loss: 2.532192\n",
      "Ep: 114, steps: 21, D loss: 0.243882, acc:  54%, G loss: 2.050632\n",
      "Ep: 114, steps: 22, D loss: 0.164545, acc:  81%, G loss: 2.233153\n",
      "Ep: 114, steps: 23, D loss: 0.176149, acc:  79%, G loss: 2.624201\n",
      "Ep: 114, steps: 24, D loss: 0.141858, acc:  87%, G loss: 2.279572\n",
      "Ep: 114, steps: 25, D loss: 0.182809, acc:  75%, G loss: 2.210122\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 115, steps: 1, D loss: 0.269846, acc:  52%, G loss: 2.339737\n",
      "Ep: 115, steps: 2, D loss: 0.235757, acc:  59%, G loss: 2.074635\n",
      "Ep: 115, steps: 3, D loss: 0.145514, acc:  80%, G loss: 2.607627\n",
      "Ep: 115, steps: 4, D loss: 0.190019, acc:  73%, G loss: 2.173228\n",
      "Ep: 115, steps: 5, D loss: 0.183650, acc:  74%, G loss: 2.386034\n",
      "Ep: 115, steps: 6, D loss: 0.274461, acc:  49%, G loss: 2.210335\n",
      "Ep: 115, steps: 7, D loss: 0.413744, acc:  20%, G loss: 1.726880\n",
      "Ep: 115, steps: 8, D loss: 0.219913, acc:  63%, G loss: 2.547987\n",
      "Ep: 115, steps: 9, D loss: 0.188430, acc:  75%, G loss: 2.128679\n",
      "Ep: 115, steps: 10, D loss: 0.157463, acc:  83%, G loss: 2.191472\n",
      "Ep: 115, steps: 11, D loss: 0.264229, acc:  54%, G loss: 2.515747\n",
      "Ep: 115, steps: 12, D loss: 0.326274, acc:  31%, G loss: 1.749886\n",
      "Saved Model\n",
      "Ep: 115, steps: 13, D loss: 0.294698, acc:  38%, G loss: 1.950988\n",
      "Ep: 115, steps: 14, D loss: 0.259802, acc:  50%, G loss: 1.996129\n",
      "Ep: 115, steps: 15, D loss: 0.223388, acc:  63%, G loss: 2.260358\n",
      "Ep: 115, steps: 16, D loss: 0.167960, acc:  81%, G loss: 2.189605\n",
      "Ep: 115, steps: 17, D loss: 0.306960, acc:  38%, G loss: 2.131584\n",
      "Ep: 115, steps: 18, D loss: 0.229795, acc:  62%, G loss: 2.026363\n",
      "Ep: 115, steps: 19, D loss: 0.131454, acc:  91%, G loss: 2.522203\n",
      "Ep: 115, steps: 20, D loss: 0.278054, acc:  46%, G loss: 2.082311\n",
      "Ep: 115, steps: 21, D loss: 0.164561, acc:  79%, G loss: 2.255879\n",
      "Ep: 115, steps: 22, D loss: 0.192874, acc:  75%, G loss: 2.502308\n",
      "Ep: 115, steps: 23, D loss: 0.167640, acc:  80%, G loss: 2.239126\n",
      "Ep: 115, steps: 24, D loss: 0.205142, acc:  69%, G loss: 2.156348\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 116, steps: 1, D loss: 0.248730, acc:  59%, G loss: 2.331994\n",
      "Ep: 116, steps: 2, D loss: 0.224286, acc:  65%, G loss: 2.000176\n",
      "Ep: 116, steps: 3, D loss: 0.159637, acc:  76%, G loss: 2.589420\n",
      "Ep: 116, steps: 4, D loss: 0.178133, acc:  75%, G loss: 2.185316\n",
      "Ep: 116, steps: 5, D loss: 0.189167, acc:  73%, G loss: 2.433145\n",
      "Ep: 116, steps: 6, D loss: 0.269664, acc:  50%, G loss: 2.139007\n",
      "Ep: 116, steps: 7, D loss: 0.392981, acc:  26%, G loss: 1.741152\n",
      "Ep: 116, steps: 8, D loss: 0.244474, acc:  57%, G loss: 2.500862\n",
      "Ep: 116, steps: 9, D loss: 0.223269, acc:  65%, G loss: 2.165731\n",
      "Ep: 116, steps: 10, D loss: 0.146945, acc:  84%, G loss: 2.212157\n",
      "Ep: 116, steps: 11, D loss: 0.266882, acc:  54%, G loss: 2.496357\n",
      "Ep: 116, steps: 12, D loss: 0.338728, acc:  26%, G loss: 1.860179\n",
      "Ep: 116, steps: 13, D loss: 0.309750, acc:  36%, G loss: 2.003049\n",
      "Ep: 116, steps: 14, D loss: 0.249177, acc:  51%, G loss: 2.075715\n",
      "Ep: 116, steps: 15, D loss: 0.269090, acc:  47%, G loss: 2.111905\n",
      "Ep: 116, steps: 16, D loss: 0.273055, acc:  49%, G loss: 2.231436\n",
      "Ep: 116, steps: 17, D loss: 0.158771, acc:  85%, G loss: 2.170595\n",
      "Ep: 116, steps: 18, D loss: 0.295714, acc:  37%, G loss: 2.314958\n",
      "Ep: 116, steps: 19, D loss: 0.246535, acc:  54%, G loss: 2.031559\n",
      "Ep: 116, steps: 20, D loss: 0.133077, acc:  91%, G loss: 2.473555\n",
      "Ep: 116, steps: 21, D loss: 0.265421, acc:  51%, G loss: 2.008103\n",
      "Ep: 116, steps: 22, D loss: 0.173882, acc:  78%, G loss: 2.005591\n",
      "Ep: 116, steps: 23, D loss: 0.180255, acc:  79%, G loss: 2.575726\n",
      "Ep: 116, steps: 24, D loss: 0.163947, acc:  81%, G loss: 2.365617\n",
      "Ep: 116, steps: 25, D loss: 0.182283, acc:  76%, G loss: 2.102620\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 117, steps: 1, D loss: 0.270315, acc:  50%, G loss: 2.171262\n",
      "Ep: 117, steps: 2, D loss: 0.222075, acc:  66%, G loss: 2.096970\n",
      "Ep: 117, steps: 3, D loss: 0.135510, acc:  82%, G loss: 2.441548\n",
      "Ep: 117, steps: 4, D loss: 0.178665, acc:  76%, G loss: 2.332588\n",
      "Ep: 117, steps: 5, D loss: 0.162902, acc:  80%, G loss: 2.483372\n",
      "Ep: 117, steps: 6, D loss: 0.266347, acc:  50%, G loss: 2.106421\n",
      "Ep: 117, steps: 7, D loss: 0.366140, acc:  28%, G loss: 2.296389\n",
      "Ep: 117, steps: 8, D loss: 0.217311, acc:  65%, G loss: 2.989683\n",
      "Ep: 117, steps: 9, D loss: 0.191217, acc:  74%, G loss: 2.213660\n",
      "Ep: 117, steps: 10, D loss: 0.135299, acc:  88%, G loss: 2.236484\n",
      "Ep: 117, steps: 11, D loss: 0.252231, acc:  58%, G loss: 2.465073\n",
      "Ep: 117, steps: 12, D loss: 0.343203, acc:  24%, G loss: 1.783894\n",
      "Ep: 117, steps: 13, D loss: 0.288254, acc:  42%, G loss: 1.958131\n",
      "Ep: 117, steps: 14, D loss: 0.243251, acc:  54%, G loss: 2.218798\n",
      "Ep: 117, steps: 15, D loss: 0.261963, acc:  49%, G loss: 2.241332\n",
      "Ep: 117, steps: 16, D loss: 0.252019, acc:  55%, G loss: 2.273549\n",
      "Ep: 117, steps: 17, D loss: 0.117460, acc:  92%, G loss: 2.292857\n",
      "Ep: 117, steps: 18, D loss: 0.304310, acc:  35%, G loss: 2.130123\n",
      "Ep: 117, steps: 19, D loss: 0.227044, acc:  61%, G loss: 2.004723\n",
      "Ep: 117, steps: 20, D loss: 0.123533, acc:  91%, G loss: 2.545501\n",
      "Ep: 117, steps: 21, D loss: 0.239609, acc:  59%, G loss: 1.974960\n",
      "Ep: 117, steps: 22, D loss: 0.147984, acc:  84%, G loss: 2.173527\n",
      "Ep: 117, steps: 23, D loss: 0.204120, acc:  69%, G loss: 2.338731\n",
      "Ep: 117, steps: 24, D loss: 0.184778, acc:  73%, G loss: 2.251656\n",
      "Ep: 117, steps: 25, D loss: 0.200184, acc:  72%, G loss: 2.143502\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 118, steps: 1, D loss: 0.255952, acc:  55%, G loss: 2.083596\n",
      "Ep: 118, steps: 2, D loss: 0.225327, acc:  65%, G loss: 2.029985\n",
      "Ep: 118, steps: 3, D loss: 0.145028, acc:  80%, G loss: 2.549673\n",
      "Ep: 118, steps: 4, D loss: 0.172008, acc:  78%, G loss: 2.254260\n",
      "Ep: 118, steps: 5, D loss: 0.185121, acc:  74%, G loss: 2.401787\n",
      "Ep: 118, steps: 6, D loss: 0.288182, acc:  49%, G loss: 2.045013\n",
      "Ep: 118, steps: 7, D loss: 0.340475, acc:  35%, G loss: 1.831620\n",
      "Ep: 118, steps: 8, D loss: 0.206310, acc:  66%, G loss: 3.071611\n",
      "Ep: 118, steps: 9, D loss: 0.204686, acc:  71%, G loss: 2.150151\n",
      "Ep: 118, steps: 10, D loss: 0.134649, acc:  86%, G loss: 2.257483\n",
      "Saved Model\n",
      "Ep: 118, steps: 11, D loss: 0.258694, acc:  53%, G loss: 2.468939\n",
      "Ep: 118, steps: 12, D loss: 0.318588, acc:  31%, G loss: 1.789673\n",
      "Ep: 118, steps: 13, D loss: 0.252495, acc:  50%, G loss: 2.054863\n",
      "Ep: 118, steps: 14, D loss: 0.238834, acc:  56%, G loss: 2.140055\n",
      "Ep: 118, steps: 15, D loss: 0.267090, acc:  52%, G loss: 2.243447\n",
      "Ep: 118, steps: 16, D loss: 0.125510, acc:  91%, G loss: 2.368054\n",
      "Ep: 118, steps: 17, D loss: 0.297058, acc:  40%, G loss: 2.133452\n",
      "Ep: 118, steps: 18, D loss: 0.204662, acc:  64%, G loss: 2.095915\n",
      "Ep: 118, steps: 19, D loss: 0.119520, acc:  92%, G loss: 2.561692\n",
      "Ep: 118, steps: 20, D loss: 0.273158, acc:  52%, G loss: 2.135782\n",
      "Ep: 118, steps: 21, D loss: 0.145268, acc:  83%, G loss: 2.180164\n",
      "Ep: 118, steps: 22, D loss: 0.170667, acc:  78%, G loss: 2.673532\n",
      "Ep: 118, steps: 23, D loss: 0.139424, acc:  85%, G loss: 2.299132\n",
      "Ep: 118, steps: 24, D loss: 0.259943, acc:  55%, G loss: 2.115791\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 119, steps: 1, D loss: 0.290209, acc:  46%, G loss: 2.150233\n",
      "Ep: 119, steps: 2, D loss: 0.225517, acc:  64%, G loss: 2.035767\n",
      "Ep: 119, steps: 3, D loss: 0.135463, acc:  84%, G loss: 2.529526\n",
      "Ep: 119, steps: 4, D loss: 0.172238, acc:  81%, G loss: 2.242217\n",
      "Ep: 119, steps: 5, D loss: 0.182546, acc:  73%, G loss: 2.303909\n",
      "Ep: 119, steps: 6, D loss: 0.272607, acc:  50%, G loss: 2.017984\n",
      "Ep: 119, steps: 7, D loss: 0.375594, acc:  26%, G loss: 1.714921\n",
      "Ep: 119, steps: 8, D loss: 0.203189, acc:  69%, G loss: 2.837511\n",
      "Ep: 119, steps: 9, D loss: 0.210024, acc:  69%, G loss: 2.137380\n",
      "Ep: 119, steps: 10, D loss: 0.131978, acc:  86%, G loss: 2.218577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 119, steps: 11, D loss: 0.258886, acc:  55%, G loss: 2.405849\n",
      "Ep: 119, steps: 12, D loss: 0.334763, acc:  31%, G loss: 1.727932\n",
      "Ep: 119, steps: 13, D loss: 0.293266, acc:  40%, G loss: 1.881928\n",
      "Ep: 119, steps: 14, D loss: 0.246464, acc:  52%, G loss: 2.051794\n",
      "Ep: 119, steps: 15, D loss: 0.278059, acc:  41%, G loss: 2.129648\n",
      "Ep: 119, steps: 16, D loss: 0.254792, acc:  56%, G loss: 2.181796\n",
      "Ep: 119, steps: 17, D loss: 0.134401, acc:  88%, G loss: 2.073852\n",
      "Ep: 119, steps: 18, D loss: 0.321989, acc:  33%, G loss: 2.194355\n",
      "Ep: 119, steps: 19, D loss: 0.241222, acc:  55%, G loss: 2.082512\n",
      "Ep: 119, steps: 20, D loss: 0.113212, acc:  94%, G loss: 2.492540\n",
      "Ep: 119, steps: 21, D loss: 0.262764, acc:  56%, G loss: 1.976037\n",
      "Ep: 119, steps: 22, D loss: 0.156726, acc:  82%, G loss: 2.054730\n",
      "Ep: 119, steps: 23, D loss: 0.179626, acc:  79%, G loss: 2.521402\n",
      "Ep: 119, steps: 24, D loss: 0.149431, acc:  83%, G loss: 2.323745\n",
      "Ep: 119, steps: 25, D loss: 0.182438, acc:  75%, G loss: 2.059714\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 120, steps: 1, D loss: 0.250469, acc:  57%, G loss: 2.157087\n",
      "Ep: 120, steps: 2, D loss: 0.233832, acc:  61%, G loss: 1.916236\n",
      "Ep: 120, steps: 3, D loss: 0.174948, acc:  75%, G loss: 2.495506\n",
      "Ep: 120, steps: 4, D loss: 0.198768, acc:  69%, G loss: 2.063480\n",
      "Ep: 120, steps: 5, D loss: 0.191240, acc:  70%, G loss: 2.469966\n",
      "Ep: 120, steps: 6, D loss: 0.258785, acc:  51%, G loss: 2.183046\n",
      "Ep: 120, steps: 7, D loss: 0.373948, acc:  28%, G loss: 1.745798\n",
      "Ep: 120, steps: 8, D loss: 0.202053, acc:  67%, G loss: 2.562665\n",
      "Ep: 120, steps: 9, D loss: 0.209410, acc:  71%, G loss: 2.138212\n",
      "Ep: 120, steps: 10, D loss: 0.150342, acc:  80%, G loss: 2.203516\n",
      "Ep: 120, steps: 11, D loss: 0.262376, acc:  53%, G loss: 2.498337\n",
      "Ep: 120, steps: 12, D loss: 0.329274, acc:  30%, G loss: 1.787408\n",
      "Ep: 120, steps: 13, D loss: 0.300586, acc:  38%, G loss: 1.918213\n",
      "Ep: 120, steps: 14, D loss: 0.242583, acc:  54%, G loss: 2.062605\n",
      "Ep: 120, steps: 15, D loss: 0.253918, acc:  50%, G loss: 2.074613\n",
      "Ep: 120, steps: 16, D loss: 0.261802, acc:  52%, G loss: 2.289378\n",
      "Ep: 120, steps: 17, D loss: 0.191049, acc:  73%, G loss: 2.431452\n",
      "Ep: 120, steps: 18, D loss: 0.287173, acc:  40%, G loss: 2.001250\n",
      "Ep: 120, steps: 19, D loss: 0.225513, acc:  61%, G loss: 2.019169\n",
      "Ep: 120, steps: 20, D loss: 0.116761, acc:  95%, G loss: 2.496564\n",
      "Ep: 120, steps: 21, D loss: 0.264887, acc:  51%, G loss: 2.333527\n",
      "Ep: 120, steps: 22, D loss: 0.150426, acc:  85%, G loss: 2.061686\n",
      "Ep: 120, steps: 23, D loss: 0.176260, acc:  80%, G loss: 2.572555\n",
      "Ep: 120, steps: 24, D loss: 0.141635, acc:  88%, G loss: 2.211839\n",
      "Ep: 120, steps: 25, D loss: 0.184477, acc:  76%, G loss: 2.267249\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 121, steps: 1, D loss: 0.248050, acc:  60%, G loss: 2.126832\n",
      "Ep: 121, steps: 2, D loss: 0.244183, acc:  56%, G loss: 2.056292\n",
      "Ep: 121, steps: 3, D loss: 0.128389, acc:  87%, G loss: 2.321424\n",
      "Ep: 121, steps: 4, D loss: 0.204598, acc:  69%, G loss: 2.238882\n",
      "Ep: 121, steps: 5, D loss: 0.223660, acc:  60%, G loss: 2.292439\n",
      "Ep: 121, steps: 6, D loss: 0.261384, acc:  50%, G loss: 2.104612\n",
      "Ep: 121, steps: 7, D loss: 0.387788, acc:  23%, G loss: 1.631630\n",
      "Ep: 121, steps: 8, D loss: 0.213109, acc:  65%, G loss: 2.506015\n",
      "Saved Model\n",
      "Ep: 121, steps: 9, D loss: 0.198791, acc:  72%, G loss: 2.168033\n",
      "Ep: 121, steps: 10, D loss: 0.278455, acc:  49%, G loss: 2.342013\n",
      "Ep: 121, steps: 11, D loss: 0.321481, acc:  30%, G loss: 1.773121\n",
      "Ep: 121, steps: 12, D loss: 0.288986, acc:  38%, G loss: 1.858086\n",
      "Ep: 121, steps: 13, D loss: 0.259974, acc:  48%, G loss: 2.008121\n",
      "Ep: 121, steps: 14, D loss: 0.257569, acc:  54%, G loss: 2.177050\n",
      "Ep: 121, steps: 15, D loss: 0.233444, acc:  61%, G loss: 2.324273\n",
      "Ep: 121, steps: 16, D loss: 0.126344, acc:  92%, G loss: 2.065972\n",
      "Ep: 121, steps: 17, D loss: 0.309874, acc:  35%, G loss: 2.233019\n",
      "Ep: 121, steps: 18, D loss: 0.241597, acc:  55%, G loss: 2.074364\n",
      "Ep: 121, steps: 19, D loss: 0.128933, acc:  92%, G loss: 2.436698\n",
      "Ep: 121, steps: 20, D loss: 0.278573, acc:  47%, G loss: 2.081648\n",
      "Ep: 121, steps: 21, D loss: 0.164613, acc:  80%, G loss: 2.321734\n",
      "Ep: 121, steps: 22, D loss: 0.182532, acc:  79%, G loss: 2.888272\n",
      "Ep: 121, steps: 23, D loss: 0.158136, acc:  82%, G loss: 2.442010\n",
      "Ep: 121, steps: 24, D loss: 0.192361, acc:  74%, G loss: 2.453762\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 122, steps: 1, D loss: 0.220936, acc:  68%, G loss: 2.363933\n",
      "Ep: 122, steps: 2, D loss: 0.219464, acc:  67%, G loss: 2.177049\n",
      "Ep: 122, steps: 3, D loss: 0.114512, acc:  88%, G loss: 2.674950\n",
      "Ep: 122, steps: 4, D loss: 0.170270, acc:  78%, G loss: 2.264799\n",
      "Ep: 122, steps: 5, D loss: 0.147460, acc:  81%, G loss: 2.537443\n",
      "Ep: 122, steps: 6, D loss: 0.272335, acc:  49%, G loss: 2.399960\n",
      "Ep: 122, steps: 7, D loss: 0.314926, acc:  42%, G loss: 1.828060\n",
      "Ep: 122, steps: 8, D loss: 0.227900, acc:  65%, G loss: 2.930134\n",
      "Ep: 122, steps: 9, D loss: 0.231424, acc:  62%, G loss: 2.200788\n",
      "Ep: 122, steps: 10, D loss: 0.135279, acc:  85%, G loss: 2.267164\n",
      "Ep: 122, steps: 11, D loss: 0.251746, acc:  57%, G loss: 2.531218\n",
      "Ep: 122, steps: 12, D loss: 0.350096, acc:  26%, G loss: 1.849814\n",
      "Ep: 122, steps: 13, D loss: 0.302097, acc:  36%, G loss: 1.949327\n",
      "Ep: 122, steps: 14, D loss: 0.236227, acc:  56%, G loss: 2.117231\n",
      "Ep: 122, steps: 15, D loss: 0.249780, acc:  53%, G loss: 2.046115\n",
      "Ep: 122, steps: 16, D loss: 0.229253, acc:  60%, G loss: 2.159841\n",
      "Ep: 122, steps: 17, D loss: 0.150251, acc:  85%, G loss: 2.461737\n",
      "Ep: 122, steps: 18, D loss: 0.287182, acc:  42%, G loss: 2.283089\n",
      "Ep: 122, steps: 19, D loss: 0.232803, acc:  61%, G loss: 2.033097\n",
      "Ep: 122, steps: 20, D loss: 0.115135, acc:  95%, G loss: 2.468065\n",
      "Ep: 122, steps: 21, D loss: 0.241806, acc:  57%, G loss: 2.105978\n",
      "Ep: 122, steps: 22, D loss: 0.161205, acc:  79%, G loss: 2.279494\n",
      "Ep: 122, steps: 23, D loss: 0.173097, acc:  80%, G loss: 2.620969\n",
      "Ep: 122, steps: 24, D loss: 0.150350, acc:  83%, G loss: 2.285836\n",
      "Ep: 122, steps: 25, D loss: 0.229162, acc:  63%, G loss: 2.082505\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 123, steps: 1, D loss: 0.280452, acc:  48%, G loss: 2.188349\n",
      "Ep: 123, steps: 2, D loss: 0.238460, acc:  60%, G loss: 1.970459\n",
      "Ep: 123, steps: 3, D loss: 0.156082, acc:  80%, G loss: 2.412971\n",
      "Ep: 123, steps: 4, D loss: 0.210418, acc:  69%, G loss: 2.362678\n",
      "Ep: 123, steps: 5, D loss: 0.182220, acc:  74%, G loss: 2.294995\n",
      "Ep: 123, steps: 6, D loss: 0.272104, acc:  50%, G loss: 2.259481\n",
      "Ep: 123, steps: 7, D loss: 0.518445, acc:  14%, G loss: 1.912990\n",
      "Ep: 123, steps: 8, D loss: 0.214430, acc:  61%, G loss: 2.457137\n",
      "Ep: 123, steps: 9, D loss: 0.241495, acc:  62%, G loss: 2.137225\n",
      "Ep: 123, steps: 10, D loss: 0.138091, acc:  86%, G loss: 2.343137\n",
      "Ep: 123, steps: 11, D loss: 0.247761, acc:  56%, G loss: 2.530569\n",
      "Ep: 123, steps: 12, D loss: 0.326728, acc:  32%, G loss: 1.765267\n",
      "Ep: 123, steps: 13, D loss: 0.296669, acc:  40%, G loss: 1.995655\n",
      "Ep: 123, steps: 14, D loss: 0.251356, acc:  50%, G loss: 1.961915\n",
      "Ep: 123, steps: 15, D loss: 0.255657, acc:  50%, G loss: 2.105886\n",
      "Ep: 123, steps: 16, D loss: 0.242293, acc:  59%, G loss: 2.304480\n",
      "Ep: 123, steps: 17, D loss: 0.122393, acc:  88%, G loss: 2.127744\n",
      "Ep: 123, steps: 18, D loss: 0.343254, acc:  28%, G loss: 2.231207\n",
      "Ep: 123, steps: 19, D loss: 0.231606, acc:  56%, G loss: 2.073784\n",
      "Ep: 123, steps: 20, D loss: 0.108572, acc:  93%, G loss: 2.556433\n",
      "Ep: 123, steps: 21, D loss: 0.262367, acc:  52%, G loss: 2.144023\n",
      "Ep: 123, steps: 22, D loss: 0.148785, acc:  82%, G loss: 2.237461\n",
      "Ep: 123, steps: 23, D loss: 0.168198, acc:  82%, G loss: 2.619775\n",
      "Ep: 123, steps: 24, D loss: 0.145367, acc:  85%, G loss: 2.387942\n",
      "Ep: 123, steps: 25, D loss: 0.190727, acc:  74%, G loss: 2.050635\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 124, steps: 1, D loss: 0.232771, acc:  64%, G loss: 2.451307\n",
      "Ep: 124, steps: 2, D loss: 0.228794, acc:  62%, G loss: 2.018004\n",
      "Ep: 124, steps: 3, D loss: 0.153907, acc:  80%, G loss: 2.656005\n",
      "Ep: 124, steps: 4, D loss: 0.186989, acc:  73%, G loss: 2.046121\n",
      "Ep: 124, steps: 5, D loss: 0.176126, acc:  75%, G loss: 2.339359\n",
      "Ep: 124, steps: 6, D loss: 0.241752, acc:  53%, G loss: 2.103267\n",
      "Saved Model\n",
      "Ep: 124, steps: 7, D loss: 0.370072, acc:  31%, G loss: 1.787950\n",
      "Ep: 124, steps: 8, D loss: 0.181829, acc:  73%, G loss: 2.189449\n",
      "Ep: 124, steps: 9, D loss: 0.156732, acc:  80%, G loss: 2.128778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 124, steps: 10, D loss: 0.295068, acc:  47%, G loss: 2.522679\n",
      "Ep: 124, steps: 11, D loss: 0.341129, acc:  27%, G loss: 1.704767\n",
      "Ep: 124, steps: 12, D loss: 0.307153, acc:  35%, G loss: 1.880415\n",
      "Ep: 124, steps: 13, D loss: 0.264221, acc:  48%, G loss: 2.037507\n",
      "Ep: 124, steps: 14, D loss: 0.246065, acc:  60%, G loss: 2.248083\n",
      "Ep: 124, steps: 15, D loss: 0.238888, acc:  59%, G loss: 2.322412\n",
      "Ep: 124, steps: 16, D loss: 0.189358, acc:  77%, G loss: 2.172682\n",
      "Ep: 124, steps: 17, D loss: 0.279723, acc:  45%, G loss: 2.464456\n",
      "Ep: 124, steps: 18, D loss: 0.221066, acc:  61%, G loss: 2.044625\n",
      "Ep: 124, steps: 19, D loss: 0.129259, acc:  92%, G loss: 2.403524\n",
      "Ep: 124, steps: 20, D loss: 0.281952, acc:  46%, G loss: 2.071486\n",
      "Ep: 124, steps: 21, D loss: 0.170544, acc:  77%, G loss: 2.073335\n",
      "Ep: 124, steps: 22, D loss: 0.174177, acc:  80%, G loss: 2.508495\n",
      "Ep: 124, steps: 23, D loss: 0.151892, acc:  81%, G loss: 2.316635\n",
      "Ep: 124, steps: 24, D loss: 0.201383, acc:  72%, G loss: 2.058493\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 125, steps: 1, D loss: 0.234272, acc:  63%, G loss: 2.273311\n",
      "Ep: 125, steps: 2, D loss: 0.236779, acc:  60%, G loss: 1.966878\n",
      "Ep: 125, steps: 3, D loss: 0.149176, acc:  81%, G loss: 2.521734\n",
      "Ep: 125, steps: 4, D loss: 0.190538, acc:  74%, G loss: 2.321351\n",
      "Ep: 125, steps: 5, D loss: 0.263174, acc:  66%, G loss: 2.259249\n",
      "Ep: 125, steps: 6, D loss: 0.274740, acc:  50%, G loss: 2.110541\n",
      "Ep: 125, steps: 7, D loss: 0.408122, acc:  21%, G loss: 1.617295\n",
      "Ep: 125, steps: 8, D loss: 0.236556, acc:  57%, G loss: 2.775843\n",
      "Ep: 125, steps: 9, D loss: 0.190385, acc:  73%, G loss: 2.171915\n",
      "Ep: 125, steps: 10, D loss: 0.135950, acc:  90%, G loss: 2.290847\n",
      "Ep: 125, steps: 11, D loss: 0.255712, acc:  55%, G loss: 2.465473\n",
      "Ep: 125, steps: 12, D loss: 0.349292, acc:  24%, G loss: 1.721436\n",
      "Ep: 125, steps: 13, D loss: 0.298836, acc:  39%, G loss: 1.861435\n",
      "Ep: 125, steps: 14, D loss: 0.255635, acc:  48%, G loss: 2.015780\n",
      "Ep: 125, steps: 15, D loss: 0.248372, acc:  55%, G loss: 2.127375\n",
      "Ep: 125, steps: 16, D loss: 0.232641, acc:  60%, G loss: 2.242616\n",
      "Ep: 125, steps: 17, D loss: 0.165995, acc:  78%, G loss: 2.328463\n",
      "Ep: 125, steps: 18, D loss: 0.283024, acc:  41%, G loss: 2.275964\n",
      "Ep: 125, steps: 19, D loss: 0.230191, acc:  60%, G loss: 2.055920\n",
      "Ep: 125, steps: 20, D loss: 0.128138, acc:  92%, G loss: 2.529125\n",
      "Ep: 125, steps: 21, D loss: 0.272256, acc:  49%, G loss: 2.062107\n",
      "Ep: 125, steps: 22, D loss: 0.163844, acc:  80%, G loss: 2.009993\n",
      "Ep: 125, steps: 23, D loss: 0.175102, acc:  80%, G loss: 2.494064\n",
      "Ep: 125, steps: 24, D loss: 0.147480, acc:  84%, G loss: 2.296434\n",
      "Ep: 125, steps: 25, D loss: 0.189000, acc:  76%, G loss: 2.040374\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 126, steps: 1, D loss: 0.247285, acc:  58%, G loss: 2.238184\n",
      "Ep: 126, steps: 2, D loss: 0.233117, acc:  61%, G loss: 1.894557\n",
      "Ep: 126, steps: 3, D loss: 0.138775, acc:  85%, G loss: 2.496279\n",
      "Ep: 126, steps: 4, D loss: 0.179111, acc:  77%, G loss: 2.087864\n",
      "Ep: 126, steps: 5, D loss: 0.197679, acc:  72%, G loss: 2.266852\n",
      "Ep: 126, steps: 6, D loss: 0.255791, acc:  50%, G loss: 1.933852\n",
      "Ep: 126, steps: 7, D loss: 0.347092, acc:  36%, G loss: 1.743658\n",
      "Ep: 126, steps: 8, D loss: 0.206774, acc:  69%, G loss: 2.703046\n",
      "Ep: 126, steps: 9, D loss: 0.195875, acc:  73%, G loss: 2.181072\n",
      "Ep: 126, steps: 10, D loss: 0.132960, acc:  83%, G loss: 2.271706\n",
      "Ep: 126, steps: 11, D loss: 0.244253, acc:  56%, G loss: 2.537349\n",
      "Ep: 126, steps: 12, D loss: 0.332886, acc:  28%, G loss: 1.720408\n",
      "Ep: 126, steps: 13, D loss: 0.310260, acc:  34%, G loss: 1.945747\n",
      "Ep: 126, steps: 14, D loss: 0.276466, acc:  42%, G loss: 1.961000\n",
      "Ep: 126, steps: 15, D loss: 0.244243, acc:  55%, G loss: 2.119377\n",
      "Ep: 126, steps: 16, D loss: 0.250292, acc:  56%, G loss: 2.170376\n",
      "Ep: 126, steps: 17, D loss: 0.122910, acc:  90%, G loss: 2.290276\n",
      "Ep: 126, steps: 18, D loss: 0.274109, acc:  44%, G loss: 2.150731\n",
      "Ep: 126, steps: 19, D loss: 0.208145, acc:  66%, G loss: 1.988321\n",
      "Ep: 126, steps: 20, D loss: 0.121869, acc:  91%, G loss: 2.463292\n",
      "Ep: 126, steps: 21, D loss: 0.305180, acc:  40%, G loss: 1.925437\n",
      "Ep: 126, steps: 22, D loss: 0.152810, acc:  84%, G loss: 2.144492\n",
      "Ep: 126, steps: 23, D loss: 0.199050, acc:  73%, G loss: 2.430498\n",
      "Ep: 126, steps: 24, D loss: 0.200473, acc:  66%, G loss: 2.270204\n",
      "Ep: 126, steps: 25, D loss: 0.206238, acc:  66%, G loss: 2.140616\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 127, steps: 1, D loss: 0.231620, acc:  63%, G loss: 2.187313\n",
      "Ep: 127, steps: 2, D loss: 0.222212, acc:  64%, G loss: 2.006221\n",
      "Ep: 127, steps: 3, D loss: 0.133601, acc:  85%, G loss: 2.584421\n",
      "Ep: 127, steps: 4, D loss: 0.184667, acc:  77%, G loss: 2.265596\n",
      "Saved Model\n",
      "Ep: 127, steps: 5, D loss: 0.180570, acc:  74%, G loss: 2.289309\n",
      "Ep: 127, steps: 6, D loss: 0.404105, acc:  24%, G loss: 1.500057\n",
      "Ep: 127, steps: 7, D loss: 0.176151, acc:  80%, G loss: 2.731677\n",
      "Ep: 127, steps: 8, D loss: 0.188550, acc:  73%, G loss: 2.088399\n",
      "Ep: 127, steps: 9, D loss: 0.104297, acc:  95%, G loss: 2.292483\n",
      "Ep: 127, steps: 10, D loss: 0.277507, acc:  50%, G loss: 2.483797\n",
      "Ep: 127, steps: 11, D loss: 0.351411, acc:  29%, G loss: 1.703206\n",
      "Ep: 127, steps: 12, D loss: 0.310746, acc:  37%, G loss: 1.941756\n",
      "Ep: 127, steps: 13, D loss: 0.273815, acc:  44%, G loss: 1.933523\n",
      "Ep: 127, steps: 14, D loss: 0.261444, acc:  52%, G loss: 2.072031\n",
      "Ep: 127, steps: 15, D loss: 0.275705, acc:  49%, G loss: 2.276793\n",
      "Ep: 127, steps: 16, D loss: 0.148792, acc:  85%, G loss: 2.329975\n",
      "Ep: 127, steps: 17, D loss: 0.289445, acc:  37%, G loss: 2.280060\n",
      "Ep: 127, steps: 18, D loss: 0.213224, acc:  64%, G loss: 2.082269\n",
      "Ep: 127, steps: 19, D loss: 0.119031, acc:  91%, G loss: 2.433040\n",
      "Ep: 127, steps: 20, D loss: 0.358079, acc:  38%, G loss: 2.236552\n",
      "Ep: 127, steps: 21, D loss: 0.165457, acc:  75%, G loss: 1.998021\n",
      "Ep: 127, steps: 22, D loss: 0.179679, acc:  77%, G loss: 2.504745\n",
      "Ep: 127, steps: 23, D loss: 0.153959, acc:  81%, G loss: 2.224036\n",
      "Ep: 127, steps: 24, D loss: 0.205887, acc:  68%, G loss: 1.967366\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 128, steps: 1, D loss: 0.277262, acc:  49%, G loss: 2.093785\n",
      "Ep: 128, steps: 2, D loss: 0.243608, acc:  60%, G loss: 1.956369\n",
      "Ep: 128, steps: 3, D loss: 0.172257, acc:  76%, G loss: 2.698160\n",
      "Ep: 128, steps: 4, D loss: 0.197521, acc:  74%, G loss: 2.037676\n",
      "Ep: 128, steps: 5, D loss: 0.176688, acc:  76%, G loss: 2.318192\n",
      "Ep: 128, steps: 6, D loss: 0.258362, acc:  50%, G loss: 2.070214\n",
      "Ep: 128, steps: 7, D loss: 0.391352, acc:  21%, G loss: 1.759875\n",
      "Ep: 128, steps: 8, D loss: 0.205330, acc:  65%, G loss: 2.813410\n",
      "Ep: 128, steps: 9, D loss: 0.189373, acc:  77%, G loss: 2.090465\n",
      "Ep: 128, steps: 10, D loss: 0.143181, acc:  86%, G loss: 2.203183\n",
      "Ep: 128, steps: 11, D loss: 0.258601, acc:  53%, G loss: 2.440340\n",
      "Ep: 128, steps: 12, D loss: 0.334307, acc:  27%, G loss: 1.701757\n",
      "Ep: 128, steps: 13, D loss: 0.301502, acc:  37%, G loss: 1.906098\n",
      "Ep: 128, steps: 14, D loss: 0.269461, acc:  44%, G loss: 1.915792\n",
      "Ep: 128, steps: 15, D loss: 0.259704, acc:  50%, G loss: 2.072872\n",
      "Ep: 128, steps: 16, D loss: 0.279855, acc:  48%, G loss: 2.227601\n",
      "Ep: 128, steps: 17, D loss: 0.151605, acc:  87%, G loss: 2.213201\n",
      "Ep: 128, steps: 18, D loss: 0.269550, acc:  47%, G loss: 2.366138\n",
      "Ep: 128, steps: 19, D loss: 0.215803, acc:  65%, G loss: 1.929896\n",
      "Ep: 128, steps: 20, D loss: 0.120134, acc:  93%, G loss: 2.521489\n",
      "Ep: 128, steps: 21, D loss: 0.271115, acc:  50%, G loss: 2.180159\n",
      "Ep: 128, steps: 22, D loss: 0.148152, acc:  84%, G loss: 2.187900\n",
      "Ep: 128, steps: 23, D loss: 0.168537, acc:  81%, G loss: 2.535231\n",
      "Ep: 128, steps: 24, D loss: 0.156161, acc:  80%, G loss: 2.279309\n",
      "Ep: 128, steps: 25, D loss: 0.229326, acc:  61%, G loss: 2.046681\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 129, steps: 1, D loss: 0.229603, acc:  65%, G loss: 2.241596\n",
      "Ep: 129, steps: 2, D loss: 0.224604, acc:  64%, G loss: 1.942280\n",
      "Ep: 129, steps: 3, D loss: 0.133148, acc:  88%, G loss: 2.503756\n",
      "Ep: 129, steps: 4, D loss: 0.163629, acc:  83%, G loss: 2.177075\n",
      "Ep: 129, steps: 5, D loss: 0.174589, acc:  76%, G loss: 2.215991\n",
      "Ep: 129, steps: 6, D loss: 0.276132, acc:  49%, G loss: 1.964382\n",
      "Ep: 129, steps: 7, D loss: 0.476706, acc:  12%, G loss: 1.898001\n",
      "Ep: 129, steps: 8, D loss: 0.203814, acc:  68%, G loss: 2.340053\n",
      "Ep: 129, steps: 9, D loss: 0.190878, acc:  76%, G loss: 2.069314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 129, steps: 10, D loss: 0.132557, acc:  86%, G loss: 2.214432\n",
      "Ep: 129, steps: 11, D loss: 0.242364, acc:  58%, G loss: 2.455549\n",
      "Ep: 129, steps: 12, D loss: 0.336340, acc:  31%, G loss: 1.731111\n",
      "Ep: 129, steps: 13, D loss: 0.298934, acc:  39%, G loss: 1.850365\n",
      "Ep: 129, steps: 14, D loss: 0.261265, acc:  45%, G loss: 1.968587\n",
      "Ep: 129, steps: 15, D loss: 0.254650, acc:  53%, G loss: 2.047428\n",
      "Ep: 129, steps: 16, D loss: 0.260633, acc:  53%, G loss: 2.248123\n",
      "Ep: 129, steps: 17, D loss: 0.136305, acc:  89%, G loss: 2.270671\n",
      "Ep: 129, steps: 18, D loss: 0.271098, acc:  47%, G loss: 2.370643\n",
      "Ep: 129, steps: 19, D loss: 0.212249, acc:  67%, G loss: 2.004188\n",
      "Ep: 129, steps: 20, D loss: 0.110460, acc:  94%, G loss: 2.374530\n",
      "Ep: 129, steps: 21, D loss: 0.262495, acc:  49%, G loss: 2.291322\n",
      "Ep: 129, steps: 22, D loss: 0.161854, acc:  80%, G loss: 2.057532\n",
      "Ep: 129, steps: 23, D loss: 0.170016, acc:  81%, G loss: 2.631781\n",
      "Ep: 129, steps: 24, D loss: 0.141203, acc:  83%, G loss: 2.373918\n",
      "Ep: 129, steps: 25, D loss: 0.159965, acc:  81%, G loss: 2.209847\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 130, steps: 1, D loss: 0.256914, acc:  57%, G loss: 2.489963\n",
      "Ep: 130, steps: 2, D loss: 0.232028, acc:  59%, G loss: 2.010561\n",
      "Saved Model\n",
      "Ep: 130, steps: 3, D loss: 0.131160, acc:  90%, G loss: 2.574908\n",
      "Ep: 130, steps: 4, D loss: 0.163421, acc:  79%, G loss: 2.458369\n",
      "Ep: 130, steps: 5, D loss: 0.272274, acc:  49%, G loss: 2.159608\n",
      "Ep: 130, steps: 6, D loss: 0.293618, acc:  43%, G loss: 1.832509\n",
      "Ep: 130, steps: 7, D loss: 0.186648, acc:  70%, G loss: 2.565625\n",
      "Ep: 130, steps: 8, D loss: 0.194708, acc:  75%, G loss: 2.163885\n",
      "Ep: 130, steps: 9, D loss: 0.135884, acc:  85%, G loss: 2.184486\n",
      "Ep: 130, steps: 10, D loss: 0.254477, acc:  52%, G loss: 2.484310\n",
      "Ep: 130, steps: 11, D loss: 0.337124, acc:  27%, G loss: 1.740989\n",
      "Ep: 130, steps: 12, D loss: 0.295811, acc:  37%, G loss: 1.926435\n",
      "Ep: 130, steps: 13, D loss: 0.270306, acc:  44%, G loss: 1.980327\n",
      "Ep: 130, steps: 14, D loss: 0.262370, acc:  49%, G loss: 2.001224\n",
      "Ep: 130, steps: 15, D loss: 0.256829, acc:  54%, G loss: 2.357280\n",
      "Ep: 130, steps: 16, D loss: 0.147016, acc:  86%, G loss: 2.067862\n",
      "Ep: 130, steps: 17, D loss: 0.292011, acc:  41%, G loss: 2.430847\n",
      "Ep: 130, steps: 18, D loss: 0.210136, acc:  65%, G loss: 2.015810\n",
      "Ep: 130, steps: 19, D loss: 0.114398, acc:  93%, G loss: 2.348179\n",
      "Ep: 130, steps: 20, D loss: 0.296024, acc:  47%, G loss: 2.269929\n",
      "Ep: 130, steps: 21, D loss: 0.152423, acc:  82%, G loss: 2.038624\n",
      "Ep: 130, steps: 22, D loss: 0.167830, acc:  83%, G loss: 2.580595\n",
      "Ep: 130, steps: 23, D loss: 0.146393, acc:  84%, G loss: 2.352200\n",
      "Ep: 130, steps: 24, D loss: 0.181904, acc:  74%, G loss: 2.166722\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 131, steps: 1, D loss: 0.258765, acc:  56%, G loss: 2.490435\n",
      "Ep: 131, steps: 2, D loss: 0.247861, acc:  57%, G loss: 1.962674\n",
      "Ep: 131, steps: 3, D loss: 0.168930, acc:  76%, G loss: 2.564059\n",
      "Ep: 131, steps: 4, D loss: 0.196032, acc:  75%, G loss: 2.092355\n",
      "Ep: 131, steps: 5, D loss: 0.164573, acc:  78%, G loss: 2.441309\n",
      "Ep: 131, steps: 6, D loss: 0.263976, acc:  51%, G loss: 1.933898\n",
      "Ep: 131, steps: 7, D loss: 0.411703, acc:  21%, G loss: 1.783013\n",
      "Ep: 131, steps: 8, D loss: 0.217366, acc:  63%, G loss: 2.588437\n",
      "Ep: 131, steps: 9, D loss: 0.202565, acc:  71%, G loss: 2.113105\n",
      "Ep: 131, steps: 10, D loss: 0.136496, acc:  86%, G loss: 2.136425\n",
      "Ep: 131, steps: 11, D loss: 0.258687, acc:  53%, G loss: 2.465469\n",
      "Ep: 131, steps: 12, D loss: 0.334955, acc:  25%, G loss: 1.683180\n",
      "Ep: 131, steps: 13, D loss: 0.300129, acc:  37%, G loss: 1.886777\n",
      "Ep: 131, steps: 14, D loss: 0.276522, acc:  40%, G loss: 1.958102\n",
      "Ep: 131, steps: 15, D loss: 0.258225, acc:  51%, G loss: 1.937578\n",
      "Ep: 131, steps: 16, D loss: 0.270196, acc:  52%, G loss: 2.186255\n",
      "Ep: 131, steps: 17, D loss: 0.138933, acc:  88%, G loss: 2.336613\n",
      "Ep: 131, steps: 18, D loss: 0.268516, acc:  47%, G loss: 2.516394\n",
      "Ep: 131, steps: 19, D loss: 0.213013, acc:  64%, G loss: 1.974287\n",
      "Ep: 131, steps: 20, D loss: 0.120628, acc:  94%, G loss: 2.449580\n",
      "Ep: 131, steps: 21, D loss: 0.263622, acc:  53%, G loss: 2.238819\n",
      "Ep: 131, steps: 22, D loss: 0.146622, acc:  84%, G loss: 2.247499\n",
      "Ep: 131, steps: 23, D loss: 0.158640, acc:  84%, G loss: 2.556259\n",
      "Ep: 131, steps: 24, D loss: 0.148241, acc:  81%, G loss: 2.272551\n",
      "Ep: 131, steps: 25, D loss: 0.207312, acc:  71%, G loss: 2.098641\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 132, steps: 1, D loss: 0.218497, acc:  68%, G loss: 2.386683\n",
      "Ep: 132, steps: 2, D loss: 0.246397, acc:  56%, G loss: 2.094831\n",
      "Ep: 132, steps: 3, D loss: 0.148444, acc:  81%, G loss: 2.526360\n",
      "Ep: 132, steps: 4, D loss: 0.174602, acc:  79%, G loss: 2.128371\n",
      "Ep: 132, steps: 5, D loss: 0.177012, acc:  74%, G loss: 2.301368\n",
      "Ep: 132, steps: 6, D loss: 0.275634, acc:  50%, G loss: 2.016194\n",
      "Ep: 132, steps: 7, D loss: 0.420007, acc:  20%, G loss: 2.051299\n",
      "Ep: 132, steps: 8, D loss: 0.227652, acc:  61%, G loss: 2.413485\n",
      "Ep: 132, steps: 9, D loss: 0.205078, acc:  70%, G loss: 2.133793\n",
      "Ep: 132, steps: 10, D loss: 0.126927, acc:  89%, G loss: 2.184703\n",
      "Ep: 132, steps: 11, D loss: 0.247556, acc:  57%, G loss: 2.435227\n",
      "Ep: 132, steps: 12, D loss: 0.346867, acc:  27%, G loss: 1.701906\n",
      "Ep: 132, steps: 13, D loss: 0.304330, acc:  32%, G loss: 1.970346\n",
      "Ep: 132, steps: 14, D loss: 0.256093, acc:  50%, G loss: 2.057860\n",
      "Ep: 132, steps: 15, D loss: 0.249192, acc:  54%, G loss: 1.962656\n",
      "Ep: 132, steps: 16, D loss: 0.242914, acc:  58%, G loss: 2.179835\n",
      "Ep: 132, steps: 17, D loss: 0.111415, acc:  92%, G loss: 2.266738\n",
      "Ep: 132, steps: 18, D loss: 0.282910, acc:  44%, G loss: 2.303593\n",
      "Ep: 132, steps: 19, D loss: 0.217129, acc:  61%, G loss: 2.033455\n",
      "Ep: 132, steps: 20, D loss: 0.117240, acc:  93%, G loss: 2.536579\n",
      "Ep: 132, steps: 21, D loss: 0.269347, acc:  51%, G loss: 2.146891\n",
      "Ep: 132, steps: 22, D loss: 0.151225, acc:  81%, G loss: 2.125956\n",
      "Ep: 132, steps: 23, D loss: 0.194686, acc:  73%, G loss: 2.503122\n",
      "Ep: 132, steps: 24, D loss: 0.166140, acc:  77%, G loss: 2.339260\n",
      "Ep: 132, steps: 25, D loss: 0.200159, acc:  70%, G loss: 2.041212\n",
      "Data exhausted, Re Initialize\n",
      "Saved Model\n",
      "Ep: 133, steps: 1, D loss: 0.235070, acc:  59%, G loss: 2.435570\n",
      "Ep: 133, steps: 2, D loss: 0.125855, acc:  90%, G loss: 2.549913\n",
      "Ep: 133, steps: 3, D loss: 0.216391, acc:  70%, G loss: 2.086199\n",
      "Ep: 133, steps: 4, D loss: 0.176805, acc:  75%, G loss: 2.348471\n",
      "Ep: 133, steps: 5, D loss: 0.288443, acc:  48%, G loss: 1.933716\n",
      "Ep: 133, steps: 6, D loss: 0.311541, acc:  38%, G loss: 1.703742\n",
      "Ep: 133, steps: 7, D loss: 0.186799, acc:  70%, G loss: 2.570172\n",
      "Ep: 133, steps: 8, D loss: 0.190783, acc:  74%, G loss: 2.110136\n",
      "Ep: 133, steps: 9, D loss: 0.130873, acc:  87%, G loss: 2.268651\n",
      "Ep: 133, steps: 10, D loss: 0.251305, acc:  54%, G loss: 2.451599\n",
      "Ep: 133, steps: 11, D loss: 0.333035, acc:  31%, G loss: 1.742953\n",
      "Ep: 133, steps: 12, D loss: 0.302847, acc:  33%, G loss: 1.895308\n",
      "Ep: 133, steps: 13, D loss: 0.279905, acc:  40%, G loss: 1.942650\n",
      "Ep: 133, steps: 14, D loss: 0.270633, acc:  46%, G loss: 2.052449\n",
      "Ep: 133, steps: 15, D loss: 0.253952, acc:  55%, G loss: 2.110290\n",
      "Ep: 133, steps: 16, D loss: 0.136131, acc:  89%, G loss: 2.153341\n",
      "Ep: 133, steps: 17, D loss: 0.302347, acc:  36%, G loss: 2.203328\n",
      "Ep: 133, steps: 18, D loss: 0.197340, acc:  66%, G loss: 2.044123\n",
      "Ep: 133, steps: 19, D loss: 0.116082, acc:  92%, G loss: 2.368007\n",
      "Ep: 133, steps: 20, D loss: 0.280186, acc:  45%, G loss: 2.154291\n",
      "Ep: 133, steps: 21, D loss: 0.167390, acc:  76%, G loss: 2.069261\n",
      "Ep: 133, steps: 22, D loss: 0.181719, acc:  78%, G loss: 2.575776\n",
      "Ep: 133, steps: 23, D loss: 0.155123, acc:  79%, G loss: 2.307046\n",
      "Ep: 133, steps: 24, D loss: 0.190835, acc:  73%, G loss: 2.114506\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 134, steps: 1, D loss: 0.222834, acc:  64%, G loss: 2.286246\n",
      "Ep: 134, steps: 2, D loss: 0.249970, acc:  56%, G loss: 2.024802\n",
      "Ep: 134, steps: 3, D loss: 0.135065, acc:  87%, G loss: 2.625316\n",
      "Ep: 134, steps: 4, D loss: 0.176722, acc:  79%, G loss: 2.126375\n",
      "Ep: 134, steps: 5, D loss: 0.172888, acc:  77%, G loss: 2.340561\n",
      "Ep: 134, steps: 6, D loss: 0.272887, acc:  50%, G loss: 2.022396\n",
      "Ep: 134, steps: 7, D loss: 0.421893, acc:  22%, G loss: 1.791445\n",
      "Ep: 134, steps: 8, D loss: 0.220765, acc:  64%, G loss: 2.404882\n",
      "Ep: 134, steps: 9, D loss: 0.199773, acc:  71%, G loss: 2.165456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 134, steps: 10, D loss: 0.159510, acc:  78%, G loss: 2.132850\n",
      "Ep: 134, steps: 11, D loss: 0.261764, acc:  53%, G loss: 2.481238\n",
      "Ep: 134, steps: 12, D loss: 0.331779, acc:  26%, G loss: 1.776273\n",
      "Ep: 134, steps: 13, D loss: 0.287664, acc:  39%, G loss: 1.922897\n",
      "Ep: 134, steps: 14, D loss: 0.275952, acc:  43%, G loss: 1.894377\n",
      "Ep: 134, steps: 15, D loss: 0.269703, acc:  44%, G loss: 1.934422\n",
      "Ep: 134, steps: 16, D loss: 0.254522, acc:  54%, G loss: 2.154656\n",
      "Ep: 134, steps: 17, D loss: 0.149196, acc:  86%, G loss: 2.428320\n",
      "Ep: 134, steps: 18, D loss: 0.268773, acc:  48%, G loss: 2.143886\n",
      "Ep: 134, steps: 19, D loss: 0.211643, acc:  65%, G loss: 1.995373\n",
      "Ep: 134, steps: 20, D loss: 0.122081, acc:  91%, G loss: 2.425065\n",
      "Ep: 134, steps: 21, D loss: 0.272142, acc:  48%, G loss: 2.341732\n",
      "Ep: 134, steps: 22, D loss: 0.154815, acc:  82%, G loss: 1.948577\n",
      "Ep: 134, steps: 23, D loss: 0.175882, acc:  79%, G loss: 2.523066\n",
      "Ep: 134, steps: 24, D loss: 0.147079, acc:  82%, G loss: 2.360433\n",
      "Ep: 134, steps: 25, D loss: 0.195834, acc:  69%, G loss: 2.088007\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 135, steps: 1, D loss: 0.213081, acc:  67%, G loss: 2.226192\n",
      "Ep: 135, steps: 2, D loss: 0.250846, acc:  53%, G loss: 1.923147\n",
      "Ep: 135, steps: 3, D loss: 0.152610, acc:  81%, G loss: 2.717093\n",
      "Ep: 135, steps: 4, D loss: 0.166936, acc:  84%, G loss: 2.291305\n",
      "Ep: 135, steps: 5, D loss: 0.210326, acc:  69%, G loss: 2.242314\n",
      "Ep: 135, steps: 6, D loss: 0.268447, acc:  51%, G loss: 2.011435\n",
      "Ep: 135, steps: 7, D loss: 0.425688, acc:  20%, G loss: 1.610392\n",
      "Ep: 135, steps: 8, D loss: 0.215994, acc:  64%, G loss: 2.576935\n",
      "Ep: 135, steps: 9, D loss: 0.197743, acc:  72%, G loss: 2.154912\n",
      "Ep: 135, steps: 10, D loss: 0.140470, acc:  84%, G loss: 2.270774\n",
      "Ep: 135, steps: 11, D loss: 0.251324, acc:  56%, G loss: 2.465810\n",
      "Ep: 135, steps: 12, D loss: 0.341141, acc:  28%, G loss: 1.774538\n",
      "Ep: 135, steps: 13, D loss: 0.293048, acc:  38%, G loss: 1.887325\n",
      "Ep: 135, steps: 14, D loss: 0.273918, acc:  43%, G loss: 1.903912\n",
      "Ep: 135, steps: 15, D loss: 0.247223, acc:  55%, G loss: 1.935271\n",
      "Ep: 135, steps: 16, D loss: 0.236906, acc:  60%, G loss: 2.204869\n",
      "Ep: 135, steps: 17, D loss: 0.123216, acc:  90%, G loss: 2.287801\n",
      "Ep: 135, steps: 18, D loss: 0.285721, acc:  41%, G loss: 2.031581\n",
      "Ep: 135, steps: 19, D loss: 0.194368, acc:  69%, G loss: 2.051638\n",
      "Ep: 135, steps: 20, D loss: 0.109684, acc:  93%, G loss: 2.436402\n",
      "Ep: 135, steps: 21, D loss: 0.291349, acc:  52%, G loss: 1.967945\n",
      "Ep: 135, steps: 22, D loss: 0.162651, acc:  79%, G loss: 2.134250\n",
      "Saved Model\n",
      "Ep: 135, steps: 23, D loss: 0.174410, acc:  79%, G loss: 2.540070\n",
      "Ep: 135, steps: 24, D loss: 0.221474, acc:  64%, G loss: 2.237173\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 136, steps: 1, D loss: 0.216916, acc:  67%, G loss: 2.201368\n",
      "Ep: 136, steps: 2, D loss: 0.232678, acc:  59%, G loss: 1.971481\n",
      "Ep: 136, steps: 3, D loss: 0.147289, acc:  83%, G loss: 2.616641\n",
      "Ep: 136, steps: 4, D loss: 0.178763, acc:  80%, G loss: 2.134401\n",
      "Ep: 136, steps: 5, D loss: 0.281209, acc:  62%, G loss: 2.499426\n",
      "Ep: 136, steps: 6, D loss: 0.266744, acc:  51%, G loss: 2.049563\n",
      "Ep: 136, steps: 7, D loss: 0.372287, acc:  27%, G loss: 1.822234\n",
      "Ep: 136, steps: 8, D loss: 0.216753, acc:  63%, G loss: 2.486727\n",
      "Ep: 136, steps: 9, D loss: 0.190124, acc:  75%, G loss: 2.074574\n",
      "Ep: 136, steps: 10, D loss: 0.140205, acc:  84%, G loss: 2.138776\n",
      "Ep: 136, steps: 11, D loss: 0.253824, acc:  53%, G loss: 2.455619\n",
      "Ep: 136, steps: 12, D loss: 0.328531, acc:  30%, G loss: 1.757814\n",
      "Ep: 136, steps: 13, D loss: 0.305598, acc:  33%, G loss: 1.828873\n",
      "Ep: 136, steps: 14, D loss: 0.282549, acc:  40%, G loss: 1.875110\n",
      "Ep: 136, steps: 15, D loss: 0.234318, acc:  61%, G loss: 1.965902\n",
      "Ep: 136, steps: 16, D loss: 0.252922, acc:  56%, G loss: 2.071212\n",
      "Ep: 136, steps: 17, D loss: 0.120777, acc:  93%, G loss: 2.276241\n",
      "Ep: 136, steps: 18, D loss: 0.274068, acc:  48%, G loss: 2.052163\n",
      "Ep: 136, steps: 19, D loss: 0.193076, acc:  68%, G loss: 2.025407\n",
      "Ep: 136, steps: 20, D loss: 0.120986, acc:  91%, G loss: 2.389372\n",
      "Ep: 136, steps: 21, D loss: 0.281829, acc:  48%, G loss: 2.063313\n",
      "Ep: 136, steps: 22, D loss: 0.157776, acc:  81%, G loss: 2.257372\n",
      "Ep: 136, steps: 23, D loss: 0.175575, acc:  79%, G loss: 2.576965\n",
      "Ep: 136, steps: 24, D loss: 0.155517, acc:  80%, G loss: 2.278504\n",
      "Ep: 136, steps: 25, D loss: 0.187198, acc:  73%, G loss: 2.188031\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 137, steps: 1, D loss: 0.236178, acc:  62%, G loss: 2.296343\n",
      "Ep: 137, steps: 2, D loss: 0.246038, acc:  55%, G loss: 2.331527\n",
      "Ep: 137, steps: 3, D loss: 0.136400, acc:  86%, G loss: 2.649557\n",
      "Ep: 137, steps: 4, D loss: 0.182372, acc:  80%, G loss: 2.193717\n",
      "Ep: 137, steps: 5, D loss: 0.291224, acc:  64%, G loss: 2.242081\n",
      "Ep: 137, steps: 6, D loss: 0.287380, acc:  49%, G loss: 2.495707\n",
      "Ep: 137, steps: 7, D loss: 0.426153, acc:  20%, G loss: 1.620987\n",
      "Ep: 137, steps: 8, D loss: 0.234399, acc:  59%, G loss: 2.529024\n",
      "Ep: 137, steps: 9, D loss: 0.178797, acc:  79%, G loss: 2.169492\n",
      "Ep: 137, steps: 10, D loss: 0.136832, acc:  85%, G loss: 2.198255\n",
      "Ep: 137, steps: 11, D loss: 0.234887, acc:  60%, G loss: 2.465760\n",
      "Ep: 137, steps: 12, D loss: 0.331958, acc:  31%, G loss: 1.711323\n",
      "Ep: 137, steps: 13, D loss: 0.302376, acc:  36%, G loss: 1.886708\n",
      "Ep: 137, steps: 14, D loss: 0.283595, acc:  39%, G loss: 1.866966\n",
      "Ep: 137, steps: 15, D loss: 0.236829, acc:  58%, G loss: 2.014307\n",
      "Ep: 137, steps: 16, D loss: 0.245514, acc:  57%, G loss: 2.140933\n",
      "Ep: 137, steps: 17, D loss: 0.114539, acc:  93%, G loss: 2.201916\n",
      "Ep: 137, steps: 18, D loss: 0.281845, acc:  45%, G loss: 1.998796\n",
      "Ep: 137, steps: 19, D loss: 0.196384, acc:  68%, G loss: 2.076034\n",
      "Ep: 137, steps: 20, D loss: 0.113386, acc:  92%, G loss: 2.336482\n",
      "Ep: 137, steps: 21, D loss: 0.337256, acc:  42%, G loss: 2.032872\n",
      "Ep: 137, steps: 22, D loss: 0.151050, acc:  81%, G loss: 2.218115\n",
      "Ep: 137, steps: 23, D loss: 0.160594, acc:  82%, G loss: 2.495335\n",
      "Ep: 137, steps: 24, D loss: 0.162862, acc:  79%, G loss: 2.168620\n",
      "Ep: 137, steps: 25, D loss: 0.213563, acc:  65%, G loss: 2.041799\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 138, steps: 1, D loss: 0.235280, acc:  61%, G loss: 2.241923\n",
      "Ep: 138, steps: 2, D loss: 0.228501, acc:  62%, G loss: 2.086993\n",
      "Ep: 138, steps: 3, D loss: 0.154753, acc:  82%, G loss: 2.648389\n",
      "Ep: 138, steps: 4, D loss: 0.190063, acc:  75%, G loss: 2.075657\n",
      "Ep: 138, steps: 5, D loss: 0.240861, acc:  66%, G loss: 2.257740\n",
      "Ep: 138, steps: 6, D loss: 0.273375, acc:  50%, G loss: 1.954353\n",
      "Ep: 138, steps: 7, D loss: 0.420353, acc:  19%, G loss: 1.544795\n",
      "Ep: 138, steps: 8, D loss: 0.246190, acc:  56%, G loss: 2.401690\n",
      "Ep: 138, steps: 9, D loss: 0.194664, acc:  71%, G loss: 2.160080\n",
      "Ep: 138, steps: 10, D loss: 0.135964, acc:  88%, G loss: 2.169927\n",
      "Ep: 138, steps: 11, D loss: 0.256693, acc:  53%, G loss: 2.494489\n",
      "Ep: 138, steps: 12, D loss: 0.328001, acc:  31%, G loss: 1.730773\n",
      "Ep: 138, steps: 13, D loss: 0.295515, acc:  39%, G loss: 1.937813\n",
      "Ep: 138, steps: 14, D loss: 0.286187, acc:  38%, G loss: 1.957295\n",
      "Ep: 138, steps: 15, D loss: 0.278757, acc:  41%, G loss: 2.020826\n",
      "Ep: 138, steps: 16, D loss: 0.258991, acc:  54%, G loss: 2.016836\n",
      "Ep: 138, steps: 17, D loss: 0.141252, acc:  88%, G loss: 2.318027\n",
      "Ep: 138, steps: 18, D loss: 0.265875, acc:  48%, G loss: 2.153577\n",
      "Ep: 138, steps: 19, D loss: 0.210137, acc:  66%, G loss: 1.997238\n",
      "Ep: 138, steps: 20, D loss: 0.120745, acc:  90%, G loss: 2.419019\n",
      "Saved Model\n",
      "Ep: 138, steps: 21, D loss: 0.265202, acc:  53%, G loss: 2.245738\n",
      "Ep: 138, steps: 22, D loss: 0.171250, acc:  81%, G loss: 2.504250\n",
      "Ep: 138, steps: 23, D loss: 0.152353, acc:  82%, G loss: 2.101856\n",
      "Ep: 138, steps: 24, D loss: 0.223949, acc:  66%, G loss: 2.012735\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 139, steps: 1, D loss: 0.205745, acc:  72%, G loss: 2.159361\n",
      "Ep: 139, steps: 2, D loss: 0.216022, acc:  66%, G loss: 2.203956\n",
      "Ep: 139, steps: 3, D loss: 0.133118, acc:  87%, G loss: 2.822834\n",
      "Ep: 139, steps: 4, D loss: 0.177664, acc:  81%, G loss: 2.155699\n",
      "Ep: 139, steps: 5, D loss: 0.216068, acc:  68%, G loss: 2.422973\n",
      "Ep: 139, steps: 6, D loss: 0.255033, acc:  51%, G loss: 2.231958\n",
      "Ep: 139, steps: 7, D loss: 0.359200, acc:  33%, G loss: 1.846931\n",
      "Ep: 139, steps: 8, D loss: 0.226526, acc:  68%, G loss: 2.292608\n",
      "Ep: 139, steps: 9, D loss: 0.227220, acc:  62%, G loss: 2.061325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 139, steps: 10, D loss: 0.139324, acc:  82%, G loss: 2.271411\n",
      "Ep: 139, steps: 11, D loss: 0.233837, acc:  63%, G loss: 2.481666\n",
      "Ep: 139, steps: 12, D loss: 0.316098, acc:  38%, G loss: 1.813025\n",
      "Ep: 139, steps: 13, D loss: 0.312288, acc:  34%, G loss: 1.847802\n",
      "Ep: 139, steps: 14, D loss: 0.286586, acc:  40%, G loss: 1.879481\n",
      "Ep: 139, steps: 15, D loss: 0.229982, acc:  61%, G loss: 2.009953\n",
      "Ep: 139, steps: 16, D loss: 0.249080, acc:  56%, G loss: 2.213615\n",
      "Ep: 139, steps: 17, D loss: 0.129510, acc:  91%, G loss: 2.070240\n",
      "Ep: 139, steps: 18, D loss: 0.274798, acc:  44%, G loss: 2.034064\n",
      "Ep: 139, steps: 19, D loss: 0.210120, acc:  64%, G loss: 1.981309\n",
      "Ep: 139, steps: 20, D loss: 0.119888, acc:  91%, G loss: 2.253881\n",
      "Ep: 139, steps: 21, D loss: 0.393977, acc:  32%, G loss: 1.975739\n",
      "Ep: 139, steps: 22, D loss: 0.162903, acc:  75%, G loss: 1.941093\n",
      "Ep: 139, steps: 23, D loss: 0.175682, acc:  78%, G loss: 2.450213\n",
      "Ep: 139, steps: 24, D loss: 0.166393, acc:  78%, G loss: 2.236182\n",
      "Ep: 139, steps: 25, D loss: 0.206892, acc:  66%, G loss: 1.970867\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 140, steps: 1, D loss: 0.229313, acc:  64%, G loss: 2.209774\n",
      "Ep: 140, steps: 2, D loss: 0.228750, acc:  62%, G loss: 1.936664\n",
      "Ep: 140, steps: 3, D loss: 0.143567, acc:  86%, G loss: 2.569799\n",
      "Ep: 140, steps: 4, D loss: 0.192243, acc:  75%, G loss: 2.062372\n",
      "Ep: 140, steps: 5, D loss: 0.178834, acc:  76%, G loss: 2.336601\n",
      "Ep: 140, steps: 6, D loss: 0.259581, acc:  50%, G loss: 1.954473\n",
      "Ep: 140, steps: 7, D loss: 0.450320, acc:  15%, G loss: 1.619765\n",
      "Ep: 140, steps: 8, D loss: 0.209422, acc:  62%, G loss: 2.318115\n",
      "Ep: 140, steps: 9, D loss: 0.179977, acc:  80%, G loss: 2.074082\n",
      "Ep: 140, steps: 10, D loss: 0.131110, acc:  88%, G loss: 2.143156\n",
      "Ep: 140, steps: 11, D loss: 0.243837, acc:  59%, G loss: 2.441430\n",
      "Ep: 140, steps: 12, D loss: 0.320497, acc:  32%, G loss: 1.741630\n",
      "Ep: 140, steps: 13, D loss: 0.292392, acc:  37%, G loss: 1.874282\n",
      "Ep: 140, steps: 14, D loss: 0.296049, acc:  35%, G loss: 1.818794\n",
      "Ep: 140, steps: 15, D loss: 0.249500, acc:  52%, G loss: 2.008847\n",
      "Ep: 140, steps: 16, D loss: 0.267214, acc:  52%, G loss: 2.067868\n",
      "Ep: 140, steps: 17, D loss: 0.193745, acc:  75%, G loss: 2.298701\n",
      "Ep: 140, steps: 18, D loss: 0.274592, acc:  44%, G loss: 2.047083\n",
      "Ep: 140, steps: 19, D loss: 0.214973, acc:  65%, G loss: 2.003540\n",
      "Ep: 140, steps: 20, D loss: 0.124749, acc:  90%, G loss: 2.380036\n",
      "Ep: 140, steps: 21, D loss: 0.270128, acc:  46%, G loss: 2.206389\n",
      "Ep: 140, steps: 22, D loss: 0.161018, acc:  79%, G loss: 1.922445\n",
      "Ep: 140, steps: 23, D loss: 0.169297, acc:  81%, G loss: 2.548003\n",
      "Ep: 140, steps: 24, D loss: 0.150665, acc:  83%, G loss: 2.250886\n",
      "Ep: 140, steps: 25, D loss: 0.204267, acc:  70%, G loss: 2.073371\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 141, steps: 1, D loss: 0.222636, acc:  66%, G loss: 2.285510\n",
      "Ep: 141, steps: 2, D loss: 0.239702, acc:  58%, G loss: 2.010760\n",
      "Ep: 141, steps: 3, D loss: 0.145000, acc:  86%, G loss: 2.543178\n",
      "Ep: 141, steps: 4, D loss: 0.184033, acc:  78%, G loss: 1.989090\n",
      "Ep: 141, steps: 5, D loss: 0.179132, acc:  76%, G loss: 2.418648\n",
      "Ep: 141, steps: 6, D loss: 0.259239, acc:  51%, G loss: 1.888002\n",
      "Ep: 141, steps: 7, D loss: 0.358256, acc:  33%, G loss: 1.787476\n",
      "Ep: 141, steps: 8, D loss: 0.208344, acc:  67%, G loss: 2.239508\n",
      "Ep: 141, steps: 9, D loss: 0.202039, acc:  72%, G loss: 2.037666\n",
      "Ep: 141, steps: 10, D loss: 0.134087, acc:  85%, G loss: 2.277531\n",
      "Ep: 141, steps: 11, D loss: 0.253969, acc:  54%, G loss: 2.477568\n",
      "Ep: 141, steps: 12, D loss: 0.322603, acc:  31%, G loss: 1.754667\n",
      "Ep: 141, steps: 13, D loss: 0.293267, acc:  36%, G loss: 1.874149\n",
      "Ep: 141, steps: 14, D loss: 0.304511, acc:  34%, G loss: 1.861549\n",
      "Ep: 141, steps: 15, D loss: 0.235019, acc:  60%, G loss: 1.940889\n",
      "Ep: 141, steps: 16, D loss: 0.267780, acc:  51%, G loss: 2.073426\n",
      "Ep: 141, steps: 17, D loss: 0.120951, acc:  91%, G loss: 2.540870\n",
      "Ep: 141, steps: 18, D loss: 0.269544, acc:  46%, G loss: 2.002430\n",
      "Saved Model\n",
      "Ep: 141, steps: 19, D loss: 0.189657, acc:  69%, G loss: 2.006650\n",
      "Ep: 141, steps: 20, D loss: 0.270999, acc:  44%, G loss: 2.222585\n",
      "Ep: 141, steps: 21, D loss: 0.162471, acc:  78%, G loss: 2.461325\n",
      "Ep: 141, steps: 22, D loss: 0.183582, acc:  75%, G loss: 2.557424\n",
      "Ep: 141, steps: 23, D loss: 0.145823, acc:  84%, G loss: 2.277148\n",
      "Ep: 141, steps: 24, D loss: 0.272720, acc:  54%, G loss: 2.259116\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 142, steps: 1, D loss: 0.249935, acc:  54%, G loss: 2.117592\n",
      "Ep: 142, steps: 2, D loss: 0.258443, acc:  52%, G loss: 2.247042\n",
      "Ep: 142, steps: 3, D loss: 0.222243, acc:  72%, G loss: 2.609875\n",
      "Ep: 142, steps: 4, D loss: 0.189991, acc:  77%, G loss: 2.276395\n",
      "Ep: 142, steps: 5, D loss: 0.225347, acc:  68%, G loss: 2.191751\n",
      "Ep: 142, steps: 6, D loss: 0.272068, acc:  49%, G loss: 1.996218\n",
      "Ep: 142, steps: 7, D loss: 0.432469, acc:  14%, G loss: 1.538657\n",
      "Ep: 142, steps: 8, D loss: 0.222974, acc:  64%, G loss: 2.283214\n",
      "Ep: 142, steps: 9, D loss: 0.170721, acc:  81%, G loss: 2.160535\n",
      "Ep: 142, steps: 10, D loss: 0.130684, acc:  90%, G loss: 2.160118\n",
      "Ep: 142, steps: 11, D loss: 0.234478, acc:  61%, G loss: 2.439660\n",
      "Ep: 142, steps: 12, D loss: 0.312927, acc:  36%, G loss: 1.784391\n",
      "Ep: 142, steps: 13, D loss: 0.287499, acc:  40%, G loss: 1.857138\n",
      "Ep: 142, steps: 14, D loss: 0.286440, acc:  38%, G loss: 1.870638\n",
      "Ep: 142, steps: 15, D loss: 0.251528, acc:  52%, G loss: 2.003681\n",
      "Ep: 142, steps: 16, D loss: 0.284977, acc:  50%, G loss: 2.099476\n",
      "Ep: 142, steps: 17, D loss: 0.127141, acc:  92%, G loss: 2.144435\n",
      "Ep: 142, steps: 18, D loss: 0.285637, acc:  42%, G loss: 2.267121\n",
      "Ep: 142, steps: 19, D loss: 0.188701, acc:  70%, G loss: 2.185694\n",
      "Ep: 142, steps: 20, D loss: 0.126465, acc:  88%, G loss: 2.457013\n",
      "Ep: 142, steps: 21, D loss: 0.320959, acc:  43%, G loss: 2.033380\n",
      "Ep: 142, steps: 22, D loss: 0.155687, acc:  80%, G loss: 2.153626\n",
      "Ep: 142, steps: 23, D loss: 0.168723, acc:  81%, G loss: 2.537763\n",
      "Ep: 142, steps: 24, D loss: 0.155854, acc:  79%, G loss: 2.220472\n",
      "Ep: 142, steps: 25, D loss: 0.192421, acc:  69%, G loss: 1.992658\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 143, steps: 1, D loss: 0.227483, acc:  65%, G loss: 2.589961\n",
      "Ep: 143, steps: 2, D loss: 0.227621, acc:  62%, G loss: 2.027641\n",
      "Ep: 143, steps: 3, D loss: 0.149051, acc:  86%, G loss: 2.752101\n",
      "Ep: 143, steps: 4, D loss: 0.183646, acc:  80%, G loss: 2.131039\n",
      "Ep: 143, steps: 5, D loss: 0.170102, acc:  77%, G loss: 2.231457\n",
      "Ep: 143, steps: 6, D loss: 0.246877, acc:  50%, G loss: 1.958583\n",
      "Ep: 143, steps: 7, D loss: 0.408008, acc:  21%, G loss: 1.673431\n",
      "Ep: 143, steps: 8, D loss: 0.199978, acc:  69%, G loss: 2.391908\n",
      "Ep: 143, steps: 9, D loss: 0.187189, acc:  73%, G loss: 2.113926\n",
      "Ep: 143, steps: 10, D loss: 0.130488, acc:  89%, G loss: 2.219762\n",
      "Ep: 143, steps: 11, D loss: 0.241092, acc:  59%, G loss: 2.449952\n",
      "Ep: 143, steps: 12, D loss: 0.325046, acc:  30%, G loss: 1.673418\n",
      "Ep: 143, steps: 13, D loss: 0.298157, acc:  37%, G loss: 1.817919\n",
      "Ep: 143, steps: 14, D loss: 0.298356, acc:  34%, G loss: 1.836873\n",
      "Ep: 143, steps: 15, D loss: 0.242815, acc:  56%, G loss: 1.939196\n",
      "Ep: 143, steps: 16, D loss: 0.264262, acc:  53%, G loss: 2.132962\n",
      "Ep: 143, steps: 17, D loss: 0.121247, acc:  91%, G loss: 2.283788\n",
      "Ep: 143, steps: 18, D loss: 0.258328, acc:  50%, G loss: 2.287657\n",
      "Ep: 143, steps: 19, D loss: 0.192969, acc:  70%, G loss: 2.023607\n",
      "Ep: 143, steps: 20, D loss: 0.128746, acc:  88%, G loss: 2.344026\n",
      "Ep: 143, steps: 21, D loss: 0.278937, acc:  49%, G loss: 2.047540\n",
      "Ep: 143, steps: 22, D loss: 0.151625, acc:  81%, G loss: 2.149083\n",
      "Ep: 143, steps: 23, D loss: 0.167113, acc:  81%, G loss: 2.440145\n",
      "Ep: 143, steps: 24, D loss: 0.159551, acc:  78%, G loss: 2.231816\n",
      "Ep: 143, steps: 25, D loss: 0.209385, acc:  66%, G loss: 2.062991\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 144, steps: 1, D loss: 0.250060, acc:  56%, G loss: 2.341356\n",
      "Ep: 144, steps: 2, D loss: 0.237623, acc:  58%, G loss: 1.990979\n",
      "Ep: 144, steps: 3, D loss: 0.137596, acc:  88%, G loss: 2.593972\n",
      "Ep: 144, steps: 4, D loss: 0.198525, acc:  73%, G loss: 2.020508\n",
      "Ep: 144, steps: 5, D loss: 0.230026, acc:  67%, G loss: 2.112583\n",
      "Ep: 144, steps: 6, D loss: 0.262193, acc:  51%, G loss: 2.039913\n",
      "Ep: 144, steps: 7, D loss: 0.394448, acc:  19%, G loss: 1.721823\n",
      "Ep: 144, steps: 8, D loss: 0.210766, acc:  64%, G loss: 2.754381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 144, steps: 9, D loss: 0.162940, acc:  84%, G loss: 2.173233\n",
      "Ep: 144, steps: 10, D loss: 0.125739, acc:  90%, G loss: 2.183337\n",
      "Ep: 144, steps: 11, D loss: 0.258630, acc:  52%, G loss: 2.403895\n",
      "Ep: 144, steps: 12, D loss: 0.342534, acc:  23%, G loss: 1.726302\n",
      "Ep: 144, steps: 13, D loss: 0.295862, acc:  37%, G loss: 1.841340\n",
      "Ep: 144, steps: 14, D loss: 0.285794, acc:  39%, G loss: 1.878952\n",
      "Ep: 144, steps: 15, D loss: 0.248910, acc:  56%, G loss: 1.973067\n",
      "Ep: 144, steps: 16, D loss: 0.253048, acc:  56%, G loss: 2.171670\n",
      "Saved Model\n",
      "Ep: 144, steps: 17, D loss: 0.127306, acc:  91%, G loss: 2.327887\n",
      "Ep: 144, steps: 18, D loss: 0.213305, acc:  63%, G loss: 2.039977\n",
      "Ep: 144, steps: 19, D loss: 0.114035, acc:  94%, G loss: 2.286149\n",
      "Ep: 144, steps: 20, D loss: 0.410056, acc:  34%, G loss: 2.584935\n",
      "Ep: 144, steps: 21, D loss: 0.155001, acc:  78%, G loss: 2.193133\n",
      "Ep: 144, steps: 22, D loss: 0.174927, acc:  78%, G loss: 2.441077\n",
      "Ep: 144, steps: 23, D loss: 0.154935, acc:  79%, G loss: 2.213055\n",
      "Ep: 144, steps: 24, D loss: 0.185031, acc:  76%, G loss: 2.099141\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 145, steps: 1, D loss: 0.241780, acc:  57%, G loss: 2.306913\n",
      "Ep: 145, steps: 2, D loss: 0.239591, acc:  57%, G loss: 1.838447\n",
      "Ep: 145, steps: 3, D loss: 0.137676, acc:  85%, G loss: 2.721222\n",
      "Ep: 145, steps: 4, D loss: 0.181338, acc:  78%, G loss: 2.213655\n",
      "Ep: 145, steps: 5, D loss: 0.187438, acc:  74%, G loss: 2.399105\n",
      "Ep: 145, steps: 6, D loss: 0.258857, acc:  51%, G loss: 2.103225\n",
      "Ep: 145, steps: 7, D loss: 0.345608, acc:  38%, G loss: 1.777768\n",
      "Ep: 145, steps: 8, D loss: 0.205790, acc:  65%, G loss: 2.340421\n",
      "Ep: 145, steps: 9, D loss: 0.170990, acc:  79%, G loss: 2.072537\n",
      "Ep: 145, steps: 10, D loss: 0.123928, acc:  88%, G loss: 2.326672\n",
      "Ep: 145, steps: 11, D loss: 0.242971, acc:  56%, G loss: 2.572216\n",
      "Ep: 145, steps: 12, D loss: 0.327385, acc:  35%, G loss: 1.730560\n",
      "Ep: 145, steps: 13, D loss: 0.290833, acc:  42%, G loss: 1.923503\n",
      "Ep: 145, steps: 14, D loss: 0.286865, acc:  37%, G loss: 1.875544\n",
      "Ep: 145, steps: 15, D loss: 0.249357, acc:  52%, G loss: 1.934159\n",
      "Ep: 145, steps: 16, D loss: 0.255958, acc:  51%, G loss: 2.044661\n",
      "Ep: 145, steps: 17, D loss: 0.117994, acc:  91%, G loss: 2.213019\n",
      "Ep: 145, steps: 18, D loss: 0.270568, acc:  46%, G loss: 2.071895\n",
      "Ep: 145, steps: 19, D loss: 0.202309, acc:  66%, G loss: 1.972812\n",
      "Ep: 145, steps: 20, D loss: 0.117950, acc:  91%, G loss: 2.399138\n",
      "Ep: 145, steps: 21, D loss: 0.250616, acc:  52%, G loss: 2.278334\n",
      "Ep: 145, steps: 22, D loss: 0.149656, acc:  81%, G loss: 2.018194\n",
      "Ep: 145, steps: 23, D loss: 0.182555, acc:  78%, G loss: 2.515645\n",
      "Ep: 145, steps: 24, D loss: 0.151479, acc:  81%, G loss: 2.224918\n",
      "Ep: 145, steps: 25, D loss: 0.199171, acc:  70%, G loss: 2.115468\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 146, steps: 1, D loss: 0.227546, acc:  64%, G loss: 2.265974\n",
      "Ep: 146, steps: 2, D loss: 0.234496, acc:  57%, G loss: 1.918482\n",
      "Ep: 146, steps: 3, D loss: 0.127949, acc:  90%, G loss: 2.489244\n",
      "Ep: 146, steps: 4, D loss: 0.187944, acc:  75%, G loss: 1.982147\n",
      "Ep: 146, steps: 5, D loss: 0.188717, acc:  75%, G loss: 2.211213\n",
      "Ep: 146, steps: 6, D loss: 0.271300, acc:  49%, G loss: 2.035188\n",
      "Ep: 146, steps: 7, D loss: 0.391156, acc:  20%, G loss: 1.820658\n",
      "Ep: 146, steps: 8, D loss: 0.202486, acc:  68%, G loss: 2.374839\n",
      "Ep: 146, steps: 9, D loss: 0.175653, acc:  79%, G loss: 2.080469\n",
      "Ep: 146, steps: 10, D loss: 0.128309, acc:  88%, G loss: 2.115672\n",
      "Ep: 146, steps: 11, D loss: 0.246057, acc:  57%, G loss: 2.483858\n",
      "Ep: 146, steps: 12, D loss: 0.341009, acc:  29%, G loss: 1.689365\n",
      "Ep: 146, steps: 13, D loss: 0.289327, acc:  41%, G loss: 2.014498\n",
      "Ep: 146, steps: 14, D loss: 0.286309, acc:  39%, G loss: 1.816669\n",
      "Ep: 146, steps: 15, D loss: 0.258141, acc:  50%, G loss: 2.017445\n",
      "Ep: 146, steps: 16, D loss: 0.268615, acc:  51%, G loss: 2.102989\n",
      "Ep: 146, steps: 17, D loss: 0.140466, acc:  86%, G loss: 2.160858\n",
      "Ep: 146, steps: 18, D loss: 0.273136, acc:  45%, G loss: 2.157279\n",
      "Ep: 146, steps: 19, D loss: 0.197936, acc:  65%, G loss: 2.042438\n",
      "Ep: 146, steps: 20, D loss: 0.127848, acc:  88%, G loss: 2.334629\n",
      "Ep: 146, steps: 21, D loss: 0.262863, acc:  51%, G loss: 2.152285\n",
      "Ep: 146, steps: 22, D loss: 0.174787, acc:  73%, G loss: 2.007721\n",
      "Ep: 146, steps: 23, D loss: 0.180350, acc:  77%, G loss: 2.590894\n",
      "Ep: 146, steps: 24, D loss: 0.156835, acc:  79%, G loss: 2.293572\n",
      "Ep: 146, steps: 25, D loss: 0.205383, acc:  67%, G loss: 2.020746\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 147, steps: 1, D loss: 0.217340, acc:  67%, G loss: 2.461203\n",
      "Ep: 147, steps: 2, D loss: 0.247007, acc:  53%, G loss: 2.020324\n",
      "Ep: 147, steps: 3, D loss: 0.143216, acc:  83%, G loss: 2.660813\n",
      "Ep: 147, steps: 4, D loss: 0.181969, acc:  79%, G loss: 2.097638\n",
      "Ep: 147, steps: 5, D loss: 0.212561, acc:  71%, G loss: 2.126760\n",
      "Ep: 147, steps: 6, D loss: 0.262763, acc:  49%, G loss: 1.966014\n",
      "Ep: 147, steps: 7, D loss: 0.361883, acc:  29%, G loss: 1.687229\n",
      "Ep: 147, steps: 8, D loss: 0.197434, acc:  70%, G loss: 2.464136\n",
      "Ep: 147, steps: 9, D loss: 0.174097, acc:  80%, G loss: 2.114224\n",
      "Ep: 147, steps: 10, D loss: 0.130569, acc:  86%, G loss: 2.196916\n",
      "Ep: 147, steps: 11, D loss: 0.240160, acc:  59%, G loss: 2.443620\n",
      "Ep: 147, steps: 12, D loss: 0.337548, acc:  30%, G loss: 1.625639\n",
      "Ep: 147, steps: 13, D loss: 0.302005, acc:  35%, G loss: 1.846341\n",
      "Ep: 147, steps: 14, D loss: 0.311970, acc:  33%, G loss: 1.893690\n",
      "Saved Model\n",
      "Ep: 147, steps: 15, D loss: 0.239281, acc:  57%, G loss: 1.976641\n",
      "Ep: 147, steps: 16, D loss: 0.134612, acc:  88%, G loss: 2.162540\n",
      "Ep: 147, steps: 17, D loss: 0.271915, acc:  48%, G loss: 2.052215\n",
      "Ep: 147, steps: 18, D loss: 0.190494, acc:  68%, G loss: 1.986924\n",
      "Ep: 147, steps: 19, D loss: 0.120844, acc:  90%, G loss: 2.305376\n",
      "Ep: 147, steps: 20, D loss: 0.284590, acc:  49%, G loss: 2.120427\n",
      "Ep: 147, steps: 21, D loss: 0.173630, acc:  74%, G loss: 2.066909\n",
      "Ep: 147, steps: 22, D loss: 0.158032, acc:  85%, G loss: 2.631516\n",
      "Ep: 147, steps: 23, D loss: 0.149028, acc:  80%, G loss: 2.357514\n",
      "Ep: 147, steps: 24, D loss: 0.207050, acc:  66%, G loss: 2.054117\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 148, steps: 1, D loss: 0.220938, acc:  63%, G loss: 2.363257\n",
      "Ep: 148, steps: 2, D loss: 0.222752, acc:  62%, G loss: 1.931440\n",
      "Ep: 148, steps: 3, D loss: 0.141434, acc:  85%, G loss: 2.641289\n",
      "Ep: 148, steps: 4, D loss: 0.172338, acc:  84%, G loss: 2.142411\n",
      "Ep: 148, steps: 5, D loss: 0.297503, acc:  61%, G loss: 2.204601\n",
      "Ep: 148, steps: 6, D loss: 0.261060, acc:  51%, G loss: 2.362760\n",
      "Ep: 148, steps: 7, D loss: 0.373203, acc:  25%, G loss: 1.664773\n",
      "Ep: 148, steps: 8, D loss: 0.206370, acc:  66%, G loss: 2.505040\n",
      "Ep: 148, steps: 9, D loss: 0.183408, acc:  77%, G loss: 2.089318\n",
      "Ep: 148, steps: 10, D loss: 0.126284, acc:  87%, G loss: 2.130988\n",
      "Ep: 148, steps: 11, D loss: 0.238130, acc:  62%, G loss: 2.442298\n",
      "Ep: 148, steps: 12, D loss: 0.345613, acc:  28%, G loss: 1.652970\n",
      "Ep: 148, steps: 13, D loss: 0.307183, acc:  34%, G loss: 1.808151\n",
      "Ep: 148, steps: 14, D loss: 0.291453, acc:  35%, G loss: 1.794336\n",
      "Ep: 148, steps: 15, D loss: 0.247839, acc:  54%, G loss: 1.970766\n",
      "Ep: 148, steps: 16, D loss: 0.250625, acc:  56%, G loss: 2.104071\n",
      "Ep: 148, steps: 17, D loss: 0.130828, acc:  89%, G loss: 2.246168\n",
      "Ep: 148, steps: 18, D loss: 0.259085, acc:  51%, G loss: 2.052450\n",
      "Ep: 148, steps: 19, D loss: 0.175549, acc:  72%, G loss: 1.956064\n",
      "Ep: 148, steps: 20, D loss: 0.127642, acc:  89%, G loss: 2.288903\n",
      "Ep: 148, steps: 21, D loss: 0.269263, acc:  48%, G loss: 2.066978\n",
      "Ep: 148, steps: 22, D loss: 0.153722, acc:  81%, G loss: 2.068167\n",
      "Ep: 148, steps: 23, D loss: 0.174563, acc:  80%, G loss: 2.508256\n",
      "Ep: 148, steps: 24, D loss: 0.158901, acc:  77%, G loss: 2.268938\n",
      "Ep: 148, steps: 25, D loss: 0.210111, acc:  66%, G loss: 2.176085\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 149, steps: 1, D loss: 0.223990, acc:  63%, G loss: 2.182823\n",
      "Ep: 149, steps: 2, D loss: 0.242024, acc:  55%, G loss: 2.012706\n",
      "Ep: 149, steps: 3, D loss: 0.130634, acc:  92%, G loss: 2.542507\n",
      "Ep: 149, steps: 4, D loss: 0.186003, acc:  78%, G loss: 2.013933\n",
      "Ep: 149, steps: 5, D loss: 0.199360, acc:  72%, G loss: 2.030122\n",
      "Ep: 149, steps: 6, D loss: 0.267282, acc:  50%, G loss: 2.062701\n",
      "Ep: 149, steps: 7, D loss: 0.423948, acc:  21%, G loss: 1.874219\n",
      "Ep: 149, steps: 8, D loss: 0.199319, acc:  65%, G loss: 2.493384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 149, steps: 9, D loss: 0.170543, acc:  82%, G loss: 2.173659\n",
      "Ep: 149, steps: 10, D loss: 0.127453, acc:  89%, G loss: 2.123892\n",
      "Ep: 149, steps: 11, D loss: 0.246531, acc:  57%, G loss: 2.359131\n",
      "Ep: 149, steps: 12, D loss: 0.327313, acc:  32%, G loss: 1.764353\n",
      "Ep: 149, steps: 13, D loss: 0.282742, acc:  43%, G loss: 1.907716\n",
      "Ep: 149, steps: 14, D loss: 0.276177, acc:  42%, G loss: 1.847278\n",
      "Ep: 149, steps: 15, D loss: 0.245125, acc:  54%, G loss: 1.998226\n",
      "Ep: 149, steps: 16, D loss: 0.260671, acc:  54%, G loss: 2.112931\n",
      "Ep: 149, steps: 17, D loss: 0.129974, acc:  88%, G loss: 2.362498\n",
      "Ep: 149, steps: 18, D loss: 0.259483, acc:  52%, G loss: 2.070519\n",
      "Ep: 149, steps: 19, D loss: 0.202961, acc:  66%, G loss: 1.897136\n",
      "Ep: 149, steps: 20, D loss: 0.124419, acc:  88%, G loss: 2.450000\n",
      "Ep: 149, steps: 21, D loss: 0.271631, acc:  49%, G loss: 2.356127\n",
      "Ep: 149, steps: 22, D loss: 0.149603, acc:  81%, G loss: 2.134300\n",
      "Ep: 149, steps: 23, D loss: 0.172026, acc:  78%, G loss: 2.433794\n",
      "Ep: 149, steps: 24, D loss: 0.156032, acc:  79%, G loss: 2.251208\n",
      "Ep: 149, steps: 25, D loss: 0.207453, acc:  65%, G loss: 2.210225\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 150, steps: 1, D loss: 0.225387, acc:  61%, G loss: 2.118481\n",
      "Ep: 150, steps: 2, D loss: 0.248861, acc:  52%, G loss: 2.106316\n",
      "Ep: 150, steps: 3, D loss: 0.124682, acc:  91%, G loss: 2.506578\n",
      "Ep: 150, steps: 4, D loss: 0.180962, acc:  80%, G loss: 1.979379\n",
      "Ep: 150, steps: 5, D loss: 0.218795, acc:  69%, G loss: 2.080884\n",
      "Ep: 150, steps: 6, D loss: 0.257867, acc:  50%, G loss: 2.002386\n",
      "Ep: 150, steps: 7, D loss: 0.381834, acc:  27%, G loss: 1.698990\n",
      "Ep: 150, steps: 8, D loss: 0.192342, acc:  68%, G loss: 2.252931\n",
      "Ep: 150, steps: 9, D loss: 0.173219, acc:  80%, G loss: 2.071013\n",
      "Ep: 150, steps: 10, D loss: 0.138803, acc:  84%, G loss: 2.169684\n",
      "Ep: 150, steps: 11, D loss: 0.261929, acc:  54%, G loss: 2.430913\n",
      "Ep: 150, steps: 12, D loss: 0.335959, acc:  33%, G loss: 1.714073\n",
      "Saved Model\n",
      "Ep: 150, steps: 13, D loss: 0.301333, acc:  39%, G loss: 1.875907\n",
      "Ep: 150, steps: 14, D loss: 0.224494, acc:  63%, G loss: 2.050586\n",
      "Ep: 150, steps: 15, D loss: 0.308401, acc:  42%, G loss: 2.107666\n",
      "Ep: 150, steps: 16, D loss: 0.135999, acc:  87%, G loss: 2.329893\n",
      "Ep: 150, steps: 17, D loss: 0.238450, acc:  60%, G loss: 2.055512\n",
      "Ep: 150, steps: 18, D loss: 0.172286, acc:  73%, G loss: 2.113883\n",
      "Ep: 150, steps: 19, D loss: 0.132259, acc:  87%, G loss: 2.325434\n",
      "Ep: 150, steps: 20, D loss: 0.287729, acc:  46%, G loss: 2.118175\n",
      "Ep: 150, steps: 21, D loss: 0.177994, acc:  71%, G loss: 2.382759\n",
      "Ep: 150, steps: 22, D loss: 0.160450, acc:  82%, G loss: 2.554201\n",
      "Ep: 150, steps: 23, D loss: 0.148569, acc:  78%, G loss: 2.271046\n",
      "Ep: 150, steps: 24, D loss: 0.224184, acc:  60%, G loss: 2.116851\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 151, steps: 1, D loss: 0.207836, acc:  69%, G loss: 2.475644\n",
      "Ep: 151, steps: 2, D loss: 0.233108, acc:  58%, G loss: 2.016588\n",
      "Ep: 151, steps: 3, D loss: 0.136683, acc:  89%, G loss: 2.744801\n",
      "Ep: 151, steps: 4, D loss: 0.169750, acc:  82%, G loss: 2.220469\n",
      "Ep: 151, steps: 5, D loss: 0.290805, acc:  63%, G loss: 2.162311\n",
      "Ep: 151, steps: 6, D loss: 0.263197, acc:  50%, G loss: 2.249825\n",
      "Ep: 151, steps: 7, D loss: 0.407155, acc:  23%, G loss: 1.619307\n",
      "Ep: 151, steps: 8, D loss: 0.207410, acc:  63%, G loss: 2.349827\n",
      "Ep: 151, steps: 9, D loss: 0.161579, acc:  83%, G loss: 2.093135\n",
      "Ep: 151, steps: 10, D loss: 0.127628, acc:  88%, G loss: 2.244819\n",
      "Ep: 151, steps: 11, D loss: 0.227307, acc:  64%, G loss: 2.502472\n",
      "Ep: 151, steps: 12, D loss: 0.341220, acc:  32%, G loss: 1.744594\n",
      "Ep: 151, steps: 13, D loss: 0.299096, acc:  39%, G loss: 1.832104\n",
      "Ep: 151, steps: 14, D loss: 0.301177, acc:  35%, G loss: 1.777223\n",
      "Ep: 151, steps: 15, D loss: 0.247865, acc:  54%, G loss: 2.067827\n",
      "Ep: 151, steps: 16, D loss: 0.263248, acc:  51%, G loss: 2.240263\n",
      "Ep: 151, steps: 17, D loss: 0.136692, acc:  86%, G loss: 2.364025\n",
      "Ep: 151, steps: 18, D loss: 0.255272, acc:  53%, G loss: 2.055046\n",
      "Ep: 151, steps: 19, D loss: 0.180759, acc:  71%, G loss: 1.976855\n",
      "Ep: 151, steps: 20, D loss: 0.115584, acc:  91%, G loss: 2.239353\n",
      "Ep: 151, steps: 21, D loss: 0.281354, acc:  47%, G loss: 2.047924\n",
      "Ep: 151, steps: 22, D loss: 0.152929, acc:  79%, G loss: 2.036968\n",
      "Ep: 151, steps: 23, D loss: 0.167058, acc:  81%, G loss: 2.529549\n",
      "Ep: 151, steps: 24, D loss: 0.156114, acc:  80%, G loss: 2.360979\n",
      "Ep: 151, steps: 25, D loss: 0.198035, acc:  67%, G loss: 2.159524\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 152, steps: 1, D loss: 0.211182, acc:  67%, G loss: 2.187538\n",
      "Ep: 152, steps: 2, D loss: 0.250958, acc:  53%, G loss: 2.034540\n",
      "Ep: 152, steps: 3, D loss: 0.132494, acc:  90%, G loss: 2.639753\n",
      "Ep: 152, steps: 4, D loss: 0.173875, acc:  80%, G loss: 1.989759\n",
      "Ep: 152, steps: 5, D loss: 0.302296, acc:  63%, G loss: 2.226499\n",
      "Ep: 152, steps: 6, D loss: 0.263394, acc:  51%, G loss: 2.172107\n",
      "Ep: 152, steps: 7, D loss: 0.365312, acc:  29%, G loss: 1.624410\n",
      "Ep: 152, steps: 8, D loss: 0.199567, acc:  66%, G loss: 2.346425\n",
      "Ep: 152, steps: 9, D loss: 0.160921, acc:  86%, G loss: 2.026854\n",
      "Ep: 152, steps: 10, D loss: 0.123800, acc:  86%, G loss: 2.216595\n",
      "Ep: 152, steps: 11, D loss: 0.237617, acc:  58%, G loss: 2.400254\n",
      "Ep: 152, steps: 12, D loss: 0.324201, acc:  39%, G loss: 1.679726\n",
      "Ep: 152, steps: 13, D loss: 0.296722, acc:  40%, G loss: 1.790431\n",
      "Ep: 152, steps: 14, D loss: 0.299491, acc:  38%, G loss: 1.807485\n",
      "Ep: 152, steps: 15, D loss: 0.246664, acc:  53%, G loss: 2.062961\n",
      "Ep: 152, steps: 16, D loss: 0.263901, acc:  52%, G loss: 2.198816\n",
      "Ep: 152, steps: 17, D loss: 0.119216, acc:  91%, G loss: 2.395880\n",
      "Ep: 152, steps: 18, D loss: 0.257720, acc:  51%, G loss: 2.050743\n",
      "Ep: 152, steps: 19, D loss: 0.197914, acc:  67%, G loss: 1.904048\n",
      "Ep: 152, steps: 20, D loss: 0.118003, acc:  91%, G loss: 2.205891\n",
      "Ep: 152, steps: 21, D loss: 0.264709, acc:  52%, G loss: 2.102248\n",
      "Ep: 152, steps: 22, D loss: 0.163411, acc:  78%, G loss: 2.058794\n",
      "Ep: 152, steps: 23, D loss: 0.168799, acc:  78%, G loss: 2.518179\n",
      "Ep: 152, steps: 24, D loss: 0.149943, acc:  80%, G loss: 2.308713\n",
      "Ep: 152, steps: 25, D loss: 0.215119, acc:  64%, G loss: 2.128441\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 153, steps: 1, D loss: 0.228088, acc:  63%, G loss: 2.238246\n",
      "Ep: 153, steps: 2, D loss: 0.249516, acc:  51%, G loss: 2.090142\n",
      "Ep: 153, steps: 3, D loss: 0.138177, acc:  89%, G loss: 2.763445\n",
      "Ep: 153, steps: 4, D loss: 0.187961, acc:  77%, G loss: 2.081393\n",
      "Ep: 153, steps: 5, D loss: 0.205979, acc:  70%, G loss: 2.131598\n",
      "Ep: 153, steps: 6, D loss: 0.275702, acc:  48%, G loss: 2.060462\n",
      "Ep: 153, steps: 7, D loss: 0.405399, acc:  22%, G loss: 2.100429\n",
      "Ep: 153, steps: 8, D loss: 0.209405, acc:  63%, G loss: 2.295877\n",
      "Ep: 153, steps: 9, D loss: 0.174591, acc:  81%, G loss: 2.017357\n",
      "Ep: 153, steps: 10, D loss: 0.135271, acc:  84%, G loss: 2.107989\n",
      "Saved Model\n",
      "Ep: 153, steps: 11, D loss: 0.248127, acc:  56%, G loss: 2.405071\n",
      "Ep: 153, steps: 12, D loss: 0.304411, acc:  35%, G loss: 1.861710\n",
      "Ep: 153, steps: 13, D loss: 0.307773, acc:  33%, G loss: 1.838902\n",
      "Ep: 153, steps: 14, D loss: 0.224302, acc:  64%, G loss: 2.027645\n",
      "Ep: 153, steps: 15, D loss: 0.258436, acc:  56%, G loss: 2.171104\n",
      "Ep: 153, steps: 16, D loss: 0.123085, acc:  89%, G loss: 2.626228\n",
      "Ep: 153, steps: 17, D loss: 0.259382, acc:  50%, G loss: 2.038656\n",
      "Ep: 153, steps: 18, D loss: 0.182727, acc:  71%, G loss: 1.977325\n",
      "Ep: 153, steps: 19, D loss: 0.118007, acc:  89%, G loss: 2.406823\n",
      "Ep: 153, steps: 20, D loss: 0.263779, acc:  52%, G loss: 2.025475\n",
      "Ep: 153, steps: 21, D loss: 0.167536, acc:  76%, G loss: 2.193119\n",
      "Ep: 153, steps: 22, D loss: 0.163472, acc:  81%, G loss: 2.490005\n",
      "Ep: 153, steps: 23, D loss: 0.162855, acc:  74%, G loss: 2.281114\n",
      "Ep: 153, steps: 24, D loss: 0.200398, acc:  66%, G loss: 2.159961\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 154, steps: 1, D loss: 0.230323, acc:  61%, G loss: 2.336577\n",
      "Ep: 154, steps: 2, D loss: 0.248004, acc:  53%, G loss: 2.031852\n",
      "Ep: 154, steps: 3, D loss: 0.135534, acc:  88%, G loss: 2.761595\n",
      "Ep: 154, steps: 4, D loss: 0.175818, acc:  81%, G loss: 2.223843\n",
      "Ep: 154, steps: 5, D loss: 0.258924, acc:  65%, G loss: 2.199883\n",
      "Ep: 154, steps: 6, D loss: 0.274271, acc:  51%, G loss: 2.091356\n",
      "Ep: 154, steps: 7, D loss: 0.365315, acc:  29%, G loss: 1.623136\n",
      "Ep: 154, steps: 8, D loss: 0.211012, acc:  65%, G loss: 2.354973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 154, steps: 9, D loss: 0.182436, acc:  77%, G loss: 2.095615\n",
      "Ep: 154, steps: 10, D loss: 0.142279, acc:  83%, G loss: 2.189487\n",
      "Ep: 154, steps: 11, D loss: 0.247036, acc:  57%, G loss: 2.448031\n",
      "Ep: 154, steps: 12, D loss: 0.338980, acc:  31%, G loss: 1.791565\n",
      "Ep: 154, steps: 13, D loss: 0.288974, acc:  46%, G loss: 1.889516\n",
      "Ep: 154, steps: 14, D loss: 0.310016, acc:  35%, G loss: 1.959360\n",
      "Ep: 154, steps: 15, D loss: 0.249035, acc:  51%, G loss: 2.095402\n",
      "Ep: 154, steps: 16, D loss: 0.285073, acc:  48%, G loss: 2.260885\n",
      "Ep: 154, steps: 17, D loss: 0.123525, acc:  89%, G loss: 2.434252\n",
      "Ep: 154, steps: 18, D loss: 0.244727, acc:  55%, G loss: 1.975895\n",
      "Ep: 154, steps: 19, D loss: 0.177125, acc:  72%, G loss: 1.971597\n",
      "Ep: 154, steps: 20, D loss: 0.129919, acc:  89%, G loss: 2.258031\n",
      "Ep: 154, steps: 21, D loss: 0.279804, acc:  47%, G loss: 2.051869\n",
      "Ep: 154, steps: 22, D loss: 0.176110, acc:  72%, G loss: 2.097665\n",
      "Ep: 154, steps: 23, D loss: 0.177972, acc:  78%, G loss: 2.497409\n",
      "Ep: 154, steps: 24, D loss: 0.149423, acc:  79%, G loss: 2.253717\n",
      "Ep: 154, steps: 25, D loss: 0.191646, acc:  69%, G loss: 2.200759\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 155, steps: 1, D loss: 0.225813, acc:  62%, G loss: 2.273618\n",
      "Ep: 155, steps: 2, D loss: 0.242864, acc:  56%, G loss: 2.111166\n",
      "Ep: 155, steps: 3, D loss: 0.129895, acc:  91%, G loss: 2.843965\n",
      "Ep: 155, steps: 4, D loss: 0.171765, acc:  83%, G loss: 2.296648\n",
      "Ep: 155, steps: 5, D loss: 0.250760, acc:  64%, G loss: 2.307238\n",
      "Ep: 155, steps: 6, D loss: 0.272091, acc:  49%, G loss: 2.240136\n",
      "Ep: 155, steps: 7, D loss: 0.363490, acc:  30%, G loss: 1.706226\n",
      "Ep: 155, steps: 8, D loss: 0.191581, acc:  67%, G loss: 2.289632\n",
      "Ep: 155, steps: 9, D loss: 0.169798, acc:  81%, G loss: 2.114081\n",
      "Ep: 155, steps: 10, D loss: 0.122953, acc:  90%, G loss: 2.056074\n",
      "Ep: 155, steps: 11, D loss: 0.244231, acc:  58%, G loss: 2.400095\n",
      "Ep: 155, steps: 12, D loss: 0.338641, acc:  29%, G loss: 1.805136\n",
      "Ep: 155, steps: 13, D loss: 0.294170, acc:  40%, G loss: 1.957776\n",
      "Ep: 155, steps: 14, D loss: 0.294459, acc:  38%, G loss: 1.759920\n",
      "Ep: 155, steps: 15, D loss: 0.242330, acc:  53%, G loss: 1.930154\n",
      "Ep: 155, steps: 16, D loss: 0.287927, acc:  46%, G loss: 2.316314\n",
      "Ep: 155, steps: 17, D loss: 0.134349, acc:  88%, G loss: 2.459340\n",
      "Ep: 155, steps: 18, D loss: 0.259369, acc:  51%, G loss: 2.041168\n",
      "Ep: 155, steps: 19, D loss: 0.183438, acc:  70%, G loss: 1.983515\n",
      "Ep: 155, steps: 20, D loss: 0.125885, acc:  88%, G loss: 2.254528\n",
      "Ep: 155, steps: 21, D loss: 0.302064, acc:  43%, G loss: 2.047229\n",
      "Ep: 155, steps: 22, D loss: 0.152381, acc:  80%, G loss: 2.148381\n",
      "Ep: 155, steps: 23, D loss: 0.165292, acc:  81%, G loss: 2.552552\n",
      "Ep: 155, steps: 24, D loss: 0.145280, acc:  83%, G loss: 2.218795\n",
      "Ep: 155, steps: 25, D loss: 0.182453, acc:  73%, G loss: 2.090501\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 156, steps: 1, D loss: 0.242133, acc:  58%, G loss: 2.182500\n",
      "Ep: 156, steps: 2, D loss: 0.250991, acc:  52%, G loss: 2.029993\n",
      "Ep: 156, steps: 3, D loss: 0.148426, acc:  88%, G loss: 2.567791\n",
      "Ep: 156, steps: 4, D loss: 0.184101, acc:  78%, G loss: 2.020972\n",
      "Ep: 156, steps: 5, D loss: 0.225364, acc:  67%, G loss: 2.293186\n",
      "Ep: 156, steps: 6, D loss: 0.267488, acc:  51%, G loss: 2.188105\n",
      "Ep: 156, steps: 7, D loss: 0.498918, acc:  17%, G loss: 1.937643\n",
      "Ep: 156, steps: 8, D loss: 0.229304, acc:  60%, G loss: 2.414270\n",
      "Saved Model\n",
      "Ep: 156, steps: 9, D loss: 0.164114, acc:  83%, G loss: 2.118338\n",
      "Ep: 156, steps: 10, D loss: 0.260610, acc:  54%, G loss: 2.337613\n",
      "Ep: 156, steps: 11, D loss: 0.317202, acc:  35%, G loss: 1.746309\n",
      "Ep: 156, steps: 12, D loss: 0.277796, acc:  43%, G loss: 1.948316\n",
      "Ep: 156, steps: 13, D loss: 0.298992, acc:  34%, G loss: 1.840604\n",
      "Ep: 156, steps: 14, D loss: 0.248796, acc:  54%, G loss: 1.938758\n",
      "Ep: 156, steps: 15, D loss: 0.256830, acc:  57%, G loss: 2.229339\n",
      "Ep: 156, steps: 16, D loss: 0.129741, acc:  90%, G loss: 2.284770\n",
      "Ep: 156, steps: 17, D loss: 0.262607, acc:  49%, G loss: 2.008668\n",
      "Ep: 156, steps: 18, D loss: 0.206874, acc:  65%, G loss: 1.951935\n",
      "Ep: 156, steps: 19, D loss: 0.132464, acc:  88%, G loss: 2.260887\n",
      "Ep: 156, steps: 20, D loss: 0.323393, acc:  36%, G loss: 1.979494\n",
      "Ep: 156, steps: 21, D loss: 0.153548, acc:  80%, G loss: 2.062060\n",
      "Ep: 156, steps: 22, D loss: 0.185163, acc:  77%, G loss: 2.656888\n",
      "Ep: 156, steps: 23, D loss: 0.179371, acc:  75%, G loss: 2.504188\n",
      "Ep: 156, steps: 24, D loss: 0.228298, acc:  61%, G loss: 3.046307\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 157, steps: 1, D loss: 0.208364, acc:  68%, G loss: 2.852299\n",
      "Ep: 157, steps: 2, D loss: 0.195967, acc:  73%, G loss: 2.302047\n",
      "Ep: 157, steps: 3, D loss: 0.143817, acc:  86%, G loss: 2.863995\n",
      "Ep: 157, steps: 4, D loss: 0.174845, acc:  80%, G loss: 2.389855\n",
      "Ep: 157, steps: 5, D loss: 0.153652, acc:  80%, G loss: 2.599288\n",
      "Ep: 157, steps: 6, D loss: 0.260677, acc:  49%, G loss: 2.147200\n",
      "Ep: 157, steps: 7, D loss: 0.300665, acc:  46%, G loss: 2.057051\n",
      "Ep: 157, steps: 8, D loss: 0.190898, acc:  69%, G loss: 2.612565\n",
      "Ep: 157, steps: 9, D loss: 0.173751, acc:  76%, G loss: 2.224179\n",
      "Ep: 157, steps: 10, D loss: 0.138048, acc:  85%, G loss: 2.304340\n",
      "Ep: 157, steps: 11, D loss: 0.219244, acc:  63%, G loss: 2.653244\n",
      "Ep: 157, steps: 12, D loss: 0.339801, acc:  33%, G loss: 1.986176\n",
      "Ep: 157, steps: 13, D loss: 0.263413, acc:  53%, G loss: 2.150743\n",
      "Ep: 157, steps: 14, D loss: 0.266096, acc:  47%, G loss: 2.101293\n",
      "Ep: 157, steps: 15, D loss: 0.211871, acc:  69%, G loss: 2.138756\n",
      "Ep: 157, steps: 16, D loss: 0.241599, acc:  58%, G loss: 2.229388\n",
      "Ep: 157, steps: 17, D loss: 0.090083, acc:  94%, G loss: 2.509342\n",
      "Ep: 157, steps: 18, D loss: 0.263115, acc:  49%, G loss: 2.060627\n",
      "Ep: 157, steps: 19, D loss: 0.205927, acc:  65%, G loss: 1.997586\n",
      "Ep: 157, steps: 20, D loss: 0.118565, acc:  90%, G loss: 2.451142\n",
      "Ep: 157, steps: 21, D loss: 0.238686, acc:  59%, G loss: 2.356670\n",
      "Ep: 157, steps: 22, D loss: 0.168985, acc:  78%, G loss: 2.073842\n",
      "Ep: 157, steps: 23, D loss: 0.164628, acc:  79%, G loss: 2.425057\n",
      "Ep: 157, steps: 24, D loss: 0.142918, acc:  83%, G loss: 2.176504\n",
      "Ep: 157, steps: 25, D loss: 0.202084, acc:  69%, G loss: 2.157822\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 158, steps: 1, D loss: 0.207922, acc:  68%, G loss: 2.123745\n",
      "Ep: 158, steps: 2, D loss: 0.273221, acc:  45%, G loss: 1.920071\n",
      "Ep: 158, steps: 3, D loss: 0.123562, acc:  88%, G loss: 2.793639\n",
      "Ep: 158, steps: 4, D loss: 0.182125, acc:  75%, G loss: 2.221933\n",
      "Ep: 158, steps: 5, D loss: 0.146333, acc:  82%, G loss: 2.456004\n",
      "Ep: 158, steps: 6, D loss: 0.280110, acc:  48%, G loss: 2.109885\n",
      "Ep: 158, steps: 7, D loss: 0.347983, acc:  35%, G loss: 1.768251\n",
      "Ep: 158, steps: 8, D loss: 0.180535, acc:  69%, G loss: 2.364669\n",
      "Ep: 158, steps: 9, D loss: 0.176326, acc:  79%, G loss: 2.154267\n",
      "Ep: 158, steps: 10, D loss: 0.121590, acc:  90%, G loss: 2.267275\n",
      "Ep: 158, steps: 11, D loss: 0.236249, acc:  59%, G loss: 2.473626\n",
      "Ep: 158, steps: 12, D loss: 0.326231, acc:  36%, G loss: 1.746976\n",
      "Ep: 158, steps: 13, D loss: 0.276316, acc:  46%, G loss: 1.918763\n",
      "Ep: 158, steps: 14, D loss: 0.288342, acc:  40%, G loss: 1.815037\n",
      "Ep: 158, steps: 15, D loss: 0.238134, acc:  56%, G loss: 1.994043\n",
      "Ep: 158, steps: 16, D loss: 0.289661, acc:  48%, G loss: 2.045967\n",
      "Ep: 158, steps: 17, D loss: 0.130371, acc:  86%, G loss: 2.230926\n",
      "Ep: 158, steps: 18, D loss: 0.281368, acc:  43%, G loss: 2.073702\n",
      "Ep: 158, steps: 19, D loss: 0.209570, acc:  65%, G loss: 1.951756\n",
      "Ep: 158, steps: 20, D loss: 0.128227, acc:  89%, G loss: 2.354829\n",
      "Ep: 158, steps: 21, D loss: 0.261994, acc:  49%, G loss: 2.038140\n",
      "Ep: 158, steps: 22, D loss: 0.165560, acc:  74%, G loss: 2.164906\n",
      "Ep: 158, steps: 23, D loss: 0.157987, acc:  82%, G loss: 2.545163\n",
      "Ep: 158, steps: 24, D loss: 0.148547, acc:  81%, G loss: 2.291547\n",
      "Ep: 158, steps: 25, D loss: 0.192651, acc:  71%, G loss: 2.134059\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 159, steps: 1, D loss: 0.240531, acc:  61%, G loss: 2.234482\n",
      "Ep: 159, steps: 2, D loss: 0.231558, acc:  59%, G loss: 1.952698\n",
      "Ep: 159, steps: 3, D loss: 0.124630, acc:  88%, G loss: 2.610550\n",
      "Ep: 159, steps: 4, D loss: 0.176413, acc:  80%, G loss: 2.150376\n",
      "Ep: 159, steps: 5, D loss: 0.185954, acc:  75%, G loss: 2.099875\n",
      "Ep: 159, steps: 6, D loss: 0.266030, acc:  49%, G loss: 2.311293\n",
      "Saved Model\n",
      "Ep: 159, steps: 7, D loss: 0.405187, acc:  24%, G loss: 1.793083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 159, steps: 8, D loss: 0.158245, acc:  82%, G loss: 2.342650\n",
      "Ep: 159, steps: 9, D loss: 0.137931, acc:  83%, G loss: 2.179167\n",
      "Ep: 159, steps: 10, D loss: 0.257658, acc:  57%, G loss: 2.489146\n",
      "Ep: 159, steps: 11, D loss: 0.349962, acc:  31%, G loss: 1.742421\n",
      "Ep: 159, steps: 12, D loss: 0.297772, acc:  43%, G loss: 1.841645\n",
      "Ep: 159, steps: 13, D loss: 0.288694, acc:  42%, G loss: 1.998435\n",
      "Ep: 159, steps: 14, D loss: 0.235248, acc:  61%, G loss: 1.954422\n",
      "Ep: 159, steps: 15, D loss: 0.258396, acc:  54%, G loss: 2.200164\n",
      "Ep: 159, steps: 16, D loss: 0.115536, acc:  92%, G loss: 2.220905\n",
      "Ep: 159, steps: 17, D loss: 0.277657, acc:  47%, G loss: 2.080528\n",
      "Ep: 159, steps: 18, D loss: 0.221306, acc:  60%, G loss: 1.990555\n",
      "Ep: 159, steps: 19, D loss: 0.131933, acc:  88%, G loss: 2.240825\n",
      "Ep: 159, steps: 20, D loss: 0.323573, acc:  40%, G loss: 2.330028\n",
      "Ep: 159, steps: 21, D loss: 0.146879, acc:  82%, G loss: 2.273854\n",
      "Ep: 159, steps: 22, D loss: 0.168520, acc:  79%, G loss: 2.407628\n",
      "Ep: 159, steps: 23, D loss: 0.161826, acc:  75%, G loss: 2.188526\n",
      "Ep: 159, steps: 24, D loss: 0.200724, acc:  65%, G loss: 2.144886\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 160, steps: 1, D loss: 0.232161, acc:  60%, G loss: 2.325077\n",
      "Ep: 160, steps: 2, D loss: 0.234686, acc:  59%, G loss: 1.846477\n",
      "Ep: 160, steps: 3, D loss: 0.152693, acc:  84%, G loss: 2.700135\n",
      "Ep: 160, steps: 4, D loss: 0.196616, acc:  75%, G loss: 2.155064\n",
      "Ep: 160, steps: 5, D loss: 0.264436, acc:  62%, G loss: 2.099751\n",
      "Ep: 160, steps: 6, D loss: 0.260573, acc:  49%, G loss: 2.174428\n",
      "Ep: 160, steps: 7, D loss: 0.392428, acc:  26%, G loss: 1.669134\n",
      "Ep: 160, steps: 8, D loss: 0.207428, acc:  63%, G loss: 2.393473\n",
      "Ep: 160, steps: 9, D loss: 0.169689, acc:  82%, G loss: 2.053840\n",
      "Ep: 160, steps: 10, D loss: 0.134257, acc:  87%, G loss: 2.180045\n",
      "Ep: 160, steps: 11, D loss: 0.260889, acc:  53%, G loss: 2.349811\n",
      "Ep: 160, steps: 12, D loss: 0.325333, acc:  33%, G loss: 1.653984\n",
      "Ep: 160, steps: 13, D loss: 0.297588, acc:  39%, G loss: 1.787967\n",
      "Ep: 160, steps: 14, D loss: 0.318634, acc:  28%, G loss: 1.770414\n",
      "Ep: 160, steps: 15, D loss: 0.227746, acc:  61%, G loss: 2.088564\n",
      "Ep: 160, steps: 16, D loss: 0.262115, acc:  53%, G loss: 2.188823\n",
      "Ep: 160, steps: 17, D loss: 0.128500, acc:  90%, G loss: 2.322567\n",
      "Ep: 160, steps: 18, D loss: 0.256340, acc:  52%, G loss: 2.021183\n",
      "Ep: 160, steps: 19, D loss: 0.197371, acc:  67%, G loss: 1.939695\n",
      "Ep: 160, steps: 20, D loss: 0.139672, acc:  86%, G loss: 2.267706\n",
      "Ep: 160, steps: 21, D loss: 0.262051, acc:  46%, G loss: 2.164553\n",
      "Ep: 160, steps: 22, D loss: 0.175811, acc:  74%, G loss: 1.985739\n",
      "Ep: 160, steps: 23, D loss: 0.168024, acc:  79%, G loss: 2.505785\n",
      "Ep: 160, steps: 24, D loss: 0.159418, acc:  79%, G loss: 2.241828\n",
      "Ep: 160, steps: 25, D loss: 0.216134, acc:  64%, G loss: 2.139493\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 161, steps: 1, D loss: 0.212426, acc:  68%, G loss: 2.192932\n",
      "Ep: 161, steps: 2, D loss: 0.259750, acc:  48%, G loss: 1.935209\n",
      "Ep: 161, steps: 3, D loss: 0.128858, acc:  89%, G loss: 2.643063\n",
      "Ep: 161, steps: 4, D loss: 0.175688, acc:  83%, G loss: 2.021255\n",
      "Ep: 161, steps: 5, D loss: 0.239272, acc:  66%, G loss: 2.262394\n",
      "Ep: 161, steps: 6, D loss: 0.251395, acc:  51%, G loss: 2.160783\n",
      "Ep: 161, steps: 7, D loss: 0.381390, acc:  30%, G loss: 1.820847\n",
      "Ep: 161, steps: 8, D loss: 0.193257, acc:  68%, G loss: 2.434394\n",
      "Ep: 161, steps: 9, D loss: 0.173038, acc:  81%, G loss: 2.076436\n",
      "Ep: 161, steps: 10, D loss: 0.138754, acc:  83%, G loss: 2.111364\n",
      "Ep: 161, steps: 11, D loss: 0.270543, acc:  50%, G loss: 2.385073\n",
      "Ep: 161, steps: 12, D loss: 0.328048, acc:  36%, G loss: 1.664222\n",
      "Ep: 161, steps: 13, D loss: 0.284830, acc:  43%, G loss: 1.807582\n",
      "Ep: 161, steps: 14, D loss: 0.306683, acc:  33%, G loss: 1.781878\n",
      "Ep: 161, steps: 15, D loss: 0.222979, acc:  66%, G loss: 2.044661\n",
      "Ep: 161, steps: 16, D loss: 0.290967, acc:  47%, G loss: 2.210623\n",
      "Ep: 161, steps: 17, D loss: 0.132498, acc:  89%, G loss: 2.237401\n",
      "Ep: 161, steps: 18, D loss: 0.265926, acc:  47%, G loss: 2.028281\n",
      "Ep: 161, steps: 19, D loss: 0.185911, acc:  69%, G loss: 1.998549\n",
      "Ep: 161, steps: 20, D loss: 0.128518, acc:  88%, G loss: 2.247321\n",
      "Ep: 161, steps: 21, D loss: 0.273735, acc:  46%, G loss: 2.057909\n",
      "Ep: 161, steps: 22, D loss: 0.175267, acc:  73%, G loss: 2.226953\n",
      "Ep: 161, steps: 23, D loss: 0.148691, acc:  86%, G loss: 2.539506\n",
      "Ep: 161, steps: 24, D loss: 0.146430, acc:  79%, G loss: 2.211687\n",
      "Ep: 161, steps: 25, D loss: 0.211491, acc:  65%, G loss: 2.241966\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 162, steps: 1, D loss: 0.232045, acc:  62%, G loss: 2.670467\n",
      "Ep: 162, steps: 2, D loss: 0.251298, acc:  53%, G loss: 1.894506\n",
      "Ep: 162, steps: 3, D loss: 0.140513, acc:  88%, G loss: 2.821989\n",
      "Ep: 162, steps: 4, D loss: 0.185600, acc:  80%, G loss: 2.140756\n",
      "Saved Model\n",
      "Ep: 162, steps: 5, D loss: 0.217875, acc:  69%, G loss: 2.140883\n",
      "Ep: 162, steps: 6, D loss: 0.446687, acc:  14%, G loss: 1.823282\n",
      "Ep: 162, steps: 7, D loss: 0.184264, acc:  77%, G loss: 2.837174\n",
      "Ep: 162, steps: 8, D loss: 0.164681, acc:  85%, G loss: 2.163589\n",
      "Ep: 162, steps: 9, D loss: 0.110044, acc:  94%, G loss: 2.114433\n",
      "Ep: 162, steps: 10, D loss: 0.251560, acc:  54%, G loss: 2.401349\n",
      "Ep: 162, steps: 11, D loss: 0.319987, acc:  39%, G loss: 1.670808\n",
      "Ep: 162, steps: 12, D loss: 0.282243, acc:  45%, G loss: 1.819064\n",
      "Ep: 162, steps: 13, D loss: 0.314838, acc:  30%, G loss: 1.807359\n",
      "Ep: 162, steps: 14, D loss: 0.227926, acc:  64%, G loss: 1.980754\n",
      "Ep: 162, steps: 15, D loss: 0.302171, acc:  44%, G loss: 2.205227\n",
      "Ep: 162, steps: 16, D loss: 0.135787, acc:  88%, G loss: 2.527964\n",
      "Ep: 162, steps: 17, D loss: 0.263433, acc:  49%, G loss: 2.009776\n",
      "Ep: 162, steps: 18, D loss: 0.190800, acc:  67%, G loss: 1.947152\n",
      "Ep: 162, steps: 19, D loss: 0.129701, acc:  88%, G loss: 2.270533\n",
      "Ep: 162, steps: 20, D loss: 0.264901, acc:  49%, G loss: 2.060180\n",
      "Ep: 162, steps: 21, D loss: 0.152619, acc:  81%, G loss: 2.268711\n",
      "Ep: 162, steps: 22, D loss: 0.164056, acc:  81%, G loss: 2.383352\n",
      "Ep: 162, steps: 23, D loss: 0.153038, acc:  79%, G loss: 2.134581\n",
      "Ep: 162, steps: 24, D loss: 0.223422, acc:  62%, G loss: 2.161679\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 163, steps: 1, D loss: 0.236310, acc:  58%, G loss: 2.382073\n",
      "Ep: 163, steps: 2, D loss: 0.244467, acc:  53%, G loss: 1.994185\n",
      "Ep: 163, steps: 3, D loss: 0.134821, acc:  89%, G loss: 2.646950\n",
      "Ep: 163, steps: 4, D loss: 0.184722, acc:  79%, G loss: 2.147960\n",
      "Ep: 163, steps: 5, D loss: 0.295018, acc:  61%, G loss: 2.222427\n",
      "Ep: 163, steps: 6, D loss: 0.264033, acc:  49%, G loss: 2.349917\n",
      "Ep: 163, steps: 7, D loss: 0.389214, acc:  25%, G loss: 1.715004\n",
      "Ep: 163, steps: 8, D loss: 0.202637, acc:  66%, G loss: 2.430808\n",
      "Ep: 163, steps: 9, D loss: 0.151659, acc:  86%, G loss: 2.077096\n",
      "Ep: 163, steps: 10, D loss: 0.123692, acc:  89%, G loss: 2.165873\n",
      "Ep: 163, steps: 11, D loss: 0.246021, acc:  56%, G loss: 2.351924\n",
      "Ep: 163, steps: 12, D loss: 0.338979, acc:  34%, G loss: 1.675073\n",
      "Ep: 163, steps: 13, D loss: 0.295878, acc:  38%, G loss: 1.766910\n",
      "Ep: 163, steps: 14, D loss: 0.294780, acc:  36%, G loss: 1.791596\n",
      "Ep: 163, steps: 15, D loss: 0.236075, acc:  59%, G loss: 1.966975\n",
      "Ep: 163, steps: 16, D loss: 0.269011, acc:  52%, G loss: 2.043283\n",
      "Ep: 163, steps: 17, D loss: 0.129897, acc:  89%, G loss: 2.341105\n",
      "Ep: 163, steps: 18, D loss: 0.256938, acc:  51%, G loss: 2.044515\n",
      "Ep: 163, steps: 19, D loss: 0.184059, acc:  70%, G loss: 1.966485\n",
      "Ep: 163, steps: 20, D loss: 0.134867, acc:  87%, G loss: 2.276042\n",
      "Ep: 163, steps: 21, D loss: 0.263666, acc:  52%, G loss: 1.988312\n",
      "Ep: 163, steps: 22, D loss: 0.160494, acc:  78%, G loss: 2.049505\n",
      "Ep: 163, steps: 23, D loss: 0.156203, acc:  83%, G loss: 2.469000\n",
      "Ep: 163, steps: 24, D loss: 0.160196, acc:  77%, G loss: 2.265851\n",
      "Ep: 163, steps: 25, D loss: 0.193582, acc:  69%, G loss: 2.159782\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 164, steps: 1, D loss: 0.222507, acc:  64%, G loss: 2.244455\n",
      "Ep: 164, steps: 2, D loss: 0.252997, acc:  53%, G loss: 1.892359\n",
      "Ep: 164, steps: 3, D loss: 0.113840, acc:  94%, G loss: 2.655661\n",
      "Ep: 164, steps: 4, D loss: 0.192631, acc:  78%, G loss: 2.134919\n",
      "Ep: 164, steps: 5, D loss: 0.279844, acc:  59%, G loss: 2.192744\n",
      "Ep: 164, steps: 6, D loss: 0.273197, acc:  49%, G loss: 2.103537\n",
      "Ep: 164, steps: 7, D loss: 0.432120, acc:  19%, G loss: 1.630918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 164, steps: 8, D loss: 0.183439, acc:  68%, G loss: 2.761380\n",
      "Ep: 164, steps: 9, D loss: 0.167730, acc:  84%, G loss: 2.113058\n",
      "Ep: 164, steps: 10, D loss: 0.120795, acc:  92%, G loss: 2.046583\n",
      "Ep: 164, steps: 11, D loss: 0.241210, acc:  58%, G loss: 2.360853\n",
      "Ep: 164, steps: 12, D loss: 0.329002, acc:  36%, G loss: 1.689208\n",
      "Ep: 164, steps: 13, D loss: 0.282249, acc:  44%, G loss: 1.947675\n",
      "Ep: 164, steps: 14, D loss: 0.319355, acc:  29%, G loss: 1.728932\n",
      "Ep: 164, steps: 15, D loss: 0.223009, acc:  64%, G loss: 2.094232\n",
      "Ep: 164, steps: 16, D loss: 0.293464, acc:  45%, G loss: 2.141445\n",
      "Ep: 164, steps: 17, D loss: 0.122421, acc:  91%, G loss: 2.268101\n",
      "Ep: 164, steps: 18, D loss: 0.263567, acc:  48%, G loss: 1.943889\n",
      "Ep: 164, steps: 19, D loss: 0.183965, acc:  70%, G loss: 1.937210\n",
      "Ep: 164, steps: 20, D loss: 0.131961, acc:  88%, G loss: 2.145203\n",
      "Ep: 164, steps: 21, D loss: 0.288316, acc:  45%, G loss: 2.047206\n",
      "Ep: 164, steps: 22, D loss: 0.173010, acc:  74%, G loss: 2.025957\n",
      "Ep: 164, steps: 23, D loss: 0.169696, acc:  78%, G loss: 2.431665\n",
      "Ep: 164, steps: 24, D loss: 0.154072, acc:  81%, G loss: 2.232316\n",
      "Ep: 164, steps: 25, D loss: 0.194378, acc:  68%, G loss: 2.103411\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 165, steps: 1, D loss: 0.226843, acc:  63%, G loss: 2.338686\n",
      "Saved Model\n",
      "Ep: 165, steps: 2, D loss: 0.253161, acc:  51%, G loss: 1.878287\n",
      "Ep: 165, steps: 3, D loss: 0.211644, acc:  72%, G loss: 2.182348\n",
      "Ep: 165, steps: 4, D loss: 0.256105, acc:  62%, G loss: 2.224961\n",
      "Ep: 165, steps: 5, D loss: 0.243789, acc:  50%, G loss: 1.948525\n",
      "Ep: 165, steps: 6, D loss: 0.327369, acc:  36%, G loss: 1.742033\n",
      "Ep: 165, steps: 7, D loss: 0.199141, acc:  68%, G loss: 2.166189\n",
      "Ep: 165, steps: 8, D loss: 0.162227, acc:  84%, G loss: 2.143841\n",
      "Ep: 165, steps: 9, D loss: 0.137825, acc:  84%, G loss: 2.234917\n",
      "Ep: 165, steps: 10, D loss: 0.248489, acc:  54%, G loss: 2.411048\n",
      "Ep: 165, steps: 11, D loss: 0.336205, acc:  34%, G loss: 1.601833\n",
      "Ep: 165, steps: 12, D loss: 0.288485, acc:  40%, G loss: 1.824017\n",
      "Ep: 165, steps: 13, D loss: 0.318746, acc:  31%, G loss: 1.790283\n",
      "Ep: 165, steps: 14, D loss: 0.235849, acc:  60%, G loss: 2.095536\n",
      "Ep: 165, steps: 15, D loss: 0.294426, acc:  46%, G loss: 2.260470\n",
      "Ep: 165, steps: 16, D loss: 0.133689, acc:  89%, G loss: 2.355898\n",
      "Ep: 165, steps: 17, D loss: 0.247220, acc:  55%, G loss: 2.085829\n",
      "Ep: 165, steps: 18, D loss: 0.178419, acc:  72%, G loss: 2.020569\n",
      "Ep: 165, steps: 19, D loss: 0.146223, acc:  85%, G loss: 2.190132\n",
      "Ep: 165, steps: 20, D loss: 0.256001, acc:  48%, G loss: 2.085254\n",
      "Ep: 165, steps: 21, D loss: 0.198470, acc:  68%, G loss: 1.985121\n",
      "Ep: 165, steps: 22, D loss: 0.175256, acc:  76%, G loss: 2.482855\n",
      "Ep: 165, steps: 23, D loss: 0.163763, acc:  76%, G loss: 2.246065\n",
      "Ep: 165, steps: 24, D loss: 0.195763, acc:  70%, G loss: 2.037570\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 166, steps: 1, D loss: 0.204086, acc:  70%, G loss: 2.268842\n",
      "Ep: 166, steps: 2, D loss: 0.235103, acc:  58%, G loss: 1.942032\n",
      "Ep: 166, steps: 3, D loss: 0.126657, acc:  92%, G loss: 2.745458\n",
      "Ep: 166, steps: 4, D loss: 0.179499, acc:  82%, G loss: 2.105216\n",
      "Ep: 166, steps: 5, D loss: 0.194695, acc:  75%, G loss: 2.211913\n",
      "Ep: 166, steps: 6, D loss: 0.273199, acc:  49%, G loss: 2.002711\n",
      "Ep: 166, steps: 7, D loss: 0.436861, acc:  21%, G loss: 1.801149\n",
      "Ep: 166, steps: 8, D loss: 0.199453, acc:  69%, G loss: 2.522440\n",
      "Ep: 166, steps: 9, D loss: 0.191722, acc:  75%, G loss: 2.069376\n",
      "Ep: 166, steps: 10, D loss: 0.140128, acc:  86%, G loss: 2.133673\n",
      "Ep: 166, steps: 11, D loss: 0.244260, acc:  57%, G loss: 2.341441\n",
      "Ep: 166, steps: 12, D loss: 0.324075, acc:  35%, G loss: 1.644418\n",
      "Ep: 166, steps: 13, D loss: 0.286135, acc:  41%, G loss: 1.797043\n",
      "Ep: 166, steps: 14, D loss: 0.300045, acc:  38%, G loss: 1.845182\n",
      "Ep: 166, steps: 15, D loss: 0.244891, acc:  53%, G loss: 2.047701\n",
      "Ep: 166, steps: 16, D loss: 0.293222, acc:  44%, G loss: 2.184556\n",
      "Ep: 166, steps: 17, D loss: 0.118369, acc:  92%, G loss: 2.463566\n",
      "Ep: 166, steps: 18, D loss: 0.257342, acc:  51%, G loss: 1.968793\n",
      "Ep: 166, steps: 19, D loss: 0.207735, acc:  66%, G loss: 1.866011\n",
      "Ep: 166, steps: 20, D loss: 0.145099, acc:  85%, G loss: 2.152923\n",
      "Ep: 166, steps: 21, D loss: 0.265630, acc:  49%, G loss: 1.997771\n",
      "Ep: 166, steps: 22, D loss: 0.188844, acc:  69%, G loss: 1.866060\n",
      "Ep: 166, steps: 23, D loss: 0.164332, acc:  82%, G loss: 2.370750\n",
      "Ep: 166, steps: 24, D loss: 0.158333, acc:  78%, G loss: 2.198982\n",
      "Ep: 166, steps: 25, D loss: 0.213805, acc:  64%, G loss: 2.159408\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 167, steps: 1, D loss: 0.225771, acc:  62%, G loss: 2.418485\n",
      "Ep: 167, steps: 2, D loss: 0.250634, acc:  52%, G loss: 1.957029\n",
      "Ep: 167, steps: 3, D loss: 0.140132, acc:  87%, G loss: 2.767247\n",
      "Ep: 167, steps: 4, D loss: 0.191083, acc:  80%, G loss: 2.153679\n",
      "Ep: 167, steps: 5, D loss: 0.254649, acc:  65%, G loss: 2.177302\n",
      "Ep: 167, steps: 6, D loss: 0.252420, acc:  50%, G loss: 2.044766\n",
      "Ep: 167, steps: 7, D loss: 0.390440, acc:  28%, G loss: 1.710308\n",
      "Ep: 167, steps: 8, D loss: 0.194852, acc:  68%, G loss: 2.280769\n",
      "Ep: 167, steps: 9, D loss: 0.155018, acc:  87%, G loss: 2.021368\n",
      "Ep: 167, steps: 10, D loss: 0.129745, acc:  87%, G loss: 2.134904\n",
      "Ep: 167, steps: 11, D loss: 0.263457, acc:  52%, G loss: 2.335954\n",
      "Ep: 167, steps: 12, D loss: 0.334841, acc:  34%, G loss: 1.667740\n",
      "Ep: 167, steps: 13, D loss: 0.287994, acc:  43%, G loss: 1.827316\n",
      "Ep: 167, steps: 14, D loss: 0.303308, acc:  34%, G loss: 1.817492\n",
      "Ep: 167, steps: 15, D loss: 0.235412, acc:  59%, G loss: 2.077914\n",
      "Ep: 167, steps: 16, D loss: 0.264975, acc:  54%, G loss: 2.211812\n",
      "Ep: 167, steps: 17, D loss: 0.134137, acc:  89%, G loss: 2.340904\n",
      "Ep: 167, steps: 18, D loss: 0.263495, acc:  48%, G loss: 1.990419\n",
      "Ep: 167, steps: 19, D loss: 0.185876, acc:  71%, G loss: 1.952818\n",
      "Ep: 167, steps: 20, D loss: 0.152964, acc:  83%, G loss: 2.161733\n",
      "Ep: 167, steps: 21, D loss: 0.288155, acc:  43%, G loss: 1.969836\n",
      "Ep: 167, steps: 22, D loss: 0.183327, acc:  69%, G loss: 2.001917\n",
      "Ep: 167, steps: 23, D loss: 0.164730, acc:  82%, G loss: 2.472403\n",
      "Ep: 167, steps: 24, D loss: 0.162696, acc:  79%, G loss: 2.267073\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 167, steps: 25, D loss: 0.194469, acc:  68%, G loss: 2.094981\n",
      "Ep: 167, steps: 26, D loss: 0.259709, acc:  51%, G loss: 1.967921\n",
      "Ep: 167, steps: 27, D loss: 0.132236, acc:  86%, G loss: 2.895092\n",
      "Ep: 167, steps: 28, D loss: 0.216896, acc:  74%, G loss: 2.203726\n",
      "Ep: 167, steps: 29, D loss: 0.261525, acc:  65%, G loss: 2.327449\n",
      "Ep: 167, steps: 30, D loss: 0.262741, acc:  49%, G loss: 2.004958\n",
      "Ep: 167, steps: 31, D loss: 0.347699, acc:  31%, G loss: 1.728539\n",
      "Ep: 167, steps: 32, D loss: 0.200637, acc:  67%, G loss: 2.376133\n",
      "Ep: 167, steps: 33, D loss: 0.173357, acc:  78%, G loss: 2.105346\n",
      "Ep: 167, steps: 34, D loss: 0.131316, acc:  88%, G loss: 2.174157\n",
      "Ep: 167, steps: 35, D loss: 0.254255, acc:  54%, G loss: 2.402825\n",
      "Ep: 167, steps: 36, D loss: 0.334263, acc:  31%, G loss: 1.656866\n",
      "Ep: 167, steps: 37, D loss: 0.304310, acc:  35%, G loss: 1.815860\n",
      "Ep: 167, steps: 38, D loss: 0.301249, acc:  37%, G loss: 1.820765\n",
      "Ep: 167, steps: 39, D loss: 0.237185, acc:  55%, G loss: 2.062133\n",
      "Ep: 167, steps: 40, D loss: 0.265533, acc:  53%, G loss: 2.194192\n",
      "Ep: 167, steps: 41, D loss: 0.134120, acc:  88%, G loss: 2.523302\n",
      "Ep: 167, steps: 42, D loss: 0.277259, acc:  45%, G loss: 1.920193\n",
      "Ep: 167, steps: 43, D loss: 0.192379, acc:  69%, G loss: 1.966242\n",
      "Ep: 167, steps: 44, D loss: 0.137900, acc:  87%, G loss: 2.174758\n",
      "Ep: 167, steps: 45, D loss: 0.298044, acc:  43%, G loss: 2.181520\n",
      "Ep: 167, steps: 46, D loss: 0.149686, acc:  82%, G loss: 2.083380\n",
      "Ep: 167, steps: 47, D loss: 0.166197, acc:  81%, G loss: 2.466673\n",
      "Ep: 167, steps: 48, D loss: 0.165101, acc:  77%, G loss: 2.186728\n",
      "Ep: 167, steps: 49, D loss: 0.199934, acc:  64%, G loss: 2.160617\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 168, steps: 1, D loss: 0.226007, acc:  63%, G loss: 2.205421\n",
      "Ep: 168, steps: 2, D loss: 0.252433, acc:  51%, G loss: 2.012514\n",
      "Ep: 168, steps: 3, D loss: 0.124657, acc:  91%, G loss: 2.554511\n",
      "Ep: 168, steps: 4, D loss: 0.195012, acc:  79%, G loss: 2.149919\n",
      "Ep: 168, steps: 5, D loss: 0.191970, acc:  73%, G loss: 2.142497\n",
      "Ep: 168, steps: 6, D loss: 0.260768, acc:  49%, G loss: 2.010446\n",
      "Ep: 168, steps: 7, D loss: 0.343321, acc:  38%, G loss: 1.749882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 168, steps: 8, D loss: 0.192276, acc:  69%, G loss: 2.159748\n",
      "Ep: 168, steps: 9, D loss: 0.157246, acc:  84%, G loss: 2.130411\n",
      "Ep: 168, steps: 10, D loss: 0.124502, acc:  88%, G loss: 2.259352\n",
      "Ep: 168, steps: 11, D loss: 0.245969, acc:  55%, G loss: 2.379903\n",
      "Ep: 168, steps: 12, D loss: 0.340120, acc:  31%, G loss: 1.652856\n",
      "Ep: 168, steps: 13, D loss: 0.295688, acc:  41%, G loss: 1.868715\n",
      "Ep: 168, steps: 14, D loss: 0.300358, acc:  40%, G loss: 1.828224\n",
      "Ep: 168, steps: 15, D loss: 0.242041, acc:  54%, G loss: 1.945747\n",
      "Ep: 168, steps: 16, D loss: 0.282418, acc:  51%, G loss: 2.113410\n",
      "Ep: 168, steps: 17, D loss: 0.128283, acc:  87%, G loss: 2.389083\n",
      "Ep: 168, steps: 18, D loss: 0.269575, acc:  45%, G loss: 2.050624\n",
      "Ep: 168, steps: 19, D loss: 0.185833, acc:  69%, G loss: 2.015595\n",
      "Ep: 168, steps: 20, D loss: 0.144916, acc:  83%, G loss: 2.208992\n",
      "Ep: 168, steps: 21, D loss: 0.263576, acc:  44%, G loss: 1.961654\n",
      "Ep: 168, steps: 22, D loss: 0.167435, acc:  75%, G loss: 1.961251\n",
      "Ep: 168, steps: 23, D loss: 0.164498, acc:  80%, G loss: 2.462825\n",
      "Ep: 168, steps: 24, D loss: 0.159628, acc:  78%, G loss: 2.191617\n",
      "Ep: 168, steps: 25, D loss: 0.183150, acc:  71%, G loss: 2.293760\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 169, steps: 1, D loss: 0.205928, acc:  69%, G loss: 2.491976\n",
      "Ep: 169, steps: 2, D loss: 0.244804, acc:  56%, G loss: 1.825730\n",
      "Ep: 169, steps: 3, D loss: 0.128834, acc:  92%, G loss: 2.731483\n",
      "Ep: 169, steps: 4, D loss: 0.171971, acc:  84%, G loss: 2.137534\n",
      "Ep: 169, steps: 5, D loss: 0.255756, acc:  63%, G loss: 2.182594\n",
      "Ep: 169, steps: 6, D loss: 0.272014, acc:  50%, G loss: 2.152749\n",
      "Ep: 169, steps: 7, D loss: 0.407666, acc:  20%, G loss: 1.589810\n",
      "Ep: 169, steps: 8, D loss: 0.216240, acc:  61%, G loss: 2.373434\n",
      "Ep: 169, steps: 9, D loss: 0.180241, acc:  80%, G loss: 2.023510\n",
      "Ep: 169, steps: 10, D loss: 0.124609, acc:  89%, G loss: 2.026490\n",
      "Ep: 169, steps: 11, D loss: 0.266190, acc:  50%, G loss: 2.294807\n",
      "Ep: 169, steps: 12, D loss: 0.326619, acc:  34%, G loss: 1.817750\n",
      "Ep: 169, steps: 13, D loss: 0.289943, acc:  40%, G loss: 1.853335\n",
      "Ep: 169, steps: 14, D loss: 0.313693, acc:  33%, G loss: 1.809012\n",
      "Ep: 169, steps: 15, D loss: 0.239564, acc:  56%, G loss: 2.006787\n",
      "Ep: 169, steps: 16, D loss: 0.272202, acc:  53%, G loss: 2.167397\n",
      "Ep: 169, steps: 17, D loss: 0.139852, acc:  87%, G loss: 2.329588\n",
      "Ep: 169, steps: 18, D loss: 0.257492, acc:  50%, G loss: 1.945038\n",
      "Ep: 169, steps: 19, D loss: 0.188435, acc:  69%, G loss: 1.977995\n",
      "Ep: 169, steps: 20, D loss: 0.140081, acc:  86%, G loss: 2.198100\n",
      "Ep: 169, steps: 21, D loss: 0.315191, acc:  36%, G loss: 1.981961\n",
      "Ep: 169, steps: 22, D loss: 0.168220, acc:  74%, G loss: 2.020517\n",
      "Saved Model\n",
      "Ep: 169, steps: 23, D loss: 0.168019, acc:  79%, G loss: 2.475345\n",
      "Ep: 169, steps: 24, D loss: 0.229288, acc:  56%, G loss: 1.890880\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 170, steps: 1, D loss: 0.241042, acc:  57%, G loss: 2.233912\n",
      "Ep: 170, steps: 2, D loss: 0.227019, acc:  63%, G loss: 1.812696\n",
      "Ep: 170, steps: 3, D loss: 0.159957, acc:  84%, G loss: 2.507091\n",
      "Ep: 170, steps: 4, D loss: 0.196536, acc:  77%, G loss: 1.977509\n",
      "Ep: 170, steps: 5, D loss: 0.284389, acc:  60%, G loss: 2.231336\n",
      "Ep: 170, steps: 6, D loss: 0.249675, acc:  50%, G loss: 2.079374\n",
      "Ep: 170, steps: 7, D loss: 0.369787, acc:  29%, G loss: 1.658439\n",
      "Ep: 170, steps: 8, D loss: 0.209769, acc:  65%, G loss: 2.336469\n",
      "Ep: 170, steps: 9, D loss: 0.161998, acc:  82%, G loss: 2.134436\n",
      "Ep: 170, steps: 10, D loss: 0.134867, acc:  86%, G loss: 2.116497\n",
      "Ep: 170, steps: 11, D loss: 0.239042, acc:  57%, G loss: 2.343021\n",
      "Ep: 170, steps: 12, D loss: 0.328131, acc:  35%, G loss: 1.641149\n",
      "Ep: 170, steps: 13, D loss: 0.285902, acc:  40%, G loss: 1.775222\n",
      "Ep: 170, steps: 14, D loss: 0.299147, acc:  37%, G loss: 1.850558\n",
      "Ep: 170, steps: 15, D loss: 0.236219, acc:  54%, G loss: 2.003323\n",
      "Ep: 170, steps: 16, D loss: 0.278417, acc:  50%, G loss: 2.074588\n",
      "Ep: 170, steps: 17, D loss: 0.133686, acc:  88%, G loss: 2.301332\n",
      "Ep: 170, steps: 18, D loss: 0.242982, acc:  56%, G loss: 2.001012\n",
      "Ep: 170, steps: 19, D loss: 0.173305, acc:  74%, G loss: 1.976595\n",
      "Ep: 170, steps: 20, D loss: 0.130361, acc:  88%, G loss: 2.300806\n",
      "Ep: 170, steps: 21, D loss: 0.274848, acc:  46%, G loss: 1.977537\n",
      "Ep: 170, steps: 22, D loss: 0.162651, acc:  78%, G loss: 1.994239\n",
      "Ep: 170, steps: 23, D loss: 0.155704, acc:  81%, G loss: 2.408624\n",
      "Ep: 170, steps: 24, D loss: 0.170340, acc:  73%, G loss: 2.064606\n",
      "Ep: 170, steps: 25, D loss: 0.189389, acc:  66%, G loss: 2.097855\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 171, steps: 1, D loss: 0.224983, acc:  64%, G loss: 2.151975\n",
      "Ep: 171, steps: 2, D loss: 0.235253, acc:  58%, G loss: 1.814973\n",
      "Ep: 171, steps: 3, D loss: 0.138872, acc:  89%, G loss: 2.642644\n",
      "Ep: 171, steps: 4, D loss: 0.182854, acc:  80%, G loss: 2.121846\n",
      "Ep: 171, steps: 5, D loss: 0.217879, acc:  67%, G loss: 2.338064\n",
      "Ep: 171, steps: 6, D loss: 0.267663, acc:  50%, G loss: 2.228670\n",
      "Ep: 171, steps: 7, D loss: 0.476484, acc:  12%, G loss: 2.014209\n",
      "Ep: 171, steps: 8, D loss: 0.208313, acc:  65%, G loss: 2.481557\n",
      "Ep: 171, steps: 9, D loss: 0.191448, acc:  75%, G loss: 2.231799\n",
      "Ep: 171, steps: 10, D loss: 0.119719, acc:  91%, G loss: 2.048884\n",
      "Ep: 171, steps: 11, D loss: 0.234481, acc:  60%, G loss: 2.349506\n",
      "Ep: 171, steps: 12, D loss: 0.324090, acc:  38%, G loss: 1.666647\n",
      "Ep: 171, steps: 13, D loss: 0.277956, acc:  46%, G loss: 1.757877\n",
      "Ep: 171, steps: 14, D loss: 0.294617, acc:  38%, G loss: 1.834990\n",
      "Ep: 171, steps: 15, D loss: 0.228686, acc:  63%, G loss: 2.064244\n",
      "Ep: 171, steps: 16, D loss: 0.289698, acc:  46%, G loss: 2.167391\n",
      "Ep: 171, steps: 17, D loss: 0.126335, acc:  89%, G loss: 2.206316\n",
      "Ep: 171, steps: 18, D loss: 0.251946, acc:  51%, G loss: 2.051225\n",
      "Ep: 171, steps: 19, D loss: 0.192311, acc:  67%, G loss: 1.985523\n",
      "Ep: 171, steps: 20, D loss: 0.141732, acc:  85%, G loss: 2.197599\n",
      "Ep: 171, steps: 21, D loss: 0.283295, acc:  46%, G loss: 1.966417\n",
      "Ep: 171, steps: 22, D loss: 0.162951, acc:  78%, G loss: 2.132641\n",
      "Ep: 171, steps: 23, D loss: 0.180182, acc:  76%, G loss: 2.447757\n",
      "Ep: 171, steps: 24, D loss: 0.170523, acc:  75%, G loss: 2.118085\n",
      "Ep: 171, steps: 25, D loss: 0.206940, acc:  65%, G loss: 2.226473\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 172, steps: 1, D loss: 0.231992, acc:  61%, G loss: 2.585636\n",
      "Ep: 172, steps: 2, D loss: 0.249864, acc:  51%, G loss: 1.829626\n",
      "Ep: 172, steps: 3, D loss: 0.135546, acc:  90%, G loss: 2.547271\n",
      "Ep: 172, steps: 4, D loss: 0.186708, acc:  80%, G loss: 2.045777\n",
      "Ep: 172, steps: 5, D loss: 0.220368, acc:  69%, G loss: 2.248818\n",
      "Ep: 172, steps: 6, D loss: 0.265525, acc:  49%, G loss: 1.900521\n",
      "Ep: 172, steps: 7, D loss: 0.407241, acc:  24%, G loss: 1.765173\n",
      "Ep: 172, steps: 8, D loss: 0.187802, acc:  72%, G loss: 2.729394\n",
      "Ep: 172, steps: 9, D loss: 0.178500, acc:  80%, G loss: 2.133155\n",
      "Ep: 172, steps: 10, D loss: 0.129087, acc:  89%, G loss: 2.092105\n",
      "Ep: 172, steps: 11, D loss: 0.253622, acc:  54%, G loss: 2.260143\n",
      "Ep: 172, steps: 12, D loss: 0.324123, acc:  37%, G loss: 1.671244\n",
      "Ep: 172, steps: 13, D loss: 0.276972, acc:  47%, G loss: 1.814346\n",
      "Ep: 172, steps: 14, D loss: 0.319670, acc:  32%, G loss: 1.831562\n",
      "Ep: 172, steps: 15, D loss: 0.220314, acc:  65%, G loss: 2.074888\n",
      "Ep: 172, steps: 16, D loss: 0.285658, acc:  48%, G loss: 2.167671\n",
      "Ep: 172, steps: 17, D loss: 0.165129, acc:  80%, G loss: 2.245984\n",
      "Ep: 172, steps: 18, D loss: 0.241773, acc:  57%, G loss: 1.937471\n",
      "Ep: 172, steps: 19, D loss: 0.183966, acc:  71%, G loss: 1.977869\n",
      "Saved Model\n",
      "Ep: 172, steps: 20, D loss: 0.141305, acc:  86%, G loss: 2.153991\n",
      "Ep: 172, steps: 21, D loss: 0.177011, acc:  73%, G loss: 2.248402\n",
      "Ep: 172, steps: 22, D loss: 0.161769, acc:  80%, G loss: 2.508721\n",
      "Ep: 172, steps: 23, D loss: 0.168656, acc:  74%, G loss: 2.085209\n",
      "Ep: 172, steps: 24, D loss: 0.197171, acc:  64%, G loss: 2.078616\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 173, steps: 1, D loss: 0.210796, acc:  69%, G loss: 2.481303\n",
      "Ep: 173, steps: 2, D loss: 0.235214, acc:  56%, G loss: 1.883944\n",
      "Ep: 173, steps: 3, D loss: 0.134283, acc:  91%, G loss: 2.585701\n",
      "Ep: 173, steps: 4, D loss: 0.167887, acc:  84%, G loss: 2.147260\n",
      "Ep: 173, steps: 5, D loss: 0.296187, acc:  63%, G loss: 2.663589\n",
      "Ep: 173, steps: 6, D loss: 0.267445, acc:  50%, G loss: 1.936657\n",
      "Ep: 173, steps: 7, D loss: 0.370419, acc:  26%, G loss: 1.591524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 173, steps: 8, D loss: 0.209953, acc:  65%, G loss: 2.230631\n",
      "Ep: 173, steps: 9, D loss: 0.155538, acc:  84%, G loss: 2.103717\n",
      "Ep: 173, steps: 10, D loss: 0.125289, acc:  88%, G loss: 2.039593\n",
      "Ep: 173, steps: 11, D loss: 0.226928, acc:  62%, G loss: 2.360282\n",
      "Ep: 173, steps: 12, D loss: 0.318854, acc:  39%, G loss: 1.626269\n",
      "Ep: 173, steps: 13, D loss: 0.286972, acc:  44%, G loss: 1.738605\n",
      "Ep: 173, steps: 14, D loss: 0.297807, acc:  40%, G loss: 1.772485\n",
      "Ep: 173, steps: 15, D loss: 0.230909, acc:  59%, G loss: 2.058022\n",
      "Ep: 173, steps: 16, D loss: 0.282190, acc:  49%, G loss: 1.990878\n",
      "Ep: 173, steps: 17, D loss: 0.123870, acc:  91%, G loss: 2.328224\n",
      "Ep: 173, steps: 18, D loss: 0.249181, acc:  53%, G loss: 1.937205\n",
      "Ep: 173, steps: 19, D loss: 0.180279, acc:  72%, G loss: 1.931920\n",
      "Ep: 173, steps: 20, D loss: 0.145869, acc:  84%, G loss: 2.256718\n",
      "Ep: 173, steps: 21, D loss: 0.261484, acc:  50%, G loss: 2.081317\n",
      "Ep: 173, steps: 22, D loss: 0.171735, acc:  75%, G loss: 2.008764\n",
      "Ep: 173, steps: 23, D loss: 0.181247, acc:  76%, G loss: 2.351074\n",
      "Ep: 173, steps: 24, D loss: 0.173572, acc:  73%, G loss: 1.970766\n",
      "Ep: 173, steps: 25, D loss: 0.202303, acc:  65%, G loss: 2.029887\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 174, steps: 1, D loss: 0.225002, acc:  64%, G loss: 2.249376\n",
      "Ep: 174, steps: 2, D loss: 0.234597, acc:  59%, G loss: 1.860813\n",
      "Ep: 174, steps: 3, D loss: 0.132484, acc:  91%, G loss: 2.587831\n",
      "Ep: 174, steps: 4, D loss: 0.180532, acc:  80%, G loss: 2.081653\n",
      "Ep: 174, steps: 5, D loss: 0.183414, acc:  76%, G loss: 2.191676\n",
      "Ep: 174, steps: 6, D loss: 0.272710, acc:  49%, G loss: 1.993788\n",
      "Ep: 174, steps: 7, D loss: 0.450162, acc:  17%, G loss: 1.643298\n",
      "Ep: 174, steps: 8, D loss: 0.206374, acc:  67%, G loss: 2.365346\n",
      "Ep: 174, steps: 9, D loss: 0.178269, acc:  79%, G loss: 2.050989\n",
      "Ep: 174, steps: 10, D loss: 0.130732, acc:  84%, G loss: 2.057779\n",
      "Ep: 174, steps: 11, D loss: 0.269162, acc:  48%, G loss: 2.261565\n",
      "Ep: 174, steps: 12, D loss: 0.323040, acc:  38%, G loss: 1.713802\n",
      "Ep: 174, steps: 13, D loss: 0.294525, acc:  42%, G loss: 1.822597\n",
      "Ep: 174, steps: 14, D loss: 0.314953, acc:  31%, G loss: 1.773675\n",
      "Ep: 174, steps: 15, D loss: 0.229243, acc:  63%, G loss: 2.098069\n",
      "Ep: 174, steps: 16, D loss: 0.296537, acc:  44%, G loss: 2.146158\n",
      "Ep: 174, steps: 17, D loss: 0.113494, acc:  92%, G loss: 2.309740\n",
      "Ep: 174, steps: 18, D loss: 0.256742, acc:  50%, G loss: 1.919298\n",
      "Ep: 174, steps: 19, D loss: 0.189663, acc:  69%, G loss: 1.915099\n",
      "Ep: 174, steps: 20, D loss: 0.179811, acc:  73%, G loss: 2.096601\n",
      "Ep: 174, steps: 21, D loss: 0.314762, acc:  35%, G loss: 2.085058\n",
      "Ep: 174, steps: 22, D loss: 0.181381, acc:  72%, G loss: 2.086190\n",
      "Ep: 174, steps: 23, D loss: 0.191247, acc:  73%, G loss: 2.401389\n",
      "Ep: 174, steps: 24, D loss: 0.166490, acc:  74%, G loss: 2.068243\n",
      "Ep: 174, steps: 25, D loss: 0.204019, acc:  64%, G loss: 2.198172\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 175, steps: 1, D loss: 0.240500, acc:  60%, G loss: 2.491946\n",
      "Ep: 175, steps: 2, D loss: 0.248162, acc:  53%, G loss: 1.797269\n",
      "Ep: 175, steps: 3, D loss: 0.140618, acc:  90%, G loss: 2.595190\n",
      "Ep: 175, steps: 4, D loss: 0.190709, acc:  79%, G loss: 2.116769\n",
      "Ep: 175, steps: 5, D loss: 0.191848, acc:  75%, G loss: 2.137048\n",
      "Ep: 175, steps: 6, D loss: 0.262867, acc:  49%, G loss: 1.896405\n",
      "Ep: 175, steps: 7, D loss: 0.371003, acc:  29%, G loss: 1.618541\n",
      "Ep: 175, steps: 8, D loss: 0.200103, acc:  67%, G loss: 2.411291\n",
      "Ep: 175, steps: 9, D loss: 0.174555, acc:  79%, G loss: 2.054789\n",
      "Ep: 175, steps: 10, D loss: 0.126316, acc:  89%, G loss: 2.089969\n",
      "Ep: 175, steps: 11, D loss: 0.317020, acc:  36%, G loss: 2.635277\n",
      "Ep: 175, steps: 12, D loss: 0.315557, acc:  41%, G loss: 1.819875\n",
      "Ep: 175, steps: 13, D loss: 0.297079, acc:  41%, G loss: 1.870276\n",
      "Ep: 175, steps: 14, D loss: 0.309017, acc:  33%, G loss: 1.796118\n",
      "Ep: 175, steps: 15, D loss: 0.211648, acc:  70%, G loss: 2.048220\n",
      "Ep: 175, steps: 16, D loss: 0.284032, acc:  47%, G loss: 2.086750\n",
      "Ep: 175, steps: 17, D loss: 0.137936, acc:  88%, G loss: 2.221210\n",
      "Saved Model\n",
      "Ep: 175, steps: 18, D loss: 0.245353, acc:  57%, G loss: 1.900884\n",
      "Ep: 175, steps: 19, D loss: 0.148557, acc:  84%, G loss: 2.232248\n",
      "Ep: 175, steps: 20, D loss: 0.251509, acc:  51%, G loss: 2.053785\n",
      "Ep: 175, steps: 21, D loss: 0.167467, acc:  78%, G loss: 1.957972\n",
      "Ep: 175, steps: 22, D loss: 0.185213, acc:  76%, G loss: 2.430234\n",
      "Ep: 175, steps: 23, D loss: 0.168117, acc:  78%, G loss: 2.035526\n",
      "Ep: 175, steps: 24, D loss: 0.207624, acc:  69%, G loss: 2.147508\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 176, steps: 1, D loss: 0.211587, acc:  68%, G loss: 2.343550\n",
      "Ep: 176, steps: 2, D loss: 0.244851, acc:  54%, G loss: 1.784654\n",
      "Ep: 176, steps: 3, D loss: 0.142615, acc:  86%, G loss: 2.554536\n",
      "Ep: 176, steps: 4, D loss: 0.176821, acc:  81%, G loss: 2.115216\n",
      "Ep: 176, steps: 5, D loss: 0.247016, acc:  64%, G loss: 2.229039\n",
      "Ep: 176, steps: 6, D loss: 0.256680, acc:  49%, G loss: 2.262660\n",
      "Ep: 176, steps: 7, D loss: 0.451956, acc:  19%, G loss: 1.851525\n",
      "Ep: 176, steps: 8, D loss: 0.200249, acc:  67%, G loss: 2.436537\n",
      "Ep: 176, steps: 9, D loss: 0.167017, acc:  83%, G loss: 2.159583\n",
      "Ep: 176, steps: 10, D loss: 0.124613, acc:  90%, G loss: 2.086674\n",
      "Ep: 176, steps: 11, D loss: 0.240686, acc:  57%, G loss: 2.393371\n",
      "Ep: 176, steps: 12, D loss: 0.345338, acc:  33%, G loss: 1.670152\n",
      "Ep: 176, steps: 13, D loss: 0.283097, acc:  46%, G loss: 1.698474\n",
      "Ep: 176, steps: 14, D loss: 0.300337, acc:  36%, G loss: 1.791769\n",
      "Ep: 176, steps: 15, D loss: 0.229569, acc:  62%, G loss: 2.067084\n",
      "Ep: 176, steps: 16, D loss: 0.262005, acc:  53%, G loss: 2.107838\n",
      "Ep: 176, steps: 17, D loss: 0.124064, acc:  91%, G loss: 2.255414\n",
      "Ep: 176, steps: 18, D loss: 0.263873, acc:  49%, G loss: 1.962428\n",
      "Ep: 176, steps: 19, D loss: 0.196599, acc:  67%, G loss: 1.905249\n",
      "Ep: 176, steps: 20, D loss: 0.136349, acc:  86%, G loss: 2.176344\n",
      "Ep: 176, steps: 21, D loss: 0.265357, acc:  45%, G loss: 2.039356\n",
      "Ep: 176, steps: 22, D loss: 0.160954, acc:  77%, G loss: 2.054292\n",
      "Ep: 176, steps: 23, D loss: 0.171186, acc:  79%, G loss: 2.471241\n",
      "Ep: 176, steps: 24, D loss: 0.170756, acc:  74%, G loss: 2.100452\n",
      "Ep: 176, steps: 25, D loss: 0.205827, acc:  63%, G loss: 2.174085\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 177, steps: 1, D loss: 0.225069, acc:  64%, G loss: 2.449553\n",
      "Ep: 177, steps: 2, D loss: 0.245405, acc:  54%, G loss: 1.819867\n",
      "Ep: 177, steps: 3, D loss: 0.137498, acc:  89%, G loss: 2.568871\n",
      "Ep: 177, steps: 4, D loss: 0.192821, acc:  77%, G loss: 2.063332\n",
      "Ep: 177, steps: 5, D loss: 0.285679, acc:  63%, G loss: 2.332627\n",
      "Ep: 177, steps: 6, D loss: 0.264314, acc:  49%, G loss: 2.235579\n",
      "Ep: 177, steps: 7, D loss: 0.356878, acc:  31%, G loss: 1.627082\n",
      "Ep: 177, steps: 8, D loss: 0.189112, acc:  69%, G loss: 2.388953\n",
      "Ep: 177, steps: 9, D loss: 0.164581, acc:  84%, G loss: 2.129285\n",
      "Ep: 177, steps: 10, D loss: 0.133229, acc:  85%, G loss: 2.031211\n",
      "Ep: 177, steps: 11, D loss: 0.232058, acc:  60%, G loss: 2.304257\n",
      "Ep: 177, steps: 12, D loss: 0.330493, acc:  36%, G loss: 1.667407\n",
      "Ep: 177, steps: 13, D loss: 0.300444, acc:  41%, G loss: 1.871676\n",
      "Ep: 177, steps: 14, D loss: 0.296721, acc:  38%, G loss: 1.801278\n",
      "Ep: 177, steps: 15, D loss: 0.221114, acc:  65%, G loss: 2.076174\n",
      "Ep: 177, steps: 16, D loss: 0.275312, acc:  53%, G loss: 2.192283\n",
      "Ep: 177, steps: 17, D loss: 0.120098, acc:  90%, G loss: 2.291442\n",
      "Ep: 177, steps: 18, D loss: 0.234355, acc:  59%, G loss: 1.928843\n",
      "Ep: 177, steps: 19, D loss: 0.190784, acc:  69%, G loss: 1.927108\n",
      "Ep: 177, steps: 20, D loss: 0.145142, acc:  83%, G loss: 2.236316\n",
      "Ep: 177, steps: 21, D loss: 0.273120, acc:  43%, G loss: 1.961413\n",
      "Ep: 177, steps: 22, D loss: 0.183643, acc:  69%, G loss: 2.073526\n",
      "Ep: 177, steps: 23, D loss: 0.160596, acc:  81%, G loss: 2.540809\n",
      "Ep: 177, steps: 24, D loss: 0.168313, acc:  74%, G loss: 2.258070\n",
      "Ep: 177, steps: 25, D loss: 0.194651, acc:  68%, G loss: 2.162819\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 178, steps: 1, D loss: 0.219567, acc:  67%, G loss: 2.415051\n",
      "Ep: 178, steps: 2, D loss: 0.252517, acc:  53%, G loss: 1.839188\n",
      "Ep: 178, steps: 3, D loss: 0.129892, acc:  91%, G loss: 2.530329\n",
      "Ep: 178, steps: 4, D loss: 0.182438, acc:  80%, G loss: 2.094564\n",
      "Ep: 178, steps: 5, D loss: 0.212756, acc:  71%, G loss: 2.208008\n",
      "Ep: 178, steps: 6, D loss: 0.271338, acc:  50%, G loss: 2.189998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 178, steps: 7, D loss: 0.462925, acc:  16%, G loss: 1.786592\n",
      "Ep: 178, steps: 8, D loss: 0.205227, acc:  66%, G loss: 2.739694\n",
      "Ep: 178, steps: 9, D loss: 0.178646, acc:  79%, G loss: 2.179473\n",
      "Ep: 178, steps: 10, D loss: 0.119309, acc:  91%, G loss: 2.009685\n",
      "Ep: 178, steps: 11, D loss: 0.241319, acc:  58%, G loss: 2.324126\n",
      "Ep: 178, steps: 12, D loss: 0.332251, acc:  35%, G loss: 1.634560\n",
      "Ep: 178, steps: 13, D loss: 0.282328, acc:  41%, G loss: 1.786644\n",
      "Ep: 178, steps: 14, D loss: 0.318385, acc:  32%, G loss: 1.813196\n",
      "Ep: 178, steps: 15, D loss: 0.219322, acc:  64%, G loss: 2.192182\n",
      "Saved Model\n",
      "Ep: 178, steps: 16, D loss: 0.294667, acc:  45%, G loss: 2.175333\n",
      "Ep: 178, steps: 17, D loss: 0.233942, acc:  62%, G loss: 2.027009\n",
      "Ep: 178, steps: 18, D loss: 0.186489, acc:  70%, G loss: 1.970508\n",
      "Ep: 178, steps: 19, D loss: 0.140060, acc:  86%, G loss: 2.255177\n",
      "Ep: 178, steps: 20, D loss: 0.279537, acc:  47%, G loss: 2.004889\n",
      "Ep: 178, steps: 21, D loss: 0.171635, acc:  75%, G loss: 2.062629\n",
      "Ep: 178, steps: 22, D loss: 0.191512, acc:  72%, G loss: 2.430862\n",
      "Ep: 178, steps: 23, D loss: 0.159990, acc:  78%, G loss: 2.065590\n",
      "Ep: 178, steps: 24, D loss: 0.194644, acc:  66%, G loss: 2.207988\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 179, steps: 1, D loss: 0.230056, acc:  63%, G loss: 2.380312\n",
      "Ep: 179, steps: 2, D loss: 0.241616, acc:  56%, G loss: 1.797986\n",
      "Ep: 179, steps: 3, D loss: 0.130086, acc:  94%, G loss: 2.488886\n",
      "Ep: 179, steps: 4, D loss: 0.188760, acc:  78%, G loss: 2.049863\n",
      "Ep: 179, steps: 5, D loss: 0.245142, acc:  66%, G loss: 2.282072\n",
      "Ep: 179, steps: 6, D loss: 0.280436, acc:  48%, G loss: 2.087110\n",
      "Ep: 179, steps: 7, D loss: 0.386226, acc:  23%, G loss: 1.815518\n",
      "Ep: 179, steps: 8, D loss: 0.202747, acc:  66%, G loss: 2.370619\n",
      "Ep: 179, steps: 9, D loss: 0.172540, acc:  82%, G loss: 2.202034\n",
      "Ep: 179, steps: 10, D loss: 0.126826, acc:  89%, G loss: 2.052827\n",
      "Ep: 179, steps: 11, D loss: 0.253525, acc:  52%, G loss: 2.281248\n",
      "Ep: 179, steps: 12, D loss: 0.328400, acc:  36%, G loss: 1.629093\n",
      "Ep: 179, steps: 13, D loss: 0.283669, acc:  42%, G loss: 1.819033\n",
      "Ep: 179, steps: 14, D loss: 0.281300, acc:  41%, G loss: 1.742818\n",
      "Ep: 179, steps: 15, D loss: 0.224810, acc:  61%, G loss: 2.012712\n",
      "Ep: 179, steps: 16, D loss: 0.272029, acc:  52%, G loss: 2.000012\n",
      "Ep: 179, steps: 17, D loss: 0.127786, acc:  88%, G loss: 2.235499\n",
      "Ep: 179, steps: 18, D loss: 0.257537, acc:  51%, G loss: 2.054663\n",
      "Ep: 179, steps: 19, D loss: 0.178476, acc:  72%, G loss: 1.952168\n",
      "Ep: 179, steps: 20, D loss: 0.137234, acc:  85%, G loss: 2.193302\n",
      "Ep: 179, steps: 21, D loss: 0.281617, acc:  45%, G loss: 2.083158\n",
      "Ep: 179, steps: 22, D loss: 0.183673, acc:  72%, G loss: 1.945101\n",
      "Ep: 179, steps: 23, D loss: 0.163120, acc:  79%, G loss: 2.375486\n",
      "Ep: 179, steps: 24, D loss: 0.155490, acc:  78%, G loss: 2.081691\n",
      "Ep: 179, steps: 25, D loss: 0.194431, acc:  67%, G loss: 2.224882\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 180, steps: 1, D loss: 0.229082, acc:  64%, G loss: 2.369502\n",
      "Ep: 180, steps: 2, D loss: 0.255050, acc:  51%, G loss: 1.805125\n",
      "Ep: 180, steps: 3, D loss: 0.121564, acc:  94%, G loss: 2.536387\n",
      "Ep: 180, steps: 4, D loss: 0.177012, acc:  82%, G loss: 2.114991\n",
      "Ep: 180, steps: 5, D loss: 0.222950, acc:  70%, G loss: 2.245144\n",
      "Ep: 180, steps: 6, D loss: 0.268281, acc:  49%, G loss: 2.209309\n",
      "Ep: 180, steps: 7, D loss: 0.484469, acc:  16%, G loss: 2.162307\n",
      "Ep: 180, steps: 8, D loss: 0.210661, acc:  64%, G loss: 2.344069\n",
      "Ep: 180, steps: 9, D loss: 0.187380, acc:  76%, G loss: 2.187891\n",
      "Ep: 180, steps: 10, D loss: 0.133637, acc:  87%, G loss: 1.937737\n",
      "Ep: 180, steps: 11, D loss: 0.216681, acc:  66%, G loss: 2.218776\n",
      "Ep: 180, steps: 12, D loss: 0.323488, acc:  38%, G loss: 1.654719\n",
      "Ep: 180, steps: 13, D loss: 0.270442, acc:  51%, G loss: 1.757190\n",
      "Ep: 180, steps: 14, D loss: 0.298207, acc:  36%, G loss: 1.795930\n",
      "Ep: 180, steps: 15, D loss: 0.217856, acc:  66%, G loss: 2.085441\n",
      "Ep: 180, steps: 16, D loss: 0.291005, acc:  48%, G loss: 2.025867\n",
      "Ep: 180, steps: 17, D loss: 0.142240, acc:  84%, G loss: 2.237164\n",
      "Ep: 180, steps: 18, D loss: 0.245545, acc:  56%, G loss: 1.982193\n",
      "Ep: 180, steps: 19, D loss: 0.176259, acc:  74%, G loss: 1.950630\n",
      "Ep: 180, steps: 20, D loss: 0.139679, acc:  86%, G loss: 2.179774\n",
      "Ep: 180, steps: 21, D loss: 0.279332, acc:  43%, G loss: 1.848695\n",
      "Ep: 180, steps: 22, D loss: 0.179218, acc:  72%, G loss: 1.979614\n",
      "Ep: 180, steps: 23, D loss: 0.179334, acc:  77%, G loss: 2.352047\n",
      "Ep: 180, steps: 24, D loss: 0.166996, acc:  74%, G loss: 2.037659\n",
      "Ep: 180, steps: 25, D loss: 0.194209, acc:  67%, G loss: 2.243390\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 181, steps: 1, D loss: 0.232820, acc:  59%, G loss: 2.347572\n",
      "Ep: 181, steps: 2, D loss: 0.232342, acc:  58%, G loss: 1.897510\n",
      "Ep: 181, steps: 3, D loss: 0.128363, acc:  94%, G loss: 2.575467\n",
      "Ep: 181, steps: 4, D loss: 0.195672, acc:  77%, G loss: 2.061343\n",
      "Ep: 181, steps: 5, D loss: 0.219286, acc:  68%, G loss: 2.160925\n",
      "Ep: 181, steps: 6, D loss: 0.273109, acc:  49%, G loss: 2.174087\n",
      "Ep: 181, steps: 7, D loss: 0.428736, acc:  19%, G loss: 1.921793\n",
      "Ep: 181, steps: 8, D loss: 0.190578, acc:  70%, G loss: 2.343021\n",
      "Ep: 181, steps: 9, D loss: 0.186868, acc:  77%, G loss: 2.072830\n",
      "Ep: 181, steps: 10, D loss: 0.133744, acc:  85%, G loss: 1.928787\n",
      "Ep: 181, steps: 11, D loss: 0.249583, acc:  55%, G loss: 2.405712\n",
      "Ep: 181, steps: 12, D loss: 0.314384, acc:  42%, G loss: 1.778129\n",
      "Ep: 181, steps: 13, D loss: 0.287562, acc:  41%, G loss: 1.755588\n",
      "Saved Model\n",
      "Ep: 181, steps: 14, D loss: 0.353491, acc:  28%, G loss: 1.759091\n",
      "Ep: 181, steps: 15, D loss: 0.318397, acc:  39%, G loss: 2.003402\n",
      "Ep: 181, steps: 16, D loss: 0.146969, acc:  86%, G loss: 2.203014\n",
      "Ep: 181, steps: 17, D loss: 0.255231, acc:  52%, G loss: 1.937561\n",
      "Ep: 181, steps: 18, D loss: 0.177872, acc:  74%, G loss: 1.951534\n",
      "Ep: 181, steps: 19, D loss: 0.148969, acc:  85%, G loss: 2.131074\n",
      "Ep: 181, steps: 20, D loss: 0.302203, acc:  40%, G loss: 2.063939\n",
      "Ep: 181, steps: 21, D loss: 0.176804, acc:  76%, G loss: 2.017699\n",
      "Ep: 181, steps: 22, D loss: 0.173553, acc:  78%, G loss: 2.407775\n",
      "Ep: 181, steps: 23, D loss: 0.170554, acc:  76%, G loss: 1.960070\n",
      "Ep: 181, steps: 24, D loss: 0.193971, acc:  66%, G loss: 2.141951\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 182, steps: 1, D loss: 0.213165, acc:  69%, G loss: 2.496483\n",
      "Ep: 182, steps: 2, D loss: 0.222626, acc:  63%, G loss: 1.894551\n",
      "Ep: 182, steps: 3, D loss: 0.134330, acc:  92%, G loss: 2.631119\n",
      "Ep: 182, steps: 4, D loss: 0.179475, acc:  83%, G loss: 2.006388\n",
      "Ep: 182, steps: 5, D loss: 0.244173, acc:  65%, G loss: 2.514660\n",
      "Ep: 182, steps: 6, D loss: 0.260123, acc:  50%, G loss: 2.138548\n",
      "Ep: 182, steps: 7, D loss: 0.391272, acc:  23%, G loss: 1.664744\n",
      "Ep: 182, steps: 8, D loss: 0.193422, acc:  67%, G loss: 2.422406\n",
      "Ep: 182, steps: 9, D loss: 0.167968, acc:  82%, G loss: 2.093451\n",
      "Ep: 182, steps: 10, D loss: 0.129412, acc:  85%, G loss: 2.045462\n",
      "Ep: 182, steps: 11, D loss: 0.246174, acc:  56%, G loss: 2.302291\n",
      "Ep: 182, steps: 12, D loss: 0.328351, acc:  39%, G loss: 1.720622\n",
      "Ep: 182, steps: 13, D loss: 0.274107, acc:  49%, G loss: 1.912617\n",
      "Ep: 182, steps: 14, D loss: 0.325198, acc:  28%, G loss: 1.855933\n",
      "Ep: 182, steps: 15, D loss: 0.211220, acc:  70%, G loss: 2.135533\n",
      "Ep: 182, steps: 16, D loss: 0.284656, acc:  47%, G loss: 2.140251\n",
      "Ep: 182, steps: 17, D loss: 0.125821, acc:  90%, G loss: 2.207617\n",
      "Ep: 182, steps: 18, D loss: 0.259554, acc:  51%, G loss: 1.908994\n",
      "Ep: 182, steps: 19, D loss: 0.174892, acc:  73%, G loss: 2.010481\n",
      "Ep: 182, steps: 20, D loss: 0.156287, acc:  81%, G loss: 2.176140\n",
      "Ep: 182, steps: 21, D loss: 0.308041, acc:  37%, G loss: 1.984044\n",
      "Ep: 182, steps: 22, D loss: 0.183430, acc:  70%, G loss: 2.059514\n",
      "Ep: 182, steps: 23, D loss: 0.200909, acc:  70%, G loss: 2.511193\n",
      "Ep: 182, steps: 24, D loss: 0.167006, acc:  72%, G loss: 2.113862\n",
      "Ep: 182, steps: 25, D loss: 0.191262, acc:  66%, G loss: 2.231069\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 183, steps: 1, D loss: 0.218268, acc:  66%, G loss: 2.426718\n",
      "Ep: 183, steps: 2, D loss: 0.259250, acc:  49%, G loss: 1.782591\n",
      "Ep: 183, steps: 3, D loss: 0.141999, acc:  90%, G loss: 2.498723\n",
      "Ep: 183, steps: 4, D loss: 0.189934, acc:  77%, G loss: 2.029748\n",
      "Ep: 183, steps: 5, D loss: 0.242675, acc:  65%, G loss: 2.270459\n",
      "Ep: 183, steps: 6, D loss: 0.273552, acc:  49%, G loss: 2.192469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 183, steps: 7, D loss: 0.402939, acc:  22%, G loss: 1.657995\n",
      "Ep: 183, steps: 8, D loss: 0.208297, acc:  63%, G loss: 2.233449\n",
      "Ep: 183, steps: 9, D loss: 0.176213, acc:  80%, G loss: 2.093605\n",
      "Ep: 183, steps: 10, D loss: 0.125689, acc:  91%, G loss: 2.027558\n",
      "Ep: 183, steps: 11, D loss: 0.253391, acc:  53%, G loss: 2.289222\n",
      "Ep: 183, steps: 12, D loss: 0.328451, acc:  36%, G loss: 1.640931\n",
      "Ep: 183, steps: 13, D loss: 0.282230, acc:  43%, G loss: 1.777345\n",
      "Ep: 183, steps: 14, D loss: 0.297575, acc:  37%, G loss: 1.737553\n",
      "Ep: 183, steps: 15, D loss: 0.227734, acc:  61%, G loss: 1.992268\n",
      "Ep: 183, steps: 16, D loss: 0.276863, acc:  54%, G loss: 2.043108\n",
      "Ep: 183, steps: 17, D loss: 0.126503, acc:  89%, G loss: 2.242009\n",
      "Ep: 183, steps: 18, D loss: 0.252953, acc:  54%, G loss: 1.937507\n",
      "Ep: 183, steps: 19, D loss: 0.179645, acc:  70%, G loss: 1.973396\n",
      "Ep: 183, steps: 20, D loss: 0.135014, acc:  84%, G loss: 2.203782\n",
      "Ep: 183, steps: 21, D loss: 0.308069, acc:  40%, G loss: 1.971536\n",
      "Ep: 183, steps: 22, D loss: 0.183520, acc:  70%, G loss: 1.927996\n",
      "Ep: 183, steps: 23, D loss: 0.173574, acc:  80%, G loss: 2.393113\n",
      "Ep: 183, steps: 24, D loss: 0.164516, acc:  77%, G loss: 2.001608\n",
      "Ep: 183, steps: 25, D loss: 0.187142, acc:  70%, G loss: 2.071871\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 184, steps: 1, D loss: 0.218487, acc:  68%, G loss: 2.358409\n",
      "Ep: 184, steps: 2, D loss: 0.253908, acc:  51%, G loss: 1.818846\n",
      "Ep: 184, steps: 3, D loss: 0.123438, acc:  94%, G loss: 2.566100\n",
      "Ep: 184, steps: 4, D loss: 0.181892, acc:  78%, G loss: 2.046079\n",
      "Ep: 184, steps: 5, D loss: 0.231555, acc:  65%, G loss: 2.268618\n",
      "Ep: 184, steps: 6, D loss: 0.266137, acc:  49%, G loss: 2.244954\n",
      "Ep: 184, steps: 7, D loss: 0.469416, acc:  13%, G loss: 2.043258\n",
      "Ep: 184, steps: 8, D loss: 0.215996, acc:  61%, G loss: 2.227830\n",
      "Ep: 184, steps: 9, D loss: 0.179932, acc:  81%, G loss: 2.066402\n",
      "Ep: 184, steps: 10, D loss: 0.121213, acc:  91%, G loss: 1.961542\n",
      "Ep: 184, steps: 11, D loss: 0.262443, acc:  49%, G loss: 2.382950\n",
      "Saved Model\n",
      "Ep: 184, steps: 12, D loss: 0.332352, acc:  33%, G loss: 1.629892\n",
      "Ep: 184, steps: 13, D loss: 0.316011, acc:  29%, G loss: 1.752539\n",
      "Ep: 184, steps: 14, D loss: 0.201947, acc:  74%, G loss: 2.137504\n",
      "Ep: 184, steps: 15, D loss: 0.271354, acc:  52%, G loss: 2.022332\n",
      "Ep: 184, steps: 16, D loss: 0.122875, acc:  90%, G loss: 2.133493\n",
      "Ep: 184, steps: 17, D loss: 0.241263, acc:  58%, G loss: 1.997877\n",
      "Ep: 184, steps: 18, D loss: 0.173159, acc:  72%, G loss: 2.043068\n",
      "Ep: 184, steps: 19, D loss: 0.138738, acc:  86%, G loss: 2.245023\n",
      "Ep: 184, steps: 20, D loss: 0.287616, acc:  42%, G loss: 1.996970\n",
      "Ep: 184, steps: 21, D loss: 0.213238, acc:  62%, G loss: 2.095973\n",
      "Ep: 184, steps: 22, D loss: 0.194801, acc:  74%, G loss: 2.445556\n",
      "Ep: 184, steps: 23, D loss: 0.180060, acc:  71%, G loss: 2.021093\n",
      "Ep: 184, steps: 24, D loss: 0.194678, acc:  66%, G loss: 2.193835\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 185, steps: 1, D loss: 0.233874, acc:  62%, G loss: 2.314589\n",
      "Ep: 185, steps: 2, D loss: 0.237568, acc:  59%, G loss: 1.836903\n",
      "Ep: 185, steps: 3, D loss: 0.136789, acc:  92%, G loss: 2.493283\n",
      "Ep: 185, steps: 4, D loss: 0.186814, acc:  81%, G loss: 1.978970\n",
      "Ep: 185, steps: 5, D loss: 0.258238, acc:  63%, G loss: 2.265548\n",
      "Ep: 185, steps: 6, D loss: 0.274108, acc:  49%, G loss: 2.140254\n",
      "Ep: 185, steps: 7, D loss: 0.414423, acc:  18%, G loss: 1.951434\n",
      "Ep: 185, steps: 8, D loss: 0.205211, acc:  64%, G loss: 2.313563\n",
      "Ep: 185, steps: 9, D loss: 0.161351, acc:  85%, G loss: 2.058422\n",
      "Ep: 185, steps: 10, D loss: 0.119919, acc:  91%, G loss: 2.019072\n",
      "Ep: 185, steps: 11, D loss: 0.221545, acc:  63%, G loss: 2.302084\n",
      "Ep: 185, steps: 12, D loss: 0.349672, acc:  31%, G loss: 1.710887\n",
      "Ep: 185, steps: 13, D loss: 0.287577, acc:  43%, G loss: 1.748327\n",
      "Ep: 185, steps: 14, D loss: 0.309453, acc:  32%, G loss: 1.720955\n",
      "Ep: 185, steps: 15, D loss: 0.219689, acc:  66%, G loss: 2.106341\n",
      "Ep: 185, steps: 16, D loss: 0.277745, acc:  48%, G loss: 2.125414\n",
      "Ep: 185, steps: 17, D loss: 0.112000, acc:  93%, G loss: 2.215266\n",
      "Ep: 185, steps: 18, D loss: 0.254946, acc:  52%, G loss: 1.987243\n",
      "Ep: 185, steps: 19, D loss: 0.194203, acc:  69%, G loss: 1.959040\n",
      "Ep: 185, steps: 20, D loss: 0.135187, acc:  88%, G loss: 2.162423\n",
      "Ep: 185, steps: 21, D loss: 0.322950, acc:  39%, G loss: 2.063120\n",
      "Ep: 185, steps: 22, D loss: 0.191507, acc:  71%, G loss: 2.076276\n",
      "Ep: 185, steps: 23, D loss: 0.163883, acc:  82%, G loss: 2.512353\n",
      "Ep: 185, steps: 24, D loss: 0.168347, acc:  75%, G loss: 2.083679\n",
      "Ep: 185, steps: 25, D loss: 0.181866, acc:  73%, G loss: 2.132897\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 186, steps: 1, D loss: 0.236752, acc:  60%, G loss: 2.455389\n",
      "Ep: 186, steps: 2, D loss: 0.250071, acc:  53%, G loss: 1.868921\n",
      "Ep: 186, steps: 3, D loss: 0.131768, acc:  93%, G loss: 2.410569\n",
      "Ep: 186, steps: 4, D loss: 0.196217, acc:  76%, G loss: 2.019053\n",
      "Ep: 186, steps: 5, D loss: 0.251220, acc:  61%, G loss: 2.299903\n",
      "Ep: 186, steps: 6, D loss: 0.258547, acc:  50%, G loss: 2.134398\n",
      "Ep: 186, steps: 7, D loss: 0.418509, acc:  19%, G loss: 1.749753\n",
      "Ep: 186, steps: 8, D loss: 0.190314, acc:  66%, G loss: 2.468909\n",
      "Ep: 186, steps: 9, D loss: 0.172797, acc:  81%, G loss: 2.067411\n",
      "Ep: 186, steps: 10, D loss: 0.122414, acc:  90%, G loss: 1.974272\n",
      "Ep: 186, steps: 11, D loss: 0.254860, acc:  52%, G loss: 2.218234\n",
      "Ep: 186, steps: 12, D loss: 0.326455, acc:  37%, G loss: 1.612767\n",
      "Ep: 186, steps: 13, D loss: 0.283903, acc:  42%, G loss: 1.774197\n",
      "Ep: 186, steps: 14, D loss: 0.301238, acc:  33%, G loss: 1.703490\n",
      "Ep: 186, steps: 15, D loss: 0.211384, acc:  67%, G loss: 1.997554\n",
      "Ep: 186, steps: 16, D loss: 0.278149, acc:  53%, G loss: 2.018982\n",
      "Ep: 186, steps: 17, D loss: 0.124889, acc:  90%, G loss: 2.231531\n",
      "Ep: 186, steps: 18, D loss: 0.255918, acc:  52%, G loss: 1.850100\n",
      "Ep: 186, steps: 19, D loss: 0.191598, acc:  68%, G loss: 1.879362\n",
      "Ep: 186, steps: 20, D loss: 0.171356, acc:  77%, G loss: 2.154262\n",
      "Ep: 186, steps: 21, D loss: 0.315730, acc:  34%, G loss: 2.111975\n",
      "Ep: 186, steps: 22, D loss: 0.183656, acc:  69%, G loss: 2.002250\n",
      "Ep: 186, steps: 23, D loss: 0.205534, acc:  70%, G loss: 2.448128\n",
      "Ep: 186, steps: 24, D loss: 0.169117, acc:  77%, G loss: 2.080858\n",
      "Ep: 186, steps: 25, D loss: 0.192906, acc:  68%, G loss: 2.157897\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 187, steps: 1, D loss: 0.210456, acc:  69%, G loss: 2.385966\n",
      "Ep: 187, steps: 2, D loss: 0.243749, acc:  55%, G loss: 1.771463\n",
      "Ep: 187, steps: 3, D loss: 0.137278, acc:  88%, G loss: 2.478779\n",
      "Ep: 187, steps: 4, D loss: 0.183656, acc:  80%, G loss: 1.999744\n",
      "Ep: 187, steps: 5, D loss: 0.237990, acc:  65%, G loss: 2.266830\n",
      "Ep: 187, steps: 6, D loss: 0.267429, acc:  49%, G loss: 2.077819\n",
      "Ep: 187, steps: 7, D loss: 0.425171, acc:  16%, G loss: 1.839975\n",
      "Ep: 187, steps: 8, D loss: 0.198099, acc:  66%, G loss: 2.432686\n",
      "Ep: 187, steps: 9, D loss: 0.171296, acc:  83%, G loss: 2.112961\n",
      "Saved Model\n",
      "Ep: 187, steps: 10, D loss: 0.124439, acc:  90%, G loss: 2.000924\n",
      "Ep: 187, steps: 11, D loss: 0.330993, acc:  32%, G loss: 1.719823\n",
      "Ep: 187, steps: 12, D loss: 0.268341, acc:  46%, G loss: 1.793124\n",
      "Ep: 187, steps: 13, D loss: 0.287955, acc:  41%, G loss: 1.784924\n",
      "Ep: 187, steps: 14, D loss: 0.223612, acc:  64%, G loss: 2.091585\n",
      "Ep: 187, steps: 15, D loss: 0.296894, acc:  44%, G loss: 2.040424\n",
      "Ep: 187, steps: 16, D loss: 0.132393, acc:  88%, G loss: 2.073749\n",
      "Ep: 187, steps: 17, D loss: 0.265619, acc:  48%, G loss: 1.974390\n",
      "Ep: 187, steps: 18, D loss: 0.210035, acc:  65%, G loss: 1.972978\n",
      "Ep: 187, steps: 19, D loss: 0.129534, acc:  88%, G loss: 2.263107\n",
      "Ep: 187, steps: 20, D loss: 0.256385, acc:  49%, G loss: 2.018343\n",
      "Ep: 187, steps: 21, D loss: 0.159881, acc:  79%, G loss: 2.068827\n",
      "Ep: 187, steps: 22, D loss: 0.169112, acc:  81%, G loss: 2.373737\n",
      "Ep: 187, steps: 23, D loss: 0.171605, acc:  74%, G loss: 2.054410\n",
      "Ep: 187, steps: 24, D loss: 0.196708, acc:  67%, G loss: 2.262315\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 188, steps: 1, D loss: 0.239463, acc:  62%, G loss: 2.504515\n",
      "Ep: 188, steps: 2, D loss: 0.248859, acc:  54%, G loss: 1.795038\n",
      "Ep: 188, steps: 3, D loss: 0.138847, acc:  89%, G loss: 2.462176\n",
      "Ep: 188, steps: 4, D loss: 0.191745, acc:  79%, G loss: 2.147891\n",
      "Ep: 188, steps: 5, D loss: 0.242661, acc:  63%, G loss: 2.257934\n",
      "Ep: 188, steps: 6, D loss: 0.264073, acc:  49%, G loss: 2.002841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 188, steps: 7, D loss: 0.419508, acc:  20%, G loss: 1.848571\n",
      "Ep: 188, steps: 8, D loss: 0.213019, acc:  63%, G loss: 2.379161\n",
      "Ep: 188, steps: 9, D loss: 0.188813, acc:  77%, G loss: 2.121804\n",
      "Ep: 188, steps: 10, D loss: 0.127727, acc:  88%, G loss: 1.944674\n",
      "Ep: 188, steps: 11, D loss: 0.242726, acc:  57%, G loss: 2.231717\n",
      "Ep: 188, steps: 12, D loss: 0.334727, acc:  33%, G loss: 1.656076\n",
      "Ep: 188, steps: 13, D loss: 0.290393, acc:  40%, G loss: 1.802530\n",
      "Ep: 188, steps: 14, D loss: 0.311862, acc:  31%, G loss: 1.840727\n",
      "Ep: 188, steps: 15, D loss: 0.214402, acc:  67%, G loss: 2.060148\n",
      "Ep: 188, steps: 16, D loss: 0.282477, acc:  49%, G loss: 2.117265\n",
      "Ep: 188, steps: 17, D loss: 0.121264, acc:  91%, G loss: 2.159606\n",
      "Ep: 188, steps: 18, D loss: 0.244311, acc:  55%, G loss: 1.931856\n",
      "Ep: 188, steps: 19, D loss: 0.191364, acc:  70%, G loss: 1.957878\n",
      "Ep: 188, steps: 20, D loss: 0.199147, acc:  70%, G loss: 2.419791\n",
      "Ep: 188, steps: 21, D loss: 0.276166, acc:  44%, G loss: 2.134991\n",
      "Ep: 188, steps: 22, D loss: 0.210323, acc:  64%, G loss: 2.161909\n",
      "Ep: 188, steps: 23, D loss: 0.166267, acc:  80%, G loss: 2.328330\n",
      "Ep: 188, steps: 24, D loss: 0.165293, acc:  78%, G loss: 2.009846\n",
      "Ep: 188, steps: 25, D loss: 0.209971, acc:  66%, G loss: 2.111803\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 189, steps: 1, D loss: 0.226578, acc:  62%, G loss: 2.451288\n",
      "Ep: 189, steps: 2, D loss: 0.245390, acc:  54%, G loss: 1.785380\n",
      "Ep: 189, steps: 3, D loss: 0.147064, acc:  87%, G loss: 2.393383\n",
      "Ep: 189, steps: 4, D loss: 0.179446, acc:  81%, G loss: 2.108007\n",
      "Ep: 189, steps: 5, D loss: 0.229948, acc:  67%, G loss: 2.165083\n",
      "Ep: 189, steps: 6, D loss: 0.272763, acc:  49%, G loss: 2.002069\n",
      "Ep: 189, steps: 7, D loss: 0.398549, acc:  18%, G loss: 1.805099\n",
      "Ep: 189, steps: 8, D loss: 0.205526, acc:  65%, G loss: 2.476125\n",
      "Ep: 189, steps: 9, D loss: 0.171807, acc:  82%, G loss: 2.224383\n",
      "Ep: 189, steps: 10, D loss: 0.121378, acc:  90%, G loss: 2.068358\n",
      "Ep: 189, steps: 11, D loss: 0.258007, acc:  52%, G loss: 2.247468\n",
      "Ep: 189, steps: 12, D loss: 0.332524, acc:  37%, G loss: 1.660159\n",
      "Ep: 189, steps: 13, D loss: 0.283729, acc:  45%, G loss: 1.794250\n",
      "Ep: 189, steps: 14, D loss: 0.300762, acc:  33%, G loss: 1.775965\n",
      "Ep: 189, steps: 15, D loss: 0.208750, acc:  70%, G loss: 2.028204\n",
      "Ep: 189, steps: 16, D loss: 0.274329, acc:  51%, G loss: 2.134504\n",
      "Ep: 189, steps: 17, D loss: 0.127382, acc:  89%, G loss: 2.278863\n",
      "Ep: 189, steps: 18, D loss: 0.227231, acc:  64%, G loss: 2.017595\n",
      "Ep: 189, steps: 19, D loss: 0.182783, acc:  70%, G loss: 2.011539\n",
      "Ep: 189, steps: 20, D loss: 0.165005, acc:  77%, G loss: 2.214251\n",
      "Ep: 189, steps: 21, D loss: 0.278390, acc:  45%, G loss: 1.977008\n",
      "Ep: 189, steps: 22, D loss: 0.178627, acc:  72%, G loss: 2.086705\n",
      "Ep: 189, steps: 23, D loss: 0.188655, acc:  74%, G loss: 2.428663\n",
      "Ep: 189, steps: 24, D loss: 0.161447, acc:  76%, G loss: 2.110039\n",
      "Ep: 189, steps: 25, D loss: 0.208792, acc:  64%, G loss: 2.184469\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 190, steps: 1, D loss: 0.235326, acc:  61%, G loss: 2.252455\n",
      "Ep: 190, steps: 2, D loss: 0.259667, acc:  49%, G loss: 1.796169\n",
      "Ep: 190, steps: 3, D loss: 0.140383, acc:  92%, G loss: 2.495212\n",
      "Ep: 190, steps: 4, D loss: 0.183577, acc:  82%, G loss: 1.986896\n",
      "Ep: 190, steps: 5, D loss: 0.253833, acc:  61%, G loss: 2.470995\n",
      "Ep: 190, steps: 6, D loss: 0.270239, acc:  50%, G loss: 2.133336\n",
      "Ep: 190, steps: 7, D loss: 0.388478, acc:  22%, G loss: 1.962375\n",
      "Saved Model\n",
      "Ep: 190, steps: 8, D loss: 0.198460, acc:  66%, G loss: 2.171680\n",
      "Ep: 190, steps: 9, D loss: 0.133505, acc:  86%, G loss: 2.040620\n",
      "Ep: 190, steps: 10, D loss: 0.294170, acc:  45%, G loss: 2.241873\n",
      "Ep: 190, steps: 11, D loss: 0.332746, acc:  32%, G loss: 1.618528\n",
      "Ep: 190, steps: 12, D loss: 0.270784, acc:  48%, G loss: 1.790281\n",
      "Ep: 190, steps: 13, D loss: 0.289578, acc:  38%, G loss: 1.704432\n",
      "Ep: 190, steps: 14, D loss: 0.220048, acc:  67%, G loss: 1.969661\n",
      "Ep: 190, steps: 15, D loss: 0.267629, acc:  52%, G loss: 1.954756\n",
      "Ep: 190, steps: 16, D loss: 0.125821, acc:  88%, G loss: 2.092625\n",
      "Ep: 190, steps: 17, D loss: 0.258861, acc:  51%, G loss: 2.073690\n",
      "Ep: 190, steps: 18, D loss: 0.197583, acc:  68%, G loss: 2.011981\n",
      "Ep: 190, steps: 19, D loss: 0.146383, acc:  84%, G loss: 2.170128\n",
      "Ep: 190, steps: 20, D loss: 0.259284, acc:  48%, G loss: 1.935117\n",
      "Ep: 190, steps: 21, D loss: 0.189129, acc:  71%, G loss: 2.137372\n",
      "Ep: 190, steps: 22, D loss: 0.169948, acc:  80%, G loss: 2.470346\n",
      "Ep: 190, steps: 23, D loss: 0.167975, acc:  76%, G loss: 2.106049\n",
      "Ep: 190, steps: 24, D loss: 0.188967, acc:  69%, G loss: 2.130130\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 191, steps: 1, D loss: 0.233178, acc:  63%, G loss: 2.368126\n",
      "Ep: 191, steps: 2, D loss: 0.245376, acc:  56%, G loss: 1.760198\n",
      "Ep: 191, steps: 3, D loss: 0.129793, acc:  93%, G loss: 2.286371\n",
      "Ep: 191, steps: 4, D loss: 0.180849, acc:  83%, G loss: 2.062263\n",
      "Ep: 191, steps: 5, D loss: 0.213637, acc:  70%, G loss: 2.172298\n",
      "Ep: 191, steps: 6, D loss: 0.261934, acc:  49%, G loss: 2.038630\n",
      "Ep: 191, steps: 7, D loss: 0.463299, acc:  14%, G loss: 1.952363\n",
      "Ep: 191, steps: 8, D loss: 0.203183, acc:  66%, G loss: 2.582084\n",
      "Ep: 191, steps: 9, D loss: 0.187929, acc:  76%, G loss: 2.126182\n",
      "Ep: 191, steps: 10, D loss: 0.129170, acc:  88%, G loss: 2.047143\n",
      "Ep: 191, steps: 11, D loss: 0.254353, acc:  55%, G loss: 2.239289\n",
      "Ep: 191, steps: 12, D loss: 0.323743, acc:  37%, G loss: 1.564235\n",
      "Ep: 191, steps: 13, D loss: 0.299567, acc:  37%, G loss: 1.679752\n",
      "Ep: 191, steps: 14, D loss: 0.317776, acc:  29%, G loss: 1.695654\n",
      "Ep: 191, steps: 15, D loss: 0.211235, acc:  68%, G loss: 2.107418\n",
      "Ep: 191, steps: 16, D loss: 0.270775, acc:  52%, G loss: 1.971456\n",
      "Ep: 191, steps: 17, D loss: 0.141648, acc:  86%, G loss: 2.126075\n",
      "Ep: 191, steps: 18, D loss: 0.235694, acc:  60%, G loss: 2.128077\n",
      "Ep: 191, steps: 19, D loss: 0.183161, acc:  70%, G loss: 2.020583\n",
      "Ep: 191, steps: 20, D loss: 0.139265, acc:  84%, G loss: 2.246644\n",
      "Ep: 191, steps: 21, D loss: 0.267820, acc:  47%, G loss: 1.897117\n",
      "Ep: 191, steps: 22, D loss: 0.204163, acc:  65%, G loss: 1.939563\n",
      "Ep: 191, steps: 23, D loss: 0.173057, acc:  78%, G loss: 2.474043\n",
      "Ep: 191, steps: 24, D loss: 0.165300, acc:  76%, G loss: 2.059120\n",
      "Ep: 191, steps: 25, D loss: 0.204105, acc:  65%, G loss: 2.043213\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 192, steps: 1, D loss: 0.206938, acc:  70%, G loss: 2.338038\n",
      "Ep: 192, steps: 2, D loss: 0.251670, acc:  52%, G loss: 1.889671\n",
      "Ep: 192, steps: 3, D loss: 0.123744, acc:  92%, G loss: 2.379290\n",
      "Ep: 192, steps: 4, D loss: 0.162296, acc:  86%, G loss: 2.199788\n",
      "Ep: 192, steps: 5, D loss: 0.226117, acc:  66%, G loss: 2.278774\n",
      "Ep: 192, steps: 6, D loss: 0.267244, acc:  50%, G loss: 2.228702\n",
      "Ep: 192, steps: 7, D loss: 0.471796, acc:  14%, G loss: 1.949453\n",
      "Ep: 192, steps: 8, D loss: 0.218892, acc:  62%, G loss: 2.477595\n",
      "Ep: 192, steps: 9, D loss: 0.193061, acc:  77%, G loss: 2.132786\n",
      "Ep: 192, steps: 10, D loss: 0.127891, acc:  87%, G loss: 2.181288\n",
      "Ep: 192, steps: 11, D loss: 0.255344, acc:  52%, G loss: 2.326561\n",
      "Ep: 192, steps: 12, D loss: 0.315704, acc:  40%, G loss: 1.702178\n",
      "Ep: 192, steps: 13, D loss: 0.289861, acc:  42%, G loss: 1.722826\n",
      "Ep: 192, steps: 14, D loss: 0.296325, acc:  36%, G loss: 1.847107\n",
      "Ep: 192, steps: 15, D loss: 0.229549, acc:  62%, G loss: 2.037558\n",
      "Ep: 192, steps: 16, D loss: 0.276139, acc:  52%, G loss: 2.133431\n",
      "Ep: 192, steps: 17, D loss: 0.127154, acc:  90%, G loss: 2.157494\n",
      "Ep: 192, steps: 18, D loss: 0.232374, acc:  61%, G loss: 1.977600\n",
      "Ep: 192, steps: 19, D loss: 0.189008, acc:  71%, G loss: 2.007233\n",
      "Ep: 192, steps: 20, D loss: 0.165482, acc:  78%, G loss: 2.275076\n",
      "Ep: 192, steps: 21, D loss: 0.260981, acc:  47%, G loss: 1.983977\n",
      "Ep: 192, steps: 22, D loss: 0.177369, acc:  70%, G loss: 2.057188\n",
      "Ep: 192, steps: 23, D loss: 0.216326, acc:  68%, G loss: 2.356717\n",
      "Ep: 192, steps: 24, D loss: 0.172063, acc:  73%, G loss: 2.076344\n",
      "Ep: 192, steps: 25, D loss: 0.188541, acc:  66%, G loss: 2.218773\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 193, steps: 1, D loss: 0.227595, acc:  66%, G loss: 2.247946\n",
      "Ep: 193, steps: 2, D loss: 0.250302, acc:  53%, G loss: 1.772145\n",
      "Ep: 193, steps: 3, D loss: 0.125754, acc:  93%, G loss: 2.485472\n",
      "Ep: 193, steps: 4, D loss: 0.168508, acc:  87%, G loss: 2.016744\n",
      "Ep: 193, steps: 5, D loss: 0.231493, acc:  64%, G loss: 2.337389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Ep: 193, steps: 6, D loss: 0.266808, acc:  49%, G loss: 2.035598\n",
      "Ep: 193, steps: 7, D loss: 0.197581, acc:  71%, G loss: 2.092879\n",
      "Ep: 193, steps: 8, D loss: 0.128393, acc:  91%, G loss: 2.205849\n",
      "Ep: 193, steps: 9, D loss: 0.088521, acc:  97%, G loss: 2.153898\n",
      "Ep: 193, steps: 10, D loss: 0.215393, acc:  67%, G loss: 2.341584\n",
      "Ep: 193, steps: 11, D loss: 0.416430, acc:  28%, G loss: 1.593311\n",
      "Ep: 193, steps: 12, D loss: 0.358925, acc:  28%, G loss: 1.611577\n",
      "Ep: 193, steps: 13, D loss: 0.332764, acc:  30%, G loss: 1.746457\n",
      "Ep: 193, steps: 14, D loss: 0.218304, acc:  65%, G loss: 1.973083\n",
      "Ep: 193, steps: 15, D loss: 0.277725, acc:  52%, G loss: 2.075090\n",
      "Ep: 193, steps: 16, D loss: 0.129006, acc:  88%, G loss: 2.182244\n",
      "Ep: 193, steps: 17, D loss: 0.243362, acc:  59%, G loss: 2.045443\n",
      "Ep: 193, steps: 18, D loss: 0.180818, acc:  69%, G loss: 2.060675\n",
      "Ep: 193, steps: 19, D loss: 0.142649, acc:  83%, G loss: 2.274865\n",
      "Ep: 193, steps: 20, D loss: 0.307280, acc:  37%, G loss: 1.878060\n",
      "Ep: 193, steps: 21, D loss: 0.253185, acc:  58%, G loss: 2.125654\n",
      "Ep: 193, steps: 22, D loss: 0.152783, acc:  83%, G loss: 2.494391\n",
      "Ep: 193, steps: 23, D loss: 0.167600, acc:  75%, G loss: 2.133076\n",
      "Ep: 193, steps: 24, D loss: 0.209364, acc:  64%, G loss: 2.170771\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 194, steps: 1, D loss: 0.202520, acc:  70%, G loss: 2.427107\n",
      "Ep: 194, steps: 2, D loss: 0.232795, acc:  59%, G loss: 1.801942\n",
      "Ep: 194, steps: 3, D loss: 0.113066, acc:  95%, G loss: 2.561295\n",
      "Ep: 194, steps: 4, D loss: 0.161200, acc:  85%, G loss: 2.093146\n",
      "Ep: 194, steps: 5, D loss: 0.243859, acc:  62%, G loss: 2.385590\n",
      "Ep: 194, steps: 6, D loss: 0.268516, acc:  50%, G loss: 1.926380\n",
      "Ep: 194, steps: 7, D loss: 0.419815, acc:  24%, G loss: 1.774855\n",
      "Ep: 194, steps: 8, D loss: 0.196999, acc:  67%, G loss: 2.250724\n",
      "Ep: 194, steps: 9, D loss: 0.195021, acc:  74%, G loss: 2.029608\n",
      "Ep: 194, steps: 10, D loss: 0.121637, acc:  87%, G loss: 2.034918\n",
      "Ep: 194, steps: 11, D loss: 0.345604, acc:  36%, G loss: 2.511605\n",
      "Ep: 194, steps: 12, D loss: 0.306361, acc:  42%, G loss: 1.753411\n",
      "Ep: 194, steps: 13, D loss: 0.315292, acc:  35%, G loss: 1.756679\n",
      "Ep: 194, steps: 14, D loss: 0.310471, acc:  33%, G loss: 1.737631\n",
      "Ep: 194, steps: 15, D loss: 0.203892, acc:  74%, G loss: 2.026743\n",
      "Ep: 194, steps: 16, D loss: 0.276780, acc:  48%, G loss: 2.033037\n",
      "Ep: 194, steps: 17, D loss: 0.139922, acc:  88%, G loss: 2.104467\n",
      "Ep: 194, steps: 18, D loss: 0.235020, acc:  59%, G loss: 1.922268\n",
      "Ep: 194, steps: 19, D loss: 0.177725, acc:  73%, G loss: 1.929127\n",
      "Ep: 194, steps: 20, D loss: 0.155421, acc:  81%, G loss: 2.118681\n",
      "Ep: 194, steps: 21, D loss: 0.284520, acc:  44%, G loss: 1.949965\n",
      "Ep: 194, steps: 22, D loss: 0.171092, acc:  75%, G loss: 2.048132\n",
      "Ep: 194, steps: 23, D loss: 0.178443, acc:  77%, G loss: 2.378031\n",
      "Ep: 194, steps: 24, D loss: 0.172706, acc:  75%, G loss: 2.004133\n",
      "Ep: 194, steps: 25, D loss: 0.199253, acc:  66%, G loss: 2.154611\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 195, steps: 1, D loss: 0.218453, acc:  67%, G loss: 2.311954\n",
      "Ep: 195, steps: 2, D loss: 0.242119, acc:  57%, G loss: 1.732213\n",
      "Ep: 195, steps: 3, D loss: 0.131677, acc:  93%, G loss: 2.492322\n",
      "Ep: 195, steps: 4, D loss: 0.171060, acc:  85%, G loss: 2.014240\n",
      "Ep: 195, steps: 5, D loss: 0.230828, acc:  66%, G loss: 2.451596\n",
      "Ep: 195, steps: 6, D loss: 0.252240, acc:  50%, G loss: 2.028578\n",
      "Ep: 195, steps: 7, D loss: 0.442315, acc:  17%, G loss: 1.724413\n",
      "Ep: 195, steps: 8, D loss: 0.190746, acc:  67%, G loss: 2.180342\n",
      "Ep: 195, steps: 9, D loss: 0.179614, acc:  79%, G loss: 2.075502\n",
      "Ep: 195, steps: 10, D loss: 0.122189, acc:  90%, G loss: 2.004560\n",
      "Ep: 195, steps: 11, D loss: 0.236830, acc:  60%, G loss: 2.265898\n",
      "Ep: 195, steps: 12, D loss: 0.321216, acc:  39%, G loss: 1.615136\n",
      "Ep: 195, steps: 13, D loss: 0.296333, acc:  39%, G loss: 1.761844\n",
      "Ep: 195, steps: 14, D loss: 0.306414, acc:  36%, G loss: 1.791540\n",
      "Ep: 195, steps: 15, D loss: 0.221113, acc:  64%, G loss: 2.093246\n",
      "Ep: 195, steps: 16, D loss: 0.282451, acc:  48%, G loss: 2.115417\n",
      "Ep: 195, steps: 17, D loss: 0.127055, acc:  90%, G loss: 2.127131\n",
      "Ep: 195, steps: 18, D loss: 0.251582, acc:  53%, G loss: 2.070360\n",
      "Ep: 195, steps: 19, D loss: 0.188618, acc:  69%, G loss: 1.953332\n",
      "Ep: 195, steps: 20, D loss: 0.161250, acc:  80%, G loss: 2.139888\n",
      "Ep: 195, steps: 21, D loss: 0.305744, acc:  38%, G loss: 2.081367\n",
      "Ep: 195, steps: 22, D loss: 0.215755, acc:  64%, G loss: 1.978531\n",
      "Ep: 195, steps: 23, D loss: 0.183694, acc:  75%, G loss: 2.369460\n",
      "Ep: 195, steps: 24, D loss: 0.167895, acc:  77%, G loss: 2.027695\n",
      "Ep: 195, steps: 25, D loss: 0.201581, acc:  65%, G loss: 2.202409\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 196, steps: 1, D loss: 0.208802, acc:  69%, G loss: 2.389148\n",
      "Ep: 196, steps: 2, D loss: 0.258324, acc:  50%, G loss: 1.887932\n",
      "Ep: 196, steps: 3, D loss: 0.143617, acc:  89%, G loss: 2.417524\n",
      "Saved Model\n",
      "Ep: 196, steps: 4, D loss: 0.186978, acc:  80%, G loss: 2.017845\n",
      "Ep: 196, steps: 5, D loss: 0.260430, acc:  50%, G loss: 1.866292\n",
      "Ep: 196, steps: 6, D loss: 0.486822, acc:  11%, G loss: 1.566797\n",
      "Ep: 196, steps: 7, D loss: 0.200962, acc:  67%, G loss: 2.137266\n",
      "Ep: 196, steps: 8, D loss: 0.166199, acc:  85%, G loss: 2.064860\n",
      "Ep: 196, steps: 9, D loss: 0.126152, acc:  89%, G loss: 1.921317\n",
      "Ep: 196, steps: 10, D loss: 0.221405, acc:  62%, G loss: 2.378019\n",
      "Ep: 196, steps: 11, D loss: 0.351608, acc:  30%, G loss: 1.588573\n",
      "Ep: 196, steps: 12, D loss: 0.288169, acc:  39%, G loss: 1.664521\n",
      "Ep: 196, steps: 13, D loss: 0.307168, acc:  33%, G loss: 1.762880\n",
      "Ep: 196, steps: 14, D loss: 0.222711, acc:  63%, G loss: 1.993043\n",
      "Ep: 196, steps: 15, D loss: 0.273430, acc:  56%, G loss: 2.094214\n",
      "Ep: 196, steps: 16, D loss: 0.129394, acc:  87%, G loss: 2.228958\n",
      "Ep: 196, steps: 17, D loss: 0.231810, acc:  63%, G loss: 2.036344\n",
      "Ep: 196, steps: 18, D loss: 0.181925, acc:  72%, G loss: 1.931498\n",
      "Ep: 196, steps: 19, D loss: 0.151623, acc:  83%, G loss: 2.122206\n",
      "Ep: 196, steps: 20, D loss: 0.286390, acc:  42%, G loss: 1.795101\n",
      "Ep: 196, steps: 21, D loss: 0.193949, acc:  68%, G loss: 2.147765\n",
      "Ep: 196, steps: 22, D loss: 0.242214, acc:  64%, G loss: 2.578960\n",
      "Ep: 196, steps: 23, D loss: 0.163262, acc:  75%, G loss: 2.100601\n",
      "Ep: 196, steps: 24, D loss: 0.191208, acc:  71%, G loss: 2.145861\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 197, steps: 1, D loss: 0.227664, acc:  62%, G loss: 2.355788\n",
      "Ep: 197, steps: 2, D loss: 0.257594, acc:  49%, G loss: 1.944260\n",
      "Ep: 197, steps: 3, D loss: 0.127341, acc:  92%, G loss: 2.505133\n",
      "Ep: 197, steps: 4, D loss: 0.187180, acc:  80%, G loss: 2.109307\n",
      "Ep: 197, steps: 5, D loss: 0.251473, acc:  63%, G loss: 2.246126\n",
      "Ep: 197, steps: 6, D loss: 0.267025, acc:  49%, G loss: 2.059712\n",
      "Ep: 197, steps: 7, D loss: 0.378381, acc:  26%, G loss: 1.596787\n",
      "Ep: 197, steps: 8, D loss: 0.205684, acc:  65%, G loss: 2.370003\n",
      "Ep: 197, steps: 9, D loss: 0.179693, acc:  80%, G loss: 2.114774\n",
      "Ep: 197, steps: 10, D loss: 0.134829, acc:  87%, G loss: 2.158984\n",
      "Ep: 197, steps: 11, D loss: 0.243621, acc:  56%, G loss: 2.385724\n",
      "Ep: 197, steps: 12, D loss: 0.317690, acc:  38%, G loss: 1.773268\n",
      "Ep: 197, steps: 13, D loss: 0.299935, acc:  36%, G loss: 1.907133\n",
      "Ep: 197, steps: 14, D loss: 0.270525, acc:  45%, G loss: 1.773634\n",
      "Ep: 197, steps: 15, D loss: 0.213962, acc:  66%, G loss: 2.028274\n",
      "Ep: 197, steps: 16, D loss: 0.238035, acc:  61%, G loss: 2.127938\n",
      "Ep: 197, steps: 17, D loss: 0.107975, acc:  92%, G loss: 2.209758\n",
      "Ep: 197, steps: 18, D loss: 0.266065, acc:  49%, G loss: 2.017637\n",
      "Ep: 197, steps: 19, D loss: 0.203299, acc:  66%, G loss: 2.049262\n",
      "Ep: 197, steps: 20, D loss: 0.155756, acc:  80%, G loss: 2.224722\n",
      "Ep: 197, steps: 21, D loss: 0.305474, acc:  38%, G loss: 2.224370\n",
      "Ep: 197, steps: 22, D loss: 0.181467, acc:  71%, G loss: 2.068646\n",
      "Ep: 197, steps: 23, D loss: 0.178110, acc:  77%, G loss: 2.463085\n",
      "Ep: 197, steps: 24, D loss: 0.172674, acc:  75%, G loss: 2.036992\n",
      "Ep: 197, steps: 25, D loss: 0.197918, acc:  69%, G loss: 2.192404\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 198, steps: 1, D loss: 0.215855, acc:  67%, G loss: 2.409207\n",
      "Ep: 198, steps: 2, D loss: 0.261469, acc:  50%, G loss: 1.756979\n",
      "Ep: 198, steps: 3, D loss: 0.130749, acc:  92%, G loss: 2.498115\n",
      "Ep: 198, steps: 4, D loss: 0.187557, acc:  80%, G loss: 2.037971\n",
      "Ep: 198, steps: 5, D loss: 0.201298, acc:  72%, G loss: 2.403085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 198, steps: 6, D loss: 0.268888, acc:  49%, G loss: 2.042049\n",
      "Ep: 198, steps: 7, D loss: 0.505426, acc:  16%, G loss: 2.588440\n",
      "Ep: 198, steps: 8, D loss: 0.192904, acc:  68%, G loss: 2.305667\n",
      "Ep: 198, steps: 9, D loss: 0.173184, acc:  79%, G loss: 2.200878\n",
      "Ep: 198, steps: 10, D loss: 0.129661, acc:  89%, G loss: 1.977684\n",
      "Ep: 198, steps: 11, D loss: 0.229045, acc:  60%, G loss: 2.305233\n",
      "Ep: 198, steps: 12, D loss: 0.313403, acc:  40%, G loss: 1.692826\n",
      "Ep: 198, steps: 13, D loss: 0.262675, acc:  52%, G loss: 1.771362\n",
      "Ep: 198, steps: 14, D loss: 0.291270, acc:  38%, G loss: 1.775442\n",
      "Ep: 198, steps: 15, D loss: 0.210615, acc:  69%, G loss: 2.118480\n",
      "Ep: 198, steps: 16, D loss: 0.275193, acc:  52%, G loss: 2.000066\n",
      "Ep: 198, steps: 17, D loss: 0.117306, acc:  91%, G loss: 2.044295\n",
      "Ep: 198, steps: 18, D loss: 0.248653, acc:  56%, G loss: 1.873200\n",
      "Ep: 198, steps: 19, D loss: 0.189128, acc:  71%, G loss: 1.970050\n",
      "Ep: 198, steps: 20, D loss: 0.144509, acc:  84%, G loss: 2.184187\n",
      "Ep: 198, steps: 21, D loss: 0.271187, acc:  50%, G loss: 1.981494\n",
      "Ep: 198, steps: 22, D loss: 0.173819, acc:  71%, G loss: 2.091668\n",
      "Ep: 198, steps: 23, D loss: 0.173371, acc:  78%, G loss: 2.483182\n",
      "Ep: 198, steps: 24, D loss: 0.177684, acc:  72%, G loss: 2.111344\n",
      "Ep: 198, steps: 25, D loss: 0.207099, acc:  66%, G loss: 2.099787\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 199, steps: 1, D loss: 0.239935, acc:  58%, G loss: 2.312683\n",
      "Saved Model\n",
      "Ep: 199, steps: 2, D loss: 0.252664, acc:  51%, G loss: 2.181197\n",
      "Ep: 199, steps: 3, D loss: 0.197630, acc:  76%, G loss: 2.028078\n",
      "Ep: 199, steps: 4, D loss: 0.229990, acc:  63%, G loss: 2.255281\n",
      "Ep: 199, steps: 5, D loss: 0.251529, acc:  50%, G loss: 1.866812\n",
      "Ep: 199, steps: 6, D loss: 0.313541, acc:  35%, G loss: 1.617826\n",
      "Ep: 199, steps: 7, D loss: 0.237685, acc:  58%, G loss: 2.522974\n",
      "Ep: 199, steps: 8, D loss: 0.197458, acc:  71%, G loss: 2.076232\n",
      "Ep: 199, steps: 9, D loss: 0.140389, acc:  84%, G loss: 2.054482\n",
      "Ep: 199, steps: 10, D loss: 0.229638, acc:  60%, G loss: 2.219244\n",
      "Ep: 199, steps: 11, D loss: 0.320199, acc:  40%, G loss: 1.565828\n",
      "Ep: 199, steps: 12, D loss: 0.287640, acc:  41%, G loss: 1.804599\n",
      "Ep: 199, steps: 13, D loss: 0.300122, acc:  36%, G loss: 1.757902\n",
      "Ep: 199, steps: 14, D loss: 0.225221, acc:  62%, G loss: 1.991238\n",
      "Ep: 199, steps: 15, D loss: 0.254195, acc:  57%, G loss: 2.048288\n",
      "Ep: 199, steps: 16, D loss: 0.155819, acc:  80%, G loss: 2.062169\n",
      "Ep: 199, steps: 17, D loss: 0.246144, acc:  57%, G loss: 1.983994\n",
      "Ep: 199, steps: 18, D loss: 0.193355, acc:  69%, G loss: 1.987954\n",
      "Ep: 199, steps: 19, D loss: 0.143959, acc:  85%, G loss: 2.249521\n",
      "Ep: 199, steps: 20, D loss: 0.316335, acc:  36%, G loss: 2.027864\n",
      "Ep: 199, steps: 21, D loss: 0.230949, acc:  62%, G loss: 2.150763\n",
      "Ep: 199, steps: 22, D loss: 0.172452, acc:  78%, G loss: 2.597583\n",
      "Ep: 199, steps: 23, D loss: 0.165648, acc:  74%, G loss: 2.068012\n",
      "Ep: 199, steps: 24, D loss: 0.202012, acc:  67%, G loss: 2.181824\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 200, steps: 1, D loss: 0.207756, acc:  70%, G loss: 2.469571\n",
      "Ep: 200, steps: 2, D loss: 0.255612, acc:  51%, G loss: 1.860947\n",
      "Ep: 200, steps: 3, D loss: 0.150692, acc:  87%, G loss: 2.475136\n",
      "Ep: 200, steps: 4, D loss: 0.186400, acc:  80%, G loss: 1.964532\n",
      "Ep: 200, steps: 5, D loss: 0.226880, acc:  66%, G loss: 2.230911\n",
      "Ep: 200, steps: 6, D loss: 0.283644, acc:  48%, G loss: 2.078115\n",
      "Ep: 200, steps: 7, D loss: 0.406927, acc:  23%, G loss: 1.901894\n",
      "Ep: 200, steps: 8, D loss: 0.209701, acc:  64%, G loss: 2.749552\n",
      "Ep: 200, steps: 9, D loss: 0.175361, acc:  81%, G loss: 2.050198\n",
      "Ep: 200, steps: 10, D loss: 0.120675, acc:  92%, G loss: 2.061259\n",
      "Ep: 200, steps: 11, D loss: 0.241998, acc:  57%, G loss: 2.207774\n",
      "Ep: 200, steps: 12, D loss: 0.316834, acc:  38%, G loss: 1.670463\n",
      "Ep: 200, steps: 13, D loss: 0.288795, acc:  41%, G loss: 1.767507\n",
      "Ep: 200, steps: 14, D loss: 0.298544, acc:  36%, G loss: 1.747616\n",
      "Ep: 200, steps: 15, D loss: 0.223631, acc:  63%, G loss: 2.098876\n",
      "Ep: 200, steps: 16, D loss: 0.267981, acc:  56%, G loss: 1.991731\n",
      "Ep: 200, steps: 17, D loss: 0.151085, acc:  80%, G loss: 2.071513\n",
      "Ep: 200, steps: 18, D loss: 0.240687, acc:  59%, G loss: 2.165056\n",
      "Ep: 200, steps: 19, D loss: 0.201380, acc:  66%, G loss: 2.061223\n",
      "Ep: 200, steps: 20, D loss: 0.151833, acc:  82%, G loss: 2.201436\n",
      "Ep: 200, steps: 21, D loss: 0.288337, acc:  42%, G loss: 1.932546\n",
      "Ep: 200, steps: 22, D loss: 0.180947, acc:  70%, G loss: 2.065136\n",
      "Ep: 200, steps: 23, D loss: 0.179424, acc:  76%, G loss: 2.517558\n",
      "Ep: 200, steps: 24, D loss: 0.173743, acc:  75%, G loss: 2.092776\n",
      "Ep: 200, steps: 25, D loss: 0.190707, acc:  68%, G loss: 2.194186\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 201, steps: 1, D loss: 0.239993, acc:  57%, G loss: 2.427387\n",
      "Ep: 201, steps: 2, D loss: 0.248029, acc:  54%, G loss: 1.870654\n",
      "Ep: 201, steps: 3, D loss: 0.144704, acc:  90%, G loss: 2.444364\n",
      "Ep: 201, steps: 4, D loss: 0.180527, acc:  82%, G loss: 2.097386\n",
      "Ep: 201, steps: 5, D loss: 0.291455, acc:  57%, G loss: 2.405099\n",
      "Ep: 201, steps: 6, D loss: 0.273548, acc:  49%, G loss: 2.141420\n",
      "Ep: 201, steps: 7, D loss: 0.370561, acc:  29%, G loss: 1.719528\n",
      "Ep: 201, steps: 8, D loss: 0.222879, acc:  63%, G loss: 2.479386\n",
      "Ep: 201, steps: 9, D loss: 0.164954, acc:  84%, G loss: 2.099238\n",
      "Ep: 201, steps: 10, D loss: 0.125400, acc:  91%, G loss: 2.046872\n",
      "Ep: 201, steps: 11, D loss: 0.223730, acc:  61%, G loss: 2.294375\n",
      "Ep: 201, steps: 12, D loss: 0.336883, acc:  34%, G loss: 1.578550\n",
      "Ep: 201, steps: 13, D loss: 0.293683, acc:  39%, G loss: 1.801128\n",
      "Ep: 201, steps: 14, D loss: 0.293006, acc:  39%, G loss: 1.777739\n",
      "Ep: 201, steps: 15, D loss: 0.205242, acc:  70%, G loss: 2.016405\n",
      "Ep: 201, steps: 16, D loss: 0.267335, acc:  55%, G loss: 2.017887\n",
      "Ep: 201, steps: 17, D loss: 0.170264, acc:  74%, G loss: 2.354383\n",
      "Ep: 201, steps: 18, D loss: 0.211928, acc:  69%, G loss: 2.165064\n",
      "Ep: 201, steps: 19, D loss: 0.180185, acc:  73%, G loss: 2.187596\n",
      "Ep: 201, steps: 20, D loss: 0.139677, acc:  87%, G loss: 2.217887\n",
      "Ep: 201, steps: 21, D loss: 0.259309, acc:  48%, G loss: 2.024225\n",
      "Ep: 201, steps: 22, D loss: 0.200205, acc:  66%, G loss: 2.132201\n",
      "Ep: 201, steps: 23, D loss: 0.166002, acc:  81%, G loss: 2.434206\n",
      "Ep: 201, steps: 24, D loss: 0.184543, acc:  70%, G loss: 2.075143\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 201, steps: 25, D loss: 0.199819, acc:  68%, G loss: 2.078121\n",
      "Ep: 201, steps: 26, D loss: 0.274976, acc:  50%, G loss: 1.985805\n",
      "Ep: 201, steps: 27, D loss: 0.129543, acc:  89%, G loss: 2.513927\n",
      "Ep: 201, steps: 28, D loss: 0.193979, acc:  80%, G loss: 2.091859\n",
      "Ep: 201, steps: 29, D loss: 0.185967, acc:  73%, G loss: 2.486962\n",
      "Ep: 201, steps: 30, D loss: 0.254827, acc:  50%, G loss: 1.968524\n",
      "Ep: 201, steps: 31, D loss: 0.396464, acc:  22%, G loss: 1.655696\n",
      "Ep: 201, steps: 32, D loss: 0.216025, acc:  65%, G loss: 2.365693\n",
      "Ep: 201, steps: 33, D loss: 0.189415, acc:  72%, G loss: 2.053179\n",
      "Ep: 201, steps: 34, D loss: 0.130532, acc:  84%, G loss: 2.062241\n",
      "Ep: 201, steps: 35, D loss: 0.254142, acc:  54%, G loss: 2.195391\n",
      "Ep: 201, steps: 36, D loss: 0.325152, acc:  39%, G loss: 1.509720\n",
      "Ep: 201, steps: 37, D loss: 0.309089, acc:  34%, G loss: 1.695189\n",
      "Ep: 201, steps: 38, D loss: 0.305776, acc:  34%, G loss: 1.735856\n",
      "Ep: 201, steps: 39, D loss: 0.198738, acc:  74%, G loss: 2.057534\n",
      "Ep: 201, steps: 40, D loss: 0.281005, acc:  50%, G loss: 2.006284\n",
      "Ep: 201, steps: 41, D loss: 0.122013, acc:  90%, G loss: 2.097064\n",
      "Ep: 201, steps: 42, D loss: 0.266415, acc:  51%, G loss: 1.908557\n",
      "Ep: 201, steps: 43, D loss: 0.202616, acc:  66%, G loss: 2.016774\n",
      "Ep: 201, steps: 44, D loss: 0.167114, acc:  80%, G loss: 2.187548\n",
      "Ep: 201, steps: 45, D loss: 0.288972, acc:  40%, G loss: 1.882410\n",
      "Ep: 201, steps: 46, D loss: 0.226816, acc:  62%, G loss: 1.930143\n",
      "Ep: 201, steps: 47, D loss: 0.197978, acc:  73%, G loss: 2.347814\n",
      "Ep: 201, steps: 48, D loss: 0.169811, acc:  76%, G loss: 2.041452\n",
      "Ep: 201, steps: 49, D loss: 0.206610, acc:  64%, G loss: 2.133608\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 202, steps: 1, D loss: 0.227082, acc:  64%, G loss: 2.281946\n",
      "Ep: 202, steps: 2, D loss: 0.262364, acc:  49%, G loss: 1.890716\n",
      "Ep: 202, steps: 3, D loss: 0.134533, acc:  90%, G loss: 2.617574\n",
      "Ep: 202, steps: 4, D loss: 0.187743, acc:  80%, G loss: 2.140859\n",
      "Ep: 202, steps: 5, D loss: 0.202057, acc:  71%, G loss: 2.254318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 202, steps: 6, D loss: 0.271829, acc:  49%, G loss: 1.857707\n",
      "Ep: 202, steps: 7, D loss: 0.448323, acc:  15%, G loss: 1.650147\n",
      "Ep: 202, steps: 8, D loss: 0.199557, acc:  68%, G loss: 2.410518\n",
      "Ep: 202, steps: 9, D loss: 0.187917, acc:  77%, G loss: 2.110900\n",
      "Ep: 202, steps: 10, D loss: 0.126442, acc:  91%, G loss: 2.050243\n",
      "Ep: 202, steps: 11, D loss: 0.272507, acc:  45%, G loss: 2.247412\n",
      "Ep: 202, steps: 12, D loss: 0.324432, acc:  37%, G loss: 1.608566\n",
      "Ep: 202, steps: 13, D loss: 0.306062, acc:  34%, G loss: 1.636832\n",
      "Ep: 202, steps: 14, D loss: 0.300133, acc:  36%, G loss: 1.738962\n",
      "Ep: 202, steps: 15, D loss: 0.212721, acc:  69%, G loss: 2.105279\n",
      "Ep: 202, steps: 16, D loss: 0.272835, acc:  53%, G loss: 1.949939\n",
      "Ep: 202, steps: 17, D loss: 0.137634, acc:  87%, G loss: 2.038666\n",
      "Ep: 202, steps: 18, D loss: 0.244042, acc:  58%, G loss: 2.004637\n",
      "Ep: 202, steps: 19, D loss: 0.190375, acc:  71%, G loss: 1.976050\n",
      "Ep: 202, steps: 20, D loss: 0.176971, acc:  76%, G loss: 2.243061\n",
      "Ep: 202, steps: 21, D loss: 0.280325, acc:  46%, G loss: 1.906207\n",
      "Ep: 202, steps: 22, D loss: 0.223924, acc:  62%, G loss: 2.081917\n",
      "Ep: 202, steps: 23, D loss: 0.181183, acc:  80%, G loss: 2.468049\n",
      "Ep: 202, steps: 24, D loss: 0.180604, acc:  73%, G loss: 2.017998\n",
      "Ep: 202, steps: 25, D loss: 0.210235, acc:  64%, G loss: 2.072973\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 203, steps: 1, D loss: 0.218561, acc:  68%, G loss: 2.303484\n",
      "Ep: 203, steps: 2, D loss: 0.237208, acc:  57%, G loss: 1.868667\n",
      "Ep: 203, steps: 3, D loss: 0.147094, acc:  86%, G loss: 2.468154\n",
      "Ep: 203, steps: 4, D loss: 0.172049, acc:  85%, G loss: 2.020465\n",
      "Ep: 203, steps: 5, D loss: 0.224224, acc:  67%, G loss: 2.337313\n",
      "Ep: 203, steps: 6, D loss: 0.267677, acc:  49%, G loss: 1.908324\n",
      "Ep: 203, steps: 7, D loss: 0.433238, acc:  16%, G loss: 1.684464\n",
      "Ep: 203, steps: 8, D loss: 0.201995, acc:  67%, G loss: 2.487393\n",
      "Ep: 203, steps: 9, D loss: 0.198935, acc:  72%, G loss: 2.088535\n",
      "Ep: 203, steps: 10, D loss: 0.121133, acc:  93%, G loss: 2.071822\n",
      "Ep: 203, steps: 11, D loss: 0.240397, acc:  57%, G loss: 2.225086\n",
      "Ep: 203, steps: 12, D loss: 0.317763, acc:  39%, G loss: 1.672988\n",
      "Ep: 203, steps: 13, D loss: 0.299000, acc:  38%, G loss: 1.700295\n",
      "Ep: 203, steps: 14, D loss: 0.303420, acc:  33%, G loss: 1.737844\n",
      "Ep: 203, steps: 15, D loss: 0.209622, acc:  68%, G loss: 2.017332\n",
      "Ep: 203, steps: 16, D loss: 0.274173, acc:  53%, G loss: 2.013304\n",
      "Ep: 203, steps: 17, D loss: 0.140226, acc:  86%, G loss: 2.068841\n",
      "Ep: 203, steps: 18, D loss: 0.228957, acc:  63%, G loss: 1.983717\n",
      "Ep: 203, steps: 19, D loss: 0.194860, acc:  69%, G loss: 1.963698\n",
      "Ep: 203, steps: 20, D loss: 0.153498, acc:  83%, G loss: 2.188884\n",
      "Ep: 203, steps: 21, D loss: 0.281363, acc:  42%, G loss: 1.962420\n",
      "Ep: 203, steps: 22, D loss: 0.213789, acc:  62%, G loss: 2.004922\n",
      "Saved Model\n",
      "Ep: 203, steps: 23, D loss: 0.187544, acc:  74%, G loss: 2.498891\n",
      "Ep: 203, steps: 24, D loss: 0.217996, acc:  59%, G loss: 1.907141\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 204, steps: 1, D loss: 0.226915, acc:  65%, G loss: 2.204413\n",
      "Ep: 204, steps: 2, D loss: 0.232487, acc:  61%, G loss: 1.874068\n",
      "Ep: 204, steps: 3, D loss: 0.152610, acc:  85%, G loss: 2.367231\n",
      "Ep: 204, steps: 4, D loss: 0.169197, acc:  88%, G loss: 1.889081\n",
      "Ep: 204, steps: 5, D loss: 0.215075, acc:  67%, G loss: 2.189727\n",
      "Ep: 204, steps: 6, D loss: 0.261815, acc:  49%, G loss: 1.876743\n",
      "Ep: 204, steps: 7, D loss: 0.453842, acc:  16%, G loss: 1.638844\n",
      "Ep: 204, steps: 8, D loss: 0.206373, acc:  67%, G loss: 2.525907\n",
      "Ep: 204, steps: 9, D loss: 0.194745, acc:  74%, G loss: 2.063872\n",
      "Ep: 204, steps: 10, D loss: 0.125946, acc:  90%, G loss: 2.139930\n",
      "Ep: 204, steps: 11, D loss: 0.245238, acc:  57%, G loss: 2.295809\n",
      "Ep: 204, steps: 12, D loss: 0.317281, acc:  38%, G loss: 1.642152\n",
      "Ep: 204, steps: 13, D loss: 0.307664, acc:  33%, G loss: 1.704264\n",
      "Ep: 204, steps: 14, D loss: 0.301237, acc:  32%, G loss: 1.751602\n",
      "Ep: 204, steps: 15, D loss: 0.209475, acc:  71%, G loss: 2.010111\n",
      "Ep: 204, steps: 16, D loss: 0.288067, acc:  48%, G loss: 1.952830\n",
      "Ep: 204, steps: 17, D loss: 0.141416, acc:  87%, G loss: 2.036065\n",
      "Ep: 204, steps: 18, D loss: 0.245620, acc:  58%, G loss: 1.972494\n",
      "Ep: 204, steps: 19, D loss: 0.187983, acc:  70%, G loss: 1.973141\n",
      "Ep: 204, steps: 20, D loss: 0.194711, acc:  72%, G loss: 2.221079\n",
      "Ep: 204, steps: 21, D loss: 0.256767, acc:  48%, G loss: 2.045191\n",
      "Ep: 204, steps: 22, D loss: 0.213886, acc:  64%, G loss: 1.960037\n",
      "Ep: 204, steps: 23, D loss: 0.208010, acc:  71%, G loss: 2.451468\n",
      "Ep: 204, steps: 24, D loss: 0.168461, acc:  75%, G loss: 2.048921\n",
      "Ep: 204, steps: 25, D loss: 0.205151, acc:  65%, G loss: 2.081892\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 205, steps: 1, D loss: 0.215490, acc:  67%, G loss: 2.321302\n",
      "Ep: 205, steps: 2, D loss: 0.241619, acc:  56%, G loss: 1.818408\n",
      "Ep: 205, steps: 3, D loss: 0.140267, acc:  88%, G loss: 2.491958\n",
      "Ep: 205, steps: 4, D loss: 0.172009, acc:  84%, G loss: 2.236613\n",
      "Ep: 205, steps: 5, D loss: 0.230476, acc:  65%, G loss: 2.263513\n",
      "Ep: 205, steps: 6, D loss: 0.263007, acc:  49%, G loss: 1.790086\n",
      "Ep: 205, steps: 7, D loss: 0.415764, acc:  19%, G loss: 1.559679\n",
      "Ep: 205, steps: 8, D loss: 0.198397, acc:  68%, G loss: 2.323265\n",
      "Ep: 205, steps: 9, D loss: 0.171835, acc:  81%, G loss: 2.025824\n",
      "Ep: 205, steps: 10, D loss: 0.128594, acc:  90%, G loss: 2.068211\n",
      "Ep: 205, steps: 11, D loss: 0.246558, acc:  55%, G loss: 2.187246\n",
      "Ep: 205, steps: 12, D loss: 0.323142, acc:  40%, G loss: 1.607192\n",
      "Ep: 205, steps: 13, D loss: 0.307919, acc:  34%, G loss: 1.755451\n",
      "Ep: 205, steps: 14, D loss: 0.303213, acc:  34%, G loss: 1.770538\n",
      "Ep: 205, steps: 15, D loss: 0.204614, acc:  71%, G loss: 1.868520\n",
      "Ep: 205, steps: 16, D loss: 0.272093, acc:  52%, G loss: 2.074254\n",
      "Ep: 205, steps: 17, D loss: 0.123974, acc:  91%, G loss: 1.950611\n",
      "Ep: 205, steps: 18, D loss: 0.238001, acc:  60%, G loss: 1.980056\n",
      "Ep: 205, steps: 19, D loss: 0.192826, acc:  68%, G loss: 1.980430\n",
      "Ep: 205, steps: 20, D loss: 0.176790, acc:  77%, G loss: 2.177016\n",
      "Ep: 205, steps: 21, D loss: 0.268086, acc:  44%, G loss: 1.995520\n",
      "Ep: 205, steps: 22, D loss: 0.207523, acc:  62%, G loss: 1.983636\n",
      "Ep: 205, steps: 23, D loss: 0.191064, acc:  73%, G loss: 2.455362\n",
      "Ep: 205, steps: 24, D loss: 0.174146, acc:  77%, G loss: 2.141509\n",
      "Ep: 205, steps: 25, D loss: 0.208842, acc:  66%, G loss: 2.016752\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 206, steps: 1, D loss: 0.216273, acc:  67%, G loss: 2.256154\n",
      "Ep: 206, steps: 2, D loss: 0.241491, acc:  57%, G loss: 2.059171\n",
      "Ep: 206, steps: 3, D loss: 0.135556, acc:  89%, G loss: 2.489823\n",
      "Ep: 206, steps: 4, D loss: 0.178156, acc:  83%, G loss: 2.541848\n",
      "Ep: 206, steps: 5, D loss: 0.212729, acc:  69%, G loss: 2.329277\n",
      "Ep: 206, steps: 6, D loss: 0.269913, acc:  49%, G loss: 1.905148\n",
      "Ep: 206, steps: 7, D loss: 0.461597, acc:  13%, G loss: 1.593601\n",
      "Ep: 206, steps: 8, D loss: 0.202286, acc:  66%, G loss: 2.145515\n",
      "Ep: 206, steps: 9, D loss: 0.168419, acc:  83%, G loss: 2.100595\n",
      "Ep: 206, steps: 10, D loss: 0.112237, acc:  94%, G loss: 1.997519\n",
      "Ep: 206, steps: 11, D loss: 0.243215, acc:  54%, G loss: 2.213576\n",
      "Ep: 206, steps: 12, D loss: 0.314767, acc:  39%, G loss: 1.637342\n",
      "Ep: 206, steps: 13, D loss: 0.306343, acc:  33%, G loss: 1.774333\n",
      "Ep: 206, steps: 14, D loss: 0.296421, acc:  35%, G loss: 1.776406\n",
      "Ep: 206, steps: 15, D loss: 0.212839, acc:  69%, G loss: 1.891945\n",
      "Ep: 206, steps: 16, D loss: 0.265701, acc:  55%, G loss: 1.972727\n",
      "Ep: 206, steps: 17, D loss: 0.160424, acc:  79%, G loss: 1.941102\n",
      "Ep: 206, steps: 18, D loss: 0.247200, acc:  58%, G loss: 2.024425\n",
      "Ep: 206, steps: 19, D loss: 0.184502, acc:  71%, G loss: 2.025574\n",
      "Ep: 206, steps: 20, D loss: 0.214405, acc:  65%, G loss: 2.473860\n",
      "Saved Model\n",
      "Ep: 206, steps: 21, D loss: 0.243130, acc:  53%, G loss: 2.166570\n",
      "Ep: 206, steps: 22, D loss: 0.190616, acc:  72%, G loss: 2.432383\n",
      "Ep: 206, steps: 23, D loss: 0.168380, acc:  81%, G loss: 1.945732\n",
      "Ep: 206, steps: 24, D loss: 0.200666, acc:  67%, G loss: 2.004356\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 207, steps: 1, D loss: 0.205776, acc:  68%, G loss: 2.217427\n",
      "Ep: 207, steps: 2, D loss: 0.233380, acc:  58%, G loss: 1.866500\n",
      "Ep: 207, steps: 3, D loss: 0.132714, acc:  91%, G loss: 2.493649\n",
      "Ep: 207, steps: 4, D loss: 0.154512, acc:  90%, G loss: 2.341246\n",
      "Ep: 207, steps: 5, D loss: 0.238301, acc:  61%, G loss: 2.327297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 207, steps: 6, D loss: 0.268279, acc:  49%, G loss: 1.940294\n",
      "Ep: 207, steps: 7, D loss: 0.435089, acc:  15%, G loss: 1.593881\n",
      "Ep: 207, steps: 8, D loss: 0.212150, acc:  63%, G loss: 2.412567\n",
      "Ep: 207, steps: 9, D loss: 0.177019, acc:  79%, G loss: 2.060753\n",
      "Ep: 207, steps: 10, D loss: 0.123196, acc:  91%, G loss: 2.114863\n",
      "Ep: 207, steps: 11, D loss: 0.239190, acc:  58%, G loss: 2.249600\n",
      "Ep: 207, steps: 12, D loss: 0.322713, acc:  36%, G loss: 1.718899\n",
      "Ep: 207, steps: 13, D loss: 0.290637, acc:  41%, G loss: 1.777871\n",
      "Ep: 207, steps: 14, D loss: 0.296430, acc:  36%, G loss: 1.765030\n",
      "Ep: 207, steps: 15, D loss: 0.220680, acc:  63%, G loss: 1.914684\n",
      "Ep: 207, steps: 16, D loss: 0.253797, acc:  57%, G loss: 2.090628\n",
      "Ep: 207, steps: 17, D loss: 0.154375, acc:  81%, G loss: 2.009293\n",
      "Ep: 207, steps: 18, D loss: 0.251645, acc:  55%, G loss: 2.036016\n",
      "Ep: 207, steps: 19, D loss: 0.195632, acc:  67%, G loss: 2.092473\n",
      "Ep: 207, steps: 20, D loss: 0.144439, acc:  84%, G loss: 2.257650\n",
      "Ep: 207, steps: 21, D loss: 0.279042, acc:  42%, G loss: 1.955504\n",
      "Ep: 207, steps: 22, D loss: 0.230501, acc:  60%, G loss: 1.980922\n",
      "Ep: 207, steps: 23, D loss: 0.168282, acc:  79%, G loss: 2.428943\n",
      "Ep: 207, steps: 24, D loss: 0.172664, acc:  72%, G loss: 2.085254\n",
      "Ep: 207, steps: 25, D loss: 0.187837, acc:  68%, G loss: 2.050170\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 208, steps: 1, D loss: 0.211211, acc:  71%, G loss: 2.324857\n",
      "Ep: 208, steps: 2, D loss: 0.263595, acc:  48%, G loss: 2.060026\n",
      "Ep: 208, steps: 3, D loss: 0.127628, acc:  93%, G loss: 2.557707\n",
      "Ep: 208, steps: 4, D loss: 0.171827, acc:  85%, G loss: 2.249848\n",
      "Ep: 208, steps: 5, D loss: 0.226037, acc:  67%, G loss: 2.357300\n",
      "Ep: 208, steps: 6, D loss: 0.260085, acc:  50%, G loss: 1.856042\n",
      "Ep: 208, steps: 7, D loss: 0.445391, acc:  15%, G loss: 1.747659\n",
      "Ep: 208, steps: 8, D loss: 0.210396, acc:  64%, G loss: 2.395319\n",
      "Ep: 208, steps: 9, D loss: 0.179413, acc:  79%, G loss: 2.045367\n",
      "Ep: 208, steps: 10, D loss: 0.123386, acc:  92%, G loss: 2.105011\n",
      "Ep: 208, steps: 11, D loss: 0.257596, acc:  50%, G loss: 2.211034\n",
      "Ep: 208, steps: 12, D loss: 0.324659, acc:  36%, G loss: 1.585980\n",
      "Ep: 208, steps: 13, D loss: 0.294120, acc:  40%, G loss: 1.733330\n",
      "Ep: 208, steps: 14, D loss: 0.297579, acc:  38%, G loss: 1.736343\n",
      "Ep: 208, steps: 15, D loss: 0.224173, acc:  65%, G loss: 1.997987\n",
      "Ep: 208, steps: 16, D loss: 0.264351, acc:  53%, G loss: 1.957957\n",
      "Ep: 208, steps: 17, D loss: 0.143087, acc:  86%, G loss: 2.022724\n",
      "Ep: 208, steps: 18, D loss: 0.277135, acc:  50%, G loss: 1.935009\n",
      "Ep: 208, steps: 19, D loss: 0.194800, acc:  68%, G loss: 2.004599\n",
      "Ep: 208, steps: 20, D loss: 0.163238, acc:  81%, G loss: 2.215667\n",
      "Ep: 208, steps: 21, D loss: 0.265246, acc:  44%, G loss: 1.917736\n",
      "Ep: 208, steps: 22, D loss: 0.258700, acc:  57%, G loss: 2.069055\n",
      "Ep: 208, steps: 23, D loss: 0.170946, acc:  78%, G loss: 2.472879\n",
      "Ep: 208, steps: 24, D loss: 0.172790, acc:  74%, G loss: 2.056190\n",
      "Ep: 208, steps: 25, D loss: 0.192868, acc:  68%, G loss: 1.984695\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 209, steps: 1, D loss: 0.210329, acc:  70%, G loss: 2.363682\n",
      "Ep: 209, steps: 2, D loss: 0.247055, acc:  54%, G loss: 1.918789\n",
      "Ep: 209, steps: 3, D loss: 0.128310, acc:  91%, G loss: 2.495812\n",
      "Ep: 209, steps: 4, D loss: 0.165387, acc:  86%, G loss: 2.230301\n",
      "Ep: 209, steps: 5, D loss: 0.216179, acc:  68%, G loss: 2.374455\n",
      "Ep: 209, steps: 6, D loss: 0.266897, acc:  49%, G loss: 1.812345\n",
      "Ep: 209, steps: 7, D loss: 0.440873, acc:  15%, G loss: 1.668975\n",
      "Ep: 209, steps: 8, D loss: 0.208324, acc:  64%, G loss: 2.312897\n",
      "Ep: 209, steps: 9, D loss: 0.194759, acc:  75%, G loss: 2.005321\n",
      "Ep: 209, steps: 10, D loss: 0.115353, acc:  95%, G loss: 2.136503\n",
      "Ep: 209, steps: 11, D loss: 0.250500, acc:  52%, G loss: 2.270005\n",
      "Ep: 209, steps: 12, D loss: 0.324664, acc:  36%, G loss: 1.632360\n",
      "Ep: 209, steps: 13, D loss: 0.303093, acc:  36%, G loss: 1.763249\n",
      "Ep: 209, steps: 14, D loss: 0.297099, acc:  37%, G loss: 1.756055\n",
      "Ep: 209, steps: 15, D loss: 0.213334, acc:  69%, G loss: 1.921942\n",
      "Ep: 209, steps: 16, D loss: 0.257381, acc:  57%, G loss: 1.970953\n",
      "Ep: 209, steps: 17, D loss: 0.170036, acc:  75%, G loss: 2.059608\n",
      "Ep: 209, steps: 18, D loss: 0.225905, acc:  63%, G loss: 2.007930\n",
      "Saved Model\n",
      "Ep: 209, steps: 19, D loss: 0.196343, acc:  67%, G loss: 2.009580\n",
      "Ep: 209, steps: 20, D loss: 0.275568, acc:  42%, G loss: 1.885234\n",
      "Ep: 209, steps: 21, D loss: 0.175342, acc:  72%, G loss: 2.032405\n",
      "Ep: 209, steps: 22, D loss: 0.186351, acc:  75%, G loss: 2.469142\n",
      "Ep: 209, steps: 23, D loss: 0.180505, acc:  73%, G loss: 2.067563\n",
      "Ep: 209, steps: 24, D loss: 0.202314, acc:  65%, G loss: 2.077772\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 210, steps: 1, D loss: 0.194570, acc:  72%, G loss: 2.442153\n",
      "Ep: 210, steps: 2, D loss: 0.244284, acc:  55%, G loss: 1.896930\n",
      "Ep: 210, steps: 3, D loss: 0.141319, acc:  90%, G loss: 2.398302\n",
      "Ep: 210, steps: 4, D loss: 0.166729, acc:  88%, G loss: 2.228945\n",
      "Ep: 210, steps: 5, D loss: 0.237752, acc:  62%, G loss: 2.300124\n",
      "Ep: 210, steps: 6, D loss: 0.263166, acc:  50%, G loss: 1.979928\n",
      "Ep: 210, steps: 7, D loss: 0.417855, acc:  18%, G loss: 1.733006\n",
      "Ep: 210, steps: 8, D loss: 0.202883, acc:  67%, G loss: 2.609639\n",
      "Ep: 210, steps: 9, D loss: 0.177909, acc:  82%, G loss: 1.997524\n",
      "Ep: 210, steps: 10, D loss: 0.122262, acc:  92%, G loss: 2.052033\n",
      "Ep: 210, steps: 11, D loss: 0.251686, acc:  52%, G loss: 2.244565\n",
      "Ep: 210, steps: 12, D loss: 0.318232, acc:  38%, G loss: 1.648675\n",
      "Ep: 210, steps: 13, D loss: 0.288860, acc:  41%, G loss: 1.846612\n",
      "Ep: 210, steps: 14, D loss: 0.302888, acc:  35%, G loss: 1.820808\n",
      "Ep: 210, steps: 15, D loss: 0.216021, acc:  66%, G loss: 1.830034\n",
      "Ep: 210, steps: 16, D loss: 0.278962, acc:  50%, G loss: 1.947180\n",
      "Ep: 210, steps: 17, D loss: 0.156028, acc:  79%, G loss: 2.332921\n",
      "Ep: 210, steps: 18, D loss: 0.223761, acc:  67%, G loss: 2.127044\n",
      "Ep: 210, steps: 19, D loss: 0.182040, acc:  72%, G loss: 2.082673\n",
      "Ep: 210, steps: 20, D loss: 0.162287, acc:  81%, G loss: 2.216729\n",
      "Ep: 210, steps: 21, D loss: 0.290761, acc:  41%, G loss: 1.931966\n",
      "Ep: 210, steps: 22, D loss: 0.196782, acc:  66%, G loss: 1.918548\n",
      "Ep: 210, steps: 23, D loss: 0.223001, acc:  68%, G loss: 2.489763\n",
      "Ep: 210, steps: 24, D loss: 0.169031, acc:  76%, G loss: 2.052565\n",
      "Ep: 210, steps: 25, D loss: 0.208766, acc:  67%, G loss: 2.022413\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 211, steps: 1, D loss: 0.211617, acc:  67%, G loss: 2.257771\n",
      "Ep: 211, steps: 2, D loss: 0.249738, acc:  53%, G loss: 1.794381\n",
      "Ep: 211, steps: 3, D loss: 0.137942, acc:  90%, G loss: 2.525864\n",
      "Ep: 211, steps: 4, D loss: 0.167960, acc:  87%, G loss: 2.176951\n",
      "Ep: 211, steps: 5, D loss: 0.213384, acc:  66%, G loss: 2.307291\n",
      "Ep: 211, steps: 6, D loss: 0.257256, acc:  50%, G loss: 1.989356\n",
      "Ep: 211, steps: 7, D loss: 0.446403, acc:  15%, G loss: 1.728309\n",
      "Ep: 211, steps: 8, D loss: 0.206927, acc:  66%, G loss: 2.660458\n",
      "Ep: 211, steps: 9, D loss: 0.211206, acc:  69%, G loss: 2.081516\n",
      "Ep: 211, steps: 10, D loss: 0.126737, acc:  91%, G loss: 2.109253\n",
      "Ep: 211, steps: 11, D loss: 0.237273, acc:  58%, G loss: 2.239894\n",
      "Ep: 211, steps: 12, D loss: 0.314314, acc:  38%, G loss: 1.763018\n",
      "Ep: 211, steps: 13, D loss: 0.290704, acc:  38%, G loss: 1.796308\n",
      "Ep: 211, steps: 14, D loss: 0.288399, acc:  39%, G loss: 1.757877\n",
      "Ep: 211, steps: 15, D loss: 0.233752, acc:  59%, G loss: 1.803474\n",
      "Ep: 211, steps: 16, D loss: 0.256695, acc:  56%, G loss: 1.897442\n",
      "Ep: 211, steps: 17, D loss: 0.115930, acc:  92%, G loss: 2.122902\n",
      "Ep: 211, steps: 18, D loss: 0.253160, acc:  55%, G loss: 1.936895\n",
      "Ep: 211, steps: 19, D loss: 0.194496, acc:  68%, G loss: 2.090098\n",
      "Ep: 211, steps: 20, D loss: 0.179098, acc:  74%, G loss: 2.267446\n",
      "Ep: 211, steps: 21, D loss: 0.286514, acc:  40%, G loss: 1.991107\n",
      "Ep: 211, steps: 22, D loss: 0.225734, acc:  61%, G loss: 2.018420\n",
      "Ep: 211, steps: 23, D loss: 0.166461, acc:  78%, G loss: 2.456198\n",
      "Ep: 211, steps: 24, D loss: 0.171911, acc:  77%, G loss: 2.021590\n",
      "Ep: 211, steps: 25, D loss: 0.209932, acc:  64%, G loss: 1.969942\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 212, steps: 1, D loss: 0.210509, acc:  68%, G loss: 2.287030\n",
      "Ep: 212, steps: 2, D loss: 0.244073, acc:  56%, G loss: 1.874983\n",
      "Ep: 212, steps: 3, D loss: 0.126435, acc:  92%, G loss: 2.484580\n",
      "Ep: 212, steps: 4, D loss: 0.180860, acc:  82%, G loss: 2.155572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 212, steps: 5, D loss: 0.251421, acc:  62%, G loss: 2.197746\n",
      "Ep: 212, steps: 6, D loss: 0.260283, acc:  49%, G loss: 1.850130\n",
      "Ep: 212, steps: 7, D loss: 0.481444, acc:  15%, G loss: 1.780856\n",
      "Ep: 212, steps: 8, D loss: 0.208838, acc:  66%, G loss: 2.375894\n",
      "Ep: 212, steps: 9, D loss: 0.186612, acc:  78%, G loss: 2.000461\n",
      "Ep: 212, steps: 10, D loss: 0.122045, acc:  91%, G loss: 1.992125\n",
      "Ep: 212, steps: 11, D loss: 0.246135, acc:  54%, G loss: 2.229677\n",
      "Ep: 212, steps: 12, D loss: 0.323169, acc:  37%, G loss: 1.675153\n",
      "Ep: 212, steps: 13, D loss: 0.296740, acc:  37%, G loss: 1.768155\n",
      "Ep: 212, steps: 14, D loss: 0.304576, acc:  34%, G loss: 1.686286\n",
      "Ep: 212, steps: 15, D loss: 0.225191, acc:  63%, G loss: 1.834604\n",
      "Ep: 212, steps: 16, D loss: 0.260334, acc:  55%, G loss: 1.906432\n",
      "Saved Model\n",
      "Ep: 212, steps: 17, D loss: 0.149086, acc:  83%, G loss: 1.997791\n",
      "Ep: 212, steps: 18, D loss: 0.210532, acc:  64%, G loss: 1.885095\n",
      "Ep: 212, steps: 19, D loss: 0.157969, acc:  80%, G loss: 2.110374\n",
      "Ep: 212, steps: 20, D loss: 0.290467, acc:  43%, G loss: 1.917579\n",
      "Ep: 212, steps: 21, D loss: 0.182701, acc:  71%, G loss: 2.060589\n",
      "Ep: 212, steps: 22, D loss: 0.145930, acc:  86%, G loss: 2.341063\n",
      "Ep: 212, steps: 23, D loss: 0.166549, acc:  77%, G loss: 2.064505\n",
      "Ep: 212, steps: 24, D loss: 0.214718, acc:  61%, G loss: 2.083564\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 213, steps: 1, D loss: 0.240397, acc:  58%, G loss: 2.452050\n",
      "Ep: 213, steps: 2, D loss: 0.253581, acc:  55%, G loss: 2.000176\n",
      "Ep: 213, steps: 3, D loss: 0.143709, acc:  90%, G loss: 2.406503\n",
      "Ep: 213, steps: 4, D loss: 0.182165, acc:  81%, G loss: 2.082930\n",
      "Ep: 213, steps: 5, D loss: 0.195097, acc:  72%, G loss: 2.189021\n",
      "Ep: 213, steps: 6, D loss: 0.258441, acc:  49%, G loss: 1.957522\n",
      "Ep: 213, steps: 7, D loss: 0.410760, acc:  18%, G loss: 1.667129\n",
      "Ep: 213, steps: 8, D loss: 0.208004, acc:  66%, G loss: 2.493946\n",
      "Ep: 213, steps: 9, D loss: 0.200107, acc:  72%, G loss: 1.972112\n",
      "Ep: 213, steps: 10, D loss: 0.128129, acc:  91%, G loss: 2.009047\n",
      "Ep: 213, steps: 11, D loss: 0.250034, acc:  54%, G loss: 2.182249\n",
      "Ep: 213, steps: 12, D loss: 0.323073, acc:  37%, G loss: 1.589377\n",
      "Ep: 213, steps: 13, D loss: 0.295879, acc:  38%, G loss: 1.798773\n",
      "Ep: 213, steps: 14, D loss: 0.298861, acc:  37%, G loss: 1.805586\n",
      "Ep: 213, steps: 15, D loss: 0.223616, acc:  64%, G loss: 1.810458\n",
      "Ep: 213, steps: 16, D loss: 0.249005, acc:  56%, G loss: 1.901687\n",
      "Ep: 213, steps: 17, D loss: 0.161365, acc:  79%, G loss: 2.340560\n",
      "Ep: 213, steps: 18, D loss: 0.206844, acc:  73%, G loss: 2.021740\n",
      "Ep: 213, steps: 19, D loss: 0.181104, acc:  74%, G loss: 2.002580\n",
      "Ep: 213, steps: 20, D loss: 0.141610, acc:  86%, G loss: 2.239712\n",
      "Ep: 213, steps: 21, D loss: 0.246833, acc:  53%, G loss: 2.085232\n",
      "Ep: 213, steps: 22, D loss: 0.213376, acc:  62%, G loss: 1.928733\n",
      "Ep: 213, steps: 23, D loss: 0.140419, acc:  85%, G loss: 2.410094\n",
      "Ep: 213, steps: 24, D loss: 0.159543, acc:  77%, G loss: 2.143077\n",
      "Ep: 213, steps: 25, D loss: 0.206859, acc:  66%, G loss: 1.964832\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 214, steps: 1, D loss: 0.211677, acc:  67%, G loss: 2.404397\n",
      "Ep: 214, steps: 2, D loss: 0.256303, acc:  52%, G loss: 1.917461\n",
      "Ep: 214, steps: 3, D loss: 0.133168, acc:  88%, G loss: 2.636153\n",
      "Ep: 214, steps: 4, D loss: 0.156597, acc:  88%, G loss: 2.265681\n",
      "Ep: 214, steps: 5, D loss: 0.252986, acc:  61%, G loss: 2.251282\n",
      "Ep: 214, steps: 6, D loss: 0.259552, acc:  50%, G loss: 1.852356\n",
      "Ep: 214, steps: 7, D loss: 0.443479, acc:  19%, G loss: 1.789021\n",
      "Ep: 214, steps: 8, D loss: 0.209944, acc:  62%, G loss: 2.258382\n",
      "Ep: 214, steps: 9, D loss: 0.158296, acc:  87%, G loss: 2.081792\n",
      "Ep: 214, steps: 10, D loss: 0.111260, acc:  95%, G loss: 2.036702\n",
      "Ep: 214, steps: 11, D loss: 0.243029, acc:  55%, G loss: 2.277624\n",
      "Ep: 214, steps: 12, D loss: 0.326818, acc:  37%, G loss: 1.644687\n",
      "Ep: 214, steps: 13, D loss: 0.280789, acc:  44%, G loss: 1.834584\n",
      "Ep: 214, steps: 14, D loss: 0.283352, acc:  39%, G loss: 1.797709\n",
      "Ep: 214, steps: 15, D loss: 0.223616, acc:  61%, G loss: 1.819190\n",
      "Ep: 214, steps: 16, D loss: 0.232616, acc:  59%, G loss: 1.938420\n",
      "Ep: 214, steps: 17, D loss: 0.120947, acc:  91%, G loss: 2.088513\n",
      "Ep: 214, steps: 18, D loss: 0.259583, acc:  53%, G loss: 1.952713\n",
      "Ep: 214, steps: 19, D loss: 0.214157, acc:  61%, G loss: 1.941286\n",
      "Ep: 214, steps: 20, D loss: 0.141437, acc:  84%, G loss: 2.300038\n",
      "Ep: 214, steps: 21, D loss: 0.263561, acc:  46%, G loss: 2.006750\n",
      "Ep: 214, steps: 22, D loss: 0.201176, acc:  62%, G loss: 2.064051\n",
      "Ep: 214, steps: 23, D loss: 0.202870, acc:  71%, G loss: 2.579926\n",
      "Ep: 214, steps: 24, D loss: 0.162332, acc:  74%, G loss: 2.110430\n",
      "Ep: 214, steps: 25, D loss: 0.205053, acc:  64%, G loss: 2.004400\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 215, steps: 1, D loss: 0.207598, acc:  68%, G loss: 2.316118\n",
      "Ep: 215, steps: 2, D loss: 0.252506, acc:  54%, G loss: 1.822849\n",
      "Ep: 215, steps: 3, D loss: 0.139556, acc:  88%, G loss: 2.389894\n",
      "Ep: 215, steps: 4, D loss: 0.170663, acc:  84%, G loss: 2.136892\n",
      "Ep: 215, steps: 5, D loss: 0.197076, acc:  72%, G loss: 2.237973\n",
      "Ep: 215, steps: 6, D loss: 0.263516, acc:  50%, G loss: 1.979992\n",
      "Ep: 215, steps: 7, D loss: 0.409410, acc:  22%, G loss: 1.629060\n",
      "Ep: 215, steps: 8, D loss: 0.206671, acc:  67%, G loss: 2.292746\n",
      "Ep: 215, steps: 9, D loss: 0.199535, acc:  73%, G loss: 1.979669\n",
      "Ep: 215, steps: 10, D loss: 0.127412, acc:  88%, G loss: 2.060180\n",
      "Ep: 215, steps: 11, D loss: 0.267508, acc:  49%, G loss: 2.131498\n",
      "Ep: 215, steps: 12, D loss: 0.336277, acc:  34%, G loss: 1.667805\n",
      "Ep: 215, steps: 13, D loss: 0.319809, acc:  32%, G loss: 1.804945\n",
      "Ep: 215, steps: 14, D loss: 0.312628, acc:  33%, G loss: 1.754697\n",
      "Saved Model\n",
      "Ep: 215, steps: 15, D loss: 0.233792, acc:  60%, G loss: 1.924912\n",
      "Ep: 215, steps: 16, D loss: 0.118269, acc:  94%, G loss: 2.153646\n",
      "Ep: 215, steps: 17, D loss: 0.250793, acc:  54%, G loss: 1.909525\n",
      "Ep: 215, steps: 18, D loss: 0.198162, acc:  68%, G loss: 1.963874\n",
      "Ep: 215, steps: 19, D loss: 0.167129, acc:  77%, G loss: 2.247835\n",
      "Ep: 215, steps: 20, D loss: 0.286861, acc:  40%, G loss: 1.900217\n",
      "Ep: 215, steps: 21, D loss: 0.206121, acc:  62%, G loss: 1.965700\n",
      "Ep: 215, steps: 22, D loss: 0.224053, acc:  65%, G loss: 2.443167\n",
      "Ep: 215, steps: 23, D loss: 0.189641, acc:  65%, G loss: 2.007077\n",
      "Ep: 215, steps: 24, D loss: 0.208012, acc:  65%, G loss: 2.074682\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 216, steps: 1, D loss: 0.218601, acc:  66%, G loss: 2.264460\n",
      "Ep: 216, steps: 2, D loss: 0.267585, acc:  46%, G loss: 1.999804\n",
      "Ep: 216, steps: 3, D loss: 0.145742, acc:  87%, G loss: 2.359800\n",
      "Ep: 216, steps: 4, D loss: 0.175282, acc:  84%, G loss: 2.167332\n",
      "Ep: 216, steps: 5, D loss: 0.218934, acc:  68%, G loss: 2.168440\n",
      "Ep: 216, steps: 6, D loss: 0.260027, acc:  49%, G loss: 1.920469\n",
      "Ep: 216, steps: 7, D loss: 0.419977, acc:  18%, G loss: 1.502355\n",
      "Ep: 216, steps: 8, D loss: 0.209598, acc:  65%, G loss: 2.454165\n",
      "Ep: 216, steps: 9, D loss: 0.173077, acc:  82%, G loss: 2.029205\n",
      "Ep: 216, steps: 10, D loss: 0.124790, acc:  92%, G loss: 2.100427\n",
      "Ep: 216, steps: 11, D loss: 0.266970, acc:  46%, G loss: 2.406557\n",
      "Ep: 216, steps: 12, D loss: 0.323245, acc:  38%, G loss: 1.776210\n",
      "Ep: 216, steps: 13, D loss: 0.299420, acc:  39%, G loss: 1.859437\n",
      "Ep: 216, steps: 14, D loss: 0.301782, acc:  36%, G loss: 1.822388\n",
      "Ep: 216, steps: 15, D loss: 0.215156, acc:  68%, G loss: 1.841407\n",
      "Ep: 216, steps: 16, D loss: 0.249660, acc:  57%, G loss: 1.937168\n",
      "Ep: 216, steps: 17, D loss: 0.135079, acc:  86%, G loss: 1.961220\n",
      "Ep: 216, steps: 18, D loss: 0.247130, acc:  56%, G loss: 1.906454\n",
      "Ep: 216, steps: 19, D loss: 0.191467, acc:  69%, G loss: 1.893072\n",
      "Ep: 216, steps: 20, D loss: 0.163160, acc:  80%, G loss: 2.182889\n",
      "Ep: 216, steps: 21, D loss: 0.269847, acc:  41%, G loss: 1.907107\n",
      "Ep: 216, steps: 22, D loss: 0.211947, acc:  64%, G loss: 1.991998\n",
      "Ep: 216, steps: 23, D loss: 0.192181, acc:  73%, G loss: 2.431005\n",
      "Ep: 216, steps: 24, D loss: 0.169207, acc:  76%, G loss: 2.013343\n",
      "Ep: 216, steps: 25, D loss: 0.198471, acc:  67%, G loss: 1.967946\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 217, steps: 1, D loss: 0.200565, acc:  70%, G loss: 2.278330\n",
      "Ep: 217, steps: 2, D loss: 0.253482, acc:  52%, G loss: 1.969305\n",
      "Ep: 217, steps: 3, D loss: 0.142582, acc:  89%, G loss: 2.576782\n",
      "Ep: 217, steps: 4, D loss: 0.169528, acc:  86%, G loss: 2.229995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 217, steps: 5, D loss: 0.234113, acc:  63%, G loss: 2.229481\n",
      "Ep: 217, steps: 6, D loss: 0.258705, acc:  50%, G loss: 1.795023\n",
      "Ep: 217, steps: 7, D loss: 0.405574, acc:  22%, G loss: 1.547024\n",
      "Ep: 217, steps: 8, D loss: 0.215603, acc:  63%, G loss: 2.430699\n",
      "Ep: 217, steps: 9, D loss: 0.179339, acc:  79%, G loss: 2.008473\n",
      "Ep: 217, steps: 10, D loss: 0.123675, acc:  90%, G loss: 2.012419\n",
      "Ep: 217, steps: 11, D loss: 0.256238, acc:  52%, G loss: 2.139349\n",
      "Ep: 217, steps: 12, D loss: 0.322705, acc:  38%, G loss: 1.679938\n",
      "Ep: 217, steps: 13, D loss: 0.292352, acc:  42%, G loss: 1.774418\n",
      "Ep: 217, steps: 14, D loss: 0.323645, acc:  33%, G loss: 1.791140\n",
      "Ep: 217, steps: 15, D loss: 0.223016, acc:  63%, G loss: 1.892189\n",
      "Ep: 217, steps: 16, D loss: 0.268016, acc:  55%, G loss: 1.992874\n",
      "Ep: 217, steps: 17, D loss: 0.151831, acc:  81%, G loss: 2.143090\n",
      "Ep: 217, steps: 18, D loss: 0.247817, acc:  59%, G loss: 2.033031\n",
      "Ep: 217, steps: 19, D loss: 0.191532, acc:  69%, G loss: 2.095182\n",
      "Ep: 217, steps: 20, D loss: 0.143437, acc:  85%, G loss: 2.267693\n",
      "Ep: 217, steps: 21, D loss: 0.291743, acc:  41%, G loss: 1.895749\n",
      "Ep: 217, steps: 22, D loss: 0.228890, acc:  60%, G loss: 2.062752\n",
      "Ep: 217, steps: 23, D loss: 0.172388, acc:  79%, G loss: 2.501612\n",
      "Ep: 217, steps: 24, D loss: 0.161161, acc:  78%, G loss: 2.071803\n",
      "Ep: 217, steps: 25, D loss: 0.199297, acc:  67%, G loss: 2.109600\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 218, steps: 1, D loss: 0.210677, acc:  69%, G loss: 2.258965\n",
      "Ep: 218, steps: 2, D loss: 0.255759, acc:  51%, G loss: 2.011678\n",
      "Ep: 218, steps: 3, D loss: 0.141908, acc:  89%, G loss: 2.351480\n",
      "Ep: 218, steps: 4, D loss: 0.176757, acc:  85%, G loss: 2.195462\n",
      "Ep: 218, steps: 5, D loss: 0.226972, acc:  65%, G loss: 2.267748\n",
      "Ep: 218, steps: 6, D loss: 0.262670, acc:  49%, G loss: 1.854653\n",
      "Ep: 218, steps: 7, D loss: 0.463659, acc:  13%, G loss: 1.729043\n",
      "Ep: 218, steps: 8, D loss: 0.228304, acc:  61%, G loss: 2.401704\n",
      "Ep: 218, steps: 9, D loss: 0.194319, acc:  74%, G loss: 2.022957\n",
      "Ep: 218, steps: 10, D loss: 0.123929, acc:  92%, G loss: 2.057898\n",
      "Ep: 218, steps: 11, D loss: 0.252888, acc:  52%, G loss: 2.214285\n",
      "Ep: 218, steps: 12, D loss: 0.316963, acc:  38%, G loss: 1.735278\n",
      "Saved Model\n",
      "Ep: 218, steps: 13, D loss: 0.297067, acc:  36%, G loss: 1.786571\n",
      "Ep: 218, steps: 14, D loss: 0.202254, acc:  73%, G loss: 2.116105\n",
      "Ep: 218, steps: 15, D loss: 0.281700, acc:  51%, G loss: 1.894830\n",
      "Ep: 218, steps: 16, D loss: 0.136652, acc:  84%, G loss: 2.061209\n",
      "Ep: 218, steps: 17, D loss: 0.224853, acc:  64%, G loss: 1.927043\n",
      "Ep: 218, steps: 18, D loss: 0.178219, acc:  72%, G loss: 1.956814\n",
      "Ep: 218, steps: 19, D loss: 0.156746, acc:  80%, G loss: 2.155668\n",
      "Ep: 218, steps: 20, D loss: 0.279402, acc:  41%, G loss: 1.968640\n",
      "Ep: 218, steps: 21, D loss: 0.201406, acc:  63%, G loss: 2.004292\n",
      "Ep: 218, steps: 22, D loss: 0.213274, acc:  67%, G loss: 2.669909\n",
      "Ep: 218, steps: 23, D loss: 0.165080, acc:  75%, G loss: 2.128027\n",
      "Ep: 218, steps: 24, D loss: 0.203883, acc:  68%, G loss: 1.979978\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 219, steps: 1, D loss: 0.189539, acc:  73%, G loss: 2.186727\n",
      "Ep: 219, steps: 2, D loss: 0.238233, acc:  58%, G loss: 1.824767\n",
      "Ep: 219, steps: 3, D loss: 0.140181, acc:  90%, G loss: 2.520388\n",
      "Ep: 219, steps: 4, D loss: 0.167292, acc:  85%, G loss: 2.134684\n",
      "Ep: 219, steps: 5, D loss: 0.210544, acc:  70%, G loss: 2.317690\n",
      "Ep: 219, steps: 6, D loss: 0.248495, acc:  50%, G loss: 1.906010\n",
      "Ep: 219, steps: 7, D loss: 0.434028, acc:  18%, G loss: 1.609337\n",
      "Ep: 219, steps: 8, D loss: 0.231951, acc:  59%, G loss: 2.272018\n",
      "Ep: 219, steps: 9, D loss: 0.186885, acc:  76%, G loss: 2.001856\n",
      "Ep: 219, steps: 10, D loss: 0.127713, acc:  88%, G loss: 2.014973\n",
      "Ep: 219, steps: 11, D loss: 0.258944, acc:  52%, G loss: 2.234868\n",
      "Ep: 219, steps: 12, D loss: 0.320683, acc:  40%, G loss: 1.698873\n",
      "Ep: 219, steps: 13, D loss: 0.303640, acc:  39%, G loss: 1.686456\n",
      "Ep: 219, steps: 14, D loss: 0.316920, acc:  33%, G loss: 1.695178\n",
      "Ep: 219, steps: 15, D loss: 0.222888, acc:  65%, G loss: 1.966364\n",
      "Ep: 219, steps: 16, D loss: 0.283959, acc:  49%, G loss: 2.007387\n",
      "Ep: 219, steps: 17, D loss: 0.148094, acc:  85%, G loss: 1.982006\n",
      "Ep: 219, steps: 18, D loss: 0.272460, acc:  51%, G loss: 2.005253\n",
      "Ep: 219, steps: 19, D loss: 0.188797, acc:  71%, G loss: 1.922745\n",
      "Ep: 219, steps: 20, D loss: 0.165843, acc:  80%, G loss: 2.130181\n",
      "Ep: 219, steps: 21, D loss: 0.264920, acc:  44%, G loss: 1.901561\n",
      "Ep: 219, steps: 22, D loss: 0.201575, acc:  67%, G loss: 1.963259\n",
      "Ep: 219, steps: 23, D loss: 0.154624, acc:  82%, G loss: 2.430866\n",
      "Ep: 219, steps: 24, D loss: 0.171636, acc:  77%, G loss: 1.933876\n",
      "Ep: 219, steps: 25, D loss: 0.213759, acc:  63%, G loss: 2.023539\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 220, steps: 1, D loss: 0.202451, acc:  71%, G loss: 2.240068\n",
      "Ep: 220, steps: 2, D loss: 0.229392, acc:  60%, G loss: 2.023651\n",
      "Ep: 220, steps: 3, D loss: 0.137911, acc:  89%, G loss: 2.454204\n",
      "Ep: 220, steps: 4, D loss: 0.166368, acc:  86%, G loss: 2.212037\n",
      "Ep: 220, steps: 5, D loss: 0.253857, acc:  58%, G loss: 2.348465\n",
      "Ep: 220, steps: 6, D loss: 0.255867, acc:  51%, G loss: 1.843744\n",
      "Ep: 220, steps: 7, D loss: 0.399049, acc:  25%, G loss: 1.578086\n",
      "Ep: 220, steps: 8, D loss: 0.221551, acc:  62%, G loss: 2.150987\n",
      "Ep: 220, steps: 9, D loss: 0.188302, acc:  73%, G loss: 1.998507\n",
      "Ep: 220, steps: 10, D loss: 0.135600, acc:  85%, G loss: 1.905511\n",
      "Ep: 220, steps: 11, D loss: 0.246888, acc:  56%, G loss: 2.192533\n",
      "Ep: 220, steps: 12, D loss: 0.327101, acc:  35%, G loss: 1.671647\n",
      "Ep: 220, steps: 13, D loss: 0.301042, acc:  37%, G loss: 1.772372\n",
      "Ep: 220, steps: 14, D loss: 0.297737, acc:  39%, G loss: 1.689476\n",
      "Ep: 220, steps: 15, D loss: 0.220116, acc:  64%, G loss: 1.813098\n",
      "Ep: 220, steps: 16, D loss: 0.256531, acc:  56%, G loss: 1.986260\n",
      "Ep: 220, steps: 17, D loss: 0.152998, acc:  79%, G loss: 1.981059\n",
      "Ep: 220, steps: 18, D loss: 0.252785, acc:  57%, G loss: 1.979603\n",
      "Ep: 220, steps: 19, D loss: 0.198087, acc:  67%, G loss: 2.024562\n",
      "Ep: 220, steps: 20, D loss: 0.153765, acc:  80%, G loss: 2.187654\n",
      "Ep: 220, steps: 21, D loss: 0.272103, acc:  44%, G loss: 1.980469\n",
      "Ep: 220, steps: 22, D loss: 0.221965, acc:  61%, G loss: 1.948273\n",
      "Ep: 220, steps: 23, D loss: 0.158959, acc:  82%, G loss: 2.441304\n",
      "Ep: 220, steps: 24, D loss: 0.166894, acc:  76%, G loss: 1.998215\n",
      "Ep: 220, steps: 25, D loss: 0.205751, acc:  66%, G loss: 2.093483\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 221, steps: 1, D loss: 0.213797, acc:  68%, G loss: 2.251382\n",
      "Ep: 221, steps: 2, D loss: 0.253853, acc:  52%, G loss: 2.017411\n",
      "Ep: 221, steps: 3, D loss: 0.137442, acc:  92%, G loss: 2.361312\n",
      "Ep: 221, steps: 4, D loss: 0.179705, acc:  82%, G loss: 2.377884\n",
      "Ep: 221, steps: 5, D loss: 0.215430, acc:  67%, G loss: 2.206999\n",
      "Ep: 221, steps: 6, D loss: 0.262085, acc:  48%, G loss: 1.985816\n",
      "Ep: 221, steps: 7, D loss: 0.475140, acc:  15%, G loss: 1.805596\n",
      "Ep: 221, steps: 8, D loss: 0.223973, acc:  62%, G loss: 2.440671\n",
      "Ep: 221, steps: 9, D loss: 0.197404, acc:  70%, G loss: 1.977207\n",
      "Ep: 221, steps: 10, D loss: 0.132792, acc:  87%, G loss: 2.040324\n",
      "Saved Model\n",
      "Ep: 221, steps: 11, D loss: 0.248629, acc:  54%, G loss: 2.106210\n",
      "Ep: 221, steps: 12, D loss: 0.322409, acc:  32%, G loss: 1.594586\n",
      "Ep: 221, steps: 13, D loss: 0.303415, acc:  36%, G loss: 1.657148\n",
      "Ep: 221, steps: 14, D loss: 0.212659, acc:  66%, G loss: 1.877140\n",
      "Ep: 221, steps: 15, D loss: 0.268051, acc:  54%, G loss: 1.932932\n",
      "Ep: 221, steps: 16, D loss: 0.158111, acc:  77%, G loss: 1.968794\n",
      "Ep: 221, steps: 17, D loss: 0.232852, acc:  61%, G loss: 1.955719\n",
      "Ep: 221, steps: 18, D loss: 0.187534, acc:  69%, G loss: 1.974675\n",
      "Ep: 221, steps: 19, D loss: 0.154014, acc:  82%, G loss: 2.185670\n",
      "Ep: 221, steps: 20, D loss: 0.246683, acc:  49%, G loss: 1.933658\n",
      "Ep: 221, steps: 21, D loss: 0.229973, acc:  59%, G loss: 1.976744\n",
      "Ep: 221, steps: 22, D loss: 0.171808, acc:  78%, G loss: 2.388757\n",
      "Ep: 221, steps: 23, D loss: 0.179346, acc:  73%, G loss: 2.050417\n",
      "Ep: 221, steps: 24, D loss: 0.211720, acc:  64%, G loss: 2.035089\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 222, steps: 1, D loss: 0.209393, acc:  69%, G loss: 2.223858\n",
      "Ep: 222, steps: 2, D loss: 0.245588, acc:  55%, G loss: 1.969983\n",
      "Ep: 222, steps: 3, D loss: 0.136981, acc:  91%, G loss: 2.351312\n",
      "Ep: 222, steps: 4, D loss: 0.176214, acc:  85%, G loss: 2.218963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 222, steps: 5, D loss: 0.243877, acc:  61%, G loss: 2.189253\n",
      "Ep: 222, steps: 6, D loss: 0.260154, acc:  50%, G loss: 1.905925\n",
      "Ep: 222, steps: 7, D loss: 0.442576, acc:  12%, G loss: 1.600114\n",
      "Ep: 222, steps: 8, D loss: 0.226885, acc:  60%, G loss: 2.411712\n",
      "Ep: 222, steps: 9, D loss: 0.183001, acc:  76%, G loss: 2.017204\n",
      "Ep: 222, steps: 10, D loss: 0.132713, acc:  87%, G loss: 2.012494\n",
      "Ep: 222, steps: 11, D loss: 0.248118, acc:  54%, G loss: 2.299783\n",
      "Ep: 222, steps: 12, D loss: 0.323614, acc:  36%, G loss: 1.620746\n",
      "Ep: 222, steps: 13, D loss: 0.309084, acc:  34%, G loss: 1.815027\n",
      "Ep: 222, steps: 14, D loss: 0.300989, acc:  36%, G loss: 1.729881\n",
      "Ep: 222, steps: 15, D loss: 0.213524, acc:  69%, G loss: 1.741682\n",
      "Ep: 222, steps: 16, D loss: 0.258255, acc:  55%, G loss: 1.881254\n",
      "Ep: 222, steps: 17, D loss: 0.162485, acc:  79%, G loss: 1.982852\n",
      "Ep: 222, steps: 18, D loss: 0.243199, acc:  60%, G loss: 1.939881\n",
      "Ep: 222, steps: 19, D loss: 0.198225, acc:  67%, G loss: 1.874790\n",
      "Ep: 222, steps: 20, D loss: 0.162198, acc:  80%, G loss: 2.108682\n",
      "Ep: 222, steps: 21, D loss: 0.263189, acc:  45%, G loss: 1.913776\n",
      "Ep: 222, steps: 22, D loss: 0.195278, acc:  65%, G loss: 1.907820\n",
      "Ep: 222, steps: 23, D loss: 0.183462, acc:  74%, G loss: 2.366241\n",
      "Ep: 222, steps: 24, D loss: 0.171314, acc:  76%, G loss: 1.965352\n",
      "Ep: 222, steps: 25, D loss: 0.210296, acc:  66%, G loss: 2.052263\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 223, steps: 1, D loss: 0.210707, acc:  67%, G loss: 2.313962\n",
      "Ep: 223, steps: 2, D loss: 0.242279, acc:  57%, G loss: 1.968774\n",
      "Ep: 223, steps: 3, D loss: 0.138679, acc:  91%, G loss: 2.282873\n",
      "Ep: 223, steps: 4, D loss: 0.172081, acc:  85%, G loss: 2.151513\n",
      "Ep: 223, steps: 5, D loss: 0.253460, acc:  59%, G loss: 2.227193\n",
      "Ep: 223, steps: 6, D loss: 0.249362, acc:  51%, G loss: 1.869713\n",
      "Ep: 223, steps: 7, D loss: 0.436395, acc:  15%, G loss: 1.626144\n",
      "Ep: 223, steps: 8, D loss: 0.212348, acc:  64%, G loss: 2.577561\n",
      "Ep: 223, steps: 9, D loss: 0.179484, acc:  80%, G loss: 2.029650\n",
      "Ep: 223, steps: 10, D loss: 0.143829, acc:  83%, G loss: 1.943738\n",
      "Ep: 223, steps: 11, D loss: 0.265014, acc:  50%, G loss: 2.226493\n",
      "Ep: 223, steps: 12, D loss: 0.323003, acc:  37%, G loss: 1.704557\n",
      "Ep: 223, steps: 13, D loss: 0.297115, acc:  36%, G loss: 1.726005\n",
      "Ep: 223, steps: 14, D loss: 0.298358, acc:  35%, G loss: 1.699993\n",
      "Ep: 223, steps: 15, D loss: 0.233256, acc:  59%, G loss: 1.916839\n",
      "Ep: 223, steps: 16, D loss: 0.280224, acc:  52%, G loss: 1.941159\n",
      "Ep: 223, steps: 17, D loss: 0.159524, acc:  79%, G loss: 2.065792\n",
      "Ep: 223, steps: 18, D loss: 0.238934, acc:  62%, G loss: 2.118706\n",
      "Ep: 223, steps: 19, D loss: 0.182975, acc:  73%, G loss: 2.012040\n",
      "Ep: 223, steps: 20, D loss: 0.135449, acc:  87%, G loss: 2.331477\n",
      "Ep: 223, steps: 21, D loss: 0.260448, acc:  47%, G loss: 1.914485\n",
      "Ep: 223, steps: 22, D loss: 0.204707, acc:  64%, G loss: 2.051067\n",
      "Ep: 223, steps: 23, D loss: 0.165906, acc:  80%, G loss: 2.384895\n",
      "Ep: 223, steps: 24, D loss: 0.165952, acc:  77%, G loss: 1.998132\n",
      "Ep: 223, steps: 25, D loss: 0.203491, acc:  67%, G loss: 2.046615\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 224, steps: 1, D loss: 0.201995, acc:  70%, G loss: 2.310757\n",
      "Ep: 224, steps: 2, D loss: 0.251923, acc:  55%, G loss: 1.985844\n",
      "Ep: 224, steps: 3, D loss: 0.135390, acc:  91%, G loss: 2.402568\n",
      "Ep: 224, steps: 4, D loss: 0.172450, acc:  84%, G loss: 2.198009\n",
      "Ep: 224, steps: 5, D loss: 0.234245, acc:  63%, G loss: 2.228500\n",
      "Ep: 224, steps: 6, D loss: 0.264055, acc:  49%, G loss: 1.831240\n",
      "Ep: 224, steps: 7, D loss: 0.398540, acc:  26%, G loss: 1.664725\n",
      "Ep: 224, steps: 8, D loss: 0.210270, acc:  64%, G loss: 2.354958\n",
      "Saved Model\n",
      "Ep: 224, steps: 9, D loss: 0.198377, acc:  72%, G loss: 2.026063\n",
      "Ep: 224, steps: 10, D loss: 0.275160, acc:  46%, G loss: 2.018934\n",
      "Ep: 224, steps: 11, D loss: 0.313867, acc:  34%, G loss: 1.646610\n",
      "Ep: 224, steps: 12, D loss: 0.285462, acc:  40%, G loss: 1.915472\n",
      "Ep: 224, steps: 13, D loss: 0.280137, acc:  41%, G loss: 1.831285\n",
      "Ep: 224, steps: 14, D loss: 0.226947, acc:  62%, G loss: 1.745843\n",
      "Ep: 224, steps: 15, D loss: 0.241811, acc:  59%, G loss: 1.912962\n",
      "Ep: 224, steps: 16, D loss: 0.165337, acc:  80%, G loss: 2.099216\n",
      "Ep: 224, steps: 17, D loss: 0.226490, acc:  64%, G loss: 1.908669\n",
      "Ep: 224, steps: 18, D loss: 0.200801, acc:  66%, G loss: 1.856904\n",
      "Ep: 224, steps: 19, D loss: 0.163715, acc:  79%, G loss: 2.129179\n",
      "Ep: 224, steps: 20, D loss: 0.292301, acc:  38%, G loss: 1.866043\n",
      "Ep: 224, steps: 21, D loss: 0.197590, acc:  65%, G loss: 1.933515\n",
      "Ep: 224, steps: 22, D loss: 0.186204, acc:  73%, G loss: 2.371465\n",
      "Ep: 224, steps: 23, D loss: 0.168747, acc:  78%, G loss: 2.205496\n",
      "Ep: 224, steps: 24, D loss: 0.212798, acc:  63%, G loss: 2.028342\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 225, steps: 1, D loss: 0.213956, acc:  68%, G loss: 2.299261\n",
      "Ep: 225, steps: 2, D loss: 0.248093, acc:  54%, G loss: 1.964149\n",
      "Ep: 225, steps: 3, D loss: 0.145259, acc:  88%, G loss: 2.330781\n",
      "Ep: 225, steps: 4, D loss: 0.170608, acc:  85%, G loss: 2.180441\n",
      "Ep: 225, steps: 5, D loss: 0.225698, acc:  66%, G loss: 2.160072\n",
      "Ep: 225, steps: 6, D loss: 0.259930, acc:  49%, G loss: 1.944119\n",
      "Ep: 225, steps: 7, D loss: 0.455934, acc:  14%, G loss: 1.652758\n",
      "Ep: 225, steps: 8, D loss: 0.198437, acc:  68%, G loss: 2.443168\n",
      "Ep: 225, steps: 9, D loss: 0.178192, acc:  80%, G loss: 2.030286\n",
      "Ep: 225, steps: 10, D loss: 0.137090, acc:  86%, G loss: 2.076715\n",
      "Ep: 225, steps: 11, D loss: 0.262525, acc:  51%, G loss: 2.214045\n",
      "Ep: 225, steps: 12, D loss: 0.315565, acc:  40%, G loss: 1.736176\n",
      "Ep: 225, steps: 13, D loss: 0.295729, acc:  38%, G loss: 1.743690\n",
      "Ep: 225, steps: 14, D loss: 0.309416, acc:  33%, G loss: 1.769812\n",
      "Ep: 225, steps: 15, D loss: 0.225544, acc:  64%, G loss: 1.803810\n",
      "Ep: 225, steps: 16, D loss: 0.280785, acc:  49%, G loss: 1.827574\n",
      "Ep: 225, steps: 17, D loss: 0.153636, acc:  83%, G loss: 1.971323\n",
      "Ep: 225, steps: 18, D loss: 0.223071, acc:  66%, G loss: 1.918062\n",
      "Ep: 225, steps: 19, D loss: 0.190768, acc:  69%, G loss: 1.975147\n",
      "Ep: 225, steps: 20, D loss: 0.150755, acc:  82%, G loss: 2.080507\n",
      "Ep: 225, steps: 21, D loss: 0.295321, acc:  35%, G loss: 1.790276\n",
      "Ep: 225, steps: 22, D loss: 0.208703, acc:  60%, G loss: 2.070541\n",
      "Ep: 225, steps: 23, D loss: 0.196361, acc:  70%, G loss: 2.449656\n",
      "Ep: 225, steps: 24, D loss: 0.172068, acc:  72%, G loss: 1.978652\n",
      "Ep: 225, steps: 25, D loss: 0.212336, acc:  64%, G loss: 2.056966\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 226, steps: 1, D loss: 0.219326, acc:  65%, G loss: 2.261096\n",
      "Ep: 226, steps: 2, D loss: 0.246032, acc:  58%, G loss: 1.951320\n",
      "Ep: 226, steps: 3, D loss: 0.135376, acc:  93%, G loss: 2.401153\n",
      "Ep: 226, steps: 4, D loss: 0.175257, acc:  84%, G loss: 2.119950\n",
      "Ep: 226, steps: 5, D loss: 0.215805, acc:  67%, G loss: 2.312390\n",
      "Ep: 226, steps: 6, D loss: 0.264378, acc:  49%, G loss: 2.045509\n",
      "Ep: 226, steps: 7, D loss: 0.443252, acc:  14%, G loss: 1.771844\n",
      "Ep: 226, steps: 8, D loss: 0.220486, acc:  62%, G loss: 2.383723\n",
      "Ep: 226, steps: 9, D loss: 0.190109, acc:  78%, G loss: 2.044230\n",
      "Ep: 226, steps: 10, D loss: 0.134986, acc:  88%, G loss: 2.126923\n",
      "Ep: 226, steps: 11, D loss: 0.255025, acc:  51%, G loss: 2.184605\n",
      "Ep: 226, steps: 12, D loss: 0.320376, acc:  39%, G loss: 1.660797\n",
      "Ep: 226, steps: 13, D loss: 0.307885, acc:  34%, G loss: 1.742548\n",
      "Ep: 226, steps: 14, D loss: 0.308739, acc:  32%, G loss: 1.780217\n",
      "Ep: 226, steps: 15, D loss: 0.219454, acc:  66%, G loss: 1.814976\n",
      "Ep: 226, steps: 16, D loss: 0.271505, acc:  52%, G loss: 1.870808\n",
      "Ep: 226, steps: 17, D loss: 0.161505, acc:  79%, G loss: 2.201112\n",
      "Ep: 226, steps: 18, D loss: 0.244532, acc:  58%, G loss: 1.990015\n",
      "Ep: 226, steps: 19, D loss: 0.201501, acc:  67%, G loss: 1.917215\n",
      "Ep: 226, steps: 20, D loss: 0.160730, acc:  78%, G loss: 2.121488\n",
      "Ep: 226, steps: 21, D loss: 0.289279, acc:  38%, G loss: 1.774423\n",
      "Ep: 226, steps: 22, D loss: 0.220434, acc:  58%, G loss: 2.008506\n",
      "Ep: 226, steps: 23, D loss: 0.180671, acc:  77%, G loss: 2.408879\n",
      "Ep: 226, steps: 24, D loss: 0.173694, acc:  75%, G loss: 1.968522\n",
      "Ep: 226, steps: 25, D loss: 0.201721, acc:  69%, G loss: 2.003306\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 227, steps: 1, D loss: 0.217795, acc:  66%, G loss: 2.196831\n",
      "Ep: 227, steps: 2, D loss: 0.236075, acc:  61%, G loss: 1.884941\n",
      "Ep: 227, steps: 3, D loss: 0.150058, acc:  87%, G loss: 2.409497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 227, steps: 4, D loss: 0.173959, acc:  86%, G loss: 2.086974\n",
      "Ep: 227, steps: 5, D loss: 0.219007, acc:  66%, G loss: 2.194084\n",
      "Ep: 227, steps: 6, D loss: 0.257464, acc:  49%, G loss: 1.805339\n",
      "Saved Model\n",
      "Ep: 227, steps: 7, D loss: 0.407924, acc:  19%, G loss: 1.616696\n",
      "Ep: 227, steps: 8, D loss: 0.170202, acc:  80%, G loss: 2.131375\n",
      "Ep: 227, steps: 9, D loss: 0.122046, acc:  90%, G loss: 1.959880\n",
      "Ep: 227, steps: 10, D loss: 0.262942, acc:  54%, G loss: 2.331607\n",
      "Ep: 227, steps: 11, D loss: 0.315438, acc:  36%, G loss: 1.754583\n",
      "Ep: 227, steps: 12, D loss: 0.312246, acc:  31%, G loss: 1.695547\n",
      "Ep: 227, steps: 13, D loss: 0.306675, acc:  34%, G loss: 1.768767\n",
      "Ep: 227, steps: 14, D loss: 0.219614, acc:  68%, G loss: 1.781865\n",
      "Ep: 227, steps: 15, D loss: 0.273252, acc:  54%, G loss: 1.885314\n",
      "Ep: 227, steps: 16, D loss: 0.160095, acc:  79%, G loss: 1.941478\n",
      "Ep: 227, steps: 17, D loss: 0.260069, acc:  55%, G loss: 1.929384\n",
      "Ep: 227, steps: 18, D loss: 0.191285, acc:  71%, G loss: 2.024618\n",
      "Ep: 227, steps: 19, D loss: 0.160581, acc:  80%, G loss: 2.075496\n",
      "Ep: 227, steps: 20, D loss: 0.283096, acc:  39%, G loss: 1.922565\n",
      "Ep: 227, steps: 21, D loss: 0.214304, acc:  61%, G loss: 1.962011\n",
      "Ep: 227, steps: 22, D loss: 0.191869, acc:  71%, G loss: 2.380583\n",
      "Ep: 227, steps: 23, D loss: 0.161106, acc:  79%, G loss: 1.954160\n",
      "Ep: 227, steps: 24, D loss: 0.197215, acc:  68%, G loss: 2.061658\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 228, steps: 1, D loss: 0.216763, acc:  66%, G loss: 2.118481\n",
      "Ep: 228, steps: 2, D loss: 0.254011, acc:  54%, G loss: 1.891880\n",
      "Ep: 228, steps: 3, D loss: 0.147020, acc:  90%, G loss: 2.418866\n",
      "Ep: 228, steps: 4, D loss: 0.171388, acc:  85%, G loss: 2.115313\n",
      "Ep: 228, steps: 5, D loss: 0.253110, acc:  60%, G loss: 2.264148\n",
      "Ep: 228, steps: 6, D loss: 0.262407, acc:  50%, G loss: 1.835830\n",
      "Ep: 228, steps: 7, D loss: 0.418024, acc:  17%, G loss: 1.640473\n",
      "Ep: 228, steps: 8, D loss: 0.228723, acc:  58%, G loss: 2.481922\n",
      "Ep: 228, steps: 9, D loss: 0.185145, acc:  80%, G loss: 1.968961\n",
      "Ep: 228, steps: 10, D loss: 0.140143, acc:  85%, G loss: 2.037881\n",
      "Ep: 228, steps: 11, D loss: 0.251691, acc:  55%, G loss: 2.228123\n",
      "Ep: 228, steps: 12, D loss: 0.314161, acc:  41%, G loss: 1.818076\n",
      "Ep: 228, steps: 13, D loss: 0.293931, acc:  40%, G loss: 1.803135\n",
      "Ep: 228, steps: 14, D loss: 0.296698, acc:  36%, G loss: 1.812505\n",
      "Ep: 228, steps: 15, D loss: 0.216263, acc:  67%, G loss: 1.801321\n",
      "Ep: 228, steps: 16, D loss: 0.285696, acc:  49%, G loss: 1.905247\n",
      "Ep: 228, steps: 17, D loss: 0.147899, acc:  85%, G loss: 1.976941\n",
      "Ep: 228, steps: 18, D loss: 0.272853, acc:  54%, G loss: 1.989164\n",
      "Ep: 228, steps: 19, D loss: 0.189623, acc:  70%, G loss: 1.984515\n",
      "Ep: 228, steps: 20, D loss: 0.170410, acc:  78%, G loss: 2.117312\n",
      "Ep: 228, steps: 21, D loss: 0.301639, acc:  35%, G loss: 2.067138\n",
      "Ep: 228, steps: 22, D loss: 0.197145, acc:  66%, G loss: 1.971400\n",
      "Ep: 228, steps: 23, D loss: 0.226313, acc:  62%, G loss: 2.480923\n",
      "Ep: 228, steps: 24, D loss: 0.177419, acc:  77%, G loss: 1.963886\n",
      "Ep: 228, steps: 25, D loss: 0.211985, acc:  66%, G loss: 1.902249\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 229, steps: 1, D loss: 0.207996, acc:  68%, G loss: 2.077315\n",
      "Ep: 229, steps: 2, D loss: 0.240641, acc:  60%, G loss: 1.836941\n",
      "Ep: 229, steps: 3, D loss: 0.145182, acc:  90%, G loss: 2.324069\n",
      "Ep: 229, steps: 4, D loss: 0.178141, acc:  85%, G loss: 2.039485\n",
      "Ep: 229, steps: 5, D loss: 0.237601, acc:  62%, G loss: 2.202915\n",
      "Ep: 229, steps: 6, D loss: 0.256934, acc:  50%, G loss: 1.845552\n",
      "Ep: 229, steps: 7, D loss: 0.403003, acc:  19%, G loss: 1.617468\n",
      "Ep: 229, steps: 8, D loss: 0.214059, acc:  63%, G loss: 2.412488\n",
      "Ep: 229, steps: 9, D loss: 0.190252, acc:  77%, G loss: 2.003503\n",
      "Ep: 229, steps: 10, D loss: 0.137084, acc:  85%, G loss: 1.986071\n",
      "Ep: 229, steps: 11, D loss: 0.263621, acc:  51%, G loss: 2.110851\n",
      "Ep: 229, steps: 12, D loss: 0.308925, acc:  40%, G loss: 1.753763\n",
      "Ep: 229, steps: 13, D loss: 0.293406, acc:  39%, G loss: 1.799889\n",
      "Ep: 229, steps: 14, D loss: 0.297938, acc:  37%, G loss: 1.837014\n",
      "Ep: 229, steps: 15, D loss: 0.211416, acc:  70%, G loss: 1.793297\n",
      "Ep: 229, steps: 16, D loss: 0.277598, acc:  54%, G loss: 1.826541\n",
      "Ep: 229, steps: 17, D loss: 0.137543, acc:  87%, G loss: 1.975232\n",
      "Ep: 229, steps: 18, D loss: 0.223206, acc:  64%, G loss: 2.031680\n",
      "Ep: 229, steps: 19, D loss: 0.187342, acc:  71%, G loss: 2.006874\n",
      "Ep: 229, steps: 20, D loss: 0.218307, acc:  66%, G loss: 2.493620\n",
      "Ep: 229, steps: 21, D loss: 0.251380, acc:  50%, G loss: 2.035987\n",
      "Ep: 229, steps: 22, D loss: 0.232487, acc:  56%, G loss: 2.151217\n",
      "Ep: 229, steps: 23, D loss: 0.164281, acc:  78%, G loss: 2.448220\n",
      "Ep: 229, steps: 24, D loss: 0.166906, acc:  80%, G loss: 2.070226\n",
      "Ep: 229, steps: 25, D loss: 0.208435, acc:  65%, G loss: 1.954425\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 230, steps: 1, D loss: 0.214807, acc:  65%, G loss: 2.093791\n",
      "Ep: 230, steps: 2, D loss: 0.228370, acc:  62%, G loss: 1.816522\n",
      "Ep: 230, steps: 3, D loss: 0.139694, acc:  90%, G loss: 2.477504\n",
      "Ep: 230, steps: 4, D loss: 0.171036, acc:  85%, G loss: 2.077177\n",
      "Saved Model\n",
      "Ep: 230, steps: 5, D loss: 0.207093, acc:  71%, G loss: 2.109186\n",
      "Ep: 230, steps: 6, D loss: 0.457509, acc:   9%, G loss: 1.385601\n",
      "Ep: 230, steps: 7, D loss: 0.226213, acc:  65%, G loss: 2.519460\n",
      "Ep: 230, steps: 8, D loss: 0.198938, acc:  71%, G loss: 2.055650\n",
      "Ep: 230, steps: 9, D loss: 0.109476, acc:  96%, G loss: 2.067617\n",
      "Ep: 230, steps: 10, D loss: 0.256362, acc:  56%, G loss: 2.206040\n",
      "Ep: 230, steps: 11, D loss: 0.330473, acc:  37%, G loss: 1.782127\n",
      "Ep: 230, steps: 12, D loss: 0.308255, acc:  35%, G loss: 1.847856\n",
      "Ep: 230, steps: 13, D loss: 0.312053, acc:  34%, G loss: 1.845091\n",
      "Ep: 230, steps: 14, D loss: 0.211858, acc:  71%, G loss: 1.894962\n",
      "Ep: 230, steps: 15, D loss: 0.281923, acc:  50%, G loss: 1.891801\n",
      "Ep: 230, steps: 16, D loss: 0.158901, acc:  81%, G loss: 2.134093\n",
      "Ep: 230, steps: 17, D loss: 0.267887, acc:  49%, G loss: 1.936045\n",
      "Ep: 230, steps: 18, D loss: 0.194009, acc:  68%, G loss: 1.924502\n",
      "Ep: 230, steps: 19, D loss: 0.144695, acc:  84%, G loss: 2.156027\n",
      "Ep: 230, steps: 20, D loss: 0.334814, acc:  29%, G loss: 1.964112\n",
      "Ep: 230, steps: 21, D loss: 0.162738, acc:  75%, G loss: 2.084741\n",
      "Ep: 230, steps: 22, D loss: 0.197941, acc:  70%, G loss: 2.390210\n",
      "Ep: 230, steps: 23, D loss: 0.168853, acc:  76%, G loss: 1.970847\n",
      "Ep: 230, steps: 24, D loss: 0.206296, acc:  66%, G loss: 2.032995\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 231, steps: 1, D loss: 0.213503, acc:  70%, G loss: 2.246007\n",
      "Ep: 231, steps: 2, D loss: 0.237695, acc:  59%, G loss: 1.924540\n",
      "Ep: 231, steps: 3, D loss: 0.137855, acc:  91%, G loss: 2.289647\n",
      "Ep: 231, steps: 4, D loss: 0.175773, acc:  84%, G loss: 2.052754\n",
      "Ep: 231, steps: 5, D loss: 0.231223, acc:  61%, G loss: 2.273594\n",
      "Ep: 231, steps: 6, D loss: 0.261475, acc:  49%, G loss: 1.841685\n",
      "Ep: 231, steps: 7, D loss: 0.399684, acc:  20%, G loss: 1.615742\n",
      "Ep: 231, steps: 8, D loss: 0.228312, acc:  58%, G loss: 2.350310\n",
      "Ep: 231, steps: 9, D loss: 0.190603, acc:  77%, G loss: 1.988449\n",
      "Ep: 231, steps: 10, D loss: 0.135830, acc:  88%, G loss: 1.989578\n",
      "Ep: 231, steps: 11, D loss: 0.252116, acc:  53%, G loss: 2.296034\n",
      "Ep: 231, steps: 12, D loss: 0.305531, acc:  42%, G loss: 1.786188\n",
      "Ep: 231, steps: 13, D loss: 0.302033, acc:  35%, G loss: 1.801778\n",
      "Ep: 231, steps: 14, D loss: 0.304496, acc:  36%, G loss: 1.760571\n",
      "Ep: 231, steps: 15, D loss: 0.215558, acc:  67%, G loss: 1.804746\n",
      "Ep: 231, steps: 16, D loss: 0.261673, acc:  56%, G loss: 1.946154\n",
      "Ep: 231, steps: 17, D loss: 0.157723, acc:  82%, G loss: 1.881721\n",
      "Ep: 231, steps: 18, D loss: 0.294981, acc:  51%, G loss: 1.976158\n",
      "Ep: 231, steps: 19, D loss: 0.190527, acc:  70%, G loss: 1.977463\n",
      "Ep: 231, steps: 20, D loss: 0.161985, acc:  81%, G loss: 2.151894\n",
      "Ep: 231, steps: 21, D loss: 0.306233, acc:  31%, G loss: 1.851595\n",
      "Ep: 231, steps: 22, D loss: 0.191436, acc:  70%, G loss: 1.973868\n",
      "Ep: 231, steps: 23, D loss: 0.222901, acc:  67%, G loss: 2.401415\n",
      "Ep: 231, steps: 24, D loss: 0.177177, acc:  75%, G loss: 1.945312\n",
      "Ep: 231, steps: 25, D loss: 0.213255, acc:  64%, G loss: 1.940115\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 232, steps: 1, D loss: 0.210304, acc:  68%, G loss: 2.098773\n",
      "Ep: 232, steps: 2, D loss: 0.233513, acc:  60%, G loss: 1.963798\n",
      "Ep: 232, steps: 3, D loss: 0.143133, acc:  90%, G loss: 2.341475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 232, steps: 4, D loss: 0.172361, acc:  85%, G loss: 2.070441\n",
      "Ep: 232, steps: 5, D loss: 0.213820, acc:  66%, G loss: 2.199339\n",
      "Ep: 232, steps: 6, D loss: 0.261733, acc:  50%, G loss: 1.754482\n",
      "Ep: 232, steps: 7, D loss: 0.442104, acc:  16%, G loss: 1.593095\n",
      "Ep: 232, steps: 8, D loss: 0.208576, acc:  63%, G loss: 2.193760\n",
      "Ep: 232, steps: 9, D loss: 0.199200, acc:  72%, G loss: 1.996207\n",
      "Ep: 232, steps: 10, D loss: 0.138395, acc:  88%, G loss: 2.004679\n",
      "Ep: 232, steps: 11, D loss: 0.242968, acc:  58%, G loss: 2.129862\n",
      "Ep: 232, steps: 12, D loss: 0.313927, acc:  39%, G loss: 1.760920\n",
      "Ep: 232, steps: 13, D loss: 0.294058, acc:  40%, G loss: 1.871534\n",
      "Ep: 232, steps: 14, D loss: 0.314094, acc:  31%, G loss: 1.852937\n",
      "Ep: 232, steps: 15, D loss: 0.224392, acc:  65%, G loss: 1.797545\n",
      "Ep: 232, steps: 16, D loss: 0.269752, acc:  54%, G loss: 1.846544\n",
      "Ep: 232, steps: 17, D loss: 0.160975, acc:  78%, G loss: 2.093195\n",
      "Ep: 232, steps: 18, D loss: 0.246152, acc:  58%, G loss: 2.058883\n",
      "Ep: 232, steps: 19, D loss: 0.179199, acc:  72%, G loss: 1.976054\n",
      "Ep: 232, steps: 20, D loss: 0.168433, acc:  76%, G loss: 2.157572\n",
      "Ep: 232, steps: 21, D loss: 0.280676, acc:  40%, G loss: 1.956345\n",
      "Ep: 232, steps: 22, D loss: 0.190815, acc:  68%, G loss: 2.093119\n",
      "Ep: 232, steps: 23, D loss: 0.190181, acc:  73%, G loss: 2.459696\n",
      "Ep: 232, steps: 24, D loss: 0.178049, acc:  74%, G loss: 1.980360\n",
      "Ep: 232, steps: 25, D loss: 0.209524, acc:  65%, G loss: 1.983333\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 233, steps: 1, D loss: 0.219018, acc:  65%, G loss: 2.161026\n",
      "Ep: 233, steps: 2, D loss: 0.250353, acc:  57%, G loss: 1.828486\n",
      "Saved Model\n",
      "Ep: 233, steps: 3, D loss: 0.141640, acc:  91%, G loss: 2.289784\n",
      "Ep: 233, steps: 4, D loss: 0.230193, acc:  63%, G loss: 2.419908\n",
      "Ep: 233, steps: 5, D loss: 0.268983, acc:  48%, G loss: 1.845772\n",
      "Ep: 233, steps: 6, D loss: 0.393698, acc:  22%, G loss: 2.021862\n",
      "Ep: 233, steps: 7, D loss: 0.220350, acc:  64%, G loss: 2.403597\n",
      "Ep: 233, steps: 8, D loss: 0.200171, acc:  74%, G loss: 1.985786\n",
      "Ep: 233, steps: 9, D loss: 0.148517, acc:  86%, G loss: 1.970179\n",
      "Ep: 233, steps: 10, D loss: 0.273093, acc:  47%, G loss: 2.154921\n",
      "Ep: 233, steps: 11, D loss: 0.307869, acc:  40%, G loss: 1.767209\n",
      "Ep: 233, steps: 12, D loss: 0.287197, acc:  42%, G loss: 1.827153\n",
      "Ep: 233, steps: 13, D loss: 0.291156, acc:  37%, G loss: 1.795066\n",
      "Ep: 233, steps: 14, D loss: 0.222895, acc:  64%, G loss: 1.793738\n",
      "Ep: 233, steps: 15, D loss: 0.257235, acc:  55%, G loss: 1.853327\n",
      "Ep: 233, steps: 16, D loss: 0.160094, acc:  80%, G loss: 2.028147\n",
      "Ep: 233, steps: 17, D loss: 0.245903, acc:  58%, G loss: 2.012708\n",
      "Ep: 233, steps: 18, D loss: 0.198468, acc:  67%, G loss: 1.925478\n",
      "Ep: 233, steps: 19, D loss: 0.169757, acc:  78%, G loss: 2.132248\n",
      "Ep: 233, steps: 20, D loss: 0.290120, acc:  37%, G loss: 2.073020\n",
      "Ep: 233, steps: 21, D loss: 0.190443, acc:  67%, G loss: 2.082874\n",
      "Ep: 233, steps: 22, D loss: 0.201210, acc:  70%, G loss: 2.401536\n",
      "Ep: 233, steps: 23, D loss: 0.167483, acc:  76%, G loss: 2.004437\n",
      "Ep: 233, steps: 24, D loss: 0.205387, acc:  67%, G loss: 1.997743\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 234, steps: 1, D loss: 0.234412, acc:  60%, G loss: 2.065403\n",
      "Ep: 234, steps: 2, D loss: 0.248358, acc:  55%, G loss: 1.791946\n",
      "Ep: 234, steps: 3, D loss: 0.143449, acc:  90%, G loss: 2.304276\n",
      "Ep: 234, steps: 4, D loss: 0.185831, acc:  81%, G loss: 2.155123\n",
      "Ep: 234, steps: 5, D loss: 0.222948, acc:  64%, G loss: 2.139531\n",
      "Ep: 234, steps: 6, D loss: 0.265114, acc:  49%, G loss: 1.784447\n",
      "Ep: 234, steps: 7, D loss: 0.421623, acc:  17%, G loss: 1.605986\n",
      "Ep: 234, steps: 8, D loss: 0.228914, acc:  60%, G loss: 2.390285\n",
      "Ep: 234, steps: 9, D loss: 0.187595, acc:  78%, G loss: 1.956062\n",
      "Ep: 234, steps: 10, D loss: 0.139249, acc:  86%, G loss: 2.013024\n",
      "Ep: 234, steps: 11, D loss: 0.253337, acc:  53%, G loss: 2.183029\n",
      "Ep: 234, steps: 12, D loss: 0.306720, acc:  39%, G loss: 1.697719\n",
      "Ep: 234, steps: 13, D loss: 0.296180, acc:  38%, G loss: 1.884923\n",
      "Ep: 234, steps: 14, D loss: 0.290526, acc:  40%, G loss: 1.795092\n",
      "Ep: 234, steps: 15, D loss: 0.231064, acc:  61%, G loss: 1.791786\n",
      "Ep: 234, steps: 16, D loss: 0.262521, acc:  56%, G loss: 1.920719\n",
      "Ep: 234, steps: 17, D loss: 0.145059, acc:  86%, G loss: 1.948069\n",
      "Ep: 234, steps: 18, D loss: 0.280691, acc:  50%, G loss: 2.040430\n",
      "Ep: 234, steps: 19, D loss: 0.196908, acc:  68%, G loss: 1.996567\n",
      "Ep: 234, steps: 20, D loss: 0.159173, acc:  79%, G loss: 2.109650\n",
      "Ep: 234, steps: 21, D loss: 0.304463, acc:  34%, G loss: 1.870834\n",
      "Ep: 234, steps: 22, D loss: 0.192767, acc:  66%, G loss: 2.039099\n",
      "Ep: 234, steps: 23, D loss: 0.198645, acc:  70%, G loss: 2.422626\n",
      "Ep: 234, steps: 24, D loss: 0.174303, acc:  76%, G loss: 2.044254\n",
      "Ep: 234, steps: 25, D loss: 0.208300, acc:  66%, G loss: 2.282704\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 235, steps: 1, D loss: 0.226980, acc:  62%, G loss: 2.126032\n",
      "Ep: 235, steps: 2, D loss: 0.245410, acc:  57%, G loss: 1.639932\n",
      "Ep: 235, steps: 3, D loss: 0.138601, acc:  92%, G loss: 2.188618\n",
      "Ep: 235, steps: 4, D loss: 0.177180, acc:  85%, G loss: 1.959511\n",
      "Ep: 235, steps: 5, D loss: 0.228287, acc:  66%, G loss: 2.128614\n",
      "Ep: 235, steps: 6, D loss: 0.266894, acc:  49%, G loss: 1.762614\n",
      "Ep: 235, steps: 7, D loss: 0.422083, acc:  19%, G loss: 1.524747\n",
      "Ep: 235, steps: 8, D loss: 0.229542, acc:  59%, G loss: 2.295337\n",
      "Ep: 235, steps: 9, D loss: 0.179450, acc:  80%, G loss: 1.930960\n",
      "Ep: 235, steps: 10, D loss: 0.137046, acc:  84%, G loss: 2.012407\n",
      "Ep: 235, steps: 11, D loss: 0.249174, acc:  54%, G loss: 2.148123\n",
      "Ep: 235, steps: 12, D loss: 0.310943, acc:  40%, G loss: 1.718784\n",
      "Ep: 235, steps: 13, D loss: 0.293623, acc:  41%, G loss: 1.767792\n",
      "Ep: 235, steps: 14, D loss: 0.317871, acc:  32%, G loss: 1.861330\n",
      "Ep: 235, steps: 15, D loss: 0.231594, acc:  60%, G loss: 1.735764\n",
      "Ep: 235, steps: 16, D loss: 0.269705, acc:  52%, G loss: 1.868799\n",
      "Ep: 235, steps: 17, D loss: 0.155990, acc:  83%, G loss: 1.999455\n",
      "Ep: 235, steps: 18, D loss: 0.274213, acc:  54%, G loss: 2.074230\n",
      "Ep: 235, steps: 19, D loss: 0.183820, acc:  72%, G loss: 1.889396\n",
      "Ep: 235, steps: 20, D loss: 0.170953, acc:  77%, G loss: 2.010232\n",
      "Ep: 235, steps: 21, D loss: 0.290305, acc:  39%, G loss: 1.768814\n",
      "Ep: 235, steps: 22, D loss: 0.185685, acc:  69%, G loss: 2.041542\n",
      "Ep: 235, steps: 23, D loss: 0.204183, acc:  69%, G loss: 2.455027\n",
      "Ep: 235, steps: 24, D loss: 0.168736, acc:  76%, G loss: 2.004515\n",
      "Ep: 235, steps: 25, D loss: 0.199266, acc:  69%, G loss: 1.865862\n",
      "Data exhausted, Re Initialize\n",
      "Saved Model\n",
      "Ep: 236, steps: 1, D loss: 0.216518, acc:  66%, G loss: 2.179130\n",
      "Ep: 236, steps: 2, D loss: 0.147058, acc:  90%, G loss: 2.336136\n",
      "Ep: 236, steps: 3, D loss: 0.215431, acc:  73%, G loss: 2.096516\n",
      "Ep: 236, steps: 4, D loss: 0.209859, acc:  68%, G loss: 2.083015\n",
      "Ep: 236, steps: 5, D loss: 0.289390, acc:  46%, G loss: 2.000653\n",
      "Ep: 236, steps: 6, D loss: 0.405217, acc:  21%, G loss: 1.606659\n",
      "Ep: 236, steps: 7, D loss: 0.215630, acc:  63%, G loss: 2.325043\n",
      "Ep: 236, steps: 8, D loss: 0.179814, acc:  79%, G loss: 1.977546\n",
      "Ep: 236, steps: 9, D loss: 0.135969, acc:  85%, G loss: 2.046765\n",
      "Ep: 236, steps: 10, D loss: 0.266052, acc:  47%, G loss: 2.203312\n",
      "Ep: 236, steps: 11, D loss: 0.315721, acc:  38%, G loss: 1.657627\n",
      "Ep: 236, steps: 12, D loss: 0.304671, acc:  35%, G loss: 1.720066\n",
      "Ep: 236, steps: 13, D loss: 0.307846, acc:  31%, G loss: 1.860606\n",
      "Ep: 236, steps: 14, D loss: 0.226684, acc:  63%, G loss: 1.731272\n",
      "Ep: 236, steps: 15, D loss: 0.276399, acc:  52%, G loss: 1.864886\n",
      "Ep: 236, steps: 16, D loss: 0.174841, acc:  80%, G loss: 2.002624\n",
      "Ep: 236, steps: 17, D loss: 0.248832, acc:  59%, G loss: 1.890530\n",
      "Ep: 236, steps: 18, D loss: 0.175277, acc:  73%, G loss: 1.936900\n",
      "Ep: 236, steps: 19, D loss: 0.167944, acc:  78%, G loss: 2.534266\n",
      "Ep: 236, steps: 20, D loss: 0.301923, acc:  38%, G loss: 1.828665\n",
      "Ep: 236, steps: 21, D loss: 0.190734, acc:  66%, G loss: 1.992291\n",
      "Ep: 236, steps: 22, D loss: 0.176660, acc:  76%, G loss: 2.381067\n",
      "Ep: 236, steps: 23, D loss: 0.163155, acc:  79%, G loss: 2.030556\n",
      "Ep: 236, steps: 24, D loss: 0.205715, acc:  67%, G loss: 1.976512\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 237, steps: 1, D loss: 0.210259, acc:  68%, G loss: 2.103373\n",
      "Ep: 237, steps: 2, D loss: 0.241122, acc:  60%, G loss: 1.659929\n",
      "Ep: 237, steps: 3, D loss: 0.142932, acc:  89%, G loss: 2.358505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 237, steps: 4, D loss: 0.169253, acc:  86%, G loss: 2.025610\n",
      "Ep: 237, steps: 5, D loss: 0.206133, acc:  69%, G loss: 2.133681\n",
      "Ep: 237, steps: 6, D loss: 0.270411, acc:  49%, G loss: 1.823426\n",
      "Ep: 237, steps: 7, D loss: 0.436952, acc:  17%, G loss: 1.804427\n",
      "Ep: 237, steps: 8, D loss: 0.221925, acc:  62%, G loss: 2.416599\n",
      "Ep: 237, steps: 9, D loss: 0.190574, acc:  75%, G loss: 1.964119\n",
      "Ep: 237, steps: 10, D loss: 0.130069, acc:  89%, G loss: 2.055064\n",
      "Ep: 237, steps: 11, D loss: 0.255874, acc:  51%, G loss: 2.133598\n",
      "Ep: 237, steps: 12, D loss: 0.315579, acc:  36%, G loss: 1.697140\n",
      "Ep: 237, steps: 13, D loss: 0.281237, acc:  44%, G loss: 1.773447\n",
      "Ep: 237, steps: 14, D loss: 0.310132, acc:  30%, G loss: 1.736645\n",
      "Ep: 237, steps: 15, D loss: 0.219415, acc:  67%, G loss: 1.792467\n",
      "Ep: 237, steps: 16, D loss: 0.268369, acc:  52%, G loss: 1.837415\n",
      "Ep: 237, steps: 17, D loss: 0.133152, acc:  89%, G loss: 1.943428\n",
      "Ep: 237, steps: 18, D loss: 0.232148, acc:  64%, G loss: 2.047482\n",
      "Ep: 237, steps: 19, D loss: 0.186630, acc:  71%, G loss: 1.903533\n",
      "Ep: 237, steps: 20, D loss: 0.131269, acc:  87%, G loss: 2.167485\n",
      "Ep: 237, steps: 21, D loss: 0.312735, acc:  35%, G loss: 1.819993\n",
      "Ep: 237, steps: 22, D loss: 0.204366, acc:  63%, G loss: 1.997020\n",
      "Ep: 237, steps: 23, D loss: 0.188395, acc:  74%, G loss: 2.459829\n",
      "Ep: 237, steps: 24, D loss: 0.170602, acc:  78%, G loss: 1.993168\n",
      "Ep: 237, steps: 25, D loss: 0.198427, acc:  69%, G loss: 1.892926\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 238, steps: 1, D loss: 0.206647, acc:  68%, G loss: 2.144543\n",
      "Ep: 238, steps: 2, D loss: 0.243806, acc:  55%, G loss: 1.752531\n",
      "Ep: 238, steps: 3, D loss: 0.130518, acc:  92%, G loss: 2.286251\n",
      "Ep: 238, steps: 4, D loss: 0.163771, acc:  87%, G loss: 2.061889\n",
      "Ep: 238, steps: 5, D loss: 0.231542, acc:  65%, G loss: 2.048858\n",
      "Ep: 238, steps: 6, D loss: 0.274870, acc:  48%, G loss: 1.963090\n",
      "Ep: 238, steps: 7, D loss: 0.429160, acc:  17%, G loss: 1.554324\n",
      "Ep: 238, steps: 8, D loss: 0.229541, acc:  59%, G loss: 2.255873\n",
      "Ep: 238, steps: 9, D loss: 0.192731, acc:  73%, G loss: 1.982987\n",
      "Ep: 238, steps: 10, D loss: 0.147939, acc:  83%, G loss: 2.060530\n",
      "Ep: 238, steps: 11, D loss: 0.240572, acc:  56%, G loss: 2.207820\n",
      "Ep: 238, steps: 12, D loss: 0.319448, acc:  39%, G loss: 1.706897\n",
      "Ep: 238, steps: 13, D loss: 0.290297, acc:  40%, G loss: 1.774430\n",
      "Ep: 238, steps: 14, D loss: 0.307852, acc:  31%, G loss: 1.810241\n",
      "Ep: 238, steps: 15, D loss: 0.221343, acc:  66%, G loss: 1.825985\n",
      "Ep: 238, steps: 16, D loss: 0.274031, acc:  51%, G loss: 1.833086\n",
      "Ep: 238, steps: 17, D loss: 0.152830, acc:  82%, G loss: 1.916472\n",
      "Ep: 238, steps: 18, D loss: 0.236528, acc:  60%, G loss: 1.968347\n",
      "Ep: 238, steps: 19, D loss: 0.195124, acc:  68%, G loss: 1.897645\n",
      "Ep: 238, steps: 20, D loss: 0.148418, acc:  83%, G loss: 2.119251\n",
      "Ep: 238, steps: 21, D loss: 0.318611, acc:  30%, G loss: 1.946064\n",
      "Ep: 238, steps: 22, D loss: 0.188163, acc:  67%, G loss: 2.102438\n",
      "Ep: 238, steps: 23, D loss: 0.193230, acc:  71%, G loss: 2.418195\n",
      "Saved Model\n",
      "Ep: 238, steps: 24, D loss: 0.167497, acc:  78%, G loss: 1.966042\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 239, steps: 1, D loss: 0.205428, acc:  69%, G loss: 2.139488\n",
      "Ep: 239, steps: 2, D loss: 0.219213, acc:  68%, G loss: 1.859668\n",
      "Ep: 239, steps: 3, D loss: 0.143896, acc:  88%, G loss: 2.347480\n",
      "Ep: 239, steps: 4, D loss: 0.147759, acc:  91%, G loss: 2.141698\n",
      "Ep: 239, steps: 5, D loss: 0.302184, acc:  51%, G loss: 2.196191\n",
      "Ep: 239, steps: 6, D loss: 0.264453, acc:  49%, G loss: 1.799299\n",
      "Ep: 239, steps: 7, D loss: 0.428703, acc:  20%, G loss: 1.657502\n",
      "Ep: 239, steps: 8, D loss: 0.226075, acc:  64%, G loss: 2.410178\n",
      "Ep: 239, steps: 9, D loss: 0.182749, acc:  78%, G loss: 1.923466\n",
      "Ep: 239, steps: 10, D loss: 0.148519, acc:  85%, G loss: 1.935674\n",
      "Ep: 239, steps: 11, D loss: 0.242095, acc:  58%, G loss: 2.186778\n",
      "Ep: 239, steps: 12, D loss: 0.324453, acc:  33%, G loss: 1.641881\n",
      "Ep: 239, steps: 13, D loss: 0.284914, acc:  44%, G loss: 1.799639\n",
      "Ep: 239, steps: 14, D loss: 0.324456, acc:  29%, G loss: 1.729122\n",
      "Ep: 239, steps: 15, D loss: 0.212658, acc:  68%, G loss: 1.770351\n",
      "Ep: 239, steps: 16, D loss: 0.259815, acc:  54%, G loss: 1.823365\n",
      "Ep: 239, steps: 17, D loss: 0.145704, acc:  83%, G loss: 1.975938\n",
      "Ep: 239, steps: 18, D loss: 0.257194, acc:  55%, G loss: 1.939778\n",
      "Ep: 239, steps: 19, D loss: 0.191381, acc:  72%, G loss: 1.951623\n",
      "Ep: 239, steps: 20, D loss: 0.149985, acc:  84%, G loss: 2.055814\n",
      "Ep: 239, steps: 21, D loss: 0.307675, acc:  31%, G loss: 1.958490\n",
      "Ep: 239, steps: 22, D loss: 0.192728, acc:  69%, G loss: 1.991898\n",
      "Ep: 239, steps: 23, D loss: 0.178325, acc:  76%, G loss: 2.351537\n",
      "Ep: 239, steps: 24, D loss: 0.169739, acc:  78%, G loss: 2.046287\n",
      "Ep: 239, steps: 25, D loss: 0.207017, acc:  66%, G loss: 1.899981\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 240, steps: 1, D loss: 0.217652, acc:  66%, G loss: 2.015750\n",
      "Ep: 240, steps: 2, D loss: 0.251418, acc:  57%, G loss: 1.709731\n",
      "Ep: 240, steps: 3, D loss: 0.144766, acc:  88%, G loss: 2.227738\n",
      "Ep: 240, steps: 4, D loss: 0.171059, acc:  84%, G loss: 1.999201\n",
      "Ep: 240, steps: 5, D loss: 0.222128, acc:  65%, G loss: 2.147603\n",
      "Ep: 240, steps: 6, D loss: 0.273373, acc:  48%, G loss: 1.758633\n",
      "Ep: 240, steps: 7, D loss: 0.439239, acc:  16%, G loss: 1.611227\n",
      "Ep: 240, steps: 8, D loss: 0.216967, acc:  63%, G loss: 2.264884\n",
      "Ep: 240, steps: 9, D loss: 0.194389, acc:  75%, G loss: 1.923008\n",
      "Ep: 240, steps: 10, D loss: 0.150080, acc:  83%, G loss: 2.060194\n",
      "Ep: 240, steps: 11, D loss: 0.263293, acc:  49%, G loss: 2.198420\n",
      "Ep: 240, steps: 12, D loss: 0.314003, acc:  40%, G loss: 1.670757\n",
      "Ep: 240, steps: 13, D loss: 0.297879, acc:  37%, G loss: 1.732213\n",
      "Ep: 240, steps: 14, D loss: 0.314189, acc:  29%, G loss: 1.873511\n",
      "Ep: 240, steps: 15, D loss: 0.206266, acc:  72%, G loss: 1.744305\n",
      "Ep: 240, steps: 16, D loss: 0.268823, acc:  51%, G loss: 1.932342\n",
      "Ep: 240, steps: 17, D loss: 0.149644, acc:  86%, G loss: 1.942714\n",
      "Ep: 240, steps: 18, D loss: 0.257478, acc:  55%, G loss: 1.930469\n",
      "Ep: 240, steps: 19, D loss: 0.197824, acc:  70%, G loss: 1.896667\n",
      "Ep: 240, steps: 20, D loss: 0.150352, acc:  83%, G loss: 2.038618\n",
      "Ep: 240, steps: 21, D loss: 0.286700, acc:  39%, G loss: 1.724456\n",
      "Ep: 240, steps: 22, D loss: 0.199864, acc:  65%, G loss: 1.949117\n",
      "Ep: 240, steps: 23, D loss: 0.180334, acc:  76%, G loss: 2.474740\n",
      "Ep: 240, steps: 24, D loss: 0.162099, acc:  79%, G loss: 2.091200\n",
      "Ep: 240, steps: 25, D loss: 0.209250, acc:  66%, G loss: 1.941575\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 241, steps: 1, D loss: 0.209245, acc:  67%, G loss: 2.084762\n",
      "Ep: 241, steps: 2, D loss: 0.248403, acc:  56%, G loss: 1.760102\n",
      "Ep: 241, steps: 3, D loss: 0.131805, acc:  93%, G loss: 2.255383\n",
      "Ep: 241, steps: 4, D loss: 0.169800, acc:  85%, G loss: 2.166667\n",
      "Ep: 241, steps: 5, D loss: 0.257488, acc:  61%, G loss: 2.180349\n",
      "Ep: 241, steps: 6, D loss: 0.265478, acc:  50%, G loss: 2.088531\n",
      "Ep: 241, steps: 7, D loss: 0.417979, acc:  14%, G loss: 1.577035\n",
      "Ep: 241, steps: 8, D loss: 0.228167, acc:  61%, G loss: 2.121429\n",
      "Ep: 241, steps: 9, D loss: 0.176526, acc:  80%, G loss: 1.910812\n",
      "Ep: 241, steps: 10, D loss: 0.145611, acc:  84%, G loss: 1.984708\n",
      "Ep: 241, steps: 11, D loss: 0.249633, acc:  54%, G loss: 2.127999\n",
      "Ep: 241, steps: 12, D loss: 0.318314, acc:  37%, G loss: 1.696170\n",
      "Ep: 241, steps: 13, D loss: 0.293897, acc:  37%, G loss: 1.710548\n",
      "Ep: 241, steps: 14, D loss: 0.305650, acc:  34%, G loss: 1.772299\n",
      "Ep: 241, steps: 15, D loss: 0.229562, acc:  61%, G loss: 1.777522\n",
      "Ep: 241, steps: 16, D loss: 0.263574, acc:  55%, G loss: 1.900607\n",
      "Ep: 241, steps: 17, D loss: 0.151129, acc:  84%, G loss: 1.886933\n",
      "Ep: 241, steps: 18, D loss: 0.253395, acc:  54%, G loss: 1.960283\n",
      "Ep: 241, steps: 19, D loss: 0.186663, acc:  71%, G loss: 1.911167\n",
      "Ep: 241, steps: 20, D loss: 0.155471, acc:  81%, G loss: 2.024115\n",
      "Ep: 241, steps: 21, D loss: 0.307981, acc:  32%, G loss: 1.718819\n",
      "Saved Model\n",
      "Ep: 241, steps: 22, D loss: 0.188690, acc:  67%, G loss: 1.957130\n",
      "Ep: 241, steps: 23, D loss: 0.194949, acc:  73%, G loss: 1.878574\n",
      "Ep: 241, steps: 24, D loss: 0.232424, acc:  60%, G loss: 1.875317\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 242, steps: 1, D loss: 0.229317, acc:  62%, G loss: 2.185406\n",
      "Ep: 242, steps: 2, D loss: 0.250396, acc:  55%, G loss: 1.782857\n",
      "Ep: 242, steps: 3, D loss: 0.152428, acc:  87%, G loss: 2.275368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 242, steps: 4, D loss: 0.189335, acc:  81%, G loss: 2.098832\n",
      "Ep: 242, steps: 5, D loss: 0.215692, acc:  69%, G loss: 2.002822\n",
      "Ep: 242, steps: 6, D loss: 0.255396, acc:  50%, G loss: 2.005880\n",
      "Ep: 242, steps: 7, D loss: 0.397136, acc:  20%, G loss: 1.604755\n",
      "Ep: 242, steps: 8, D loss: 0.230365, acc:  60%, G loss: 2.333493\n",
      "Ep: 242, steps: 9, D loss: 0.192982, acc:  74%, G loss: 1.996596\n",
      "Ep: 242, steps: 10, D loss: 0.140500, acc:  86%, G loss: 2.031500\n",
      "Ep: 242, steps: 11, D loss: 0.255991, acc:  52%, G loss: 2.205312\n",
      "Ep: 242, steps: 12, D loss: 0.307086, acc:  40%, G loss: 1.739561\n",
      "Ep: 242, steps: 13, D loss: 0.287408, acc:  40%, G loss: 1.753012\n",
      "Ep: 242, steps: 14, D loss: 0.305850, acc:  34%, G loss: 1.848806\n",
      "Ep: 242, steps: 15, D loss: 0.223923, acc:  65%, G loss: 1.787431\n",
      "Ep: 242, steps: 16, D loss: 0.276140, acc:  50%, G loss: 1.855997\n",
      "Ep: 242, steps: 17, D loss: 0.160441, acc:  81%, G loss: 1.943156\n",
      "Ep: 242, steps: 18, D loss: 0.242626, acc:  59%, G loss: 1.954960\n",
      "Ep: 242, steps: 19, D loss: 0.183544, acc:  71%, G loss: 1.946630\n",
      "Ep: 242, steps: 20, D loss: 0.164874, acc:  76%, G loss: 2.081206\n",
      "Ep: 242, steps: 21, D loss: 0.300291, acc:  34%, G loss: 1.849128\n",
      "Ep: 242, steps: 22, D loss: 0.198370, acc:  63%, G loss: 2.031655\n",
      "Ep: 242, steps: 23, D loss: 0.196612, acc:  71%, G loss: 2.466830\n",
      "Ep: 242, steps: 24, D loss: 0.174107, acc:  77%, G loss: 2.077289\n",
      "Ep: 242, steps: 25, D loss: 0.214344, acc:  65%, G loss: 1.908745\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 243, steps: 1, D loss: 0.221986, acc:  64%, G loss: 2.146505\n",
      "Ep: 243, steps: 2, D loss: 0.247984, acc:  57%, G loss: 1.830757\n",
      "Ep: 243, steps: 3, D loss: 0.146307, acc:  91%, G loss: 2.282658\n",
      "Ep: 243, steps: 4, D loss: 0.178933, acc:  83%, G loss: 1.977414\n",
      "Ep: 243, steps: 5, D loss: 0.226554, acc:  65%, G loss: 2.207027\n",
      "Ep: 243, steps: 6, D loss: 0.266441, acc:  49%, G loss: 1.787915\n",
      "Ep: 243, steps: 7, D loss: 0.428126, acc:  16%, G loss: 1.640432\n",
      "Ep: 243, steps: 8, D loss: 0.226183, acc:  60%, G loss: 2.224829\n",
      "Ep: 243, steps: 9, D loss: 0.189636, acc:  76%, G loss: 1.969413\n",
      "Ep: 243, steps: 10, D loss: 0.128429, acc:  91%, G loss: 1.918571\n",
      "Ep: 243, steps: 11, D loss: 0.253317, acc:  54%, G loss: 2.222660\n",
      "Ep: 243, steps: 12, D loss: 0.319225, acc:  37%, G loss: 1.671709\n",
      "Ep: 243, steps: 13, D loss: 0.289306, acc:  41%, G loss: 1.738959\n",
      "Ep: 243, steps: 14, D loss: 0.315722, acc:  30%, G loss: 1.910116\n",
      "Ep: 243, steps: 15, D loss: 0.228844, acc:  61%, G loss: 1.726084\n",
      "Ep: 243, steps: 16, D loss: 0.270466, acc:  51%, G loss: 1.853936\n",
      "Ep: 243, steps: 17, D loss: 0.160361, acc:  80%, G loss: 2.077368\n",
      "Ep: 243, steps: 18, D loss: 0.261362, acc:  52%, G loss: 1.937611\n",
      "Ep: 243, steps: 19, D loss: 0.196095, acc:  68%, G loss: 1.909912\n",
      "Ep: 243, steps: 20, D loss: 0.164491, acc:  78%, G loss: 1.962233\n",
      "Ep: 243, steps: 21, D loss: 0.311714, acc:  32%, G loss: 1.911111\n",
      "Ep: 243, steps: 22, D loss: 0.197408, acc:  64%, G loss: 1.918131\n",
      "Ep: 243, steps: 23, D loss: 0.188439, acc:  73%, G loss: 2.475755\n",
      "Ep: 243, steps: 24, D loss: 0.165222, acc:  79%, G loss: 1.925371\n",
      "Ep: 243, steps: 25, D loss: 0.208246, acc:  66%, G loss: 1.877011\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 244, steps: 1, D loss: 0.210438, acc:  68%, G loss: 2.107172\n",
      "Ep: 244, steps: 2, D loss: 0.242786, acc:  58%, G loss: 1.847996\n",
      "Ep: 244, steps: 3, D loss: 0.143551, acc:  91%, G loss: 2.274783\n",
      "Ep: 244, steps: 4, D loss: 0.173114, acc:  86%, G loss: 2.112981\n",
      "Ep: 244, steps: 5, D loss: 0.211689, acc:  68%, G loss: 2.147464\n",
      "Ep: 244, steps: 6, D loss: 0.257273, acc:  49%, G loss: 1.930645\n",
      "Ep: 244, steps: 7, D loss: 0.430176, acc:  17%, G loss: 1.550338\n",
      "Ep: 244, steps: 8, D loss: 0.217413, acc:  64%, G loss: 2.276363\n",
      "Ep: 244, steps: 9, D loss: 0.217763, acc:  66%, G loss: 2.013572\n",
      "Ep: 244, steps: 10, D loss: 0.140320, acc:  88%, G loss: 2.047089\n",
      "Ep: 244, steps: 11, D loss: 0.220911, acc:  64%, G loss: 2.244080\n",
      "Ep: 244, steps: 12, D loss: 0.309687, acc:  39%, G loss: 1.704092\n",
      "Ep: 244, steps: 13, D loss: 0.285576, acc:  40%, G loss: 1.822557\n",
      "Ep: 244, steps: 14, D loss: 0.303035, acc:  35%, G loss: 1.771348\n",
      "Ep: 244, steps: 15, D loss: 0.231993, acc:  60%, G loss: 1.761037\n",
      "Ep: 244, steps: 16, D loss: 0.282964, acc:  47%, G loss: 1.838747\n",
      "Ep: 244, steps: 17, D loss: 0.163753, acc:  83%, G loss: 1.821196\n",
      "Ep: 244, steps: 18, D loss: 0.269284, acc:  56%, G loss: 1.880752\n",
      "Ep: 244, steps: 19, D loss: 0.203297, acc:  68%, G loss: 1.812516\n",
      "Saved Model\n",
      "Ep: 244, steps: 20, D loss: 0.186085, acc:  74%, G loss: 2.026860\n",
      "Ep: 244, steps: 21, D loss: 0.210701, acc:  63%, G loss: 1.961073\n",
      "Ep: 244, steps: 22, D loss: 0.174780, acc:  77%, G loss: 2.310019\n",
      "Ep: 244, steps: 23, D loss: 0.177855, acc:  76%, G loss: 1.978922\n",
      "Ep: 244, steps: 24, D loss: 0.214286, acc:  64%, G loss: 1.883583\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 245, steps: 1, D loss: 0.196762, acc:  72%, G loss: 2.200892\n",
      "Ep: 245, steps: 2, D loss: 0.229536, acc:  64%, G loss: 1.883061\n",
      "Ep: 245, steps: 3, D loss: 0.146095, acc:  88%, G loss: 2.325293\n",
      "Ep: 245, steps: 4, D loss: 0.161352, acc:  87%, G loss: 2.089498\n",
      "Ep: 245, steps: 5, D loss: 0.246807, acc:  58%, G loss: 2.213536\n",
      "Ep: 245, steps: 6, D loss: 0.277783, acc:  49%, G loss: 1.828274\n",
      "Ep: 245, steps: 7, D loss: 0.462668, acc:  14%, G loss: 1.665157\n",
      "Ep: 245, steps: 8, D loss: 0.224495, acc:  57%, G loss: 2.310968\n",
      "Ep: 245, steps: 9, D loss: 0.189373, acc:  73%, G loss: 1.948714\n",
      "Ep: 245, steps: 10, D loss: 0.144541, acc:  88%, G loss: 1.962975\n",
      "Ep: 245, steps: 11, D loss: 0.235675, acc:  60%, G loss: 2.108889\n",
      "Ep: 245, steps: 12, D loss: 0.324523, acc:  34%, G loss: 1.664838\n",
      "Ep: 245, steps: 13, D loss: 0.275620, acc:  49%, G loss: 1.749729\n",
      "Ep: 245, steps: 14, D loss: 0.301776, acc:  37%, G loss: 1.792987\n",
      "Ep: 245, steps: 15, D loss: 0.231168, acc:  61%, G loss: 1.816707\n",
      "Ep: 245, steps: 16, D loss: 0.278418, acc:  50%, G loss: 1.856670\n",
      "Ep: 245, steps: 17, D loss: 0.160781, acc:  82%, G loss: 1.859415\n",
      "Ep: 245, steps: 18, D loss: 0.276118, acc:  48%, G loss: 1.992124\n",
      "Ep: 245, steps: 19, D loss: 0.181165, acc:  75%, G loss: 1.960600\n",
      "Ep: 245, steps: 20, D loss: 0.150755, acc:  84%, G loss: 2.134855\n",
      "Ep: 245, steps: 21, D loss: 0.295604, acc:  37%, G loss: 1.712949\n",
      "Ep: 245, steps: 22, D loss: 0.205126, acc:  65%, G loss: 1.875236\n",
      "Ep: 245, steps: 23, D loss: 0.169431, acc:  79%, G loss: 2.384832\n",
      "Ep: 245, steps: 24, D loss: 0.163017, acc:  78%, G loss: 2.029273\n",
      "Ep: 245, steps: 25, D loss: 0.203546, acc:  67%, G loss: 2.028479\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 246, steps: 1, D loss: 0.208986, acc:  67%, G loss: 2.127346\n",
      "Ep: 246, steps: 2, D loss: 0.249058, acc:  55%, G loss: 1.802444\n",
      "Ep: 246, steps: 3, D loss: 0.139324, acc:  91%, G loss: 2.378511\n",
      "Ep: 246, steps: 4, D loss: 0.183162, acc:  82%, G loss: 2.101542\n",
      "Ep: 246, steps: 5, D loss: 0.214313, acc:  67%, G loss: 2.160846\n",
      "Ep: 246, steps: 6, D loss: 0.262287, acc:  49%, G loss: 1.945559\n",
      "Ep: 246, steps: 7, D loss: 0.407624, acc:  20%, G loss: 1.571385\n",
      "Ep: 246, steps: 8, D loss: 0.227742, acc:  61%, G loss: 2.263816\n",
      "Ep: 246, steps: 9, D loss: 0.221723, acc:  65%, G loss: 1.969875\n",
      "Ep: 246, steps: 10, D loss: 0.144995, acc:  86%, G loss: 2.022783\n",
      "Ep: 246, steps: 11, D loss: 0.248041, acc:  53%, G loss: 2.234376\n",
      "Ep: 246, steps: 12, D loss: 0.313978, acc:  37%, G loss: 1.680040\n",
      "Ep: 246, steps: 13, D loss: 0.283669, acc:  41%, G loss: 1.825125\n",
      "Ep: 246, steps: 14, D loss: 0.304271, acc:  33%, G loss: 1.858846\n",
      "Ep: 246, steps: 15, D loss: 0.236891, acc:  58%, G loss: 1.837197\n",
      "Ep: 246, steps: 16, D loss: 0.263992, acc:  54%, G loss: 1.863657\n",
      "Ep: 246, steps: 17, D loss: 0.152556, acc:  85%, G loss: 1.886711\n",
      "Ep: 246, steps: 18, D loss: 0.238552, acc:  60%, G loss: 1.867371\n",
      "Ep: 246, steps: 19, D loss: 0.193102, acc:  69%, G loss: 1.912633\n",
      "Ep: 246, steps: 20, D loss: 0.183190, acc:  73%, G loss: 2.059945\n",
      "Ep: 246, steps: 21, D loss: 0.307250, acc:  32%, G loss: 1.722587\n",
      "Ep: 246, steps: 22, D loss: 0.178906, acc:  71%, G loss: 1.992598\n",
      "Ep: 246, steps: 23, D loss: 0.208139, acc:  69%, G loss: 2.291389\n",
      "Ep: 246, steps: 24, D loss: 0.169469, acc:  78%, G loss: 1.981822\n",
      "Ep: 246, steps: 25, D loss: 0.200151, acc:  67%, G loss: 1.892650\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 247, steps: 1, D loss: 0.233516, acc:  62%, G loss: 2.029339\n",
      "Ep: 247, steps: 2, D loss: 0.241188, acc:  58%, G loss: 1.715353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 247, steps: 3, D loss: 0.146696, acc:  89%, G loss: 2.255375\n",
      "Ep: 247, steps: 4, D loss: 0.183121, acc:  83%, G loss: 2.081827\n",
      "Ep: 247, steps: 5, D loss: 0.239100, acc:  60%, G loss: 2.324395\n",
      "Ep: 247, steps: 6, D loss: 0.259903, acc:  49%, G loss: 1.960784\n",
      "Ep: 247, steps: 7, D loss: 0.369976, acc:  30%, G loss: 1.636484\n",
      "Ep: 247, steps: 8, D loss: 0.221266, acc:  63%, G loss: 2.114179\n",
      "Ep: 247, steps: 9, D loss: 0.183782, acc:  77%, G loss: 1.922810\n",
      "Ep: 247, steps: 10, D loss: 0.136535, acc:  87%, G loss: 1.945361\n",
      "Ep: 247, steps: 11, D loss: 0.259365, acc:  52%, G loss: 2.050929\n",
      "Ep: 247, steps: 12, D loss: 0.315842, acc:  38%, G loss: 1.640694\n",
      "Ep: 247, steps: 13, D loss: 0.279773, acc:  45%, G loss: 1.718522\n",
      "Ep: 247, steps: 14, D loss: 0.317987, acc:  31%, G loss: 1.864955\n",
      "Ep: 247, steps: 15, D loss: 0.231808, acc:  62%, G loss: 1.696270\n",
      "Ep: 247, steps: 16, D loss: 0.262017, acc:  55%, G loss: 1.822508\n",
      "Ep: 247, steps: 17, D loss: 0.165458, acc:  80%, G loss: 1.909999\n",
      "Saved Model\n",
      "Ep: 247, steps: 18, D loss: 0.232869, acc:  63%, G loss: 1.948418\n",
      "Ep: 247, steps: 19, D loss: 0.158912, acc:  83%, G loss: 2.109951\n",
      "Ep: 247, steps: 20, D loss: 0.289320, acc:  39%, G loss: 1.864453\n",
      "Ep: 247, steps: 21, D loss: 0.175278, acc:  74%, G loss: 1.909393\n",
      "Ep: 247, steps: 22, D loss: 0.183348, acc:  74%, G loss: 2.227495\n",
      "Ep: 247, steps: 23, D loss: 0.150641, acc:  87%, G loss: 1.947676\n",
      "Ep: 247, steps: 24, D loss: 0.196952, acc:  70%, G loss: 1.947475\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 248, steps: 1, D loss: 0.227039, acc:  64%, G loss: 1.993279\n",
      "Ep: 248, steps: 2, D loss: 0.258124, acc:  51%, G loss: 1.775553\n",
      "Ep: 248, steps: 3, D loss: 0.156362, acc:  85%, G loss: 2.238712\n",
      "Ep: 248, steps: 4, D loss: 0.188075, acc:  82%, G loss: 2.066992\n",
      "Ep: 248, steps: 5, D loss: 0.199670, acc:  69%, G loss: 2.165360\n",
      "Ep: 248, steps: 6, D loss: 0.261304, acc:  49%, G loss: 1.893797\n",
      "Ep: 248, steps: 7, D loss: 0.392154, acc:  25%, G loss: 1.560878\n",
      "Ep: 248, steps: 8, D loss: 0.228374, acc:  61%, G loss: 2.210648\n",
      "Ep: 248, steps: 9, D loss: 0.205199, acc:  67%, G loss: 1.925316\n",
      "Ep: 248, steps: 10, D loss: 0.163392, acc:  79%, G loss: 2.011990\n",
      "Ep: 248, steps: 11, D loss: 0.245837, acc:  56%, G loss: 2.177609\n",
      "Ep: 248, steps: 12, D loss: 0.323836, acc:  36%, G loss: 1.701469\n",
      "Ep: 248, steps: 13, D loss: 0.289466, acc:  42%, G loss: 1.825441\n",
      "Ep: 248, steps: 14, D loss: 0.298366, acc:  35%, G loss: 1.857585\n",
      "Ep: 248, steps: 15, D loss: 0.238725, acc:  57%, G loss: 1.910116\n",
      "Ep: 248, steps: 16, D loss: 0.271624, acc:  51%, G loss: 1.880882\n",
      "Ep: 248, steps: 17, D loss: 0.161240, acc:  81%, G loss: 1.891264\n",
      "Ep: 248, steps: 18, D loss: 0.237035, acc:  62%, G loss: 1.956711\n",
      "Ep: 248, steps: 19, D loss: 0.181796, acc:  72%, G loss: 1.883480\n",
      "Ep: 248, steps: 20, D loss: 0.167248, acc:  78%, G loss: 2.067115\n",
      "Ep: 248, steps: 21, D loss: 0.286969, acc:  42%, G loss: 1.860241\n",
      "Ep: 248, steps: 22, D loss: 0.188523, acc:  66%, G loss: 1.963572\n",
      "Ep: 248, steps: 23, D loss: 0.200521, acc:  69%, G loss: 2.260605\n",
      "Ep: 248, steps: 24, D loss: 0.166835, acc:  77%, G loss: 1.928616\n",
      "Ep: 248, steps: 25, D loss: 0.202536, acc:  66%, G loss: 1.858228\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 249, steps: 1, D loss: 0.213652, acc:  68%, G loss: 2.009907\n",
      "Ep: 249, steps: 2, D loss: 0.256978, acc:  54%, G loss: 1.798262\n",
      "Ep: 249, steps: 3, D loss: 0.155780, acc:  87%, G loss: 2.289117\n",
      "Ep: 249, steps: 4, D loss: 0.180049, acc:  83%, G loss: 2.063369\n",
      "Ep: 249, steps: 5, D loss: 0.227809, acc:  61%, G loss: 2.115625\n",
      "Ep: 249, steps: 6, D loss: 0.266621, acc:  49%, G loss: 1.972972\n",
      "Ep: 249, steps: 7, D loss: 0.429815, acc:  17%, G loss: 1.530229\n",
      "Ep: 249, steps: 8, D loss: 0.225561, acc:  61%, G loss: 2.190507\n",
      "Ep: 249, steps: 9, D loss: 0.177947, acc:  79%, G loss: 2.044073\n",
      "Ep: 249, steps: 10, D loss: 0.140187, acc:  88%, G loss: 1.999556\n",
      "Ep: 249, steps: 11, D loss: 0.252210, acc:  55%, G loss: 2.079415\n",
      "Ep: 249, steps: 12, D loss: 0.323488, acc:  32%, G loss: 1.635425\n",
      "Ep: 249, steps: 13, D loss: 0.292458, acc:  39%, G loss: 1.724871\n",
      "Ep: 249, steps: 14, D loss: 0.295403, acc:  36%, G loss: 1.771625\n",
      "Ep: 249, steps: 15, D loss: 0.237985, acc:  56%, G loss: 1.932151\n",
      "Ep: 249, steps: 16, D loss: 0.256408, acc:  55%, G loss: 1.817715\n",
      "Ep: 249, steps: 17, D loss: 0.162586, acc:  81%, G loss: 1.824977\n",
      "Ep: 249, steps: 18, D loss: 0.261527, acc:  53%, G loss: 1.900049\n",
      "Ep: 249, steps: 19, D loss: 0.188276, acc:  71%, G loss: 1.837341\n",
      "Ep: 249, steps: 20, D loss: 0.146110, acc:  84%, G loss: 2.060841\n",
      "Ep: 249, steps: 21, D loss: 0.291333, acc:  39%, G loss: 1.943290\n",
      "Ep: 249, steps: 22, D loss: 0.192658, acc:  64%, G loss: 1.993151\n",
      "Ep: 249, steps: 23, D loss: 0.166472, acc:  80%, G loss: 2.376115\n",
      "Ep: 249, steps: 24, D loss: 0.160880, acc:  79%, G loss: 2.033578\n",
      "Ep: 249, steps: 25, D loss: 0.210549, acc:  66%, G loss: 1.918282\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 250, steps: 1, D loss: 0.209765, acc:  69%, G loss: 2.094341\n",
      "Ep: 250, steps: 2, D loss: 0.247840, acc:  57%, G loss: 1.786759\n",
      "Ep: 250, steps: 3, D loss: 0.149812, acc:  88%, G loss: 2.215407\n",
      "Ep: 250, steps: 4, D loss: 0.167873, acc:  87%, G loss: 2.230619\n",
      "Ep: 250, steps: 5, D loss: 0.219317, acc:  66%, G loss: 2.123721\n",
      "Ep: 250, steps: 6, D loss: 0.262825, acc:  50%, G loss: 1.839204\n",
      "Ep: 250, steps: 7, D loss: 0.373767, acc:  25%, G loss: 1.501953\n",
      "Ep: 250, steps: 8, D loss: 0.214818, acc:  64%, G loss: 2.122621\n",
      "Ep: 250, steps: 9, D loss: 0.206776, acc:  68%, G loss: 1.981943\n",
      "Ep: 250, steps: 10, D loss: 0.148425, acc:  81%, G loss: 1.994658\n",
      "Ep: 250, steps: 11, D loss: 0.246998, acc:  56%, G loss: 2.124676\n",
      "Ep: 250, steps: 12, D loss: 0.321338, acc:  35%, G loss: 1.654755\n",
      "Ep: 250, steps: 13, D loss: 0.290123, acc:  41%, G loss: 1.777978\n",
      "Ep: 250, steps: 14, D loss: 0.297308, acc:  37%, G loss: 1.820461\n",
      "Ep: 250, steps: 15, D loss: 0.239384, acc:  57%, G loss: 1.938706\n",
      "Saved Model\n",
      "Ep: 250, steps: 16, D loss: 0.254341, acc:  56%, G loss: 1.895616\n",
      "Ep: 250, steps: 17, D loss: 0.230190, acc:  64%, G loss: 1.843453\n",
      "Ep: 250, steps: 18, D loss: 0.203354, acc:  67%, G loss: 1.832048\n",
      "Ep: 250, steps: 19, D loss: 0.160301, acc:  81%, G loss: 2.107341\n",
      "Ep: 250, steps: 20, D loss: 0.277654, acc:  42%, G loss: 1.815271\n",
      "Ep: 250, steps: 21, D loss: 0.192955, acc:  67%, G loss: 1.895607\n",
      "Ep: 250, steps: 22, D loss: 0.182794, acc:  76%, G loss: 2.234954\n",
      "Ep: 250, steps: 23, D loss: 0.177546, acc:  77%, G loss: 1.964544\n",
      "Ep: 250, steps: 24, D loss: 0.203431, acc:  65%, G loss: 1.873825\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 251, steps: 1, D loss: 0.221691, acc:  66%, G loss: 2.046392\n",
      "Ep: 251, steps: 2, D loss: 0.248149, acc:  57%, G loss: 1.650781\n",
      "Ep: 251, steps: 3, D loss: 0.162636, acc:  84%, G loss: 2.230107\n",
      "Ep: 251, steps: 4, D loss: 0.184649, acc:  82%, G loss: 1.849744\n",
      "Ep: 251, steps: 5, D loss: 0.235646, acc:  60%, G loss: 2.134145\n",
      "Ep: 251, steps: 6, D loss: 0.268629, acc:  49%, G loss: 1.959472\n",
      "Ep: 251, steps: 7, D loss: 0.458386, acc:  14%, G loss: 1.832094\n",
      "Ep: 251, steps: 8, D loss: 0.239314, acc:  58%, G loss: 2.192838\n",
      "Ep: 251, steps: 9, D loss: 0.193530, acc:  72%, G loss: 1.941333\n",
      "Ep: 251, steps: 10, D loss: 0.147915, acc:  85%, G loss: 1.959399\n",
      "Ep: 251, steps: 11, D loss: 0.241749, acc:  56%, G loss: 2.124905\n",
      "Ep: 251, steps: 12, D loss: 0.327800, acc:  33%, G loss: 1.584579\n",
      "Ep: 251, steps: 13, D loss: 0.281648, acc:  43%, G loss: 1.716332\n",
      "Ep: 251, steps: 14, D loss: 0.303879, acc:  35%, G loss: 1.786856\n",
      "Ep: 251, steps: 15, D loss: 0.225798, acc:  62%, G loss: 1.777331\n",
      "Ep: 251, steps: 16, D loss: 0.259611, acc:  53%, G loss: 1.792323\n",
      "Ep: 251, steps: 17, D loss: 0.156845, acc:  84%, G loss: 1.809634\n",
      "Ep: 251, steps: 18, D loss: 0.251016, acc:  56%, G loss: 1.852417\n",
      "Ep: 251, steps: 19, D loss: 0.188394, acc:  71%, G loss: 1.911130\n",
      "Ep: 251, steps: 20, D loss: 0.151241, acc:  82%, G loss: 2.103247\n",
      "Ep: 251, steps: 21, D loss: 0.287412, acc:  37%, G loss: 1.623767\n",
      "Ep: 251, steps: 22, D loss: 0.196420, acc:  65%, G loss: 1.877474\n",
      "Ep: 251, steps: 23, D loss: 0.178955, acc:  75%, G loss: 2.218197\n",
      "Ep: 251, steps: 24, D loss: 0.162382, acc:  78%, G loss: 1.981767\n",
      "Ep: 251, steps: 25, D loss: 0.207446, acc:  65%, G loss: 2.008513\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 252, steps: 1, D loss: 0.211208, acc:  68%, G loss: 2.137679\n",
      "Ep: 252, steps: 2, D loss: 0.240983, acc:  59%, G loss: 1.801978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 252, steps: 3, D loss: 0.149190, acc:  87%, G loss: 2.329105\n",
      "Ep: 252, steps: 4, D loss: 0.174952, acc:  86%, G loss: 2.103253\n",
      "Ep: 252, steps: 5, D loss: 0.242780, acc:  63%, G loss: 2.146409\n",
      "Ep: 252, steps: 6, D loss: 0.273229, acc:  48%, G loss: 2.003690\n",
      "Ep: 252, steps: 7, D loss: 0.407980, acc:  18%, G loss: 1.570999\n",
      "Ep: 252, steps: 8, D loss: 0.211763, acc:  65%, G loss: 2.286585\n",
      "Ep: 252, steps: 9, D loss: 0.183299, acc:  78%, G loss: 1.946841\n",
      "Ep: 252, steps: 10, D loss: 0.153678, acc:  82%, G loss: 2.041639\n",
      "Ep: 252, steps: 11, D loss: 0.235883, acc:  61%, G loss: 2.128661\n",
      "Ep: 252, steps: 12, D loss: 0.314147, acc:  37%, G loss: 1.679970\n",
      "Ep: 252, steps: 13, D loss: 0.284560, acc:  43%, G loss: 1.712563\n",
      "Ep: 252, steps: 14, D loss: 0.312763, acc:  33%, G loss: 1.776287\n",
      "Ep: 252, steps: 15, D loss: 0.225857, acc:  63%, G loss: 1.914233\n",
      "Ep: 252, steps: 16, D loss: 0.267858, acc:  54%, G loss: 1.803448\n",
      "Ep: 252, steps: 17, D loss: 0.147470, acc:  86%, G loss: 1.831308\n",
      "Ep: 252, steps: 18, D loss: 0.235547, acc:  61%, G loss: 1.916369\n",
      "Ep: 252, steps: 19, D loss: 0.170729, acc:  76%, G loss: 1.823894\n",
      "Ep: 252, steps: 20, D loss: 0.164037, acc:  78%, G loss: 2.099460\n",
      "Ep: 252, steps: 21, D loss: 0.305025, acc:  34%, G loss: 1.801193\n",
      "Ep: 252, steps: 22, D loss: 0.202897, acc:  63%, G loss: 1.967947\n",
      "Ep: 252, steps: 23, D loss: 0.167553, acc:  78%, G loss: 2.326526\n",
      "Ep: 252, steps: 24, D loss: 0.160668, acc:  81%, G loss: 2.073240\n",
      "Ep: 252, steps: 25, D loss: 0.210475, acc:  66%, G loss: 1.960018\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 253, steps: 1, D loss: 0.214227, acc:  66%, G loss: 2.155687\n",
      "Ep: 253, steps: 2, D loss: 0.242398, acc:  58%, G loss: 1.778746\n",
      "Ep: 253, steps: 3, D loss: 0.133199, acc:  92%, G loss: 2.251311\n",
      "Ep: 253, steps: 4, D loss: 0.175253, acc:  84%, G loss: 2.068711\n",
      "Ep: 253, steps: 5, D loss: 0.247706, acc:  61%, G loss: 2.161830\n",
      "Ep: 253, steps: 6, D loss: 0.274156, acc:  49%, G loss: 1.932408\n",
      "Ep: 253, steps: 7, D loss: 0.431314, acc:  19%, G loss: 1.719386\n",
      "Ep: 253, steps: 8, D loss: 0.222248, acc:  63%, G loss: 2.279492\n",
      "Ep: 253, steps: 9, D loss: 0.179510, acc:  76%, G loss: 1.984590\n",
      "Ep: 253, steps: 10, D loss: 0.139580, acc:  88%, G loss: 1.921018\n",
      "Ep: 253, steps: 11, D loss: 0.226588, acc:  61%, G loss: 2.091959\n",
      "Ep: 253, steps: 12, D loss: 0.321322, acc:  34%, G loss: 1.633908\n",
      "Ep: 253, steps: 13, D loss: 0.278579, acc:  44%, G loss: 1.846410\n",
      "Saved Model\n",
      "Ep: 253, steps: 14, D loss: 0.314606, acc:  33%, G loss: 1.795466\n",
      "Ep: 253, steps: 15, D loss: 0.277490, acc:  47%, G loss: 1.984633\n",
      "Ep: 253, steps: 16, D loss: 0.168113, acc:  80%, G loss: 1.907036\n",
      "Ep: 253, steps: 17, D loss: 0.249221, acc:  57%, G loss: 1.842390\n",
      "Ep: 253, steps: 18, D loss: 0.189188, acc:  70%, G loss: 1.884783\n",
      "Ep: 253, steps: 19, D loss: 0.151556, acc:  81%, G loss: 2.040437\n",
      "Ep: 253, steps: 20, D loss: 0.302226, acc:  35%, G loss: 1.750015\n",
      "Ep: 253, steps: 21, D loss: 0.175376, acc:  73%, G loss: 1.876333\n",
      "Ep: 253, steps: 22, D loss: 0.184573, acc:  74%, G loss: 2.332184\n",
      "Ep: 253, steps: 23, D loss: 0.168364, acc:  79%, G loss: 2.000295\n",
      "Ep: 253, steps: 24, D loss: 0.206365, acc:  67%, G loss: 1.950476\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 254, steps: 1, D loss: 0.222120, acc:  65%, G loss: 2.099336\n",
      "Ep: 254, steps: 2, D loss: 0.243667, acc:  59%, G loss: 1.807833\n",
      "Ep: 254, steps: 3, D loss: 0.149448, acc:  87%, G loss: 2.185371\n",
      "Ep: 254, steps: 4, D loss: 0.179969, acc:  84%, G loss: 2.119344\n",
      "Ep: 254, steps: 5, D loss: 0.293756, acc:  54%, G loss: 2.136998\n",
      "Ep: 254, steps: 6, D loss: 0.255661, acc:  51%, G loss: 1.799503\n",
      "Ep: 254, steps: 7, D loss: 0.383265, acc:  22%, G loss: 1.592418\n",
      "Ep: 254, steps: 8, D loss: 0.229085, acc:  61%, G loss: 2.190586\n",
      "Ep: 254, steps: 9, D loss: 0.179031, acc:  78%, G loss: 1.922268\n",
      "Ep: 254, steps: 10, D loss: 0.160883, acc:  80%, G loss: 1.887920\n",
      "Ep: 254, steps: 11, D loss: 0.230173, acc:  62%, G loss: 2.178133\n",
      "Ep: 254, steps: 12, D loss: 0.315404, acc:  35%, G loss: 1.712241\n",
      "Ep: 254, steps: 13, D loss: 0.279604, acc:  44%, G loss: 1.772472\n",
      "Ep: 254, steps: 14, D loss: 0.298557, acc:  35%, G loss: 1.767828\n",
      "Ep: 254, steps: 15, D loss: 0.220845, acc:  64%, G loss: 2.015632\n",
      "Ep: 254, steps: 16, D loss: 0.264092, acc:  56%, G loss: 1.795678\n",
      "Ep: 254, steps: 17, D loss: 0.161330, acc:  81%, G loss: 1.862541\n",
      "Ep: 254, steps: 18, D loss: 0.242713, acc:  61%, G loss: 1.886569\n",
      "Ep: 254, steps: 19, D loss: 0.200611, acc:  69%, G loss: 1.851214\n",
      "Ep: 254, steps: 20, D loss: 0.167718, acc:  78%, G loss: 1.964002\n",
      "Ep: 254, steps: 21, D loss: 0.283078, acc:  39%, G loss: 1.781006\n",
      "Ep: 254, steps: 22, D loss: 0.195189, acc:  64%, G loss: 2.003695\n",
      "Ep: 254, steps: 23, D loss: 0.176851, acc:  76%, G loss: 2.287947\n",
      "Ep: 254, steps: 24, D loss: 0.174325, acc:  76%, G loss: 2.013534\n",
      "Ep: 254, steps: 25, D loss: 0.224833, acc:  63%, G loss: 1.968508\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 255, steps: 1, D loss: 0.220180, acc:  65%, G loss: 2.263716\n",
      "Ep: 255, steps: 2, D loss: 0.242154, acc:  59%, G loss: 1.793728\n",
      "Ep: 255, steps: 3, D loss: 0.141935, acc:  91%, G loss: 2.229663\n",
      "Ep: 255, steps: 4, D loss: 0.178195, acc:  83%, G loss: 2.066159\n",
      "Ep: 255, steps: 5, D loss: 0.217196, acc:  67%, G loss: 2.038756\n",
      "Ep: 255, steps: 6, D loss: 0.275687, acc:  48%, G loss: 1.839005\n",
      "Ep: 255, steps: 7, D loss: 0.439508, acc:  17%, G loss: 1.449474\n",
      "Ep: 255, steps: 8, D loss: 0.235125, acc:  58%, G loss: 2.028581\n",
      "Ep: 255, steps: 9, D loss: 0.188042, acc:  75%, G loss: 2.047105\n",
      "Ep: 255, steps: 10, D loss: 0.141989, acc:  88%, G loss: 1.909431\n",
      "Ep: 255, steps: 11, D loss: 0.234974, acc:  59%, G loss: 2.154094\n",
      "Ep: 255, steps: 12, D loss: 0.325422, acc:  34%, G loss: 1.757848\n",
      "Ep: 255, steps: 13, D loss: 0.295615, acc:  39%, G loss: 1.822326\n",
      "Ep: 255, steps: 14, D loss: 0.311811, acc:  34%, G loss: 1.816075\n",
      "Ep: 255, steps: 15, D loss: 0.242478, acc:  54%, G loss: 2.078206\n",
      "Ep: 255, steps: 16, D loss: 0.266188, acc:  54%, G loss: 1.805665\n",
      "Ep: 255, steps: 17, D loss: 0.162140, acc:  84%, G loss: 1.852035\n",
      "Ep: 255, steps: 18, D loss: 0.264015, acc:  57%, G loss: 1.890981\n",
      "Ep: 255, steps: 19, D loss: 0.190027, acc:  71%, G loss: 1.847824\n",
      "Ep: 255, steps: 20, D loss: 0.176694, acc:  76%, G loss: 2.024782\n",
      "Ep: 255, steps: 21, D loss: 0.279189, acc:  43%, G loss: 1.822744\n",
      "Ep: 255, steps: 22, D loss: 0.198133, acc:  65%, G loss: 1.923349\n",
      "Ep: 255, steps: 23, D loss: 0.212129, acc:  65%, G loss: 2.333154\n",
      "Ep: 255, steps: 24, D loss: 0.166402, acc:  79%, G loss: 2.012144\n",
      "Ep: 255, steps: 25, D loss: 0.216202, acc:  65%, G loss: 1.945082\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 256, steps: 1, D loss: 0.217476, acc:  65%, G loss: 2.233735\n",
      "Ep: 256, steps: 2, D loss: 0.247319, acc:  57%, G loss: 1.872077\n",
      "Ep: 256, steps: 3, D loss: 0.143913, acc:  89%, G loss: 2.166576\n",
      "Ep: 256, steps: 4, D loss: 0.172619, acc:  86%, G loss: 2.014796\n",
      "Ep: 256, steps: 5, D loss: 0.254459, acc:  59%, G loss: 2.104670\n",
      "Ep: 256, steps: 6, D loss: 0.260954, acc:  50%, G loss: 1.832177\n",
      "Ep: 256, steps: 7, D loss: 0.402064, acc:  22%, G loss: 1.564576\n",
      "Ep: 256, steps: 8, D loss: 0.234827, acc:  58%, G loss: 2.204913\n",
      "Ep: 256, steps: 9, D loss: 0.176027, acc:  80%, G loss: 1.958945\n",
      "Ep: 256, steps: 10, D loss: 0.144950, acc:  86%, G loss: 1.883201\n",
      "Ep: 256, steps: 11, D loss: 0.237366, acc:  59%, G loss: 2.072764\n",
      "Saved Model\n",
      "Ep: 256, steps: 12, D loss: 0.300260, acc:  41%, G loss: 1.655061\n",
      "Ep: 256, steps: 13, D loss: 0.350798, acc:  24%, G loss: 1.690639\n",
      "Ep: 256, steps: 14, D loss: 0.204833, acc:  71%, G loss: 1.784640\n",
      "Ep: 256, steps: 15, D loss: 0.276229, acc:  52%, G loss: 1.836089\n",
      "Ep: 256, steps: 16, D loss: 0.164809, acc:  83%, G loss: 1.812611\n",
      "Ep: 256, steps: 17, D loss: 0.232761, acc:  63%, G loss: 1.926182\n",
      "Ep: 256, steps: 18, D loss: 0.173463, acc:  75%, G loss: 1.846146\n",
      "Ep: 256, steps: 19, D loss: 0.162323, acc:  80%, G loss: 2.147976\n",
      "Ep: 256, steps: 20, D loss: 0.296333, acc:  38%, G loss: 1.792929\n",
      "Ep: 256, steps: 21, D loss: 0.215835, acc:  61%, G loss: 1.935310\n",
      "Ep: 256, steps: 22, D loss: 0.178482, acc:  75%, G loss: 2.357998\n",
      "Ep: 256, steps: 23, D loss: 0.164967, acc:  80%, G loss: 1.996701\n",
      "Ep: 256, steps: 24, D loss: 0.203856, acc:  67%, G loss: 1.951376\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 257, steps: 1, D loss: 0.213597, acc:  66%, G loss: 2.138521\n",
      "Ep: 257, steps: 2, D loss: 0.245237, acc:  57%, G loss: 1.774390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 257, steps: 3, D loss: 0.130807, acc:  94%, G loss: 2.255053\n",
      "Ep: 257, steps: 4, D loss: 0.176529, acc:  84%, G loss: 2.115273\n",
      "Ep: 257, steps: 5, D loss: 0.236141, acc:  62%, G loss: 1.994718\n",
      "Ep: 257, steps: 6, D loss: 0.262885, acc:  50%, G loss: 1.799832\n",
      "Ep: 257, steps: 7, D loss: 0.416173, acc:  18%, G loss: 1.447821\n",
      "Ep: 257, steps: 8, D loss: 0.231278, acc:  58%, G loss: 2.225991\n",
      "Ep: 257, steps: 9, D loss: 0.191060, acc:  75%, G loss: 1.914036\n",
      "Ep: 257, steps: 10, D loss: 0.141370, acc:  89%, G loss: 1.908459\n",
      "Ep: 257, steps: 11, D loss: 0.250054, acc:  54%, G loss: 2.141018\n",
      "Ep: 257, steps: 12, D loss: 0.326279, acc:  35%, G loss: 1.678506\n",
      "Ep: 257, steps: 13, D loss: 0.297306, acc:  38%, G loss: 1.769735\n",
      "Ep: 257, steps: 14, D loss: 0.305468, acc:  35%, G loss: 1.712154\n",
      "Ep: 257, steps: 15, D loss: 0.228460, acc:  62%, G loss: 1.965366\n",
      "Ep: 257, steps: 16, D loss: 0.275356, acc:  49%, G loss: 1.838630\n",
      "Ep: 257, steps: 17, D loss: 0.168495, acc:  82%, G loss: 1.883368\n",
      "Ep: 257, steps: 18, D loss: 0.247278, acc:  58%, G loss: 1.911715\n",
      "Ep: 257, steps: 19, D loss: 0.186330, acc:  72%, G loss: 1.833465\n",
      "Ep: 257, steps: 20, D loss: 0.158417, acc:  82%, G loss: 2.021040\n",
      "Ep: 257, steps: 21, D loss: 0.293485, acc:  37%, G loss: 1.684415\n",
      "Ep: 257, steps: 22, D loss: 0.202619, acc:  64%, G loss: 1.911144\n",
      "Ep: 257, steps: 23, D loss: 0.176965, acc:  75%, G loss: 2.307606\n",
      "Ep: 257, steps: 24, D loss: 0.161815, acc:  79%, G loss: 1.998769\n",
      "Ep: 257, steps: 25, D loss: 0.209311, acc:  67%, G loss: 1.982331\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 258, steps: 1, D loss: 0.216618, acc:  66%, G loss: 2.068188\n",
      "Ep: 258, steps: 2, D loss: 0.253535, acc:  52%, G loss: 1.801003\n",
      "Ep: 258, steps: 3, D loss: 0.141989, acc:  90%, G loss: 2.156835\n",
      "Ep: 258, steps: 4, D loss: 0.181391, acc:  84%, G loss: 2.053801\n",
      "Ep: 258, steps: 5, D loss: 0.247400, acc:  59%, G loss: 2.223849\n",
      "Ep: 258, steps: 6, D loss: 0.257195, acc:  50%, G loss: 1.943449\n",
      "Ep: 258, steps: 7, D loss: 0.404095, acc:  19%, G loss: 1.539335\n",
      "Ep: 258, steps: 8, D loss: 0.234006, acc:  59%, G loss: 2.064456\n",
      "Ep: 258, steps: 9, D loss: 0.186957, acc:  77%, G loss: 1.935453\n",
      "Ep: 258, steps: 10, D loss: 0.148335, acc:  84%, G loss: 1.908015\n",
      "Ep: 258, steps: 11, D loss: 0.244279, acc:  55%, G loss: 2.095399\n",
      "Ep: 258, steps: 12, D loss: 0.315931, acc:  40%, G loss: 1.675174\n",
      "Ep: 258, steps: 13, D loss: 0.282097, acc:  46%, G loss: 1.809613\n",
      "Ep: 258, steps: 14, D loss: 0.310404, acc:  36%, G loss: 1.796277\n",
      "Ep: 258, steps: 15, D loss: 0.227225, acc:  62%, G loss: 1.689908\n",
      "Ep: 258, steps: 16, D loss: 0.281424, acc:  48%, G loss: 1.864151\n",
      "Ep: 258, steps: 17, D loss: 0.163158, acc:  81%, G loss: 1.765311\n",
      "Ep: 258, steps: 18, D loss: 0.273238, acc:  54%, G loss: 1.900012\n",
      "Ep: 258, steps: 19, D loss: 0.194124, acc:  70%, G loss: 1.786667\n",
      "Ep: 258, steps: 20, D loss: 0.164494, acc:  78%, G loss: 1.970853\n",
      "Ep: 258, steps: 21, D loss: 0.281344, acc:  42%, G loss: 1.735906\n",
      "Ep: 258, steps: 22, D loss: 0.190173, acc:  67%, G loss: 2.031736\n",
      "Ep: 258, steps: 23, D loss: 0.181184, acc:  76%, G loss: 2.404468\n",
      "Ep: 258, steps: 24, D loss: 0.169100, acc:  77%, G loss: 2.008529\n",
      "Ep: 258, steps: 25, D loss: 0.225376, acc:  62%, G loss: 1.880666\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 259, steps: 1, D loss: 0.228551, acc:  64%, G loss: 2.125466\n",
      "Ep: 259, steps: 2, D loss: 0.246833, acc:  57%, G loss: 1.833873\n",
      "Ep: 259, steps: 3, D loss: 0.144946, acc:  89%, G loss: 2.149809\n",
      "Ep: 259, steps: 4, D loss: 0.183232, acc:  83%, G loss: 2.149273\n",
      "Ep: 259, steps: 5, D loss: 0.246387, acc:  57%, G loss: 2.074172\n",
      "Ep: 259, steps: 6, D loss: 0.260064, acc:  50%, G loss: 1.811875\n",
      "Ep: 259, steps: 7, D loss: 0.448232, acc:  13%, G loss: 2.098656\n",
      "Ep: 259, steps: 8, D loss: 0.230064, acc:  58%, G loss: 2.112871\n",
      "Ep: 259, steps: 9, D loss: 0.188634, acc:  76%, G loss: 2.033164\n",
      "Saved Model\n",
      "Ep: 259, steps: 10, D loss: 0.136957, acc:  90%, G loss: 1.842691\n",
      "Ep: 259, steps: 11, D loss: 0.296942, acc:  37%, G loss: 1.511328\n",
      "Ep: 259, steps: 12, D loss: 0.264392, acc:  48%, G loss: 1.700271\n",
      "Ep: 259, steps: 13, D loss: 0.295702, acc:  40%, G loss: 1.825775\n",
      "Ep: 259, steps: 14, D loss: 0.266320, acc:  45%, G loss: 1.819394\n",
      "Ep: 259, steps: 15, D loss: 0.278368, acc:  48%, G loss: 1.903115\n",
      "Ep: 259, steps: 16, D loss: 0.173349, acc:  75%, G loss: 1.839427\n",
      "Ep: 259, steps: 17, D loss: 0.257152, acc:  55%, G loss: 1.965726\n",
      "Ep: 259, steps: 18, D loss: 0.207182, acc:  65%, G loss: 1.828602\n",
      "Ep: 259, steps: 19, D loss: 0.173436, acc:  77%, G loss: 2.095483\n",
      "Ep: 259, steps: 20, D loss: 0.258267, acc:  49%, G loss: 1.859297\n",
      "Ep: 259, steps: 21, D loss: 0.183955, acc:  71%, G loss: 1.892260\n",
      "Ep: 259, steps: 22, D loss: 0.192129, acc:  73%, G loss: 2.220255\n",
      "Ep: 259, steps: 23, D loss: 0.170210, acc:  80%, G loss: 1.981394\n",
      "Ep: 259, steps: 24, D loss: 0.210288, acc:  64%, G loss: 1.851930\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 260, steps: 1, D loss: 0.244662, acc:  59%, G loss: 1.904420\n",
      "Ep: 260, steps: 2, D loss: 0.250962, acc:  55%, G loss: 1.772401\n",
      "Ep: 260, steps: 3, D loss: 0.153938, acc:  86%, G loss: 2.135687\n",
      "Ep: 260, steps: 4, D loss: 0.182674, acc:  84%, G loss: 2.027854\n",
      "Ep: 260, steps: 5, D loss: 0.230261, acc:  63%, G loss: 2.118610\n",
      "Ep: 260, steps: 6, D loss: 0.257206, acc:  51%, G loss: 1.837119\n",
      "Ep: 260, steps: 7, D loss: 0.361125, acc:  25%, G loss: 1.603846\n",
      "Ep: 260, steps: 8, D loss: 0.218518, acc:  64%, G loss: 2.435804\n",
      "Ep: 260, steps: 9, D loss: 0.203862, acc:  71%, G loss: 2.082180\n",
      "Ep: 260, steps: 10, D loss: 0.148321, acc:  84%, G loss: 1.916801\n",
      "Ep: 260, steps: 11, D loss: 0.252553, acc:  54%, G loss: 2.081244\n",
      "Ep: 260, steps: 12, D loss: 0.325541, acc:  31%, G loss: 1.604226\n",
      "Ep: 260, steps: 13, D loss: 0.294961, acc:  39%, G loss: 1.717483\n",
      "Ep: 260, steps: 14, D loss: 0.304615, acc:  34%, G loss: 1.769240\n",
      "Ep: 260, steps: 15, D loss: 0.226266, acc:  63%, G loss: 1.936940\n",
      "Ep: 260, steps: 16, D loss: 0.275364, acc:  49%, G loss: 1.905633\n",
      "Ep: 260, steps: 17, D loss: 0.170274, acc:  78%, G loss: 1.841061\n",
      "Ep: 260, steps: 18, D loss: 0.251673, acc:  55%, G loss: 1.953614\n",
      "Ep: 260, steps: 19, D loss: 0.183930, acc:  73%, G loss: 1.854966\n",
      "Ep: 260, steps: 20, D loss: 0.170853, acc:  79%, G loss: 1.994638\n",
      "Ep: 260, steps: 21, D loss: 0.314975, acc:  31%, G loss: 1.779282\n",
      "Ep: 260, steps: 22, D loss: 0.186657, acc:  68%, G loss: 1.856334\n",
      "Ep: 260, steps: 23, D loss: 0.185879, acc:  74%, G loss: 2.277099\n",
      "Ep: 260, steps: 24, D loss: 0.165622, acc:  78%, G loss: 1.895141\n",
      "Ep: 260, steps: 25, D loss: 0.218424, acc:  65%, G loss: 1.897249\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 261, steps: 1, D loss: 0.221481, acc:  65%, G loss: 2.051206\n",
      "Ep: 261, steps: 2, D loss: 0.240242, acc:  57%, G loss: 1.745260\n",
      "Ep: 261, steps: 3, D loss: 0.143766, acc:  90%, G loss: 2.185035\n",
      "Ep: 261, steps: 4, D loss: 0.179092, acc:  84%, G loss: 2.074766\n",
      "Ep: 261, steps: 5, D loss: 0.259756, acc:  58%, G loss: 2.021109\n",
      "Ep: 261, steps: 6, D loss: 0.260660, acc:  50%, G loss: 1.827885\n",
      "Ep: 261, steps: 7, D loss: 0.433130, acc:  13%, G loss: 1.788985\n",
      "Ep: 261, steps: 8, D loss: 0.252272, acc:  53%, G loss: 2.329068\n",
      "Ep: 261, steps: 9, D loss: 0.182393, acc:  77%, G loss: 2.038904\n",
      "Ep: 261, steps: 10, D loss: 0.146693, acc:  86%, G loss: 2.089675\n",
      "Ep: 261, steps: 11, D loss: 0.232986, acc:  59%, G loss: 2.032329\n",
      "Ep: 261, steps: 12, D loss: 0.331296, acc:  28%, G loss: 1.633768\n",
      "Ep: 261, steps: 13, D loss: 0.281000, acc:  45%, G loss: 1.727731\n",
      "Ep: 261, steps: 14, D loss: 0.305624, acc:  35%, G loss: 1.742572\n",
      "Ep: 261, steps: 15, D loss: 0.232274, acc:  61%, G loss: 1.873705\n",
      "Ep: 261, steps: 16, D loss: 0.276818, acc:  51%, G loss: 1.831596\n",
      "Ep: 261, steps: 17, D loss: 0.166051, acc:  80%, G loss: 1.801599\n",
      "Ep: 261, steps: 18, D loss: 0.239040, acc:  60%, G loss: 1.854273\n",
      "Ep: 261, steps: 19, D loss: 0.197882, acc:  70%, G loss: 1.842730\n",
      "Ep: 261, steps: 20, D loss: 0.167668, acc:  80%, G loss: 2.102824\n",
      "Ep: 261, steps: 21, D loss: 0.274384, acc:  46%, G loss: 1.806009\n",
      "Ep: 261, steps: 22, D loss: 0.191978, acc:  66%, G loss: 1.856856\n",
      "Ep: 261, steps: 23, D loss: 0.183620, acc:  76%, G loss: 2.243550\n",
      "Ep: 261, steps: 24, D loss: 0.175026, acc:  78%, G loss: 1.967409\n",
      "Ep: 261, steps: 25, D loss: 0.211523, acc:  65%, G loss: 1.924773\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 262, steps: 1, D loss: 0.224690, acc:  66%, G loss: 2.129174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 262, steps: 2, D loss: 0.258825, acc:  53%, G loss: 1.817661\n",
      "Ep: 262, steps: 3, D loss: 0.150725, acc:  87%, G loss: 2.215558\n",
      "Ep: 262, steps: 4, D loss: 0.184623, acc:  82%, G loss: 2.024159\n",
      "Ep: 262, steps: 5, D loss: 0.223232, acc:  64%, G loss: 1.991518\n",
      "Ep: 262, steps: 6, D loss: 0.263782, acc:  49%, G loss: 1.823966\n",
      "Ep: 262, steps: 7, D loss: 0.387193, acc:  25%, G loss: 1.530631\n",
      "Saved Model\n",
      "Ep: 262, steps: 8, D loss: 0.218550, acc:  63%, G loss: 2.189036\n",
      "Ep: 262, steps: 9, D loss: 0.171441, acc:  79%, G loss: 1.749318\n",
      "Ep: 262, steps: 10, D loss: 0.282598, acc:  46%, G loss: 1.941935\n",
      "Ep: 262, steps: 11, D loss: 0.307728, acc:  37%, G loss: 1.618661\n",
      "Ep: 262, steps: 12, D loss: 0.276803, acc:  46%, G loss: 1.775429\n",
      "Ep: 262, steps: 13, D loss: 0.305440, acc:  32%, G loss: 1.884037\n",
      "Ep: 262, steps: 14, D loss: 0.236660, acc:  58%, G loss: 1.882082\n",
      "Ep: 262, steps: 15, D loss: 0.260496, acc:  56%, G loss: 1.818089\n",
      "Ep: 262, steps: 16, D loss: 0.146119, acc:  86%, G loss: 1.776981\n",
      "Ep: 262, steps: 17, D loss: 0.245476, acc:  58%, G loss: 1.908440\n",
      "Ep: 262, steps: 18, D loss: 0.210580, acc:  65%, G loss: 1.792359\n",
      "Ep: 262, steps: 19, D loss: 0.186582, acc:  76%, G loss: 1.951217\n",
      "Ep: 262, steps: 20, D loss: 0.300038, acc:  31%, G loss: 1.737517\n",
      "Ep: 262, steps: 21, D loss: 0.216909, acc:  60%, G loss: 1.871973\n",
      "Ep: 262, steps: 22, D loss: 0.196408, acc:  71%, G loss: 2.189260\n",
      "Ep: 262, steps: 23, D loss: 0.166598, acc:  82%, G loss: 1.987305\n",
      "Ep: 262, steps: 24, D loss: 0.233045, acc:  59%, G loss: 1.943874\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 263, steps: 1, D loss: 0.219159, acc:  67%, G loss: 2.128587\n",
      "Ep: 263, steps: 2, D loss: 0.249050, acc:  57%, G loss: 1.823696\n",
      "Ep: 263, steps: 3, D loss: 0.154596, acc:  88%, G loss: 2.101514\n",
      "Ep: 263, steps: 4, D loss: 0.184517, acc:  82%, G loss: 1.899137\n",
      "Ep: 263, steps: 5, D loss: 0.263455, acc:  58%, G loss: 2.096429\n",
      "Ep: 263, steps: 6, D loss: 0.251347, acc:  51%, G loss: 1.811958\n",
      "Ep: 263, steps: 7, D loss: 0.398002, acc:  21%, G loss: 1.584833\n",
      "Ep: 263, steps: 8, D loss: 0.235006, acc:  58%, G loss: 1.999275\n",
      "Ep: 263, steps: 9, D loss: 0.190263, acc:  74%, G loss: 2.088292\n",
      "Ep: 263, steps: 10, D loss: 0.149871, acc:  86%, G loss: 1.862095\n",
      "Ep: 263, steps: 11, D loss: 0.242749, acc:  59%, G loss: 2.052521\n",
      "Ep: 263, steps: 12, D loss: 0.313231, acc:  38%, G loss: 1.618852\n",
      "Ep: 263, steps: 13, D loss: 0.299466, acc:  37%, G loss: 1.698778\n",
      "Ep: 263, steps: 14, D loss: 0.300891, acc:  34%, G loss: 1.686450\n",
      "Ep: 263, steps: 15, D loss: 0.229133, acc:  61%, G loss: 1.830198\n",
      "Ep: 263, steps: 16, D loss: 0.266895, acc:  54%, G loss: 1.783295\n",
      "Ep: 263, steps: 17, D loss: 0.164066, acc:  82%, G loss: 1.907134\n",
      "Ep: 263, steps: 18, D loss: 0.236860, acc:  64%, G loss: 1.862586\n",
      "Ep: 263, steps: 19, D loss: 0.187236, acc:  71%, G loss: 1.882493\n",
      "Ep: 263, steps: 20, D loss: 0.195712, acc:  72%, G loss: 2.058362\n",
      "Ep: 263, steps: 21, D loss: 0.277205, acc:  42%, G loss: 1.674972\n",
      "Ep: 263, steps: 22, D loss: 0.212310, acc:  61%, G loss: 1.876416\n",
      "Ep: 263, steps: 23, D loss: 0.201662, acc:  67%, G loss: 2.361179\n",
      "Ep: 263, steps: 24, D loss: 0.179465, acc:  76%, G loss: 1.991288\n",
      "Ep: 263, steps: 25, D loss: 0.219452, acc:  65%, G loss: 1.884471\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 264, steps: 1, D loss: 0.227851, acc:  63%, G loss: 2.074592\n",
      "Ep: 264, steps: 2, D loss: 0.245865, acc:  56%, G loss: 1.814799\n",
      "Ep: 264, steps: 3, D loss: 0.148730, acc:  90%, G loss: 2.226431\n",
      "Ep: 264, steps: 4, D loss: 0.182799, acc:  83%, G loss: 1.864652\n",
      "Ep: 264, steps: 5, D loss: 0.261725, acc:  56%, G loss: 1.993409\n",
      "Ep: 264, steps: 6, D loss: 0.264105, acc:  50%, G loss: 1.824321\n",
      "Ep: 264, steps: 7, D loss: 0.409431, acc:  18%, G loss: 1.726335\n",
      "Ep: 264, steps: 8, D loss: 0.237079, acc:  58%, G loss: 2.136823\n",
      "Ep: 264, steps: 9, D loss: 0.178351, acc:  80%, G loss: 2.059348\n",
      "Ep: 264, steps: 10, D loss: 0.136945, acc:  90%, G loss: 2.031281\n",
      "Ep: 264, steps: 11, D loss: 0.253123, acc:  52%, G loss: 2.195482\n",
      "Ep: 264, steps: 12, D loss: 0.318576, acc:  34%, G loss: 1.650137\n",
      "Ep: 264, steps: 13, D loss: 0.287654, acc:  41%, G loss: 1.735331\n",
      "Ep: 264, steps: 14, D loss: 0.302624, acc:  35%, G loss: 1.785743\n",
      "Ep: 264, steps: 15, D loss: 0.222295, acc:  63%, G loss: 1.863893\n",
      "Ep: 264, steps: 16, D loss: 0.266732, acc:  51%, G loss: 1.849610\n",
      "Ep: 264, steps: 17, D loss: 0.189261, acc:  76%, G loss: 1.795894\n",
      "Ep: 264, steps: 18, D loss: 0.262003, acc:  58%, G loss: 1.950251\n",
      "Ep: 264, steps: 19, D loss: 0.194191, acc:  70%, G loss: 1.829068\n",
      "Ep: 264, steps: 20, D loss: 0.158352, acc:  82%, G loss: 2.026272\n",
      "Ep: 264, steps: 21, D loss: 0.277106, acc:  45%, G loss: 1.733557\n",
      "Ep: 264, steps: 22, D loss: 0.192225, acc:  67%, G loss: 1.858175\n",
      "Ep: 264, steps: 23, D loss: 0.185295, acc:  74%, G loss: 2.317323\n",
      "Ep: 264, steps: 24, D loss: 0.177949, acc:  77%, G loss: 1.960199\n",
      "Ep: 264, steps: 25, D loss: 0.216615, acc:  63%, G loss: 1.869287\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 265, steps: 1, D loss: 0.214874, acc:  68%, G loss: 2.012718\n",
      "Ep: 265, steps: 2, D loss: 0.258494, acc:  55%, G loss: 1.768247\n",
      "Ep: 265, steps: 3, D loss: 0.146433, acc:  89%, G loss: 2.164158\n",
      "Ep: 265, steps: 4, D loss: 0.179258, acc:  84%, G loss: 2.003708\n",
      "Ep: 265, steps: 5, D loss: 0.246182, acc:  59%, G loss: 2.130042\n",
      "Saved Model\n",
      "Ep: 265, steps: 6, D loss: 0.255884, acc:  51%, G loss: 1.819836\n",
      "Ep: 265, steps: 7, D loss: 0.259225, acc:  47%, G loss: 2.139132\n",
      "Ep: 265, steps: 8, D loss: 0.127323, acc:  89%, G loss: 2.117690\n",
      "Ep: 265, steps: 9, D loss: 0.108484, acc:  94%, G loss: 1.997031\n",
      "Ep: 265, steps: 10, D loss: 0.206189, acc:  70%, G loss: 2.136992\n",
      "Ep: 265, steps: 11, D loss: 0.407124, acc:  20%, G loss: 1.645523\n",
      "Ep: 265, steps: 12, D loss: 0.327602, acc:  35%, G loss: 1.739831\n",
      "Ep: 265, steps: 13, D loss: 0.315158, acc:  35%, G loss: 1.643636\n",
      "Ep: 265, steps: 14, D loss: 0.223864, acc:  62%, G loss: 1.871350\n",
      "Ep: 265, steps: 15, D loss: 0.263725, acc:  53%, G loss: 1.922359\n",
      "Ep: 265, steps: 16, D loss: 0.153771, acc:  83%, G loss: 1.943531\n",
      "Ep: 265, steps: 17, D loss: 0.234040, acc:  61%, G loss: 1.954349\n",
      "Ep: 265, steps: 18, D loss: 0.175098, acc:  77%, G loss: 1.912224\n",
      "Ep: 265, steps: 19, D loss: 0.177799, acc:  77%, G loss: 1.995682\n",
      "Ep: 265, steps: 20, D loss: 0.313293, acc:  33%, G loss: 1.776874\n",
      "Ep: 265, steps: 21, D loss: 0.225660, acc:  59%, G loss: 1.840476\n",
      "Ep: 265, steps: 22, D loss: 0.169235, acc:  79%, G loss: 2.219533\n",
      "Ep: 265, steps: 23, D loss: 0.155441, acc:  80%, G loss: 1.921186\n",
      "Ep: 265, steps: 24, D loss: 0.223894, acc:  63%, G loss: 1.818434\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 266, steps: 1, D loss: 0.208815, acc:  68%, G loss: 2.018471\n",
      "Ep: 266, steps: 2, D loss: 0.251718, acc:  55%, G loss: 1.694506\n",
      "Ep: 266, steps: 3, D loss: 0.135529, acc:  91%, G loss: 2.190832\n",
      "Ep: 266, steps: 4, D loss: 0.170040, acc:  86%, G loss: 1.974836\n",
      "Ep: 266, steps: 5, D loss: 0.260723, acc:  55%, G loss: 2.058165\n",
      "Ep: 266, steps: 6, D loss: 0.256294, acc:  51%, G loss: 1.856978\n",
      "Ep: 266, steps: 7, D loss: 0.463764, acc:  15%, G loss: 1.642337\n",
      "Ep: 266, steps: 8, D loss: 0.236388, acc:  58%, G loss: 2.043991\n",
      "Ep: 266, steps: 9, D loss: 0.193705, acc:  74%, G loss: 1.963927\n",
      "Ep: 266, steps: 10, D loss: 0.166130, acc:  80%, G loss: 1.854771\n",
      "Ep: 266, steps: 11, D loss: 0.226728, acc:  63%, G loss: 2.059200\n",
      "Ep: 266, steps: 12, D loss: 0.321936, acc:  32%, G loss: 1.686243\n",
      "Ep: 266, steps: 13, D loss: 0.294129, acc:  43%, G loss: 1.723749\n",
      "Ep: 266, steps: 14, D loss: 0.310771, acc:  34%, G loss: 1.760469\n",
      "Ep: 266, steps: 15, D loss: 0.225365, acc:  63%, G loss: 1.947978\n",
      "Ep: 266, steps: 16, D loss: 0.284731, acc:  48%, G loss: 1.791863\n",
      "Ep: 266, steps: 17, D loss: 0.164977, acc:  82%, G loss: 1.808081\n",
      "Ep: 266, steps: 18, D loss: 0.235091, acc:  61%, G loss: 1.845989\n",
      "Ep: 266, steps: 19, D loss: 0.193794, acc:  70%, G loss: 1.804983\n",
      "Ep: 266, steps: 20, D loss: 0.201047, acc:  71%, G loss: 2.110763\n",
      "Ep: 266, steps: 21, D loss: 0.293700, acc:  38%, G loss: 1.925851\n",
      "Ep: 266, steps: 22, D loss: 0.203657, acc:  65%, G loss: 1.856151\n",
      "Ep: 266, steps: 23, D loss: 0.178778, acc:  77%, G loss: 2.164268\n",
      "Ep: 266, steps: 24, D loss: 0.176050, acc:  78%, G loss: 1.963601\n",
      "Ep: 266, steps: 25, D loss: 0.217177, acc:  66%, G loss: 1.902493\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 267, steps: 1, D loss: 0.221979, acc:  65%, G loss: 1.981520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 267, steps: 2, D loss: 0.234544, acc:  62%, G loss: 1.688056\n",
      "Ep: 267, steps: 3, D loss: 0.146478, acc:  89%, G loss: 2.214668\n",
      "Ep: 267, steps: 4, D loss: 0.170761, acc:  87%, G loss: 1.835203\n",
      "Ep: 267, steps: 5, D loss: 0.224921, acc:  64%, G loss: 2.056297\n",
      "Ep: 267, steps: 6, D loss: 0.258973, acc:  49%, G loss: 1.762159\n",
      "Ep: 267, steps: 7, D loss: 0.372224, acc:  24%, G loss: 1.652221\n",
      "Ep: 267, steps: 8, D loss: 0.234289, acc:  58%, G loss: 2.025306\n",
      "Ep: 267, steps: 9, D loss: 0.200034, acc:  71%, G loss: 1.901295\n",
      "Ep: 267, steps: 10, D loss: 0.143644, acc:  87%, G loss: 1.858877\n",
      "Ep: 267, steps: 11, D loss: 0.240632, acc:  59%, G loss: 2.056631\n",
      "Ep: 267, steps: 12, D loss: 0.321933, acc:  36%, G loss: 1.570457\n",
      "Ep: 267, steps: 13, D loss: 0.294957, acc:  38%, G loss: 1.758281\n",
      "Ep: 267, steps: 14, D loss: 0.307608, acc:  33%, G loss: 1.704111\n",
      "Ep: 267, steps: 15, D loss: 0.238397, acc:  55%, G loss: 1.918322\n",
      "Ep: 267, steps: 16, D loss: 0.250843, acc:  56%, G loss: 1.925523\n",
      "Ep: 267, steps: 17, D loss: 0.165917, acc:  82%, G loss: 1.958746\n",
      "Ep: 267, steps: 18, D loss: 0.226584, acc:  65%, G loss: 1.888122\n",
      "Ep: 267, steps: 19, D loss: 0.189398, acc:  72%, G loss: 1.800144\n",
      "Ep: 267, steps: 20, D loss: 0.161478, acc:  82%, G loss: 1.932710\n",
      "Ep: 267, steps: 21, D loss: 0.277479, acc:  40%, G loss: 1.816009\n",
      "Ep: 267, steps: 22, D loss: 0.190383, acc:  69%, G loss: 1.863231\n",
      "Ep: 267, steps: 23, D loss: 0.186481, acc:  75%, G loss: 2.234655\n",
      "Ep: 267, steps: 24, D loss: 0.171785, acc:  78%, G loss: 1.937694\n",
      "Ep: 267, steps: 25, D loss: 0.204722, acc:  66%, G loss: 1.884415\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 268, steps: 1, D loss: 0.215213, acc:  67%, G loss: 2.039612\n",
      "Ep: 268, steps: 2, D loss: 0.264805, acc:  52%, G loss: 1.760917\n",
      "Ep: 268, steps: 3, D loss: 0.139872, acc:  91%, G loss: 2.163502\n",
      "Saved Model\n",
      "Ep: 268, steps: 4, D loss: 0.169876, acc:  86%, G loss: 2.067537\n",
      "Ep: 268, steps: 5, D loss: 0.259450, acc:  50%, G loss: 1.859874\n",
      "Ep: 268, steps: 6, D loss: 0.438052, acc:  16%, G loss: 1.562149\n",
      "Ep: 268, steps: 7, D loss: 0.233901, acc:  60%, G loss: 2.083600\n",
      "Ep: 268, steps: 8, D loss: 0.164806, acc:  83%, G loss: 1.878106\n",
      "Ep: 268, steps: 9, D loss: 0.139029, acc:  88%, G loss: 1.782503\n",
      "Ep: 268, steps: 10, D loss: 0.218395, acc:  66%, G loss: 2.062653\n",
      "Ep: 268, steps: 11, D loss: 0.319239, acc:  35%, G loss: 1.624703\n",
      "Ep: 268, steps: 12, D loss: 0.309384, acc:  35%, G loss: 1.652101\n",
      "Ep: 268, steps: 13, D loss: 0.341705, acc:  27%, G loss: 1.896504\n",
      "Ep: 268, steps: 14, D loss: 0.232214, acc:  60%, G loss: 1.801779\n",
      "Ep: 268, steps: 15, D loss: 0.267762, acc:  52%, G loss: 1.822739\n",
      "Ep: 268, steps: 16, D loss: 0.191597, acc:  74%, G loss: 1.852262\n",
      "Ep: 268, steps: 17, D loss: 0.256473, acc:  57%, G loss: 1.856824\n",
      "Ep: 268, steps: 18, D loss: 0.193277, acc:  70%, G loss: 1.824304\n",
      "Ep: 268, steps: 19, D loss: 0.179556, acc:  77%, G loss: 1.960226\n",
      "Ep: 268, steps: 20, D loss: 0.304567, acc:  35%, G loss: 1.858066\n",
      "Ep: 268, steps: 21, D loss: 0.191640, acc:  65%, G loss: 1.889785\n",
      "Ep: 268, steps: 22, D loss: 0.202266, acc:  70%, G loss: 2.241011\n",
      "Ep: 268, steps: 23, D loss: 0.163580, acc:  80%, G loss: 1.981638\n",
      "Ep: 268, steps: 24, D loss: 0.218546, acc:  64%, G loss: 1.966882\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 269, steps: 1, D loss: 0.217139, acc:  68%, G loss: 1.942971\n",
      "Ep: 269, steps: 2, D loss: 0.256369, acc:  51%, G loss: 1.706810\n",
      "Ep: 269, steps: 3, D loss: 0.148494, acc:  89%, G loss: 2.149635\n",
      "Ep: 269, steps: 4, D loss: 0.182049, acc:  83%, G loss: 2.020099\n",
      "Ep: 269, steps: 5, D loss: 0.243340, acc:  60%, G loss: 1.979616\n",
      "Ep: 269, steps: 6, D loss: 0.248413, acc:  51%, G loss: 1.862177\n",
      "Ep: 269, steps: 7, D loss: 0.381453, acc:  27%, G loss: 1.616323\n",
      "Ep: 269, steps: 8, D loss: 0.248283, acc:  53%, G loss: 2.063803\n",
      "Ep: 269, steps: 9, D loss: 0.196706, acc:  73%, G loss: 2.022869\n",
      "Ep: 269, steps: 10, D loss: 0.155229, acc:  84%, G loss: 1.850448\n",
      "Ep: 269, steps: 11, D loss: 0.242800, acc:  57%, G loss: 2.073205\n",
      "Ep: 269, steps: 12, D loss: 0.311880, acc:  37%, G loss: 1.634250\n",
      "Ep: 269, steps: 13, D loss: 0.293203, acc:  41%, G loss: 1.715748\n",
      "Ep: 269, steps: 14, D loss: 0.298538, acc:  38%, G loss: 1.708948\n",
      "Ep: 269, steps: 15, D loss: 0.232075, acc:  60%, G loss: 1.848007\n",
      "Ep: 269, steps: 16, D loss: 0.267653, acc:  51%, G loss: 1.885031\n",
      "Ep: 269, steps: 17, D loss: 0.185994, acc:  74%, G loss: 1.885567\n",
      "Ep: 269, steps: 18, D loss: 0.234133, acc:  64%, G loss: 1.854955\n",
      "Ep: 269, steps: 19, D loss: 0.181870, acc:  75%, G loss: 1.837701\n",
      "Ep: 269, steps: 20, D loss: 0.182504, acc:  74%, G loss: 2.286538\n",
      "Ep: 269, steps: 21, D loss: 0.256990, acc:  46%, G loss: 1.771449\n",
      "Ep: 269, steps: 22, D loss: 0.204839, acc:  62%, G loss: 1.911717\n",
      "Ep: 269, steps: 23, D loss: 0.189813, acc:  74%, G loss: 2.269206\n",
      "Ep: 269, steps: 24, D loss: 0.173366, acc:  79%, G loss: 1.963319\n",
      "Ep: 269, steps: 25, D loss: 0.211261, acc:  66%, G loss: 1.781978\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 270, steps: 1, D loss: 0.228800, acc:  62%, G loss: 1.984446\n",
      "Ep: 270, steps: 2, D loss: 0.242408, acc:  57%, G loss: 1.711053\n",
      "Ep: 270, steps: 3, D loss: 0.143337, acc:  91%, G loss: 2.154308\n",
      "Ep: 270, steps: 4, D loss: 0.172395, acc:  85%, G loss: 1.930426\n",
      "Ep: 270, steps: 5, D loss: 0.291806, acc:  49%, G loss: 2.040207\n",
      "Ep: 270, steps: 6, D loss: 0.268586, acc:  49%, G loss: 1.851628\n",
      "Ep: 270, steps: 7, D loss: 0.428894, acc:  14%, G loss: 1.544438\n",
      "Ep: 270, steps: 8, D loss: 0.239756, acc:  57%, G loss: 2.277929\n",
      "Ep: 270, steps: 9, D loss: 0.177141, acc:  81%, G loss: 1.989727\n",
      "Ep: 270, steps: 10, D loss: 0.144362, acc:  86%, G loss: 1.962992\n",
      "Ep: 270, steps: 11, D loss: 0.252631, acc:  54%, G loss: 2.137765\n",
      "Ep: 270, steps: 12, D loss: 0.334568, acc:  26%, G loss: 1.666631\n",
      "Ep: 270, steps: 13, D loss: 0.289429, acc:  44%, G loss: 1.711334\n",
      "Ep: 270, steps: 14, D loss: 0.292547, acc:  36%, G loss: 1.717195\n",
      "Ep: 270, steps: 15, D loss: 0.234447, acc:  60%, G loss: 1.939545\n",
      "Ep: 270, steps: 16, D loss: 0.258583, acc:  54%, G loss: 1.932873\n",
      "Ep: 270, steps: 17, D loss: 0.162629, acc:  82%, G loss: 1.812233\n",
      "Ep: 270, steps: 18, D loss: 0.259431, acc:  56%, G loss: 1.888237\n",
      "Ep: 270, steps: 19, D loss: 0.184633, acc:  73%, G loss: 1.795242\n",
      "Ep: 270, steps: 20, D loss: 0.157607, acc:  80%, G loss: 2.021027\n",
      "Ep: 270, steps: 21, D loss: 0.326925, acc:  31%, G loss: 1.703375\n",
      "Ep: 270, steps: 22, D loss: 0.196929, acc:  64%, G loss: 1.850846\n",
      "Ep: 270, steps: 23, D loss: 0.202850, acc:  69%, G loss: 2.195979\n",
      "Ep: 270, steps: 24, D loss: 0.173763, acc:  77%, G loss: 1.914516\n",
      "Ep: 270, steps: 25, D loss: 0.219826, acc:  65%, G loss: 1.967709\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 271, steps: 1, D loss: 0.216299, acc:  66%, G loss: 2.029850\n",
      "Saved Model\n",
      "Ep: 271, steps: 2, D loss: 0.264080, acc:  50%, G loss: 1.713836\n",
      "Ep: 271, steps: 3, D loss: 0.200896, acc:  77%, G loss: 1.955458\n",
      "Ep: 271, steps: 4, D loss: 0.251631, acc:  58%, G loss: 2.086061\n",
      "Ep: 271, steps: 5, D loss: 0.249244, acc:  51%, G loss: 1.712883\n",
      "Ep: 271, steps: 6, D loss: 0.387140, acc:  22%, G loss: 1.984068\n",
      "Ep: 271, steps: 7, D loss: 0.236087, acc:  58%, G loss: 2.268929\n",
      "Ep: 271, steps: 8, D loss: 0.175979, acc:  82%, G loss: 1.896493\n",
      "Ep: 271, steps: 9, D loss: 0.161269, acc:  82%, G loss: 1.817345\n",
      "Ep: 271, steps: 10, D loss: 0.236558, acc:  59%, G loss: 2.022489\n",
      "Ep: 271, steps: 11, D loss: 0.322992, acc:  33%, G loss: 1.602032\n",
      "Ep: 271, steps: 12, D loss: 0.291072, acc:  43%, G loss: 1.678986\n",
      "Ep: 271, steps: 13, D loss: 0.281647, acc:  42%, G loss: 1.736114\n",
      "Ep: 271, steps: 14, D loss: 0.226529, acc:  62%, G loss: 1.940060\n",
      "Ep: 271, steps: 15, D loss: 0.269787, acc:  53%, G loss: 1.820528\n",
      "Ep: 271, steps: 16, D loss: 0.155178, acc:  85%, G loss: 1.715842\n",
      "Ep: 271, steps: 17, D loss: 0.254914, acc:  57%, G loss: 1.902735\n",
      "Ep: 271, steps: 18, D loss: 0.201712, acc:  68%, G loss: 1.831639\n",
      "Ep: 271, steps: 19, D loss: 0.167704, acc:  81%, G loss: 2.014127\n",
      "Ep: 271, steps: 20, D loss: 0.288060, acc:  38%, G loss: 1.739770\n",
      "Ep: 271, steps: 21, D loss: 0.185314, acc:  70%, G loss: 1.789579\n",
      "Ep: 271, steps: 22, D loss: 0.205387, acc:  68%, G loss: 2.144022\n",
      "Ep: 271, steps: 23, D loss: 0.165452, acc:  81%, G loss: 1.871731\n",
      "Ep: 271, steps: 24, D loss: 0.212496, acc:  66%, G loss: 1.919571\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 272, steps: 1, D loss: 0.220213, acc:  64%, G loss: 2.110219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 272, steps: 2, D loss: 0.260259, acc:  51%, G loss: 1.730800\n",
      "Ep: 272, steps: 3, D loss: 0.163419, acc:  85%, G loss: 2.084175\n",
      "Ep: 272, steps: 4, D loss: 0.189122, acc:  81%, G loss: 2.093632\n",
      "Ep: 272, steps: 5, D loss: 0.254079, acc:  58%, G loss: 2.113077\n",
      "Ep: 272, steps: 6, D loss: 0.272467, acc:  48%, G loss: 1.777747\n",
      "Ep: 272, steps: 7, D loss: 0.383392, acc:  22%, G loss: 1.481876\n",
      "Ep: 272, steps: 8, D loss: 0.231563, acc:  58%, G loss: 2.103365\n",
      "Ep: 272, steps: 9, D loss: 0.188887, acc:  75%, G loss: 1.921203\n",
      "Ep: 272, steps: 10, D loss: 0.148671, acc:  87%, G loss: 1.937929\n",
      "Ep: 272, steps: 11, D loss: 0.243481, acc:  56%, G loss: 2.056813\n",
      "Ep: 272, steps: 12, D loss: 0.320027, acc:  34%, G loss: 1.614770\n",
      "Ep: 272, steps: 13, D loss: 0.284867, acc:  42%, G loss: 1.767071\n",
      "Ep: 272, steps: 14, D loss: 0.296000, acc:  35%, G loss: 1.771404\n",
      "Ep: 272, steps: 15, D loss: 0.233599, acc:  59%, G loss: 1.864795\n",
      "Ep: 272, steps: 16, D loss: 0.259066, acc:  53%, G loss: 1.810756\n",
      "Ep: 272, steps: 17, D loss: 0.168299, acc:  82%, G loss: 1.843090\n",
      "Ep: 272, steps: 18, D loss: 0.243736, acc:  60%, G loss: 1.888131\n",
      "Ep: 272, steps: 19, D loss: 0.193093, acc:  69%, G loss: 1.763231\n",
      "Ep: 272, steps: 20, D loss: 0.193452, acc:  73%, G loss: 1.979671\n",
      "Ep: 272, steps: 21, D loss: 0.312708, acc:  31%, G loss: 1.926934\n",
      "Ep: 272, steps: 22, D loss: 0.202540, acc:  64%, G loss: 1.871620\n",
      "Ep: 272, steps: 23, D loss: 0.207824, acc:  64%, G loss: 2.261528\n",
      "Ep: 272, steps: 24, D loss: 0.164551, acc:  81%, G loss: 1.923553\n",
      "Ep: 272, steps: 25, D loss: 0.225887, acc:  64%, G loss: 2.025961\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 273, steps: 1, D loss: 0.211689, acc:  68%, G loss: 1.954241\n",
      "Ep: 273, steps: 2, D loss: 0.252972, acc:  55%, G loss: 1.645089\n",
      "Ep: 273, steps: 3, D loss: 0.146275, acc:  90%, G loss: 2.167361\n",
      "Ep: 273, steps: 4, D loss: 0.179895, acc:  85%, G loss: 2.090134\n",
      "Ep: 273, steps: 5, D loss: 0.263643, acc:  53%, G loss: 2.064002\n",
      "Ep: 273, steps: 6, D loss: 0.263752, acc:  51%, G loss: 1.836990\n",
      "Ep: 273, steps: 7, D loss: 0.388266, acc:  23%, G loss: 1.624061\n",
      "Ep: 273, steps: 8, D loss: 0.235126, acc:  58%, G loss: 2.213307\n",
      "Ep: 273, steps: 9, D loss: 0.177783, acc:  79%, G loss: 2.036128\n",
      "Ep: 273, steps: 10, D loss: 0.155377, acc:  83%, G loss: 1.853835\n",
      "Ep: 273, steps: 11, D loss: 0.233561, acc:  60%, G loss: 2.088514\n",
      "Ep: 273, steps: 12, D loss: 0.312288, acc:  37%, G loss: 1.584433\n",
      "Ep: 273, steps: 13, D loss: 0.291926, acc:  42%, G loss: 1.672015\n",
      "Ep: 273, steps: 14, D loss: 0.311710, acc:  32%, G loss: 1.751344\n",
      "Ep: 273, steps: 15, D loss: 0.236250, acc:  56%, G loss: 1.970265\n",
      "Ep: 273, steps: 16, D loss: 0.252862, acc:  55%, G loss: 1.856908\n",
      "Ep: 273, steps: 17, D loss: 0.172007, acc:  81%, G loss: 2.102559\n",
      "Ep: 273, steps: 18, D loss: 0.240376, acc:  61%, G loss: 1.882246\n",
      "Ep: 273, steps: 19, D loss: 0.191416, acc:  71%, G loss: 1.846138\n",
      "Ep: 273, steps: 20, D loss: 0.190412, acc:  72%, G loss: 2.123418\n",
      "Ep: 273, steps: 21, D loss: 0.277325, acc:  42%, G loss: 1.693779\n",
      "Ep: 273, steps: 22, D loss: 0.188219, acc:  68%, G loss: 1.786307\n",
      "Ep: 273, steps: 23, D loss: 0.190129, acc:  73%, G loss: 2.220997\n",
      "Ep: 273, steps: 24, D loss: 0.173521, acc:  77%, G loss: 1.820436\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 273, steps: 25, D loss: 0.213733, acc:  66%, G loss: 1.843485\n",
      "Ep: 273, steps: 26, D loss: 0.251721, acc:  55%, G loss: 1.614246\n",
      "Ep: 273, steps: 27, D loss: 0.149247, acc:  85%, G loss: 2.161671\n",
      "Ep: 273, steps: 28, D loss: 0.205979, acc:  75%, G loss: 1.911211\n",
      "Ep: 273, steps: 29, D loss: 0.261174, acc:  54%, G loss: 2.267724\n",
      "Ep: 273, steps: 30, D loss: 0.264786, acc:  50%, G loss: 1.797250\n",
      "Ep: 273, steps: 31, D loss: 0.398485, acc:  18%, G loss: 1.617161\n",
      "Ep: 273, steps: 32, D loss: 0.232874, acc:  59%, G loss: 2.318924\n",
      "Ep: 273, steps: 33, D loss: 0.175207, acc:  81%, G loss: 1.988237\n",
      "Ep: 273, steps: 34, D loss: 0.147433, acc:  86%, G loss: 1.912480\n",
      "Ep: 273, steps: 35, D loss: 0.254999, acc:  55%, G loss: 2.041940\n",
      "Ep: 273, steps: 36, D loss: 0.329448, acc:  32%, G loss: 1.600889\n",
      "Ep: 273, steps: 37, D loss: 0.288035, acc:  45%, G loss: 1.757994\n",
      "Ep: 273, steps: 38, D loss: 0.296419, acc:  37%, G loss: 1.766687\n",
      "Ep: 273, steps: 39, D loss: 0.239622, acc:  58%, G loss: 1.914008\n",
      "Ep: 273, steps: 40, D loss: 0.274161, acc:  49%, G loss: 1.867020\n",
      "Ep: 273, steps: 41, D loss: 0.162555, acc:  85%, G loss: 1.805252\n",
      "Ep: 273, steps: 42, D loss: 0.263000, acc:  53%, G loss: 1.815615\n",
      "Ep: 273, steps: 43, D loss: 0.207904, acc:  66%, G loss: 1.807523\n",
      "Ep: 273, steps: 44, D loss: 0.153312, acc:  85%, G loss: 1.946967\n",
      "Ep: 273, steps: 45, D loss: 0.290395, acc:  40%, G loss: 1.810435\n",
      "Ep: 273, steps: 46, D loss: 0.190825, acc:  67%, G loss: 1.810688\n",
      "Ep: 273, steps: 47, D loss: 0.173557, acc:  77%, G loss: 2.200227\n",
      "Ep: 273, steps: 48, D loss: 0.172632, acc:  79%, G loss: 1.906280\n",
      "Ep: 273, steps: 49, D loss: 0.211254, acc:  66%, G loss: 1.936153\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 274, steps: 1, D loss: 0.231229, acc:  61%, G loss: 2.059489\n",
      "Ep: 274, steps: 2, D loss: 0.246076, acc:  57%, G loss: 1.677212\n",
      "Ep: 274, steps: 3, D loss: 0.149719, acc:  88%, G loss: 2.211592\n",
      "Ep: 274, steps: 4, D loss: 0.185779, acc:  81%, G loss: 1.865775\n",
      "Ep: 274, steps: 5, D loss: 0.273427, acc:  54%, G loss: 2.230256\n",
      "Ep: 274, steps: 6, D loss: 0.266454, acc:  49%, G loss: 1.864370\n",
      "Ep: 274, steps: 7, D loss: 0.405627, acc:  22%, G loss: 1.604322\n",
      "Ep: 274, steps: 8, D loss: 0.243989, acc:  54%, G loss: 2.082096\n",
      "Ep: 274, steps: 9, D loss: 0.188452, acc:  75%, G loss: 1.947544\n",
      "Ep: 274, steps: 10, D loss: 0.158130, acc:  82%, G loss: 1.940673\n",
      "Ep: 274, steps: 11, D loss: 0.229875, acc:  61%, G loss: 2.090432\n",
      "Ep: 274, steps: 12, D loss: 0.325086, acc:  34%, G loss: 1.708989\n",
      "Ep: 274, steps: 13, D loss: 0.286490, acc:  40%, G loss: 1.724051\n",
      "Ep: 274, steps: 14, D loss: 0.291730, acc:  38%, G loss: 1.651384\n",
      "Ep: 274, steps: 15, D loss: 0.236117, acc:  59%, G loss: 1.952280\n",
      "Ep: 274, steps: 16, D loss: 0.257431, acc:  54%, G loss: 1.897328\n",
      "Ep: 274, steps: 17, D loss: 0.164133, acc:  82%, G loss: 1.791977\n",
      "Ep: 274, steps: 18, D loss: 0.232560, acc:  61%, G loss: 1.858379\n",
      "Ep: 274, steps: 19, D loss: 0.196000, acc:  70%, G loss: 1.763413\n",
      "Ep: 274, steps: 20, D loss: 0.203279, acc:  69%, G loss: 1.985973\n",
      "Ep: 274, steps: 21, D loss: 0.318009, acc:  28%, G loss: 1.907158\n",
      "Ep: 274, steps: 22, D loss: 0.197406, acc:  69%, G loss: 1.953760\n",
      "Ep: 274, steps: 23, D loss: 0.217048, acc:  66%, G loss: 2.297792\n",
      "Ep: 274, steps: 24, D loss: 0.181481, acc:  78%, G loss: 1.951493\n",
      "Ep: 274, steps: 25, D loss: 0.217703, acc:  66%, G loss: 1.811706\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 275, steps: 1, D loss: 0.229055, acc:  63%, G loss: 1.954625\n",
      "Ep: 275, steps: 2, D loss: 0.250992, acc:  55%, G loss: 1.606771\n",
      "Ep: 275, steps: 3, D loss: 0.153457, acc:  89%, G loss: 2.100701\n",
      "Ep: 275, steps: 4, D loss: 0.179016, acc:  84%, G loss: 2.053191\n",
      "Ep: 275, steps: 5, D loss: 0.243899, acc:  57%, G loss: 2.036582\n",
      "Ep: 275, steps: 6, D loss: 0.259074, acc:  51%, G loss: 1.850564\n",
      "Ep: 275, steps: 7, D loss: 0.440751, acc:  14%, G loss: 1.629900\n",
      "Ep: 275, steps: 8, D loss: 0.231826, acc:  59%, G loss: 2.364286\n",
      "Ep: 275, steps: 9, D loss: 0.181242, acc:  80%, G loss: 1.897847\n",
      "Ep: 275, steps: 10, D loss: 0.136620, acc:  91%, G loss: 1.891652\n",
      "Ep: 275, steps: 11, D loss: 0.218076, acc:  66%, G loss: 2.116914\n",
      "Ep: 275, steps: 12, D loss: 0.319649, acc:  32%, G loss: 1.623317\n",
      "Ep: 275, steps: 13, D loss: 0.293576, acc:  38%, G loss: 1.659129\n",
      "Ep: 275, steps: 14, D loss: 0.294856, acc:  38%, G loss: 1.758328\n",
      "Ep: 275, steps: 15, D loss: 0.227775, acc:  62%, G loss: 1.888217\n",
      "Ep: 275, steps: 16, D loss: 0.253788, acc:  56%, G loss: 1.824215\n",
      "Ep: 275, steps: 17, D loss: 0.166130, acc:  81%, G loss: 1.767498\n",
      "Ep: 275, steps: 18, D loss: 0.258056, acc:  54%, G loss: 1.821306\n",
      "Ep: 275, steps: 19, D loss: 0.197346, acc:  69%, G loss: 1.766205\n",
      "Ep: 275, steps: 20, D loss: 0.194013, acc:  73%, G loss: 1.997750\n",
      "Ep: 275, steps: 21, D loss: 0.297193, acc:  35%, G loss: 1.637801\n",
      "Ep: 275, steps: 22, D loss: 0.200297, acc:  64%, G loss: 1.914343\n",
      "Saved Model\n",
      "Ep: 275, steps: 23, D loss: 0.192960, acc:  72%, G loss: 2.247642\n",
      "Ep: 275, steps: 24, D loss: 0.240159, acc:  62%, G loss: 1.969880\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 276, steps: 1, D loss: 0.226444, acc:  63%, G loss: 2.035022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 276, steps: 2, D loss: 0.236581, acc:  63%, G loss: 1.690857\n",
      "Ep: 276, steps: 3, D loss: 0.161863, acc:  83%, G loss: 2.077326\n",
      "Ep: 276, steps: 4, D loss: 0.189428, acc:  81%, G loss: 1.911363\n",
      "Ep: 276, steps: 5, D loss: 0.256553, acc:  55%, G loss: 1.949288\n",
      "Ep: 276, steps: 6, D loss: 0.264227, acc:  49%, G loss: 1.884547\n",
      "Ep: 276, steps: 7, D loss: 0.415096, acc:  15%, G loss: 1.652997\n",
      "Ep: 276, steps: 8, D loss: 0.248731, acc:  54%, G loss: 2.089635\n",
      "Ep: 276, steps: 9, D loss: 0.170430, acc:  82%, G loss: 1.905607\n",
      "Ep: 276, steps: 10, D loss: 0.139063, acc:  92%, G loss: 1.869358\n",
      "Ep: 276, steps: 11, D loss: 0.220322, acc:  66%, G loss: 2.057737\n",
      "Ep: 276, steps: 12, D loss: 0.332822, acc:  27%, G loss: 1.624404\n",
      "Ep: 276, steps: 13, D loss: 0.291601, acc:  41%, G loss: 1.751302\n",
      "Ep: 276, steps: 14, D loss: 0.302967, acc:  35%, G loss: 1.798890\n",
      "Ep: 276, steps: 15, D loss: 0.229114, acc:  61%, G loss: 1.847988\n",
      "Ep: 276, steps: 16, D loss: 0.259875, acc:  54%, G loss: 1.838642\n",
      "Ep: 276, steps: 17, D loss: 0.181213, acc:  77%, G loss: 1.740046\n",
      "Ep: 276, steps: 18, D loss: 0.246691, acc:  61%, G loss: 1.802653\n",
      "Ep: 276, steps: 19, D loss: 0.190528, acc:  72%, G loss: 1.750021\n",
      "Ep: 276, steps: 20, D loss: 0.173558, acc:  80%, G loss: 2.029015\n",
      "Ep: 276, steps: 21, D loss: 0.291704, acc:  37%, G loss: 1.752936\n",
      "Ep: 276, steps: 22, D loss: 0.187384, acc:  69%, G loss: 1.850200\n",
      "Ep: 276, steps: 23, D loss: 0.203436, acc:  68%, G loss: 2.136942\n",
      "Ep: 276, steps: 24, D loss: 0.176156, acc:  77%, G loss: 1.832957\n",
      "Ep: 276, steps: 25, D loss: 0.218929, acc:  64%, G loss: 1.828681\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 277, steps: 1, D loss: 0.217246, acc:  66%, G loss: 1.977413\n",
      "Ep: 277, steps: 2, D loss: 0.254906, acc:  52%, G loss: 1.661450\n",
      "Ep: 277, steps: 3, D loss: 0.156465, acc:  86%, G loss: 2.162833\n",
      "Ep: 277, steps: 4, D loss: 0.182587, acc:  84%, G loss: 1.912038\n",
      "Ep: 277, steps: 5, D loss: 0.264959, acc:  55%, G loss: 2.189762\n",
      "Ep: 277, steps: 6, D loss: 0.261346, acc:  50%, G loss: 1.825963\n",
      "Ep: 277, steps: 7, D loss: 0.408370, acc:  18%, G loss: 1.488868\n",
      "Ep: 277, steps: 8, D loss: 0.229105, acc:  60%, G loss: 2.112466\n",
      "Ep: 277, steps: 9, D loss: 0.184549, acc:  79%, G loss: 1.824851\n",
      "Ep: 277, steps: 10, D loss: 0.155614, acc:  85%, G loss: 1.852355\n",
      "Ep: 277, steps: 11, D loss: 0.233933, acc:  60%, G loss: 2.080350\n",
      "Ep: 277, steps: 12, D loss: 0.328968, acc:  30%, G loss: 1.657925\n",
      "Ep: 277, steps: 13, D loss: 0.281547, acc:  42%, G loss: 1.845541\n",
      "Ep: 277, steps: 14, D loss: 0.304312, acc:  33%, G loss: 1.734708\n",
      "Ep: 277, steps: 15, D loss: 0.233285, acc:  60%, G loss: 1.756719\n",
      "Ep: 277, steps: 16, D loss: 0.255351, acc:  57%, G loss: 1.950752\n",
      "Ep: 277, steps: 17, D loss: 0.166972, acc:  83%, G loss: 1.807848\n",
      "Ep: 277, steps: 18, D loss: 0.240534, acc:  63%, G loss: 1.821757\n",
      "Ep: 277, steps: 19, D loss: 0.178979, acc:  76%, G loss: 1.839252\n",
      "Ep: 277, steps: 20, D loss: 0.169039, acc:  80%, G loss: 2.081493\n",
      "Ep: 277, steps: 21, D loss: 0.282478, acc:  41%, G loss: 1.702302\n",
      "Ep: 277, steps: 22, D loss: 0.206467, acc:  63%, G loss: 1.745422\n",
      "Ep: 277, steps: 23, D loss: 0.200353, acc:  70%, G loss: 2.151711\n",
      "Ep: 277, steps: 24, D loss: 0.158387, acc:  83%, G loss: 1.837705\n",
      "Ep: 277, steps: 25, D loss: 0.210198, acc:  66%, G loss: 1.855730\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 278, steps: 1, D loss: 0.216146, acc:  66%, G loss: 2.082969\n",
      "Ep: 278, steps: 2, D loss: 0.257036, acc:  51%, G loss: 1.685992\n",
      "Ep: 278, steps: 3, D loss: 0.143805, acc:  89%, G loss: 2.181778\n",
      "Ep: 278, steps: 4, D loss: 0.176304, acc:  84%, G loss: 2.045274\n",
      "Ep: 278, steps: 5, D loss: 0.264950, acc:  56%, G loss: 2.088284\n",
      "Ep: 278, steps: 6, D loss: 0.260920, acc:  50%, G loss: 1.786747\n",
      "Ep: 278, steps: 7, D loss: 0.423646, acc:  15%, G loss: 1.742794\n",
      "Ep: 278, steps: 8, D loss: 0.240421, acc:  56%, G loss: 2.166923\n",
      "Ep: 278, steps: 9, D loss: 0.183454, acc:  79%, G loss: 1.923686\n",
      "Ep: 278, steps: 10, D loss: 0.140295, acc:  90%, G loss: 1.865017\n",
      "Ep: 278, steps: 11, D loss: 0.232442, acc:  60%, G loss: 2.033682\n",
      "Ep: 278, steps: 12, D loss: 0.312045, acc:  36%, G loss: 1.633703\n",
      "Ep: 278, steps: 13, D loss: 0.280324, acc:  44%, G loss: 1.667764\n",
      "Ep: 278, steps: 14, D loss: 0.311322, acc:  31%, G loss: 1.693257\n",
      "Ep: 278, steps: 15, D loss: 0.222132, acc:  64%, G loss: 1.777299\n",
      "Ep: 278, steps: 16, D loss: 0.273127, acc:  52%, G loss: 1.839327\n",
      "Ep: 278, steps: 17, D loss: 0.166826, acc:  82%, G loss: 1.784214\n",
      "Ep: 278, steps: 18, D loss: 0.266908, acc:  54%, G loss: 1.889581\n",
      "Ep: 278, steps: 19, D loss: 0.199735, acc:  69%, G loss: 1.764375\n",
      "Ep: 278, steps: 20, D loss: 0.157563, acc:  83%, G loss: 1.992735\n",
      "Saved Model\n",
      "Ep: 278, steps: 21, D loss: 0.297461, acc:  35%, G loss: 1.632652\n",
      "Ep: 278, steps: 22, D loss: 0.184469, acc:  74%, G loss: 2.122549\n",
      "Ep: 278, steps: 23, D loss: 0.170301, acc:  80%, G loss: 1.794735\n",
      "Ep: 278, steps: 24, D loss: 0.212904, acc:  67%, G loss: 1.819124\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 279, steps: 1, D loss: 0.224515, acc:  65%, G loss: 2.072845\n",
      "Ep: 279, steps: 2, D loss: 0.241615, acc:  55%, G loss: 1.730815\n",
      "Ep: 279, steps: 3, D loss: 0.154609, acc:  87%, G loss: 2.128809\n",
      "Ep: 279, steps: 4, D loss: 0.168422, acc:  86%, G loss: 1.956834\n",
      "Ep: 279, steps: 5, D loss: 0.261878, acc:  57%, G loss: 2.059459\n",
      "Ep: 279, steps: 6, D loss: 0.254966, acc:  52%, G loss: 1.799748\n",
      "Ep: 279, steps: 7, D loss: 0.421179, acc:  18%, G loss: 1.591279\n",
      "Ep: 279, steps: 8, D loss: 0.238161, acc:  57%, G loss: 2.032022\n",
      "Ep: 279, steps: 9, D loss: 0.192155, acc:  74%, G loss: 2.022078\n",
      "Ep: 279, steps: 10, D loss: 0.161002, acc:  84%, G loss: 1.913996\n",
      "Ep: 279, steps: 11, D loss: 0.221575, acc:  65%, G loss: 2.049321\n",
      "Ep: 279, steps: 12, D loss: 0.346139, acc:  24%, G loss: 1.592483\n",
      "Ep: 279, steps: 13, D loss: 0.289392, acc:  43%, G loss: 1.651531\n",
      "Ep: 279, steps: 14, D loss: 0.305468, acc:  35%, G loss: 1.712523\n",
      "Ep: 279, steps: 15, D loss: 0.229386, acc:  61%, G loss: 1.731547\n",
      "Ep: 279, steps: 16, D loss: 0.267344, acc:  52%, G loss: 1.763273\n",
      "Ep: 279, steps: 17, D loss: 0.171217, acc:  79%, G loss: 1.765152\n",
      "Ep: 279, steps: 18, D loss: 0.254546, acc:  57%, G loss: 1.761408\n",
      "Ep: 279, steps: 19, D loss: 0.188660, acc:  72%, G loss: 1.803524\n",
      "Ep: 279, steps: 20, D loss: 0.170585, acc:  78%, G loss: 2.037726\n",
      "Ep: 279, steps: 21, D loss: 0.286462, acc:  37%, G loss: 1.678059\n",
      "Ep: 279, steps: 22, D loss: 0.198957, acc:  65%, G loss: 1.887457\n",
      "Ep: 279, steps: 23, D loss: 0.174722, acc:  79%, G loss: 2.204574\n",
      "Ep: 279, steps: 24, D loss: 0.172944, acc:  78%, G loss: 1.888711\n",
      "Ep: 279, steps: 25, D loss: 0.212474, acc:  64%, G loss: 1.994972\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 280, steps: 1, D loss: 0.224224, acc:  63%, G loss: 2.035838\n",
      "Ep: 280, steps: 2, D loss: 0.253942, acc:  53%, G loss: 1.586788\n",
      "Ep: 280, steps: 3, D loss: 0.132604, acc:  93%, G loss: 2.153121\n",
      "Ep: 280, steps: 4, D loss: 0.171876, acc:  85%, G loss: 2.076136\n",
      "Ep: 280, steps: 5, D loss: 0.286870, acc:  51%, G loss: 2.062643\n",
      "Ep: 280, steps: 6, D loss: 0.261362, acc:  51%, G loss: 1.850102\n",
      "Ep: 280, steps: 7, D loss: 0.392032, acc:  21%, G loss: 1.553211\n",
      "Ep: 280, steps: 8, D loss: 0.240981, acc:  56%, G loss: 2.127093\n",
      "Ep: 280, steps: 9, D loss: 0.182015, acc:  79%, G loss: 1.829260\n",
      "Ep: 280, steps: 10, D loss: 0.161486, acc:  84%, G loss: 1.825714\n",
      "Ep: 280, steps: 11, D loss: 0.233397, acc:  60%, G loss: 2.023926\n",
      "Ep: 280, steps: 12, D loss: 0.311454, acc:  37%, G loss: 1.713720\n",
      "Ep: 280, steps: 13, D loss: 0.292578, acc:  41%, G loss: 1.710769\n",
      "Ep: 280, steps: 14, D loss: 0.299496, acc:  35%, G loss: 1.645403\n",
      "Ep: 280, steps: 15, D loss: 0.248274, acc:  53%, G loss: 1.906258\n",
      "Ep: 280, steps: 16, D loss: 0.267390, acc:  52%, G loss: 1.818786\n",
      "Ep: 280, steps: 17, D loss: 0.181572, acc:  78%, G loss: 1.739084\n",
      "Ep: 280, steps: 18, D loss: 0.257411, acc:  59%, G loss: 1.930336\n",
      "Ep: 280, steps: 19, D loss: 0.199712, acc:  70%, G loss: 1.803435\n",
      "Ep: 280, steps: 20, D loss: 0.160861, acc:  82%, G loss: 1.895280\n",
      "Ep: 280, steps: 21, D loss: 0.314388, acc:  28%, G loss: 1.934169\n",
      "Ep: 280, steps: 22, D loss: 0.196738, acc:  65%, G loss: 1.802101\n",
      "Ep: 280, steps: 23, D loss: 0.181936, acc:  76%, G loss: 2.203642\n",
      "Ep: 280, steps: 24, D loss: 0.170300, acc:  80%, G loss: 1.873428\n",
      "Ep: 280, steps: 25, D loss: 0.193724, acc:  69%, G loss: 1.891501\n",
      "Data exhausted, Re Initialize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 281, steps: 1, D loss: 0.233720, acc:  62%, G loss: 1.960262\n",
      "Ep: 281, steps: 2, D loss: 0.245360, acc:  57%, G loss: 1.644471\n",
      "Ep: 281, steps: 3, D loss: 0.146037, acc:  89%, G loss: 2.193954\n",
      "Ep: 281, steps: 4, D loss: 0.183899, acc:  82%, G loss: 1.901976\n",
      "Ep: 281, steps: 5, D loss: 0.249135, acc:  55%, G loss: 1.964779\n",
      "Ep: 281, steps: 6, D loss: 0.259562, acc:  50%, G loss: 1.786214\n",
      "Ep: 281, steps: 7, D loss: 0.370900, acc:  24%, G loss: 1.679427\n",
      "Ep: 281, steps: 8, D loss: 0.238592, acc:  58%, G loss: 1.992912\n",
      "Ep: 281, steps: 9, D loss: 0.184263, acc:  79%, G loss: 1.872350\n",
      "Ep: 281, steps: 10, D loss: 0.146437, acc:  87%, G loss: 1.853166\n",
      "Ep: 281, steps: 11, D loss: 0.229824, acc:  61%, G loss: 2.078072\n",
      "Ep: 281, steps: 12, D loss: 0.318101, acc:  34%, G loss: 1.591546\n",
      "Ep: 281, steps: 13, D loss: 0.289106, acc:  40%, G loss: 1.667323\n",
      "Ep: 281, steps: 14, D loss: 0.302520, acc:  36%, G loss: 1.722698\n",
      "Ep: 281, steps: 15, D loss: 0.224591, acc:  63%, G loss: 1.724977\n",
      "Ep: 281, steps: 16, D loss: 0.254868, acc:  55%, G loss: 1.858201\n",
      "Ep: 281, steps: 17, D loss: 0.187269, acc:  76%, G loss: 1.774596\n",
      "Ep: 281, steps: 18, D loss: 0.250968, acc:  58%, G loss: 1.868487\n",
      "Saved Model\n",
      "Ep: 281, steps: 19, D loss: 0.194083, acc:  70%, G loss: 1.843502\n",
      "Ep: 281, steps: 20, D loss: 0.280627, acc:  39%, G loss: 1.877036\n",
      "Ep: 281, steps: 21, D loss: 0.214100, acc:  63%, G loss: 2.040573\n",
      "Ep: 281, steps: 22, D loss: 0.193056, acc:  74%, G loss: 2.142823\n",
      "Ep: 281, steps: 23, D loss: 0.166153, acc:  83%, G loss: 1.856305\n",
      "Ep: 281, steps: 24, D loss: 0.198978, acc:  71%, G loss: 1.883567\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 282, steps: 1, D loss: 0.215481, acc:  68%, G loss: 1.963651\n",
      "Ep: 282, steps: 2, D loss: 0.246829, acc:  56%, G loss: 1.627718\n",
      "Ep: 282, steps: 3, D loss: 0.144659, acc:  88%, G loss: 2.137626\n",
      "Ep: 282, steps: 4, D loss: 0.175056, acc:  83%, G loss: 2.029203\n",
      "Ep: 282, steps: 5, D loss: 0.276525, acc:  53%, G loss: 2.064947\n",
      "Ep: 282, steps: 6, D loss: 0.260879, acc:  51%, G loss: 1.778103\n",
      "Ep: 282, steps: 7, D loss: 0.372404, acc:  24%, G loss: 1.515239\n",
      "Ep: 282, steps: 8, D loss: 0.229645, acc:  61%, G loss: 2.056990\n",
      "Ep: 282, steps: 9, D loss: 0.214676, acc:  67%, G loss: 1.968969\n",
      "Ep: 282, steps: 10, D loss: 0.144063, acc:  90%, G loss: 1.889239\n",
      "Ep: 282, steps: 11, D loss: 0.229576, acc:  62%, G loss: 2.045753\n",
      "Ep: 282, steps: 12, D loss: 0.330499, acc:  29%, G loss: 1.678012\n",
      "Ep: 282, steps: 13, D loss: 0.291692, acc:  40%, G loss: 1.768499\n",
      "Ep: 282, steps: 14, D loss: 0.305319, acc:  33%, G loss: 1.741261\n",
      "Ep: 282, steps: 15, D loss: 0.235495, acc:  58%, G loss: 1.775488\n",
      "Ep: 282, steps: 16, D loss: 0.245685, acc:  58%, G loss: 1.864301\n",
      "Ep: 282, steps: 17, D loss: 0.185759, acc:  76%, G loss: 1.786713\n",
      "Ep: 282, steps: 18, D loss: 0.235945, acc:  62%, G loss: 1.781681\n",
      "Ep: 282, steps: 19, D loss: 0.203029, acc:  69%, G loss: 1.863283\n",
      "Ep: 282, steps: 20, D loss: 0.182969, acc:  76%, G loss: 1.936074\n",
      "Ep: 282, steps: 21, D loss: 0.299122, acc:  35%, G loss: 1.718717\n",
      "Ep: 282, steps: 22, D loss: 0.212456, acc:  62%, G loss: 1.950553\n",
      "Ep: 282, steps: 23, D loss: 0.185782, acc:  75%, G loss: 2.196026\n",
      "Ep: 282, steps: 24, D loss: 0.175167, acc:  78%, G loss: 1.821705\n",
      "Ep: 282, steps: 25, D loss: 0.209196, acc:  67%, G loss: 1.932675\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 283, steps: 1, D loss: 0.228227, acc:  64%, G loss: 1.875183\n",
      "Ep: 283, steps: 2, D loss: 0.261110, acc:  50%, G loss: 1.566847\n",
      "Ep: 283, steps: 3, D loss: 0.136684, acc:  91%, G loss: 2.195956\n",
      "Ep: 283, steps: 4, D loss: 0.185246, acc:  81%, G loss: 1.912174\n",
      "Ep: 283, steps: 5, D loss: 0.232332, acc:  61%, G loss: 2.102262\n",
      "Ep: 283, steps: 6, D loss: 0.262888, acc:  52%, G loss: 1.740428\n",
      "Ep: 283, steps: 7, D loss: 0.410420, acc:  17%, G loss: 2.104023\n",
      "Ep: 283, steps: 8, D loss: 0.245726, acc:  55%, G loss: 2.043079\n",
      "Ep: 283, steps: 9, D loss: 0.184164, acc:  78%, G loss: 1.928418\n",
      "Ep: 283, steps: 10, D loss: 0.144153, acc:  90%, G loss: 1.834195\n",
      "Ep: 283, steps: 11, D loss: 0.230094, acc:  63%, G loss: 2.003501\n",
      "Ep: 283, steps: 12, D loss: 0.304230, acc:  38%, G loss: 1.701177\n",
      "Ep: 283, steps: 13, D loss: 0.270782, acc:  47%, G loss: 1.708299\n",
      "Ep: 283, steps: 14, D loss: 0.300339, acc:  36%, G loss: 1.695224\n",
      "Ep: 283, steps: 15, D loss: 0.232160, acc:  61%, G loss: 2.187079\n",
      "Ep: 283, steps: 16, D loss: 0.240945, acc:  59%, G loss: 1.909682\n",
      "Ep: 283, steps: 17, D loss: 0.152350, acc:  84%, G loss: 1.785755\n",
      "Ep: 283, steps: 18, D loss: 0.263327, acc:  57%, G loss: 1.880690\n",
      "Ep: 283, steps: 19, D loss: 0.215672, acc:  64%, G loss: 1.737421\n",
      "Ep: 283, steps: 20, D loss: 0.167918, acc:  79%, G loss: 1.971587\n",
      "Ep: 283, steps: 21, D loss: 0.251589, acc:  49%, G loss: 1.760196\n",
      "Ep: 283, steps: 22, D loss: 0.206240, acc:  62%, G loss: 1.895585\n",
      "Ep: 283, steps: 23, D loss: 0.212613, acc:  66%, G loss: 2.123273\n",
      "Ep: 283, steps: 24, D loss: 0.177694, acc:  77%, G loss: 1.837349\n",
      "Ep: 283, steps: 25, D loss: 0.207927, acc:  67%, G loss: 1.885453\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 284, steps: 1, D loss: 0.230422, acc:  63%, G loss: 1.956215\n",
      "Ep: 284, steps: 2, D loss: 0.243188, acc:  57%, G loss: 1.623415\n",
      "Ep: 284, steps: 3, D loss: 0.153124, acc:  84%, G loss: 2.225433\n",
      "Ep: 284, steps: 4, D loss: 0.188810, acc:  81%, G loss: 1.949410\n",
      "Ep: 284, steps: 5, D loss: 0.227604, acc:  61%, G loss: 1.955830\n",
      "Ep: 284, steps: 6, D loss: 0.259989, acc:  49%, G loss: 1.734310\n",
      "Ep: 284, steps: 7, D loss: 0.345800, acc:  26%, G loss: 1.444245\n",
      "Ep: 284, steps: 8, D loss: 0.221993, acc:  62%, G loss: 2.047707\n",
      "Ep: 284, steps: 9, D loss: 0.212031, acc:  69%, G loss: 1.924355\n",
      "Ep: 284, steps: 10, D loss: 0.164289, acc:  83%, G loss: 1.929499\n",
      "Ep: 284, steps: 11, D loss: 0.234742, acc:  60%, G loss: 2.136486\n",
      "Ep: 284, steps: 12, D loss: 0.311879, acc:  36%, G loss: 1.707365\n",
      "Ep: 284, steps: 13, D loss: 0.295243, acc:  40%, G loss: 1.756485\n",
      "Ep: 284, steps: 14, D loss: 0.300455, acc:  37%, G loss: 1.734986\n",
      "Ep: 284, steps: 15, D loss: 0.229937, acc:  62%, G loss: 1.888059\n",
      "Ep: 284, steps: 16, D loss: 0.257351, acc:  55%, G loss: 1.782999\n",
      "Saved Model\n",
      "Ep: 284, steps: 17, D loss: 0.181295, acc:  76%, G loss: 1.816602\n",
      "Ep: 284, steps: 18, D loss: 0.184337, acc:  74%, G loss: 1.780488\n",
      "Ep: 284, steps: 19, D loss: 0.178152, acc:  78%, G loss: 2.122457\n",
      "Ep: 284, steps: 20, D loss: 0.280650, acc:  40%, G loss: 1.949795\n",
      "Ep: 284, steps: 21, D loss: 0.194255, acc:  66%, G loss: 1.935999\n",
      "Ep: 284, steps: 22, D loss: 0.154743, acc:  82%, G loss: 2.151765\n",
      "Ep: 284, steps: 23, D loss: 0.151972, acc:  84%, G loss: 1.863960\n",
      "Ep: 284, steps: 24, D loss: 0.199732, acc:  68%, G loss: 2.093131\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 285, steps: 1, D loss: 0.219835, acc:  65%, G loss: 2.032537\n",
      "Ep: 285, steps: 2, D loss: 0.256249, acc:  51%, G loss: 1.644876\n",
      "Ep: 285, steps: 3, D loss: 0.143287, acc:  89%, G loss: 2.212690\n",
      "Ep: 285, steps: 4, D loss: 0.177135, acc:  83%, G loss: 1.900325\n",
      "Ep: 285, steps: 5, D loss: 0.265225, acc:  52%, G loss: 2.098800\n",
      "Ep: 285, steps: 6, D loss: 0.277247, acc:  50%, G loss: 1.854667\n",
      "Ep: 285, steps: 7, D loss: 0.401060, acc:  19%, G loss: 1.618473\n",
      "Ep: 285, steps: 8, D loss: 0.225884, acc:  60%, G loss: 2.076585\n",
      "Ep: 285, steps: 9, D loss: 0.212231, acc:  70%, G loss: 2.033609\n",
      "Ep: 285, steps: 10, D loss: 0.151790, acc:  86%, G loss: 1.847023\n",
      "Ep: 285, steps: 11, D loss: 0.229363, acc:  62%, G loss: 2.106057\n",
      "Ep: 285, steps: 12, D loss: 0.312024, acc:  35%, G loss: 1.593332\n",
      "Ep: 285, steps: 13, D loss: 0.290760, acc:  37%, G loss: 1.697142\n",
      "Ep: 285, steps: 14, D loss: 0.304549, acc:  35%, G loss: 1.670160\n",
      "Ep: 285, steps: 15, D loss: 0.241734, acc:  56%, G loss: 1.760271\n",
      "Ep: 285, steps: 16, D loss: 0.250894, acc:  56%, G loss: 1.785431\n",
      "Ep: 285, steps: 17, D loss: 0.182163, acc:  75%, G loss: 1.809052\n",
      "Ep: 285, steps: 18, D loss: 0.237169, acc:  62%, G loss: 1.886131\n",
      "Ep: 285, steps: 19, D loss: 0.194268, acc:  70%, G loss: 1.754637\n",
      "Ep: 285, steps: 20, D loss: 0.169278, acc:  79%, G loss: 2.021075\n",
      "Ep: 285, steps: 21, D loss: 0.315924, acc:  30%, G loss: 1.737195\n",
      "Ep: 285, steps: 22, D loss: 0.208483, acc:  61%, G loss: 1.901088\n",
      "Ep: 285, steps: 23, D loss: 0.211032, acc:  65%, G loss: 2.372395\n",
      "Ep: 285, steps: 24, D loss: 0.163782, acc:  78%, G loss: 1.869427\n",
      "Ep: 285, steps: 25, D loss: 0.206044, acc:  68%, G loss: 1.967201\n",
      "Data exhausted, Re Initialize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 286, steps: 1, D loss: 0.219154, acc:  66%, G loss: 2.009816\n",
      "Ep: 286, steps: 2, D loss: 0.236425, acc:  59%, G loss: 1.693882\n",
      "Ep: 286, steps: 3, D loss: 0.146300, acc:  89%, G loss: 2.224893\n",
      "Ep: 286, steps: 4, D loss: 0.176237, acc:  85%, G loss: 2.036674\n",
      "Ep: 286, steps: 5, D loss: 0.259748, acc:  57%, G loss: 1.886227\n",
      "Ep: 286, steps: 6, D loss: 0.258780, acc:  50%, G loss: 1.718246\n",
      "Ep: 286, steps: 7, D loss: 0.384889, acc:  22%, G loss: 1.473949\n",
      "Ep: 286, steps: 8, D loss: 0.222837, acc:  62%, G loss: 2.059865\n",
      "Ep: 286, steps: 9, D loss: 0.190945, acc:  74%, G loss: 1.965439\n",
      "Ep: 286, steps: 10, D loss: 0.164159, acc:  81%, G loss: 1.951336\n",
      "Ep: 286, steps: 11, D loss: 0.227283, acc:  63%, G loss: 2.052340\n",
      "Ep: 286, steps: 12, D loss: 0.335325, acc:  28%, G loss: 1.646486\n",
      "Ep: 286, steps: 13, D loss: 0.288729, acc:  44%, G loss: 1.714174\n",
      "Ep: 286, steps: 14, D loss: 0.316105, acc:  33%, G loss: 1.684929\n",
      "Ep: 286, steps: 15, D loss: 0.243301, acc:  54%, G loss: 1.793316\n",
      "Ep: 286, steps: 16, D loss: 0.258091, acc:  56%, G loss: 1.808308\n",
      "Ep: 286, steps: 17, D loss: 0.186244, acc:  75%, G loss: 1.804527\n",
      "Ep: 286, steps: 18, D loss: 0.251174, acc:  57%, G loss: 1.844784\n",
      "Ep: 286, steps: 19, D loss: 0.195108, acc:  69%, G loss: 1.735076\n",
      "Ep: 286, steps: 20, D loss: 0.184060, acc:  75%, G loss: 2.004992\n",
      "Ep: 286, steps: 21, D loss: 0.296240, acc:  35%, G loss: 1.758412\n",
      "Ep: 286, steps: 22, D loss: 0.206375, acc:  63%, G loss: 1.836805\n",
      "Ep: 286, steps: 23, D loss: 0.186574, acc:  74%, G loss: 2.144959\n",
      "Ep: 286, steps: 24, D loss: 0.185151, acc:  76%, G loss: 1.822562\n",
      "Ep: 286, steps: 25, D loss: 0.230057, acc:  61%, G loss: 1.882314\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 287, steps: 1, D loss: 0.232956, acc:  64%, G loss: 1.982701\n",
      "Ep: 287, steps: 2, D loss: 0.248663, acc:  56%, G loss: 1.690585\n",
      "Ep: 287, steps: 3, D loss: 0.159367, acc:  86%, G loss: 2.116385\n",
      "Ep: 287, steps: 4, D loss: 0.188038, acc:  80%, G loss: 1.953830\n",
      "Ep: 287, steps: 5, D loss: 0.266133, acc:  54%, G loss: 2.110506\n",
      "Ep: 287, steps: 6, D loss: 0.259598, acc:  50%, G loss: 1.758991\n",
      "Ep: 287, steps: 7, D loss: 0.364251, acc:  26%, G loss: 2.210439\n",
      "Ep: 287, steps: 8, D loss: 0.229810, acc:  60%, G loss: 1.929155\n",
      "Ep: 287, steps: 9, D loss: 0.191175, acc:  74%, G loss: 1.870818\n",
      "Ep: 287, steps: 10, D loss: 0.149201, acc:  87%, G loss: 1.735964\n",
      "Ep: 287, steps: 11, D loss: 0.232118, acc:  60%, G loss: 2.005106\n",
      "Ep: 287, steps: 12, D loss: 0.312178, acc:  36%, G loss: 1.604462\n",
      "Ep: 287, steps: 13, D loss: 0.272255, acc:  47%, G loss: 1.697358\n",
      "Ep: 287, steps: 14, D loss: 0.304042, acc:  37%, G loss: 1.723213\n",
      "Saved Model\n",
      "Ep: 287, steps: 15, D loss: 0.238149, acc:  56%, G loss: 1.854816\n",
      "Ep: 287, steps: 16, D loss: 0.168975, acc:  80%, G loss: 1.851264\n",
      "Ep: 287, steps: 17, D loss: 0.230106, acc:  62%, G loss: 1.731199\n",
      "Ep: 287, steps: 18, D loss: 0.187221, acc:  73%, G loss: 1.716928\n",
      "Ep: 287, steps: 19, D loss: 0.175694, acc:  78%, G loss: 1.996369\n",
      "Ep: 287, steps: 20, D loss: 0.275401, acc:  42%, G loss: 1.854581\n",
      "Ep: 287, steps: 21, D loss: 0.216831, acc:  62%, G loss: 1.841505\n",
      "Ep: 287, steps: 22, D loss: 0.181211, acc:  76%, G loss: 2.220034\n",
      "Ep: 287, steps: 23, D loss: 0.166980, acc:  78%, G loss: 1.853678\n",
      "Ep: 287, steps: 24, D loss: 0.216482, acc:  64%, G loss: 2.024054\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 288, steps: 1, D loss: 0.234510, acc:  62%, G loss: 1.908309\n",
      "Ep: 288, steps: 2, D loss: 0.260258, acc:  50%, G loss: 1.565672\n",
      "Ep: 288, steps: 3, D loss: 0.145739, acc:  88%, G loss: 2.213243\n",
      "Ep: 288, steps: 4, D loss: 0.190138, acc:  79%, G loss: 1.845769\n",
      "Ep: 288, steps: 5, D loss: 0.261318, acc:  54%, G loss: 1.891394\n",
      "Ep: 288, steps: 6, D loss: 0.269537, acc:  50%, G loss: 1.839403\n",
      "Ep: 288, steps: 7, D loss: 0.420799, acc:  15%, G loss: 1.481865\n",
      "Ep: 288, steps: 8, D loss: 0.246155, acc:  54%, G loss: 2.141149\n",
      "Ep: 288, steps: 9, D loss: 0.200554, acc:  72%, G loss: 1.920882\n",
      "Ep: 288, steps: 10, D loss: 0.156377, acc:  84%, G loss: 1.869068\n",
      "Ep: 288, steps: 11, D loss: 0.227030, acc:  63%, G loss: 2.138515\n",
      "Ep: 288, steps: 12, D loss: 0.324043, acc:  30%, G loss: 1.669545\n",
      "Ep: 288, steps: 13, D loss: 0.297571, acc:  40%, G loss: 1.674722\n",
      "Ep: 288, steps: 14, D loss: 0.305159, acc:  36%, G loss: 1.730494\n",
      "Ep: 288, steps: 15, D loss: 0.233942, acc:  60%, G loss: 1.840003\n",
      "Ep: 288, steps: 16, D loss: 0.254700, acc:  58%, G loss: 1.803980\n",
      "Ep: 288, steps: 17, D loss: 0.177575, acc:  78%, G loss: 1.787497\n",
      "Ep: 288, steps: 18, D loss: 0.245709, acc:  59%, G loss: 1.964700\n",
      "Ep: 288, steps: 19, D loss: 0.196331, acc:  71%, G loss: 1.796036\n",
      "Ep: 288, steps: 20, D loss: 0.208773, acc:  68%, G loss: 2.015075\n",
      "Ep: 288, steps: 21, D loss: 0.318747, acc:  25%, G loss: 1.624929\n",
      "Ep: 288, steps: 22, D loss: 0.206976, acc:  63%, G loss: 1.848528\n",
      "Ep: 288, steps: 23, D loss: 0.206978, acc:  68%, G loss: 2.137377\n",
      "Ep: 288, steps: 24, D loss: 0.176915, acc:  78%, G loss: 1.842676\n",
      "Ep: 288, steps: 25, D loss: 0.201741, acc:  71%, G loss: 1.919485\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 289, steps: 1, D loss: 0.237154, acc:  61%, G loss: 2.027733\n",
      "Ep: 289, steps: 2, D loss: 0.253538, acc:  53%, G loss: 1.597609\n",
      "Ep: 289, steps: 3, D loss: 0.169110, acc:  85%, G loss: 2.135641\n",
      "Ep: 289, steps: 4, D loss: 0.178901, acc:  84%, G loss: 1.835353\n",
      "Ep: 289, steps: 5, D loss: 0.291683, acc:  49%, G loss: 2.053683\n",
      "Ep: 289, steps: 6, D loss: 0.252782, acc:  51%, G loss: 1.713457\n",
      "Ep: 289, steps: 7, D loss: 0.392988, acc:  22%, G loss: 1.455459\n",
      "Ep: 289, steps: 8, D loss: 0.241668, acc:  55%, G loss: 2.122653\n",
      "Ep: 289, steps: 9, D loss: 0.177445, acc:  82%, G loss: 1.932917\n",
      "Ep: 289, steps: 10, D loss: 0.164346, acc:  84%, G loss: 1.826382\n",
      "Ep: 289, steps: 11, D loss: 0.209662, acc:  69%, G loss: 2.082223\n",
      "Ep: 289, steps: 12, D loss: 0.336785, acc:  27%, G loss: 1.618588\n",
      "Ep: 289, steps: 13, D loss: 0.293479, acc:  42%, G loss: 1.626793\n",
      "Ep: 289, steps: 14, D loss: 0.296812, acc:  37%, G loss: 1.669701\n",
      "Ep: 289, steps: 15, D loss: 0.232379, acc:  59%, G loss: 1.818512\n",
      "Ep: 289, steps: 16, D loss: 0.270069, acc:  54%, G loss: 1.812875\n",
      "Ep: 289, steps: 17, D loss: 0.193259, acc:  74%, G loss: 1.778697\n",
      "Ep: 289, steps: 18, D loss: 0.254979, acc:  59%, G loss: 1.971835\n",
      "Ep: 289, steps: 19, D loss: 0.198265, acc:  69%, G loss: 1.806146\n",
      "Ep: 289, steps: 20, D loss: 0.195115, acc:  74%, G loss: 1.964782\n",
      "Ep: 289, steps: 21, D loss: 0.291230, acc:  36%, G loss: 1.802081\n",
      "Ep: 289, steps: 22, D loss: 0.216912, acc:  61%, G loss: 1.765696\n",
      "Ep: 289, steps: 23, D loss: 0.199391, acc:  69%, G loss: 2.128095\n",
      "Ep: 289, steps: 24, D loss: 0.165056, acc:  82%, G loss: 1.875389\n",
      "Ep: 289, steps: 25, D loss: 0.201308, acc:  68%, G loss: 1.903637\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 290, steps: 1, D loss: 0.234575, acc:  63%, G loss: 1.927552\n",
      "Ep: 290, steps: 2, D loss: 0.250122, acc:  55%, G loss: 1.611955\n",
      "Ep: 290, steps: 3, D loss: 0.151421, acc:  87%, G loss: 2.128632\n",
      "Ep: 290, steps: 4, D loss: 0.180128, acc:  83%, G loss: 1.949876\n",
      "Ep: 290, steps: 5, D loss: 0.275961, acc:  49%, G loss: 1.994705\n",
      "Ep: 290, steps: 6, D loss: 0.264484, acc:  51%, G loss: 1.755998\n",
      "Ep: 290, steps: 7, D loss: 0.396665, acc:  19%, G loss: 1.482091\n",
      "Ep: 290, steps: 8, D loss: 0.244508, acc:  55%, G loss: 2.091780\n",
      "Ep: 290, steps: 9, D loss: 0.188885, acc:  77%, G loss: 1.974473\n",
      "Ep: 290, steps: 10, D loss: 0.151618, acc:  86%, G loss: 1.908469\n",
      "Ep: 290, steps: 11, D loss: 0.226014, acc:  64%, G loss: 2.015058\n",
      "Ep: 290, steps: 12, D loss: 0.323279, acc:  29%, G loss: 1.571713\n",
      "Saved Model\n",
      "Ep: 290, steps: 13, D loss: 0.300252, acc:  40%, G loss: 1.745592\n",
      "Ep: 290, steps: 14, D loss: 0.227414, acc:  62%, G loss: 1.652649\n",
      "Ep: 290, steps: 15, D loss: 0.270131, acc:  55%, G loss: 1.782196\n",
      "Ep: 290, steps: 16, D loss: 0.174575, acc:  79%, G loss: 1.806060\n",
      "Ep: 290, steps: 17, D loss: 0.243042, acc:  59%, G loss: 1.946950\n",
      "Ep: 290, steps: 18, D loss: 0.194800, acc:  71%, G loss: 1.834159\n",
      "Ep: 290, steps: 19, D loss: 0.191484, acc:  72%, G loss: 1.926324\n",
      "Ep: 290, steps: 20, D loss: 0.300224, acc:  33%, G loss: 1.745059\n",
      "Ep: 290, steps: 21, D loss: 0.218759, acc:  60%, G loss: 1.740415\n",
      "Ep: 290, steps: 22, D loss: 0.186421, acc:  74%, G loss: 2.132287\n",
      "Ep: 290, steps: 23, D loss: 0.169486, acc:  81%, G loss: 1.795489\n",
      "Ep: 290, steps: 24, D loss: 0.199035, acc:  69%, G loss: 2.014749\n",
      "Data exhausted, Re Initialize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 291, steps: 1, D loss: 0.233954, acc:  63%, G loss: 1.947880\n",
      "Ep: 291, steps: 2, D loss: 0.252289, acc:  53%, G loss: 1.660925\n",
      "Ep: 291, steps: 3, D loss: 0.146761, acc:  89%, G loss: 2.126129\n",
      "Ep: 291, steps: 4, D loss: 0.171764, acc:  86%, G loss: 1.961477\n",
      "Ep: 291, steps: 5, D loss: 0.298708, acc:  48%, G loss: 2.303027\n",
      "Ep: 291, steps: 6, D loss: 0.253865, acc:  51%, G loss: 1.734491\n",
      "Ep: 291, steps: 7, D loss: 0.400287, acc:  21%, G loss: 1.515339\n",
      "Ep: 291, steps: 8, D loss: 0.237955, acc:  56%, G loss: 2.014552\n",
      "Ep: 291, steps: 9, D loss: 0.176060, acc:  81%, G loss: 1.913299\n",
      "Ep: 291, steps: 10, D loss: 0.142342, acc:  90%, G loss: 1.772399\n",
      "Ep: 291, steps: 11, D loss: 0.215580, acc:  67%, G loss: 1.952498\n",
      "Ep: 291, steps: 12, D loss: 0.326817, acc:  29%, G loss: 1.560703\n",
      "Ep: 291, steps: 13, D loss: 0.295520, acc:  40%, G loss: 1.615960\n",
      "Ep: 291, steps: 14, D loss: 0.307717, acc:  35%, G loss: 1.648501\n",
      "Ep: 291, steps: 15, D loss: 0.238534, acc:  57%, G loss: 1.696154\n",
      "Ep: 291, steps: 16, D loss: 0.244467, acc:  57%, G loss: 1.840650\n",
      "Ep: 291, steps: 17, D loss: 0.176816, acc:  78%, G loss: 1.741686\n",
      "Ep: 291, steps: 18, D loss: 0.256459, acc:  58%, G loss: 1.949731\n",
      "Ep: 291, steps: 19, D loss: 0.204184, acc:  69%, G loss: 1.797731\n",
      "Ep: 291, steps: 20, D loss: 0.179271, acc:  76%, G loss: 1.954336\n",
      "Ep: 291, steps: 21, D loss: 0.297871, acc:  32%, G loss: 1.636101\n",
      "Ep: 291, steps: 22, D loss: 0.208849, acc:  64%, G loss: 1.904478\n",
      "Ep: 291, steps: 23, D loss: 0.180117, acc:  75%, G loss: 2.064932\n",
      "Ep: 291, steps: 24, D loss: 0.163195, acc:  82%, G loss: 1.843213\n",
      "Ep: 291, steps: 25, D loss: 0.200006, acc:  71%, G loss: 1.933244\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 292, steps: 1, D loss: 0.240039, acc:  59%, G loss: 1.947305\n",
      "Ep: 292, steps: 2, D loss: 0.259116, acc:  51%, G loss: 1.652941\n",
      "Ep: 292, steps: 3, D loss: 0.149138, acc:  89%, G loss: 2.136385\n",
      "Ep: 292, steps: 4, D loss: 0.182479, acc:  82%, G loss: 1.983591\n",
      "Ep: 292, steps: 5, D loss: 0.273638, acc:  52%, G loss: 2.084285\n",
      "Ep: 292, steps: 6, D loss: 0.271497, acc:  50%, G loss: 1.729990\n",
      "Ep: 292, steps: 7, D loss: 0.432430, acc:  13%, G loss: 1.569309\n",
      "Ep: 292, steps: 8, D loss: 0.246633, acc:  55%, G loss: 2.121425\n",
      "Ep: 292, steps: 9, D loss: 0.185872, acc:  78%, G loss: 1.913907\n",
      "Ep: 292, steps: 10, D loss: 0.154947, acc:  85%, G loss: 1.798325\n",
      "Ep: 292, steps: 11, D loss: 0.212469, acc:  68%, G loss: 1.997865\n",
      "Ep: 292, steps: 12, D loss: 0.331282, acc:  29%, G loss: 1.555274\n",
      "Ep: 292, steps: 13, D loss: 0.288423, acc:  44%, G loss: 1.645891\n",
      "Ep: 292, steps: 14, D loss: 0.311154, acc:  35%, G loss: 1.666815\n",
      "Ep: 292, steps: 15, D loss: 0.241349, acc:  57%, G loss: 1.973743\n",
      "Ep: 292, steps: 16, D loss: 0.249223, acc:  57%, G loss: 1.817149\n",
      "Ep: 292, steps: 17, D loss: 0.176429, acc:  79%, G loss: 1.736998\n",
      "Ep: 292, steps: 18, D loss: 0.278844, acc:  56%, G loss: 1.897921\n",
      "Ep: 292, steps: 19, D loss: 0.214398, acc:  64%, G loss: 1.797330\n",
      "Ep: 292, steps: 20, D loss: 0.180319, acc:  77%, G loss: 1.968676\n",
      "Ep: 292, steps: 21, D loss: 0.290025, acc:  35%, G loss: 1.615501\n",
      "Ep: 292, steps: 22, D loss: 0.209557, acc:  63%, G loss: 1.727806\n",
      "Ep: 292, steps: 23, D loss: 0.214122, acc:  67%, G loss: 2.170454\n",
      "Ep: 292, steps: 24, D loss: 0.171964, acc:  81%, G loss: 1.840897\n",
      "Ep: 292, steps: 25, D loss: 0.207589, acc:  69%, G loss: 1.860980\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 293, steps: 1, D loss: 0.245860, acc:  58%, G loss: 1.894821\n",
      "Ep: 293, steps: 2, D loss: 0.264251, acc:  48%, G loss: 1.625628\n",
      "Ep: 293, steps: 3, D loss: 0.150550, acc:  88%, G loss: 2.031126\n",
      "Ep: 293, steps: 4, D loss: 0.195153, acc:  81%, G loss: 1.943873\n",
      "Ep: 293, steps: 5, D loss: 0.259451, acc:  55%, G loss: 2.118499\n",
      "Ep: 293, steps: 6, D loss: 0.263146, acc:  49%, G loss: 1.797792\n",
      "Ep: 293, steps: 7, D loss: 0.405152, acc:  16%, G loss: 1.717013\n",
      "Ep: 293, steps: 8, D loss: 0.239694, acc:  58%, G loss: 2.161214\n",
      "Ep: 293, steps: 9, D loss: 0.192635, acc:  78%, G loss: 1.865916\n",
      "Ep: 293, steps: 10, D loss: 0.153042, acc:  85%, G loss: 1.839981\n",
      "Saved Model\n",
      "Ep: 293, steps: 11, D loss: 0.223256, acc:  62%, G loss: 1.950037\n",
      "Ep: 293, steps: 12, D loss: 0.319571, acc:  27%, G loss: 1.467040\n",
      "Ep: 293, steps: 13, D loss: 0.310543, acc:  34%, G loss: 1.595260\n",
      "Ep: 293, steps: 14, D loss: 0.221531, acc:  66%, G loss: 1.716248\n",
      "Ep: 293, steps: 15, D loss: 0.258589, acc:  55%, G loss: 1.822318\n",
      "Ep: 293, steps: 16, D loss: 0.175247, acc:  78%, G loss: 1.755483\n",
      "Ep: 293, steps: 17, D loss: 0.239616, acc:  63%, G loss: 1.865559\n",
      "Ep: 293, steps: 18, D loss: 0.180407, acc:  75%, G loss: 1.780298\n",
      "Ep: 293, steps: 19, D loss: 0.193351, acc:  74%, G loss: 2.111961\n",
      "Ep: 293, steps: 20, D loss: 0.298942, acc:  35%, G loss: 1.595908\n",
      "Ep: 293, steps: 21, D loss: 0.211885, acc:  60%, G loss: 1.815886\n",
      "Ep: 293, steps: 22, D loss: 0.178858, acc:  77%, G loss: 2.183722\n",
      "Ep: 293, steps: 23, D loss: 0.167378, acc:  81%, G loss: 1.856398\n",
      "Ep: 293, steps: 24, D loss: 0.206749, acc:  68%, G loss: 1.902078\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 294, steps: 1, D loss: 0.249426, acc:  58%, G loss: 1.931756\n",
      "Ep: 294, steps: 2, D loss: 0.259949, acc:  51%, G loss: 1.660697\n",
      "Ep: 294, steps: 3, D loss: 0.142749, acc:  90%, G loss: 2.068091\n",
      "Ep: 294, steps: 4, D loss: 0.184496, acc:  81%, G loss: 1.883898\n",
      "Ep: 294, steps: 5, D loss: 0.270512, acc:  52%, G loss: 2.043116\n",
      "Ep: 294, steps: 6, D loss: 0.258857, acc:  53%, G loss: 1.785761\n",
      "Ep: 294, steps: 7, D loss: 0.432954, acc:  12%, G loss: 1.567510\n",
      "Ep: 294, steps: 8, D loss: 0.243929, acc:  56%, G loss: 1.996291\n",
      "Ep: 294, steps: 9, D loss: 0.187296, acc:  77%, G loss: 1.896632\n",
      "Ep: 294, steps: 10, D loss: 0.162315, acc:  83%, G loss: 1.842732\n",
      "Ep: 294, steps: 11, D loss: 0.225043, acc:  63%, G loss: 2.004938\n",
      "Ep: 294, steps: 12, D loss: 0.334608, acc:  28%, G loss: 1.533722\n",
      "Ep: 294, steps: 13, D loss: 0.290926, acc:  39%, G loss: 1.600075\n",
      "Ep: 294, steps: 14, D loss: 0.311454, acc:  31%, G loss: 1.661784\n",
      "Ep: 294, steps: 15, D loss: 0.220521, acc:  66%, G loss: 1.747013\n",
      "Ep: 294, steps: 16, D loss: 0.255127, acc:  55%, G loss: 1.809068\n",
      "Ep: 294, steps: 17, D loss: 0.187379, acc:  76%, G loss: 1.728885\n",
      "Ep: 294, steps: 18, D loss: 0.243744, acc:  62%, G loss: 1.791480\n",
      "Ep: 294, steps: 19, D loss: 0.193431, acc:  71%, G loss: 1.793462\n",
      "Ep: 294, steps: 20, D loss: 0.194096, acc:  73%, G loss: 1.980448\n",
      "Ep: 294, steps: 21, D loss: 0.300542, acc:  30%, G loss: 1.662400\n",
      "Ep: 294, steps: 22, D loss: 0.209453, acc:  64%, G loss: 1.770343\n",
      "Ep: 294, steps: 23, D loss: 0.201602, acc:  70%, G loss: 2.155534\n",
      "Ep: 294, steps: 24, D loss: 0.160422, acc:  84%, G loss: 1.866499\n",
      "Ep: 294, steps: 25, D loss: 0.206600, acc:  69%, G loss: 1.943543\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 295, steps: 1, D loss: 0.245405, acc:  59%, G loss: 1.961073\n",
      "Ep: 295, steps: 2, D loss: 0.247361, acc:  56%, G loss: 1.723082\n",
      "Ep: 295, steps: 3, D loss: 0.144817, acc:  90%, G loss: 2.190459\n",
      "Ep: 295, steps: 4, D loss: 0.182287, acc:  83%, G loss: 2.046292\n",
      "Ep: 295, steps: 5, D loss: 0.266786, acc:  54%, G loss: 1.959426\n",
      "Ep: 295, steps: 6, D loss: 0.266424, acc:  49%, G loss: 1.801038\n",
      "Ep: 295, steps: 7, D loss: 0.393612, acc:  22%, G loss: 1.686906\n",
      "Ep: 295, steps: 8, D loss: 0.252715, acc:  50%, G loss: 2.017619\n",
      "Ep: 295, steps: 9, D loss: 0.176792, acc:  81%, G loss: 1.875579\n",
      "Ep: 295, steps: 10, D loss: 0.146588, acc:  90%, G loss: 1.834012\n",
      "Ep: 295, steps: 11, D loss: 0.237084, acc:  59%, G loss: 2.048366\n",
      "Ep: 295, steps: 12, D loss: 0.343141, acc:  23%, G loss: 1.565101\n",
      "Ep: 295, steps: 13, D loss: 0.306442, acc:  35%, G loss: 1.624169\n",
      "Ep: 295, steps: 14, D loss: 0.305648, acc:  36%, G loss: 1.669097\n",
      "Ep: 295, steps: 15, D loss: 0.237225, acc:  56%, G loss: 1.830218\n",
      "Ep: 295, steps: 16, D loss: 0.255596, acc:  56%, G loss: 1.827887\n",
      "Ep: 295, steps: 17, D loss: 0.192112, acc:  76%, G loss: 1.804235\n",
      "Ep: 295, steps: 18, D loss: 0.261557, acc:  55%, G loss: 1.848243\n",
      "Ep: 295, steps: 19, D loss: 0.188095, acc:  72%, G loss: 1.763078\n",
      "Ep: 295, steps: 20, D loss: 0.189067, acc:  76%, G loss: 2.040260\n",
      "Ep: 295, steps: 21, D loss: 0.306174, acc:  31%, G loss: 1.688080\n",
      "Ep: 295, steps: 22, D loss: 0.199933, acc:  64%, G loss: 1.792856\n",
      "Ep: 295, steps: 23, D loss: 0.195714, acc:  73%, G loss: 2.143960\n",
      "Ep: 295, steps: 24, D loss: 0.159418, acc:  84%, G loss: 1.838778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 295, steps: 25, D loss: 0.211110, acc:  69%, G loss: 1.829474\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 296, steps: 1, D loss: 0.238334, acc:  59%, G loss: 1.974220\n",
      "Ep: 296, steps: 2, D loss: 0.257141, acc:  51%, G loss: 1.671167\n",
      "Ep: 296, steps: 3, D loss: 0.142856, acc:  91%, G loss: 2.149037\n",
      "Ep: 296, steps: 4, D loss: 0.185328, acc:  81%, G loss: 1.862649\n",
      "Ep: 296, steps: 5, D loss: 0.270827, acc:  52%, G loss: 2.086471\n",
      "Ep: 296, steps: 6, D loss: 0.263941, acc:  50%, G loss: 1.990077\n",
      "Ep: 296, steps: 7, D loss: 0.410071, acc:  16%, G loss: 1.413414\n",
      "Ep: 296, steps: 8, D loss: 0.244868, acc:  56%, G loss: 1.944316\n",
      "Saved Model\n",
      "Ep: 296, steps: 9, D loss: 0.182942, acc:  80%, G loss: 1.938372\n",
      "Ep: 296, steps: 10, D loss: 0.232314, acc:  60%, G loss: 1.982925\n",
      "Ep: 296, steps: 11, D loss: 0.309303, acc:  33%, G loss: 1.482787\n",
      "Ep: 296, steps: 12, D loss: 0.277943, acc:  42%, G loss: 1.576504\n",
      "Ep: 296, steps: 13, D loss: 0.292604, acc:  37%, G loss: 1.691208\n",
      "Ep: 296, steps: 14, D loss: 0.233734, acc:  60%, G loss: 1.722555\n",
      "Ep: 296, steps: 15, D loss: 0.241834, acc:  58%, G loss: 1.815599\n",
      "Ep: 296, steps: 16, D loss: 0.185248, acc:  76%, G loss: 1.684469\n",
      "Ep: 296, steps: 17, D loss: 0.247446, acc:  59%, G loss: 1.793728\n",
      "Ep: 296, steps: 18, D loss: 0.202678, acc:  68%, G loss: 1.705058\n",
      "Ep: 296, steps: 19, D loss: 0.170795, acc:  81%, G loss: 1.967600\n",
      "Ep: 296, steps: 20, D loss: 0.285233, acc:  39%, G loss: 1.859802\n",
      "Ep: 296, steps: 21, D loss: 0.205538, acc:  64%, G loss: 1.844934\n",
      "Ep: 296, steps: 22, D loss: 0.181256, acc:  76%, G loss: 2.155093\n",
      "Ep: 296, steps: 23, D loss: 0.163396, acc:  84%, G loss: 1.822937\n",
      "Ep: 296, steps: 24, D loss: 0.216077, acc:  65%, G loss: 1.917091\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 297, steps: 1, D loss: 0.252297, acc:  55%, G loss: 2.087310\n",
      "Ep: 297, steps: 2, D loss: 0.253345, acc:  53%, G loss: 1.752049\n",
      "Ep: 297, steps: 3, D loss: 0.153010, acc:  86%, G loss: 2.182430\n",
      "Ep: 297, steps: 4, D loss: 0.179391, acc:  84%, G loss: 2.065285\n",
      "Ep: 297, steps: 5, D loss: 0.233662, acc:  61%, G loss: 1.938463\n",
      "Ep: 297, steps: 6, D loss: 0.263727, acc:  50%, G loss: 1.762075\n",
      "Ep: 297, steps: 7, D loss: 0.397617, acc:  18%, G loss: 1.577108\n",
      "Ep: 297, steps: 8, D loss: 0.244355, acc:  57%, G loss: 2.020523\n",
      "Ep: 297, steps: 9, D loss: 0.194213, acc:  75%, G loss: 1.855787\n",
      "Ep: 297, steps: 10, D loss: 0.170630, acc:  80%, G loss: 1.769006\n",
      "Ep: 297, steps: 11, D loss: 0.225657, acc:  63%, G loss: 2.043961\n",
      "Ep: 297, steps: 12, D loss: 0.312343, acc:  36%, G loss: 1.598807\n",
      "Ep: 297, steps: 13, D loss: 0.297237, acc:  37%, G loss: 1.623018\n",
      "Ep: 297, steps: 14, D loss: 0.300466, acc:  36%, G loss: 1.730527\n",
      "Ep: 297, steps: 15, D loss: 0.239789, acc:  58%, G loss: 1.866414\n",
      "Ep: 297, steps: 16, D loss: 0.261451, acc:  52%, G loss: 1.903452\n",
      "Ep: 297, steps: 17, D loss: 0.183581, acc:  77%, G loss: 1.799269\n",
      "Ep: 297, steps: 18, D loss: 0.249577, acc:  59%, G loss: 1.866276\n",
      "Ep: 297, steps: 19, D loss: 0.217720, acc:  64%, G loss: 1.756875\n",
      "Ep: 297, steps: 20, D loss: 0.188640, acc:  76%, G loss: 1.968832\n",
      "Ep: 297, steps: 21, D loss: 0.279178, acc:  41%, G loss: 1.566221\n",
      "Ep: 297, steps: 22, D loss: 0.209839, acc:  67%, G loss: 1.674624\n",
      "Ep: 297, steps: 23, D loss: 0.205122, acc:  71%, G loss: 2.134974\n",
      "Ep: 297, steps: 24, D loss: 0.175057, acc:  78%, G loss: 1.808977\n",
      "Ep: 297, steps: 25, D loss: 0.207855, acc:  67%, G loss: 1.833322\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 298, steps: 1, D loss: 0.257641, acc:  54%, G loss: 2.151262\n",
      "Ep: 298, steps: 2, D loss: 0.249877, acc:  56%, G loss: 1.778229\n",
      "Ep: 298, steps: 3, D loss: 0.155800, acc:  87%, G loss: 2.232868\n",
      "Ep: 298, steps: 4, D loss: 0.180113, acc:  82%, G loss: 2.093457\n",
      "Ep: 298, steps: 5, D loss: 0.259264, acc:  51%, G loss: 2.026000\n",
      "Ep: 298, steps: 6, D loss: 0.262690, acc:  51%, G loss: 1.836763\n",
      "Ep: 298, steps: 7, D loss: 0.394296, acc:  18%, G loss: 1.485279\n",
      "Ep: 298, steps: 8, D loss: 0.234506, acc:  58%, G loss: 1.970762\n",
      "Ep: 298, steps: 9, D loss: 0.185382, acc:  79%, G loss: 1.794574\n",
      "Ep: 298, steps: 10, D loss: 0.153067, acc:  87%, G loss: 1.838048\n",
      "Ep: 298, steps: 11, D loss: 0.226725, acc:  62%, G loss: 2.025039\n",
      "Ep: 298, steps: 12, D loss: 0.326063, acc:  28%, G loss: 1.563652\n",
      "Ep: 298, steps: 13, D loss: 0.290619, acc:  42%, G loss: 1.586215\n",
      "Ep: 298, steps: 14, D loss: 0.296469, acc:  37%, G loss: 1.666289\n",
      "Ep: 298, steps: 15, D loss: 0.225172, acc:  64%, G loss: 1.703819\n",
      "Ep: 298, steps: 16, D loss: 0.248001, acc:  59%, G loss: 1.826571\n",
      "Ep: 298, steps: 17, D loss: 0.195144, acc:  72%, G loss: 1.731500\n",
      "Ep: 298, steps: 18, D loss: 0.273401, acc:  54%, G loss: 1.830966\n",
      "Ep: 298, steps: 19, D loss: 0.205168, acc:  68%, G loss: 1.780939\n",
      "Ep: 298, steps: 20, D loss: 0.183080, acc:  75%, G loss: 2.035695\n",
      "Ep: 298, steps: 21, D loss: 0.302168, acc:  32%, G loss: 1.615607\n",
      "Ep: 298, steps: 22, D loss: 0.220927, acc:  60%, G loss: 1.710887\n",
      "Ep: 298, steps: 23, D loss: 0.197551, acc:  71%, G loss: 2.118825\n",
      "Ep: 298, steps: 24, D loss: 0.170396, acc:  80%, G loss: 1.787070\n",
      "Ep: 298, steps: 25, D loss: 0.208056, acc:  68%, G loss: 1.979379\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 299, steps: 1, D loss: 0.236724, acc:  60%, G loss: 1.903498\n",
      "Ep: 299, steps: 2, D loss: 0.263875, acc:  50%, G loss: 1.726964\n",
      "Ep: 299, steps: 3, D loss: 0.158030, acc:  87%, G loss: 2.192686\n",
      "Ep: 299, steps: 4, D loss: 0.197530, acc:  79%, G loss: 2.029708\n",
      "Ep: 299, steps: 5, D loss: 0.247020, acc:  56%, G loss: 1.988463\n",
      "Ep: 299, steps: 6, D loss: 0.262529, acc:  49%, G loss: 1.752261\n",
      "Saved Model\n",
      "Ep: 299, steps: 7, D loss: 0.419707, acc:  16%, G loss: 1.841610\n",
      "Ep: 299, steps: 8, D loss: 0.155519, acc:  87%, G loss: 2.046414\n",
      "Ep: 299, steps: 9, D loss: 0.139751, acc:  91%, G loss: 1.840292\n",
      "Ep: 299, steps: 10, D loss: 0.217103, acc:  66%, G loss: 2.063371\n",
      "Ep: 299, steps: 11, D loss: 0.320837, acc:  31%, G loss: 1.625422\n",
      "Ep: 299, steps: 12, D loss: 0.287508, acc:  40%, G loss: 1.674438\n",
      "Ep: 299, steps: 13, D loss: 0.316122, acc:  33%, G loss: 1.673976\n",
      "Ep: 299, steps: 14, D loss: 0.231684, acc:  61%, G loss: 1.871000\n",
      "Ep: 299, steps: 15, D loss: 0.246688, acc:  58%, G loss: 1.809388\n",
      "Ep: 299, steps: 16, D loss: 0.167652, acc:  81%, G loss: 1.897626\n",
      "Ep: 299, steps: 17, D loss: 0.244725, acc:  59%, G loss: 1.886258\n",
      "Ep: 299, steps: 18, D loss: 0.195358, acc:  73%, G loss: 1.790951\n",
      "Ep: 299, steps: 19, D loss: 0.203464, acc:  69%, G loss: 2.126612\n",
      "Ep: 299, steps: 20, D loss: 0.260652, acc:  44%, G loss: 1.707472\n",
      "Ep: 299, steps: 21, D loss: 0.209717, acc:  64%, G loss: 1.712532\n",
      "Ep: 299, steps: 22, D loss: 0.205944, acc:  69%, G loss: 2.120924\n",
      "Ep: 299, steps: 23, D loss: 0.158480, acc:  83%, G loss: 1.921918\n",
      "Ep: 299, steps: 24, D loss: 0.191548, acc:  72%, G loss: 1.980892\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 300, steps: 1, D loss: 0.238281, acc:  59%, G loss: 1.946399\n",
      "Ep: 300, steps: 2, D loss: 0.258875, acc:  50%, G loss: 1.779257\n",
      "Ep: 300, steps: 3, D loss: 0.151597, acc:  87%, G loss: 2.207358\n",
      "Ep: 300, steps: 4, D loss: 0.165531, acc:  87%, G loss: 2.013702\n",
      "Ep: 300, steps: 5, D loss: 0.249779, acc:  53%, G loss: 2.097923\n",
      "Ep: 300, steps: 6, D loss: 0.262552, acc:  50%, G loss: 1.814128\n",
      "Ep: 300, steps: 7, D loss: 0.349450, acc:  29%, G loss: 1.455276\n",
      "Ep: 300, steps: 8, D loss: 0.240001, acc:  57%, G loss: 1.991568\n",
      "Ep: 300, steps: 9, D loss: 0.207697, acc:  69%, G loss: 1.816556\n",
      "Ep: 300, steps: 10, D loss: 0.156423, acc:  86%, G loss: 1.786533\n",
      "Ep: 300, steps: 11, D loss: 0.212891, acc:  68%, G loss: 1.926212\n",
      "Ep: 300, steps: 12, D loss: 0.314543, acc:  34%, G loss: 1.573975\n",
      "Ep: 300, steps: 13, D loss: 0.302081, acc:  35%, G loss: 1.571986\n",
      "Ep: 300, steps: 14, D loss: 0.306884, acc:  35%, G loss: 1.628800\n",
      "Ep: 300, steps: 15, D loss: 0.240795, acc:  55%, G loss: 1.804847\n",
      "Ep: 300, steps: 16, D loss: 0.247756, acc:  58%, G loss: 1.868795\n",
      "Ep: 300, steps: 17, D loss: 0.195659, acc:  74%, G loss: 1.764924\n",
      "Ep: 300, steps: 18, D loss: 0.252583, acc:  58%, G loss: 1.827175\n",
      "Ep: 300, steps: 19, D loss: 0.200359, acc:  68%, G loss: 1.764935\n",
      "Ep: 300, steps: 20, D loss: 0.171165, acc:  81%, G loss: 1.958509\n",
      "Ep: 300, steps: 21, D loss: 0.309078, acc:  30%, G loss: 1.638816\n",
      "Ep: 300, steps: 22, D loss: 0.203642, acc:  64%, G loss: 1.669786\n",
      "Ep: 300, steps: 23, D loss: 0.192670, acc:  72%, G loss: 2.168916\n",
      "Ep: 300, steps: 24, D loss: 0.164394, acc:  83%, G loss: 1.923620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 300, steps: 25, D loss: 0.203200, acc:  69%, G loss: 1.861003\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 301, steps: 1, D loss: 0.249182, acc:  57%, G loss: 2.102941\n",
      "Ep: 301, steps: 2, D loss: 0.244945, acc:  56%, G loss: 1.809070\n",
      "Ep: 301, steps: 3, D loss: 0.149465, acc:  87%, G loss: 2.172579\n",
      "Ep: 301, steps: 4, D loss: 0.175158, acc:  84%, G loss: 1.943204\n",
      "Ep: 301, steps: 5, D loss: 0.287054, acc:  50%, G loss: 2.032644\n",
      "Ep: 301, steps: 6, D loss: 0.269985, acc:  50%, G loss: 1.865515\n",
      "Ep: 301, steps: 7, D loss: 0.379482, acc:  24%, G loss: 1.591070\n",
      "Ep: 301, steps: 8, D loss: 0.238815, acc:  56%, G loss: 1.987968\n",
      "Ep: 301, steps: 9, D loss: 0.178692, acc:  79%, G loss: 1.808194\n",
      "Ep: 301, steps: 10, D loss: 0.152435, acc:  86%, G loss: 1.773474\n",
      "Ep: 301, steps: 11, D loss: 0.221654, acc:  63%, G loss: 1.973997\n",
      "Ep: 301, steps: 12, D loss: 0.320528, acc:  30%, G loss: 1.570998\n",
      "Ep: 301, steps: 13, D loss: 0.297192, acc:  41%, G loss: 1.608006\n",
      "Ep: 301, steps: 14, D loss: 0.306331, acc:  35%, G loss: 1.621676\n",
      "Ep: 301, steps: 15, D loss: 0.237515, acc:  56%, G loss: 1.819429\n",
      "Ep: 301, steps: 16, D loss: 0.255593, acc:  55%, G loss: 1.809787\n",
      "Ep: 301, steps: 17, D loss: 0.211979, acc:  68%, G loss: 1.765062\n",
      "Ep: 301, steps: 18, D loss: 0.246471, acc:  59%, G loss: 1.823152\n",
      "Ep: 301, steps: 19, D loss: 0.200007, acc:  71%, G loss: 1.767870\n",
      "Ep: 301, steps: 20, D loss: 0.184673, acc:  76%, G loss: 1.966576\n",
      "Ep: 301, steps: 21, D loss: 0.292778, acc:  35%, G loss: 1.673531\n",
      "Ep: 301, steps: 22, D loss: 0.208653, acc:  64%, G loss: 1.750453\n",
      "Ep: 301, steps: 23, D loss: 0.194159, acc:  72%, G loss: 2.155951\n",
      "Ep: 301, steps: 24, D loss: 0.172852, acc:  81%, G loss: 1.877874\n",
      "Ep: 301, steps: 25, D loss: 0.203924, acc:  68%, G loss: 1.880606\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 302, steps: 1, D loss: 0.239184, acc:  60%, G loss: 1.899806\n",
      "Ep: 302, steps: 2, D loss: 0.249767, acc:  54%, G loss: 1.710229\n",
      "Ep: 302, steps: 3, D loss: 0.148530, acc:  87%, G loss: 2.152265\n",
      "Ep: 302, steps: 4, D loss: 0.183633, acc:  83%, G loss: 2.020697\n",
      "Saved Model\n",
      "Ep: 302, steps: 5, D loss: 0.273648, acc:  51%, G loss: 2.047377\n",
      "Ep: 302, steps: 6, D loss: 0.373844, acc:  21%, G loss: 1.472515\n",
      "Ep: 302, steps: 7, D loss: 0.244612, acc:  57%, G loss: 1.973879\n",
      "Ep: 302, steps: 8, D loss: 0.231175, acc:  61%, G loss: 1.830513\n",
      "Ep: 302, steps: 9, D loss: 0.149592, acc:  89%, G loss: 1.779607\n",
      "Ep: 302, steps: 10, D loss: 0.229859, acc:  60%, G loss: 1.963103\n",
      "Ep: 302, steps: 11, D loss: 0.313053, acc:  34%, G loss: 1.514636\n",
      "Ep: 302, steps: 12, D loss: 0.297798, acc:  38%, G loss: 1.555571\n",
      "Ep: 302, steps: 13, D loss: 0.303132, acc:  34%, G loss: 1.592664\n",
      "Ep: 302, steps: 14, D loss: 0.234444, acc:  60%, G loss: 1.769118\n",
      "Ep: 302, steps: 15, D loss: 0.255758, acc:  55%, G loss: 1.817929\n",
      "Ep: 302, steps: 16, D loss: 0.195547, acc:  73%, G loss: 1.740393\n",
      "Ep: 302, steps: 17, D loss: 0.253076, acc:  56%, G loss: 2.001878\n",
      "Ep: 302, steps: 18, D loss: 0.205586, acc:  69%, G loss: 1.759161\n",
      "Ep: 302, steps: 19, D loss: 0.202006, acc:  71%, G loss: 2.075795\n",
      "Ep: 302, steps: 20, D loss: 0.302278, acc:  33%, G loss: 1.769790\n",
      "Ep: 302, steps: 21, D loss: 0.215287, acc:  61%, G loss: 1.836882\n",
      "Ep: 302, steps: 22, D loss: 0.221254, acc:  64%, G loss: 2.163933\n",
      "Ep: 302, steps: 23, D loss: 0.167834, acc:  83%, G loss: 1.931048\n",
      "Ep: 302, steps: 24, D loss: 0.203567, acc:  69%, G loss: 1.826064\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 303, steps: 1, D loss: 0.242815, acc:  60%, G loss: 1.990636\n",
      "Ep: 303, steps: 2, D loss: 0.256142, acc:  50%, G loss: 1.619378\n",
      "Ep: 303, steps: 3, D loss: 0.157092, acc:  87%, G loss: 2.087960\n",
      "Ep: 303, steps: 4, D loss: 0.186156, acc:  82%, G loss: 1.971683\n",
      "Ep: 303, steps: 5, D loss: 0.262688, acc:  51%, G loss: 1.948644\n",
      "Ep: 303, steps: 6, D loss: 0.268548, acc:  48%, G loss: 1.890617\n",
      "Ep: 303, steps: 7, D loss: 0.372331, acc:  22%, G loss: 1.578780\n",
      "Ep: 303, steps: 8, D loss: 0.248222, acc:  52%, G loss: 1.853629\n",
      "Ep: 303, steps: 9, D loss: 0.179664, acc:  81%, G loss: 1.816256\n",
      "Ep: 303, steps: 10, D loss: 0.152935, acc:  87%, G loss: 1.723200\n",
      "Ep: 303, steps: 11, D loss: 0.226931, acc:  61%, G loss: 1.984356\n",
      "Ep: 303, steps: 12, D loss: 0.332426, acc:  27%, G loss: 1.585506\n",
      "Ep: 303, steps: 13, D loss: 0.284260, acc:  42%, G loss: 1.661571\n",
      "Ep: 303, steps: 14, D loss: 0.293175, acc:  37%, G loss: 1.632367\n",
      "Ep: 303, steps: 15, D loss: 0.229306, acc:  62%, G loss: 1.726967\n",
      "Ep: 303, steps: 16, D loss: 0.258715, acc:  53%, G loss: 1.752305\n",
      "Ep: 303, steps: 17, D loss: 0.185819, acc:  76%, G loss: 1.721197\n",
      "Ep: 303, steps: 18, D loss: 0.233991, acc:  61%, G loss: 1.761447\n",
      "Ep: 303, steps: 19, D loss: 0.213854, acc:  66%, G loss: 1.736950\n",
      "Ep: 303, steps: 20, D loss: 0.181849, acc:  76%, G loss: 2.016578\n",
      "Ep: 303, steps: 21, D loss: 0.295801, acc:  36%, G loss: 1.784152\n",
      "Ep: 303, steps: 22, D loss: 0.176523, acc:  74%, G loss: 1.809338\n",
      "Ep: 303, steps: 23, D loss: 0.195388, acc:  71%, G loss: 2.225562\n",
      "Ep: 303, steps: 24, D loss: 0.164748, acc:  82%, G loss: 1.789752\n",
      "Ep: 303, steps: 25, D loss: 0.207920, acc:  69%, G loss: 1.872800\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 304, steps: 1, D loss: 0.257364, acc:  55%, G loss: 1.895038\n",
      "Ep: 304, steps: 2, D loss: 0.248920, acc:  55%, G loss: 1.725311\n",
      "Ep: 304, steps: 3, D loss: 0.147859, acc:  90%, G loss: 2.205155\n",
      "Ep: 304, steps: 4, D loss: 0.184658, acc:  82%, G loss: 2.043805\n",
      "Ep: 304, steps: 5, D loss: 0.299170, acc:  44%, G loss: 2.180779\n",
      "Ep: 304, steps: 6, D loss: 0.270249, acc:  51%, G loss: 1.765511\n",
      "Ep: 304, steps: 7, D loss: 0.402579, acc:  17%, G loss: 1.460346\n",
      "Ep: 304, steps: 8, D loss: 0.248955, acc:  54%, G loss: 2.017419\n",
      "Ep: 304, steps: 9, D loss: 0.195280, acc:  74%, G loss: 1.788473\n",
      "Ep: 304, steps: 10, D loss: 0.145191, acc:  89%, G loss: 1.808930\n",
      "Ep: 304, steps: 11, D loss: 0.223893, acc:  64%, G loss: 2.024701\n",
      "Ep: 304, steps: 12, D loss: 0.321482, acc:  30%, G loss: 1.486907\n",
      "Ep: 304, steps: 13, D loss: 0.286010, acc:  40%, G loss: 1.539303\n",
      "Ep: 304, steps: 14, D loss: 0.298795, acc:  36%, G loss: 1.589867\n",
      "Ep: 304, steps: 15, D loss: 0.246400, acc:  53%, G loss: 1.876768\n",
      "Ep: 304, steps: 16, D loss: 0.236377, acc:  59%, G loss: 1.791803\n",
      "Ep: 304, steps: 17, D loss: 0.187663, acc:  76%, G loss: 1.650654\n",
      "Ep: 304, steps: 18, D loss: 0.259457, acc:  56%, G loss: 1.819045\n",
      "Ep: 304, steps: 19, D loss: 0.209765, acc:  66%, G loss: 1.741416\n",
      "Ep: 304, steps: 20, D loss: 0.190896, acc:  74%, G loss: 2.124016\n",
      "Ep: 304, steps: 21, D loss: 0.281708, acc:  37%, G loss: 1.674817\n",
      "Ep: 304, steps: 22, D loss: 0.223023, acc:  62%, G loss: 1.725232\n",
      "Ep: 304, steps: 23, D loss: 0.190607, acc:  74%, G loss: 2.078216\n",
      "Ep: 304, steps: 24, D loss: 0.166294, acc:  82%, G loss: 1.803136\n",
      "Ep: 304, steps: 25, D loss: 0.202572, acc:  71%, G loss: 1.878318\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 305, steps: 1, D loss: 0.274658, acc:  50%, G loss: 2.023969\n",
      "Ep: 305, steps: 2, D loss: 0.257135, acc:  52%, G loss: 1.700778\n",
      "Saved Model\n",
      "Ep: 305, steps: 3, D loss: 0.149955, acc:  88%, G loss: 2.158922\n",
      "Ep: 305, steps: 4, D loss: 0.258335, acc:  52%, G loss: 2.135038\n",
      "Ep: 305, steps: 5, D loss: 0.264774, acc:  48%, G loss: 1.737338\n",
      "Ep: 305, steps: 6, D loss: 0.334671, acc:  28%, G loss: 1.595521\n",
      "Ep: 305, steps: 7, D loss: 0.229951, acc:  60%, G loss: 1.996006\n",
      "Ep: 305, steps: 8, D loss: 0.198862, acc:  74%, G loss: 1.803360\n",
      "Ep: 305, steps: 9, D loss: 0.157756, acc:  88%, G loss: 1.853898\n",
      "Ep: 305, steps: 10, D loss: 0.227750, acc:  61%, G loss: 1.979128\n",
      "Ep: 305, steps: 11, D loss: 0.331478, acc:  25%, G loss: 1.555859\n",
      "Ep: 305, steps: 12, D loss: 0.281404, acc:  45%, G loss: 1.546658\n",
      "Ep: 305, steps: 13, D loss: 0.295680, acc:  36%, G loss: 1.571038\n",
      "Ep: 305, steps: 14, D loss: 0.227193, acc:  63%, G loss: 1.718717\n",
      "Ep: 305, steps: 15, D loss: 0.274630, acc:  51%, G loss: 1.887659\n",
      "Ep: 305, steps: 16, D loss: 0.204746, acc:  71%, G loss: 1.784918\n",
      "Ep: 305, steps: 17, D loss: 0.222768, acc:  67%, G loss: 1.884626\n",
      "Ep: 305, steps: 18, D loss: 0.209459, acc:  69%, G loss: 1.737659\n",
      "Ep: 305, steps: 19, D loss: 0.176863, acc:  78%, G loss: 1.897164\n",
      "Ep: 305, steps: 20, D loss: 0.318308, acc:  29%, G loss: 1.744336\n",
      "Ep: 305, steps: 21, D loss: 0.195204, acc:  65%, G loss: 1.844573\n",
      "Ep: 305, steps: 22, D loss: 0.192805, acc:  72%, G loss: 2.135731\n",
      "Ep: 305, steps: 23, D loss: 0.181171, acc:  76%, G loss: 1.790441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 305, steps: 24, D loss: 0.207360, acc:  69%, G loss: 1.932309\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 306, steps: 1, D loss: 0.252175, acc:  55%, G loss: 2.008878\n",
      "Ep: 306, steps: 2, D loss: 0.262043, acc:  49%, G loss: 1.747957\n",
      "Ep: 306, steps: 3, D loss: 0.166739, acc:  83%, G loss: 2.139816\n",
      "Ep: 306, steps: 4, D loss: 0.189252, acc:  80%, G loss: 1.960017\n",
      "Ep: 306, steps: 5, D loss: 0.255210, acc:  54%, G loss: 2.014977\n",
      "Ep: 306, steps: 6, D loss: 0.262952, acc:  50%, G loss: 1.771145\n",
      "Ep: 306, steps: 7, D loss: 0.396240, acc:  16%, G loss: 1.551355\n",
      "Ep: 306, steps: 8, D loss: 0.241099, acc:  55%, G loss: 2.019700\n",
      "Ep: 306, steps: 9, D loss: 0.192757, acc:  77%, G loss: 1.886453\n",
      "Ep: 306, steps: 10, D loss: 0.162836, acc:  85%, G loss: 1.797065\n",
      "Ep: 306, steps: 11, D loss: 0.222140, acc:  63%, G loss: 1.999235\n",
      "Ep: 306, steps: 12, D loss: 0.314677, acc:  31%, G loss: 1.536625\n",
      "Ep: 306, steps: 13, D loss: 0.283223, acc:  40%, G loss: 1.607302\n",
      "Ep: 306, steps: 14, D loss: 0.288712, acc:  39%, G loss: 1.586763\n",
      "Ep: 306, steps: 15, D loss: 0.234224, acc:  59%, G loss: 1.691980\n",
      "Ep: 306, steps: 16, D loss: 0.246676, acc:  55%, G loss: 1.838166\n",
      "Ep: 306, steps: 17, D loss: 0.188878, acc:  74%, G loss: 1.691306\n",
      "Ep: 306, steps: 18, D loss: 0.278927, acc:  52%, G loss: 1.852085\n",
      "Ep: 306, steps: 19, D loss: 0.207772, acc:  67%, G loss: 1.743989\n",
      "Ep: 306, steps: 20, D loss: 0.196579, acc:  71%, G loss: 2.058519\n",
      "Ep: 306, steps: 21, D loss: 0.285760, acc:  36%, G loss: 1.684460\n",
      "Ep: 306, steps: 22, D loss: 0.218804, acc:  61%, G loss: 1.835320\n",
      "Ep: 306, steps: 23, D loss: 0.213994, acc:  67%, G loss: 2.235104\n",
      "Ep: 306, steps: 24, D loss: 0.161103, acc:  83%, G loss: 1.826381\n",
      "Ep: 306, steps: 25, D loss: 0.206801, acc:  69%, G loss: 1.947018\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 307, steps: 1, D loss: 0.248029, acc:  56%, G loss: 1.880277\n",
      "Ep: 307, steps: 2, D loss: 0.263400, acc:  48%, G loss: 1.778575\n",
      "Ep: 307, steps: 3, D loss: 0.151076, acc:  88%, G loss: 2.063837\n",
      "Ep: 307, steps: 4, D loss: 0.191749, acc:  80%, G loss: 1.906606\n",
      "Ep: 307, steps: 5, D loss: 0.269915, acc:  49%, G loss: 2.014466\n",
      "Ep: 307, steps: 6, D loss: 0.254410, acc:  50%, G loss: 1.795101\n",
      "Ep: 307, steps: 7, D loss: 0.372013, acc:  21%, G loss: 1.768539\n",
      "Ep: 307, steps: 8, D loss: 0.241496, acc:  57%, G loss: 2.060256\n",
      "Ep: 307, steps: 9, D loss: 0.188446, acc:  80%, G loss: 1.875253\n",
      "Ep: 307, steps: 10, D loss: 0.149747, acc:  89%, G loss: 1.834042\n",
      "Ep: 307, steps: 11, D loss: 0.220373, acc:  64%, G loss: 2.005215\n",
      "Ep: 307, steps: 12, D loss: 0.326761, acc:  28%, G loss: 1.509500\n",
      "Ep: 307, steps: 13, D loss: 0.280317, acc:  45%, G loss: 1.542695\n",
      "Ep: 307, steps: 14, D loss: 0.296064, acc:  35%, G loss: 1.649142\n",
      "Ep: 307, steps: 15, D loss: 0.238484, acc:  58%, G loss: 1.787119\n",
      "Ep: 307, steps: 16, D loss: 0.247302, acc:  56%, G loss: 1.875234\n",
      "Ep: 307, steps: 17, D loss: 0.206221, acc:  70%, G loss: 1.710252\n",
      "Ep: 307, steps: 18, D loss: 0.246092, acc:  60%, G loss: 1.797160\n",
      "Ep: 307, steps: 19, D loss: 0.202465, acc:  70%, G loss: 1.736759\n",
      "Ep: 307, steps: 20, D loss: 0.188657, acc:  75%, G loss: 1.959846\n",
      "Ep: 307, steps: 21, D loss: 0.298782, acc:  34%, G loss: 1.727147\n",
      "Ep: 307, steps: 22, D loss: 0.194268, acc:  68%, G loss: 1.863902\n",
      "Ep: 307, steps: 23, D loss: 0.197992, acc:  69%, G loss: 2.091223\n",
      "Ep: 307, steps: 24, D loss: 0.169206, acc:  83%, G loss: 1.842424\n",
      "Ep: 307, steps: 25, D loss: 0.207282, acc:  68%, G loss: 1.907688\n",
      "Data exhausted, Re Initialize\n",
      "Saved Model\n",
      "Ep: 308, steps: 1, D loss: 0.259733, acc:  53%, G loss: 1.996958\n",
      "Ep: 308, steps: 2, D loss: 0.149026, acc:  87%, G loss: 2.132721\n",
      "Ep: 308, steps: 3, D loss: 0.216328, acc:  71%, G loss: 1.803916\n",
      "Ep: 308, steps: 4, D loss: 0.234232, acc:  60%, G loss: 1.957133\n",
      "Ep: 308, steps: 5, D loss: 0.283467, acc:  48%, G loss: 1.756027\n",
      "Ep: 308, steps: 6, D loss: 0.380729, acc:  20%, G loss: 1.487957\n",
      "Ep: 308, steps: 7, D loss: 0.242209, acc:  55%, G loss: 2.007577\n",
      "Ep: 308, steps: 8, D loss: 0.205461, acc:  72%, G loss: 1.894034\n",
      "Ep: 308, steps: 9, D loss: 0.151859, acc:  86%, G loss: 1.851219\n",
      "Ep: 308, steps: 10, D loss: 0.237938, acc:  57%, G loss: 1.930484\n",
      "Ep: 308, steps: 11, D loss: 0.327227, acc:  27%, G loss: 1.542325\n",
      "Ep: 308, steps: 12, D loss: 0.304724, acc:  34%, G loss: 1.521095\n",
      "Ep: 308, steps: 13, D loss: 0.302353, acc:  33%, G loss: 1.579714\n",
      "Ep: 308, steps: 14, D loss: 0.241098, acc:  56%, G loss: 1.744954\n",
      "Ep: 308, steps: 15, D loss: 0.254414, acc:  54%, G loss: 1.837979\n",
      "Ep: 308, steps: 16, D loss: 0.208247, acc:  70%, G loss: 1.720136\n",
      "Ep: 308, steps: 17, D loss: 0.257645, acc:  55%, G loss: 1.866696\n",
      "Ep: 308, steps: 18, D loss: 0.200807, acc:  70%, G loss: 1.741475\n",
      "Ep: 308, steps: 19, D loss: 0.190371, acc:  73%, G loss: 2.059756\n",
      "Ep: 308, steps: 20, D loss: 0.294205, acc:  35%, G loss: 1.641024\n",
      "Ep: 308, steps: 21, D loss: 0.206838, acc:  65%, G loss: 1.732003\n",
      "Ep: 308, steps: 22, D loss: 0.202713, acc:  69%, G loss: 2.131423\n",
      "Ep: 308, steps: 23, D loss: 0.170467, acc:  81%, G loss: 1.765833\n",
      "Ep: 308, steps: 24, D loss: 0.216662, acc:  67%, G loss: 1.848550\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 309, steps: 1, D loss: 0.250242, acc:  57%, G loss: 2.072655\n",
      "Ep: 309, steps: 2, D loss: 0.263554, acc:  50%, G loss: 1.701620\n",
      "Ep: 309, steps: 3, D loss: 0.173941, acc:  79%, G loss: 2.084915\n",
      "Ep: 309, steps: 4, D loss: 0.184234, acc:  82%, G loss: 1.944262\n",
      "Ep: 309, steps: 5, D loss: 0.281961, acc:  48%, G loss: 2.081923\n",
      "Ep: 309, steps: 6, D loss: 0.254142, acc:  50%, G loss: 1.752138\n",
      "Ep: 309, steps: 7, D loss: 0.395894, acc:  16%, G loss: 1.468756\n",
      "Ep: 309, steps: 8, D loss: 0.244356, acc:  55%, G loss: 2.031186\n",
      "Ep: 309, steps: 9, D loss: 0.197000, acc:  75%, G loss: 1.807466\n",
      "Ep: 309, steps: 10, D loss: 0.151094, acc:  90%, G loss: 1.802482\n",
      "Ep: 309, steps: 11, D loss: 0.235629, acc:  58%, G loss: 1.976250\n",
      "Ep: 309, steps: 12, D loss: 0.319047, acc:  30%, G loss: 1.514233\n",
      "Ep: 309, steps: 13, D loss: 0.283491, acc:  39%, G loss: 1.567561\n",
      "Ep: 309, steps: 14, D loss: 0.301128, acc:  33%, G loss: 1.627980\n",
      "Ep: 309, steps: 15, D loss: 0.217835, acc:  68%, G loss: 1.690836\n",
      "Ep: 309, steps: 16, D loss: 0.243165, acc:  58%, G loss: 1.753136\n",
      "Ep: 309, steps: 17, D loss: 0.201608, acc:  70%, G loss: 1.719194\n",
      "Ep: 309, steps: 18, D loss: 0.244637, acc:  60%, G loss: 1.785198\n",
      "Ep: 309, steps: 19, D loss: 0.207799, acc:  67%, G loss: 1.749167\n",
      "Ep: 309, steps: 20, D loss: 0.189606, acc:  75%, G loss: 1.899903\n",
      "Ep: 309, steps: 21, D loss: 0.316032, acc:  27%, G loss: 1.538716\n",
      "Ep: 309, steps: 22, D loss: 0.212347, acc:  64%, G loss: 1.911480\n",
      "Ep: 309, steps: 23, D loss: 0.205732, acc:  70%, G loss: 2.256457\n",
      "Ep: 309, steps: 24, D loss: 0.170869, acc:  81%, G loss: 1.784966\n",
      "Ep: 309, steps: 25, D loss: 0.217063, acc:  67%, G loss: 1.753237\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 310, steps: 1, D loss: 0.257301, acc:  53%, G loss: 2.041022\n",
      "Ep: 310, steps: 2, D loss: 0.256513, acc:  51%, G loss: 1.836559\n",
      "Ep: 310, steps: 3, D loss: 0.158227, acc:  86%, G loss: 2.048039\n",
      "Ep: 310, steps: 4, D loss: 0.181239, acc:  84%, G loss: 1.891573\n",
      "Ep: 310, steps: 5, D loss: 0.260783, acc:  51%, G loss: 1.893044\n",
      "Ep: 310, steps: 6, D loss: 0.256911, acc:  51%, G loss: 1.710235\n",
      "Ep: 310, steps: 7, D loss: 0.407841, acc:  15%, G loss: 1.417634\n",
      "Ep: 310, steps: 8, D loss: 0.241616, acc:  55%, G loss: 2.076451\n",
      "Ep: 310, steps: 9, D loss: 0.197755, acc:  76%, G loss: 1.831155\n",
      "Ep: 310, steps: 10, D loss: 0.161680, acc:  85%, G loss: 1.800227\n",
      "Ep: 310, steps: 11, D loss: 0.221911, acc:  64%, G loss: 1.981363\n",
      "Ep: 310, steps: 12, D loss: 0.330383, acc:  26%, G loss: 1.520584\n",
      "Ep: 310, steps: 13, D loss: 0.286556, acc:  39%, G loss: 1.571107\n",
      "Ep: 310, steps: 14, D loss: 0.304735, acc:  35%, G loss: 1.673169\n",
      "Ep: 310, steps: 15, D loss: 0.228721, acc:  63%, G loss: 1.749430\n",
      "Ep: 310, steps: 16, D loss: 0.243258, acc:  59%, G loss: 1.777159\n",
      "Ep: 310, steps: 17, D loss: 0.191738, acc:  74%, G loss: 1.713887\n",
      "Ep: 310, steps: 18, D loss: 0.249761, acc:  57%, G loss: 1.722324\n",
      "Ep: 310, steps: 19, D loss: 0.213169, acc:  67%, G loss: 1.741005\n",
      "Ep: 310, steps: 20, D loss: 0.209625, acc:  69%, G loss: 1.887825\n",
      "Ep: 310, steps: 21, D loss: 0.302794, acc:  32%, G loss: 1.527137\n",
      "Ep: 310, steps: 22, D loss: 0.191941, acc:  71%, G loss: 1.822993\n",
      "Ep: 310, steps: 23, D loss: 0.204654, acc:  68%, G loss: 2.253052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Ep: 310, steps: 24, D loss: 0.174055, acc:  81%, G loss: 1.768726\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 311, steps: 1, D loss: 0.240874, acc:  61%, G loss: 1.972564\n",
      "Ep: 311, steps: 2, D loss: 0.236002, acc:  61%, G loss: 1.764360\n",
      "Ep: 311, steps: 3, D loss: 0.157005, acc:  83%, G loss: 2.089214\n",
      "Ep: 311, steps: 4, D loss: 0.162935, acc:  86%, G loss: 1.973787\n",
      "Ep: 311, steps: 5, D loss: 0.297564, acc:  44%, G loss: 1.895313\n",
      "Ep: 311, steps: 6, D loss: 0.251419, acc:  52%, G loss: 1.797571\n",
      "Ep: 311, steps: 7, D loss: 0.412348, acc:  15%, G loss: 1.556745\n",
      "Ep: 311, steps: 8, D loss: 0.247317, acc:  54%, G loss: 2.099262\n",
      "Ep: 311, steps: 9, D loss: 0.189927, acc:  78%, G loss: 1.849811\n",
      "Ep: 311, steps: 10, D loss: 0.151585, acc:  90%, G loss: 1.779260\n",
      "Ep: 311, steps: 11, D loss: 0.221028, acc:  65%, G loss: 1.980937\n",
      "Ep: 311, steps: 12, D loss: 0.312405, acc:  28%, G loss: 1.503381\n",
      "Ep: 311, steps: 13, D loss: 0.279850, acc:  41%, G loss: 1.559969\n",
      "Ep: 311, steps: 14, D loss: 0.295095, acc:  35%, G loss: 1.655410\n",
      "Ep: 311, steps: 15, D loss: 0.238868, acc:  57%, G loss: 1.732484\n",
      "Ep: 311, steps: 16, D loss: 0.254282, acc:  53%, G loss: 1.750572\n",
      "Ep: 311, steps: 17, D loss: 0.203672, acc:  72%, G loss: 1.729677\n",
      "Ep: 311, steps: 18, D loss: 0.249828, acc:  59%, G loss: 1.713965\n",
      "Ep: 311, steps: 19, D loss: 0.206913, acc:  68%, G loss: 1.668147\n",
      "Ep: 311, steps: 20, D loss: 0.180560, acc:  77%, G loss: 1.989692\n",
      "Ep: 311, steps: 21, D loss: 0.324064, acc:  25%, G loss: 1.847107\n",
      "Ep: 311, steps: 22, D loss: 0.205469, acc:  66%, G loss: 1.911342\n",
      "Ep: 311, steps: 23, D loss: 0.196950, acc:  72%, G loss: 2.094378\n",
      "Ep: 311, steps: 24, D loss: 0.169778, acc:  81%, G loss: 1.819111\n",
      "Ep: 311, steps: 25, D loss: 0.209671, acc:  70%, G loss: 1.762052\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 312, steps: 1, D loss: 0.246296, acc:  59%, G loss: 1.973605\n",
      "Ep: 312, steps: 2, D loss: 0.258593, acc:  51%, G loss: 1.809682\n",
      "Ep: 312, steps: 3, D loss: 0.152763, acc:  87%, G loss: 2.109841\n",
      "Ep: 312, steps: 4, D loss: 0.178239, acc:  83%, G loss: 1.855606\n",
      "Ep: 312, steps: 5, D loss: 0.241885, acc:  55%, G loss: 1.944500\n",
      "Ep: 312, steps: 6, D loss: 0.272213, acc:  48%, G loss: 1.700943\n",
      "Ep: 312, steps: 7, D loss: 0.374259, acc:  22%, G loss: 1.595399\n",
      "Ep: 312, steps: 8, D loss: 0.247705, acc:  55%, G loss: 2.018550\n",
      "Ep: 312, steps: 9, D loss: 0.199296, acc:  75%, G loss: 1.841526\n",
      "Ep: 312, steps: 10, D loss: 0.158503, acc:  88%, G loss: 1.803338\n",
      "Ep: 312, steps: 11, D loss: 0.228444, acc:  61%, G loss: 2.001940\n",
      "Ep: 312, steps: 12, D loss: 0.318218, acc:  29%, G loss: 1.531255\n",
      "Ep: 312, steps: 13, D loss: 0.286427, acc:  38%, G loss: 1.638710\n",
      "Ep: 312, steps: 14, D loss: 0.298635, acc:  36%, G loss: 1.624345\n",
      "Ep: 312, steps: 15, D loss: 0.240369, acc:  56%, G loss: 1.706595\n",
      "Ep: 312, steps: 16, D loss: 0.251948, acc:  56%, G loss: 1.781729\n",
      "Ep: 312, steps: 17, D loss: 0.184415, acc:  79%, G loss: 1.762384\n",
      "Ep: 312, steps: 18, D loss: 0.241282, acc:  59%, G loss: 1.781779\n",
      "Ep: 312, steps: 19, D loss: 0.217449, acc:  65%, G loss: 1.709812\n",
      "Ep: 312, steps: 20, D loss: 0.183206, acc:  75%, G loss: 2.140178\n",
      "Ep: 312, steps: 21, D loss: 0.281554, acc:  38%, G loss: 1.654766\n",
      "Ep: 312, steps: 22, D loss: 0.211977, acc:  63%, G loss: 1.857669\n",
      "Ep: 312, steps: 23, D loss: 0.213805, acc:  67%, G loss: 2.277181\n",
      "Ep: 312, steps: 24, D loss: 0.167719, acc:  82%, G loss: 1.862855\n",
      "Ep: 312, steps: 25, D loss: 0.213160, acc:  68%, G loss: 1.780369\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 313, steps: 1, D loss: 0.261695, acc:  51%, G loss: 2.008516\n",
      "Ep: 313, steps: 2, D loss: 0.257095, acc:  53%, G loss: 1.793178\n",
      "Ep: 313, steps: 3, D loss: 0.150748, acc:  87%, G loss: 2.145399\n",
      "Ep: 313, steps: 4, D loss: 0.185659, acc:  82%, G loss: 1.906074\n",
      "Ep: 313, steps: 5, D loss: 0.272276, acc:  50%, G loss: 1.993143\n",
      "Ep: 313, steps: 6, D loss: 0.264012, acc:  50%, G loss: 1.708648\n",
      "Ep: 313, steps: 7, D loss: 0.403301, acc:  17%, G loss: 1.774932\n",
      "Ep: 313, steps: 8, D loss: 0.235003, acc:  58%, G loss: 2.077991\n",
      "Ep: 313, steps: 9, D loss: 0.196793, acc:  76%, G loss: 1.847899\n",
      "Ep: 313, steps: 10, D loss: 0.146270, acc:  91%, G loss: 1.809352\n",
      "Ep: 313, steps: 11, D loss: 0.224390, acc:  62%, G loss: 1.932876\n",
      "Ep: 313, steps: 12, D loss: 0.321674, acc:  30%, G loss: 1.542701\n",
      "Ep: 313, steps: 13, D loss: 0.276263, acc:  44%, G loss: 1.588814\n",
      "Ep: 313, steps: 14, D loss: 0.290935, acc:  35%, G loss: 1.647418\n",
      "Ep: 313, steps: 15, D loss: 0.234661, acc:  59%, G loss: 1.635698\n",
      "Ep: 313, steps: 16, D loss: 0.232631, acc:  61%, G loss: 1.825400\n",
      "Ep: 313, steps: 17, D loss: 0.197294, acc:  75%, G loss: 1.851460\n",
      "Ep: 313, steps: 18, D loss: 0.255197, acc:  57%, G loss: 1.757391\n",
      "Ep: 313, steps: 19, D loss: 0.196418, acc:  71%, G loss: 1.719901\n",
      "Ep: 313, steps: 20, D loss: 0.166502, acc:  83%, G loss: 2.020901\n",
      "Ep: 313, steps: 21, D loss: 0.276984, acc:  41%, G loss: 1.764317\n",
      "Saved Model\n",
      "Ep: 313, steps: 22, D loss: 0.193325, acc:  69%, G loss: 1.809413\n",
      "Ep: 313, steps: 23, D loss: 0.196169, acc:  74%, G loss: 2.154078\n",
      "Ep: 313, steps: 24, D loss: 0.217832, acc:  65%, G loss: 1.904980\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 314, steps: 1, D loss: 0.258065, acc:  55%, G loss: 1.961000\n",
      "Ep: 314, steps: 2, D loss: 0.255074, acc:  53%, G loss: 1.775503\n",
      "Ep: 314, steps: 3, D loss: 0.154473, acc:  86%, G loss: 2.186592\n",
      "Ep: 314, steps: 4, D loss: 0.201672, acc:  77%, G loss: 1.938947\n",
      "Ep: 314, steps: 5, D loss: 0.235519, acc:  61%, G loss: 1.893471\n",
      "Ep: 314, steps: 6, D loss: 0.247218, acc:  52%, G loss: 1.724282\n",
      "Ep: 314, steps: 7, D loss: 0.341826, acc:  27%, G loss: 1.573576\n",
      "Ep: 314, steps: 8, D loss: 0.236988, acc:  58%, G loss: 2.026734\n",
      "Ep: 314, steps: 9, D loss: 0.205198, acc:  70%, G loss: 1.888729\n",
      "Ep: 314, steps: 10, D loss: 0.160681, acc:  86%, G loss: 1.803154\n",
      "Ep: 314, steps: 11, D loss: 0.230779, acc:  60%, G loss: 1.971218\n",
      "Ep: 314, steps: 12, D loss: 0.317213, acc:  28%, G loss: 1.494060\n",
      "Ep: 314, steps: 13, D loss: 0.298574, acc:  36%, G loss: 1.595682\n",
      "Ep: 314, steps: 14, D loss: 0.293022, acc:  37%, G loss: 1.676969\n",
      "Ep: 314, steps: 15, D loss: 0.240389, acc:  58%, G loss: 1.785820\n",
      "Ep: 314, steps: 16, D loss: 0.271482, acc:  51%, G loss: 1.807215\n",
      "Ep: 314, steps: 17, D loss: 0.213623, acc:  71%, G loss: 1.777454\n",
      "Ep: 314, steps: 18, D loss: 0.254628, acc:  56%, G loss: 1.719218\n",
      "Ep: 314, steps: 19, D loss: 0.199602, acc:  71%, G loss: 1.688705\n",
      "Ep: 314, steps: 20, D loss: 0.178069, acc:  77%, G loss: 2.079512\n",
      "Ep: 314, steps: 21, D loss: 0.298305, acc:  32%, G loss: 1.844913\n",
      "Ep: 314, steps: 22, D loss: 0.220004, acc:  61%, G loss: 1.804445\n",
      "Ep: 314, steps: 23, D loss: 0.216510, acc:  64%, G loss: 2.205766\n",
      "Ep: 314, steps: 24, D loss: 0.169304, acc:  83%, G loss: 1.788330\n",
      "Ep: 314, steps: 25, D loss: 0.219296, acc:  64%, G loss: 1.845411\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 315, steps: 1, D loss: 0.237104, acc:  61%, G loss: 2.016169\n",
      "Ep: 315, steps: 2, D loss: 0.256268, acc:  52%, G loss: 1.715215\n",
      "Ep: 315, steps: 3, D loss: 0.147618, acc:  87%, G loss: 2.115212\n",
      "Ep: 315, steps: 4, D loss: 0.184741, acc:  82%, G loss: 1.882166\n",
      "Ep: 315, steps: 5, D loss: 0.252160, acc:  52%, G loss: 1.989182\n",
      "Ep: 315, steps: 6, D loss: 0.271980, acc:  48%, G loss: 1.751872\n",
      "Ep: 315, steps: 7, D loss: 0.364855, acc:  23%, G loss: 1.468253\n",
      "Ep: 315, steps: 8, D loss: 0.240427, acc:  58%, G loss: 1.985687\n",
      "Ep: 315, steps: 9, D loss: 0.197601, acc:  75%, G loss: 1.851237\n",
      "Ep: 315, steps: 10, D loss: 0.154164, acc:  86%, G loss: 1.841339\n",
      "Ep: 315, steps: 11, D loss: 0.221202, acc:  65%, G loss: 1.982765\n",
      "Ep: 315, steps: 12, D loss: 0.323083, acc:  28%, G loss: 1.497672\n",
      "Ep: 315, steps: 13, D loss: 0.291195, acc:  39%, G loss: 1.550542\n",
      "Ep: 315, steps: 14, D loss: 0.294404, acc:  35%, G loss: 1.624141\n",
      "Ep: 315, steps: 15, D loss: 0.250359, acc:  52%, G loss: 1.872973\n",
      "Ep: 315, steps: 16, D loss: 0.256799, acc:  54%, G loss: 1.773776\n",
      "Ep: 315, steps: 17, D loss: 0.211356, acc:  69%, G loss: 1.756442\n",
      "Ep: 315, steps: 18, D loss: 0.247078, acc:  59%, G loss: 1.716377\n",
      "Ep: 315, steps: 19, D loss: 0.202460, acc:  69%, G loss: 1.739171\n",
      "Ep: 315, steps: 20, D loss: 0.173491, acc:  81%, G loss: 2.022809\n",
      "Ep: 315, steps: 21, D loss: 0.257253, acc:  46%, G loss: 1.690623\n",
      "Ep: 315, steps: 22, D loss: 0.198774, acc:  67%, G loss: 1.834048\n",
      "Ep: 315, steps: 23, D loss: 0.187652, acc:  76%, G loss: 2.140808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 315, steps: 24, D loss: 0.173229, acc:  78%, G loss: 1.769705\n",
      "Ep: 315, steps: 25, D loss: 0.223152, acc:  62%, G loss: 1.926932\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 316, steps: 1, D loss: 0.252893, acc:  55%, G loss: 1.963577\n",
      "Ep: 316, steps: 2, D loss: 0.254782, acc:  53%, G loss: 1.755470\n",
      "Ep: 316, steps: 3, D loss: 0.154589, acc:  85%, G loss: 2.170257\n",
      "Ep: 316, steps: 4, D loss: 0.188487, acc:  81%, G loss: 1.964444\n",
      "Ep: 316, steps: 5, D loss: 0.294417, acc:  46%, G loss: 2.273298\n",
      "Ep: 316, steps: 6, D loss: 0.265104, acc:  51%, G loss: 1.808225\n",
      "Ep: 316, steps: 7, D loss: 0.367753, acc:  24%, G loss: 1.464695\n",
      "Ep: 316, steps: 8, D loss: 0.239683, acc:  57%, G loss: 1.975003\n",
      "Ep: 316, steps: 9, D loss: 0.195257, acc:  75%, G loss: 1.813981\n",
      "Ep: 316, steps: 10, D loss: 0.159155, acc:  86%, G loss: 1.753426\n",
      "Ep: 316, steps: 11, D loss: 0.224423, acc:  62%, G loss: 1.943255\n",
      "Ep: 316, steps: 12, D loss: 0.316743, acc:  32%, G loss: 1.492419\n",
      "Ep: 316, steps: 13, D loss: 0.286980, acc:  42%, G loss: 1.588487\n",
      "Ep: 316, steps: 14, D loss: 0.295856, acc:  36%, G loss: 1.620053\n",
      "Ep: 316, steps: 15, D loss: 0.228651, acc:  63%, G loss: 1.725041\n",
      "Ep: 316, steps: 16, D loss: 0.234162, acc:  60%, G loss: 1.806283\n",
      "Ep: 316, steps: 17, D loss: 0.203666, acc:  71%, G loss: 1.843874\n",
      "Ep: 316, steps: 18, D loss: 0.256672, acc:  57%, G loss: 1.744126\n",
      "Ep: 316, steps: 19, D loss: 0.202231, acc:  71%, G loss: 1.680595\n",
      "Saved Model\n",
      "Ep: 316, steps: 20, D loss: 0.183522, acc:  76%, G loss: 2.000014\n",
      "Ep: 316, steps: 21, D loss: 0.214533, acc:  61%, G loss: 1.708498\n",
      "Ep: 316, steps: 22, D loss: 0.180955, acc:  75%, G loss: 2.123431\n",
      "Ep: 316, steps: 23, D loss: 0.167919, acc:  81%, G loss: 1.779333\n",
      "Ep: 316, steps: 24, D loss: 0.211762, acc:  66%, G loss: 1.802029\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 317, steps: 1, D loss: 0.229039, acc:  62%, G loss: 2.010447\n",
      "Ep: 317, steps: 2, D loss: 0.251193, acc:  54%, G loss: 1.791839\n",
      "Ep: 317, steps: 3, D loss: 0.147661, acc:  87%, G loss: 2.207353\n",
      "Ep: 317, steps: 4, D loss: 0.164419, acc:  89%, G loss: 1.892974\n",
      "Ep: 317, steps: 5, D loss: 0.250523, acc:  55%, G loss: 1.887257\n",
      "Ep: 317, steps: 6, D loss: 0.260107, acc:  49%, G loss: 1.771534\n",
      "Ep: 317, steps: 7, D loss: 0.447126, acc:  15%, G loss: 1.432827\n",
      "Ep: 317, steps: 8, D loss: 0.244109, acc:  56%, G loss: 2.003516\n",
      "Ep: 317, steps: 9, D loss: 0.194255, acc:  75%, G loss: 1.887505\n",
      "Ep: 317, steps: 10, D loss: 0.156009, acc:  85%, G loss: 1.749147\n",
      "Ep: 317, steps: 11, D loss: 0.211756, acc:  66%, G loss: 1.913143\n",
      "Ep: 317, steps: 12, D loss: 0.322532, acc:  28%, G loss: 1.428851\n",
      "Ep: 317, steps: 13, D loss: 0.291810, acc:  39%, G loss: 1.508824\n",
      "Ep: 317, steps: 14, D loss: 0.299578, acc:  36%, G loss: 1.594142\n",
      "Ep: 317, steps: 15, D loss: 0.233812, acc:  59%, G loss: 1.708527\n",
      "Ep: 317, steps: 16, D loss: 0.259020, acc:  53%, G loss: 1.915404\n",
      "Ep: 317, steps: 17, D loss: 0.182886, acc:  78%, G loss: 1.901104\n",
      "Ep: 317, steps: 18, D loss: 0.247818, acc:  56%, G loss: 1.659070\n",
      "Ep: 317, steps: 19, D loss: 0.202473, acc:  69%, G loss: 1.696851\n",
      "Ep: 317, steps: 20, D loss: 0.174235, acc:  79%, G loss: 1.893686\n",
      "Ep: 317, steps: 21, D loss: 0.297655, acc:  39%, G loss: 1.609449\n",
      "Ep: 317, steps: 22, D loss: 0.198932, acc:  69%, G loss: 1.729980\n",
      "Ep: 317, steps: 23, D loss: 0.203224, acc:  69%, G loss: 2.073235\n",
      "Ep: 317, steps: 24, D loss: 0.163508, acc:  81%, G loss: 1.856371\n",
      "Ep: 317, steps: 25, D loss: 0.205524, acc:  68%, G loss: 1.809145\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 318, steps: 1, D loss: 0.255716, acc:  52%, G loss: 1.998419\n",
      "Ep: 318, steps: 2, D loss: 0.259504, acc:  52%, G loss: 1.829049\n",
      "Ep: 318, steps: 3, D loss: 0.157103, acc:  86%, G loss: 2.080960\n",
      "Ep: 318, steps: 4, D loss: 0.180876, acc:  82%, G loss: 1.922073\n",
      "Ep: 318, steps: 5, D loss: 0.287946, acc:  46%, G loss: 2.025052\n",
      "Ep: 318, steps: 6, D loss: 0.272444, acc:  49%, G loss: 1.702305\n",
      "Ep: 318, steps: 7, D loss: 0.397802, acc:  15%, G loss: 1.451990\n",
      "Ep: 318, steps: 8, D loss: 0.233141, acc:  60%, G loss: 2.034803\n",
      "Ep: 318, steps: 9, D loss: 0.205480, acc:  74%, G loss: 1.810692\n",
      "Ep: 318, steps: 10, D loss: 0.149872, acc:  92%, G loss: 1.804276\n",
      "Ep: 318, steps: 11, D loss: 0.224835, acc:  63%, G loss: 2.027864\n",
      "Ep: 318, steps: 12, D loss: 0.330841, acc:  26%, G loss: 1.538139\n",
      "Ep: 318, steps: 13, D loss: 0.289846, acc:  37%, G loss: 1.585548\n",
      "Ep: 318, steps: 14, D loss: 0.297187, acc:  35%, G loss: 1.582586\n",
      "Ep: 318, steps: 15, D loss: 0.230048, acc:  63%, G loss: 1.785594\n",
      "Ep: 318, steps: 16, D loss: 0.245645, acc:  57%, G loss: 1.749141\n",
      "Ep: 318, steps: 17, D loss: 0.186731, acc:  78%, G loss: 1.676045\n",
      "Ep: 318, steps: 18, D loss: 0.270272, acc:  54%, G loss: 1.673316\n",
      "Ep: 318, steps: 19, D loss: 0.209768, acc:  67%, G loss: 1.674955\n",
      "Ep: 318, steps: 20, D loss: 0.169086, acc:  79%, G loss: 1.990572\n",
      "Ep: 318, steps: 21, D loss: 0.316722, acc:  31%, G loss: 1.619179\n",
      "Ep: 318, steps: 22, D loss: 0.192619, acc:  69%, G loss: 1.744705\n",
      "Ep: 318, steps: 23, D loss: 0.204427, acc:  71%, G loss: 2.163093\n",
      "Ep: 318, steps: 24, D loss: 0.173035, acc:  82%, G loss: 1.808372\n",
      "Ep: 318, steps: 25, D loss: 0.219828, acc:  64%, G loss: 1.803553\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 319, steps: 1, D loss: 0.259282, acc:  53%, G loss: 1.973937\n",
      "Ep: 319, steps: 2, D loss: 0.249050, acc:  55%, G loss: 1.747695\n",
      "Ep: 319, steps: 3, D loss: 0.157305, acc:  86%, G loss: 2.109741\n",
      "Ep: 319, steps: 4, D loss: 0.179638, acc:  84%, G loss: 1.776013\n",
      "Ep: 319, steps: 5, D loss: 0.262529, acc:  51%, G loss: 1.947798\n",
      "Ep: 319, steps: 6, D loss: 0.262623, acc:  51%, G loss: 1.782366\n",
      "Ep: 319, steps: 7, D loss: 0.368085, acc:  22%, G loss: 1.472672\n",
      "Ep: 319, steps: 8, D loss: 0.246625, acc:  54%, G loss: 1.937911\n",
      "Ep: 319, steps: 9, D loss: 0.200141, acc:  73%, G loss: 1.845696\n",
      "Ep: 319, steps: 10, D loss: 0.151776, acc:  89%, G loss: 1.769762\n",
      "Ep: 319, steps: 11, D loss: 0.218454, acc:  65%, G loss: 2.001336\n",
      "Ep: 319, steps: 12, D loss: 0.321773, acc:  28%, G loss: 1.524823\n",
      "Ep: 319, steps: 13, D loss: 0.291688, acc:  36%, G loss: 1.550777\n",
      "Ep: 319, steps: 14, D loss: 0.297299, acc:  35%, G loss: 1.663789\n",
      "Ep: 319, steps: 15, D loss: 0.234391, acc:  58%, G loss: 1.700356\n",
      "Ep: 319, steps: 16, D loss: 0.255384, acc:  56%, G loss: 1.900425\n",
      "Ep: 319, steps: 17, D loss: 0.195518, acc:  75%, G loss: 1.777791\n",
      "Saved Model\n",
      "Ep: 319, steps: 18, D loss: 0.230015, acc:  64%, G loss: 1.777824\n",
      "Ep: 319, steps: 19, D loss: 0.176532, acc:  79%, G loss: 1.957667\n",
      "Ep: 319, steps: 20, D loss: 0.286967, acc:  38%, G loss: 1.725448\n",
      "Ep: 319, steps: 21, D loss: 0.197473, acc:  69%, G loss: 1.719412\n",
      "Ep: 319, steps: 22, D loss: 0.226757, acc:  62%, G loss: 2.118773\n",
      "Ep: 319, steps: 23, D loss: 0.173242, acc:  80%, G loss: 1.856076\n",
      "Ep: 319, steps: 24, D loss: 0.212675, acc:  67%, G loss: 1.777936\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 320, steps: 1, D loss: 0.250658, acc:  55%, G loss: 1.952246\n",
      "Ep: 320, steps: 2, D loss: 0.251301, acc:  55%, G loss: 1.685775\n",
      "Ep: 320, steps: 3, D loss: 0.150054, acc:  88%, G loss: 2.022843\n",
      "Ep: 320, steps: 4, D loss: 0.189043, acc:  81%, G loss: 1.844308\n",
      "Ep: 320, steps: 5, D loss: 0.261600, acc:  51%, G loss: 1.900731\n",
      "Ep: 320, steps: 6, D loss: 0.258569, acc:  50%, G loss: 1.728989\n",
      "Ep: 320, steps: 7, D loss: 0.409791, acc:  19%, G loss: 1.949383\n",
      "Ep: 320, steps: 8, D loss: 0.242134, acc:  56%, G loss: 2.032065\n",
      "Ep: 320, steps: 9, D loss: 0.227589, acc:  63%, G loss: 1.861549\n",
      "Ep: 320, steps: 10, D loss: 0.165585, acc:  86%, G loss: 1.837518\n",
      "Ep: 320, steps: 11, D loss: 0.208504, acc:  68%, G loss: 2.002965\n",
      "Ep: 320, steps: 12, D loss: 0.306794, acc:  34%, G loss: 1.551242\n",
      "Ep: 320, steps: 13, D loss: 0.278626, acc:  39%, G loss: 1.596887\n",
      "Ep: 320, steps: 14, D loss: 0.286515, acc:  40%, G loss: 1.625572\n",
      "Ep: 320, steps: 15, D loss: 0.235323, acc:  56%, G loss: 1.778378\n",
      "Ep: 320, steps: 16, D loss: 0.239114, acc:  58%, G loss: 1.850080\n",
      "Ep: 320, steps: 17, D loss: 0.185765, acc:  76%, G loss: 1.780902\n",
      "Ep: 320, steps: 18, D loss: 0.247697, acc:  57%, G loss: 1.694499\n",
      "Ep: 320, steps: 19, D loss: 0.207218, acc:  71%, G loss: 1.742201\n",
      "Ep: 320, steps: 20, D loss: 0.180829, acc:  77%, G loss: 1.933988\n",
      "Ep: 320, steps: 21, D loss: 0.272479, acc:  43%, G loss: 1.777675\n",
      "Ep: 320, steps: 22, D loss: 0.198651, acc:  68%, G loss: 1.641518\n",
      "Ep: 320, steps: 23, D loss: 0.191351, acc:  72%, G loss: 2.148771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 320, steps: 24, D loss: 0.151665, acc:  85%, G loss: 1.779334\n",
      "Ep: 320, steps: 25, D loss: 0.206298, acc:  67%, G loss: 1.730935\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 321, steps: 1, D loss: 0.285947, acc:  44%, G loss: 1.906975\n",
      "Ep: 321, steps: 2, D loss: 0.256574, acc:  53%, G loss: 1.770141\n",
      "Ep: 321, steps: 3, D loss: 0.150719, acc:  89%, G loss: 2.114887\n",
      "Ep: 321, steps: 4, D loss: 0.184510, acc:  82%, G loss: 1.791936\n",
      "Ep: 321, steps: 5, D loss: 0.265721, acc:  50%, G loss: 1.939115\n",
      "Ep: 321, steps: 6, D loss: 0.265605, acc:  51%, G loss: 1.716751\n",
      "Ep: 321, steps: 7, D loss: 0.338019, acc:  30%, G loss: 1.765889\n",
      "Ep: 321, steps: 8, D loss: 0.224523, acc:  62%, G loss: 2.141144\n",
      "Ep: 321, steps: 9, D loss: 0.224815, acc:  65%, G loss: 1.851857\n",
      "Ep: 321, steps: 10, D loss: 0.157680, acc:  87%, G loss: 1.785620\n",
      "Ep: 321, steps: 11, D loss: 0.223216, acc:  65%, G loss: 1.897440\n",
      "Ep: 321, steps: 12, D loss: 0.320135, acc:  30%, G loss: 1.529038\n",
      "Ep: 321, steps: 13, D loss: 0.299858, acc:  33%, G loss: 1.577774\n",
      "Ep: 321, steps: 14, D loss: 0.291564, acc:  37%, G loss: 1.657119\n",
      "Ep: 321, steps: 15, D loss: 0.235123, acc:  60%, G loss: 1.810667\n",
      "Ep: 321, steps: 16, D loss: 0.247086, acc:  57%, G loss: 1.874978\n",
      "Ep: 321, steps: 17, D loss: 0.193882, acc:  77%, G loss: 1.694813\n",
      "Ep: 321, steps: 18, D loss: 0.253904, acc:  58%, G loss: 1.683075\n",
      "Ep: 321, steps: 19, D loss: 0.213629, acc:  65%, G loss: 1.690604\n",
      "Ep: 321, steps: 20, D loss: 0.183233, acc:  76%, G loss: 1.996335\n",
      "Ep: 321, steps: 21, D loss: 0.286765, acc:  36%, G loss: 1.647488\n",
      "Ep: 321, steps: 22, D loss: 0.196855, acc:  69%, G loss: 1.666394\n",
      "Ep: 321, steps: 23, D loss: 0.204045, acc:  71%, G loss: 2.254372\n",
      "Ep: 321, steps: 24, D loss: 0.164619, acc:  83%, G loss: 1.739253\n",
      "Ep: 321, steps: 25, D loss: 0.223635, acc:  62%, G loss: 1.956523\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 322, steps: 1, D loss: 0.240930, acc:  58%, G loss: 1.940556\n",
      "Ep: 322, steps: 2, D loss: 0.262909, acc:  50%, G loss: 1.645542\n",
      "Ep: 322, steps: 3, D loss: 0.153387, acc:  87%, G loss: 2.036945\n",
      "Ep: 322, steps: 4, D loss: 0.183717, acc:  82%, G loss: 1.876238\n",
      "Ep: 322, steps: 5, D loss: 0.278632, acc:  47%, G loss: 1.994590\n",
      "Ep: 322, steps: 6, D loss: 0.253862, acc:  51%, G loss: 1.765042\n",
      "Ep: 322, steps: 7, D loss: 0.354921, acc:  24%, G loss: 1.554857\n",
      "Ep: 322, steps: 8, D loss: 0.246298, acc:  53%, G loss: 2.073115\n",
      "Ep: 322, steps: 9, D loss: 0.220645, acc:  67%, G loss: 1.847772\n",
      "Ep: 322, steps: 10, D loss: 0.162056, acc:  86%, G loss: 1.775964\n",
      "Ep: 322, steps: 11, D loss: 0.222326, acc:  64%, G loss: 1.921443\n",
      "Ep: 322, steps: 12, D loss: 0.311662, acc:  29%, G loss: 1.547947\n",
      "Ep: 322, steps: 13, D loss: 0.305095, acc:  33%, G loss: 1.597512\n",
      "Ep: 322, steps: 14, D loss: 0.298984, acc:  36%, G loss: 1.652897\n",
      "Ep: 322, steps: 15, D loss: 0.232850, acc:  60%, G loss: 1.742878\n",
      "Saved Model\n",
      "Ep: 322, steps: 16, D loss: 0.264695, acc:  52%, G loss: 1.789526\n",
      "Ep: 322, steps: 17, D loss: 0.223465, acc:  65%, G loss: 1.775004\n",
      "Ep: 322, steps: 18, D loss: 0.222867, acc:  62%, G loss: 1.699029\n",
      "Ep: 322, steps: 19, D loss: 0.166650, acc:  80%, G loss: 2.012781\n",
      "Ep: 322, steps: 20, D loss: 0.308024, acc:  33%, G loss: 1.564961\n",
      "Ep: 322, steps: 21, D loss: 0.202030, acc:  67%, G loss: 1.686302\n",
      "Ep: 322, steps: 22, D loss: 0.200687, acc:  69%, G loss: 2.148680\n",
      "Ep: 322, steps: 23, D loss: 0.186137, acc:  77%, G loss: 1.716522\n",
      "Ep: 322, steps: 24, D loss: 0.219640, acc:  61%, G loss: 1.954892\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 323, steps: 1, D loss: 0.229380, acc:  64%, G loss: 1.921645\n",
      "Ep: 323, steps: 2, D loss: 0.269007, acc:  48%, G loss: 1.702866\n",
      "Ep: 323, steps: 3, D loss: 0.158154, acc:  83%, G loss: 2.006627\n",
      "Ep: 323, steps: 4, D loss: 0.195057, acc:  79%, G loss: 1.736840\n",
      "Ep: 323, steps: 5, D loss: 0.265068, acc:  51%, G loss: 1.818027\n",
      "Ep: 323, steps: 6, D loss: 0.266739, acc:  50%, G loss: 1.659022\n",
      "Ep: 323, steps: 7, D loss: 0.353048, acc:  22%, G loss: 1.455869\n",
      "Ep: 323, steps: 8, D loss: 0.232314, acc:  59%, G loss: 1.982134\n",
      "Ep: 323, steps: 9, D loss: 0.207601, acc:  72%, G loss: 1.843293\n",
      "Ep: 323, steps: 10, D loss: 0.159476, acc:  86%, G loss: 1.740822\n",
      "Ep: 323, steps: 11, D loss: 0.213037, acc:  67%, G loss: 1.893676\n",
      "Ep: 323, steps: 12, D loss: 0.314303, acc:  29%, G loss: 1.458096\n",
      "Ep: 323, steps: 13, D loss: 0.287653, acc:  39%, G loss: 1.507906\n",
      "Ep: 323, steps: 14, D loss: 0.299166, acc:  34%, G loss: 1.585703\n",
      "Ep: 323, steps: 15, D loss: 0.225149, acc:  64%, G loss: 1.693646\n",
      "Ep: 323, steps: 16, D loss: 0.251669, acc:  54%, G loss: 1.851392\n",
      "Ep: 323, steps: 17, D loss: 0.206945, acc:  70%, G loss: 1.749477\n",
      "Ep: 323, steps: 18, D loss: 0.257897, acc:  52%, G loss: 1.681584\n",
      "Ep: 323, steps: 19, D loss: 0.206360, acc:  68%, G loss: 1.674631\n",
      "Ep: 323, steps: 20, D loss: 0.176877, acc:  78%, G loss: 1.920148\n",
      "Ep: 323, steps: 21, D loss: 0.314627, acc:  26%, G loss: 1.654168\n",
      "Ep: 323, steps: 22, D loss: 0.189410, acc:  69%, G loss: 1.707411\n",
      "Ep: 323, steps: 23, D loss: 0.187666, acc:  74%, G loss: 2.149154\n",
      "Ep: 323, steps: 24, D loss: 0.171654, acc:  81%, G loss: 1.835946\n",
      "Ep: 323, steps: 25, D loss: 0.215875, acc:  66%, G loss: 1.744616\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 324, steps: 1, D loss: 0.262061, acc:  50%, G loss: 1.993294\n",
      "Ep: 324, steps: 2, D loss: 0.257964, acc:  54%, G loss: 1.671517\n",
      "Ep: 324, steps: 3, D loss: 0.155928, acc:  85%, G loss: 2.042413\n",
      "Ep: 324, steps: 4, D loss: 0.188755, acc:  80%, G loss: 1.768064\n",
      "Ep: 324, steps: 5, D loss: 0.289640, acc:  47%, G loss: 2.146958\n",
      "Ep: 324, steps: 6, D loss: 0.257522, acc:  53%, G loss: 1.804207\n",
      "Ep: 324, steps: 7, D loss: 0.353440, acc:  25%, G loss: 1.487129\n",
      "Ep: 324, steps: 8, D loss: 0.231174, acc:  61%, G loss: 1.971244\n",
      "Ep: 324, steps: 9, D loss: 0.212728, acc:  68%, G loss: 1.846495\n",
      "Ep: 324, steps: 10, D loss: 0.159250, acc:  86%, G loss: 1.741373\n",
      "Ep: 324, steps: 11, D loss: 0.230845, acc:  61%, G loss: 1.901180\n",
      "Ep: 324, steps: 12, D loss: 0.305555, acc:  33%, G loss: 1.480395\n",
      "Ep: 324, steps: 13, D loss: 0.284704, acc:  38%, G loss: 1.543292\n",
      "Ep: 324, steps: 14, D loss: 0.282220, acc:  40%, G loss: 1.610127\n",
      "Ep: 324, steps: 15, D loss: 0.234915, acc:  59%, G loss: 1.774757\n",
      "Ep: 324, steps: 16, D loss: 0.242492, acc:  57%, G loss: 1.798234\n",
      "Ep: 324, steps: 17, D loss: 0.199093, acc:  73%, G loss: 1.812751\n",
      "Ep: 324, steps: 18, D loss: 0.248910, acc:  57%, G loss: 1.653432\n",
      "Ep: 324, steps: 19, D loss: 0.208407, acc:  68%, G loss: 1.769311\n",
      "Ep: 324, steps: 20, D loss: 0.180554, acc:  77%, G loss: 1.984238\n",
      "Ep: 324, steps: 21, D loss: 0.312982, acc:  28%, G loss: 1.700623\n",
      "Ep: 324, steps: 22, D loss: 0.195166, acc:  67%, G loss: 1.681219\n",
      "Ep: 324, steps: 23, D loss: 0.183274, acc:  76%, G loss: 2.142648\n",
      "Ep: 324, steps: 24, D loss: 0.177649, acc:  78%, G loss: 1.861694\n",
      "Ep: 324, steps: 25, D loss: 0.213236, acc:  65%, G loss: 1.796795\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 325, steps: 1, D loss: 0.243893, acc:  58%, G loss: 1.897846\n",
      "Ep: 325, steps: 2, D loss: 0.268510, acc:  48%, G loss: 1.823577\n",
      "Ep: 325, steps: 3, D loss: 0.150644, acc:  85%, G loss: 2.091772\n",
      "Ep: 325, steps: 4, D loss: 0.185694, acc:  82%, G loss: 1.803332\n",
      "Ep: 325, steps: 5, D loss: 0.237994, acc:  57%, G loss: 1.899238\n",
      "Ep: 325, steps: 6, D loss: 0.261769, acc:  49%, G loss: 1.798587\n",
      "Ep: 325, steps: 7, D loss: 0.391682, acc:  19%, G loss: 1.496547\n",
      "Ep: 325, steps: 8, D loss: 0.226988, acc:  62%, G loss: 1.900161\n",
      "Ep: 325, steps: 9, D loss: 0.212166, acc:  69%, G loss: 1.959778\n",
      "Ep: 325, steps: 10, D loss: 0.164249, acc:  85%, G loss: 1.702644\n",
      "Ep: 325, steps: 11, D loss: 0.218410, acc:  66%, G loss: 1.915311\n",
      "Ep: 325, steps: 12, D loss: 0.308904, acc:  34%, G loss: 1.484827\n",
      "Ep: 325, steps: 13, D loss: 0.310678, acc:  28%, G loss: 1.520605\n",
      "Saved Model\n",
      "Ep: 325, steps: 14, D loss: 0.291129, acc:  37%, G loss: 1.633976\n",
      "Ep: 325, steps: 15, D loss: 0.251267, acc:  55%, G loss: 1.828455\n",
      "Ep: 325, steps: 16, D loss: 0.217263, acc:  69%, G loss: 1.676134\n",
      "Ep: 325, steps: 17, D loss: 0.240013, acc:  61%, G loss: 1.636596\n",
      "Ep: 325, steps: 18, D loss: 0.221941, acc:  64%, G loss: 1.669002\n",
      "Ep: 325, steps: 19, D loss: 0.176846, acc:  76%, G loss: 2.103695\n",
      "Ep: 325, steps: 20, D loss: 0.286783, acc:  37%, G loss: 1.577417\n",
      "Ep: 325, steps: 21, D loss: 0.186350, acc:  70%, G loss: 1.824163\n",
      "Ep: 325, steps: 22, D loss: 0.220979, acc:  63%, G loss: 2.181218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 325, steps: 23, D loss: 0.173441, acc:  82%, G loss: 1.821186\n",
      "Ep: 325, steps: 24, D loss: 0.223009, acc:  62%, G loss: 1.819528\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 326, steps: 1, D loss: 0.236046, acc:  60%, G loss: 1.954558\n",
      "Ep: 326, steps: 2, D loss: 0.248846, acc:  55%, G loss: 1.733899\n",
      "Ep: 326, steps: 3, D loss: 0.165052, acc:  81%, G loss: 2.076534\n",
      "Ep: 326, steps: 4, D loss: 0.194198, acc:  80%, G loss: 1.863973\n",
      "Ep: 326, steps: 5, D loss: 0.244095, acc:  55%, G loss: 1.904237\n",
      "Ep: 326, steps: 6, D loss: 0.253815, acc:  52%, G loss: 1.770110\n",
      "Ep: 326, steps: 7, D loss: 0.357870, acc:  22%, G loss: 1.745640\n",
      "Ep: 326, steps: 8, D loss: 0.232652, acc:  59%, G loss: 1.956709\n",
      "Ep: 326, steps: 9, D loss: 0.204788, acc:  72%, G loss: 1.814201\n",
      "Ep: 326, steps: 10, D loss: 0.160076, acc:  85%, G loss: 1.746933\n",
      "Ep: 326, steps: 11, D loss: 0.233432, acc:  58%, G loss: 1.894700\n",
      "Ep: 326, steps: 12, D loss: 0.323881, acc:  29%, G loss: 1.528927\n",
      "Ep: 326, steps: 13, D loss: 0.295190, acc:  39%, G loss: 1.607617\n",
      "Ep: 326, steps: 14, D loss: 0.296862, acc:  34%, G loss: 1.624500\n",
      "Ep: 326, steps: 15, D loss: 0.244770, acc:  55%, G loss: 1.748441\n",
      "Ep: 326, steps: 16, D loss: 0.247782, acc:  55%, G loss: 1.795262\n",
      "Ep: 326, steps: 17, D loss: 0.208810, acc:  70%, G loss: 1.935505\n",
      "Ep: 326, steps: 18, D loss: 0.238685, acc:  61%, G loss: 1.731860\n",
      "Ep: 326, steps: 19, D loss: 0.202162, acc:  70%, G loss: 1.724684\n",
      "Ep: 326, steps: 20, D loss: 0.179003, acc:  79%, G loss: 1.966613\n",
      "Ep: 326, steps: 21, D loss: 0.305033, acc:  32%, G loss: 1.800710\n",
      "Ep: 326, steps: 22, D loss: 0.193990, acc:  69%, G loss: 1.657029\n",
      "Ep: 326, steps: 23, D loss: 0.184118, acc:  75%, G loss: 2.140689\n",
      "Ep: 326, steps: 24, D loss: 0.163916, acc:  85%, G loss: 1.805469\n",
      "Ep: 326, steps: 25, D loss: 0.217217, acc:  64%, G loss: 1.811414\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 327, steps: 1, D loss: 0.255497, acc:  56%, G loss: 1.958050\n",
      "Ep: 327, steps: 2, D loss: 0.260132, acc:  52%, G loss: 1.736570\n",
      "Ep: 327, steps: 3, D loss: 0.154809, acc:  89%, G loss: 2.009385\n",
      "Ep: 327, steps: 4, D loss: 0.193452, acc:  79%, G loss: 1.833213\n",
      "Ep: 327, steps: 5, D loss: 0.260514, acc:  52%, G loss: 1.949418\n",
      "Ep: 327, steps: 6, D loss: 0.255802, acc:  51%, G loss: 1.793518\n",
      "Ep: 327, steps: 7, D loss: 0.342220, acc:  25%, G loss: 1.542556\n",
      "Ep: 327, steps: 8, D loss: 0.231677, acc:  61%, G loss: 1.929755\n",
      "Ep: 327, steps: 9, D loss: 0.219949, acc:  67%, G loss: 1.866638\n",
      "Ep: 327, steps: 10, D loss: 0.158135, acc:  85%, G loss: 1.704436\n",
      "Ep: 327, steps: 11, D loss: 0.239122, acc:  56%, G loss: 1.991877\n",
      "Ep: 327, steps: 12, D loss: 0.311950, acc:  32%, G loss: 1.497675\n",
      "Ep: 327, steps: 13, D loss: 0.293565, acc:  37%, G loss: 1.541734\n",
      "Ep: 327, steps: 14, D loss: 0.286772, acc:  38%, G loss: 1.607713\n",
      "Ep: 327, steps: 15, D loss: 0.234504, acc:  59%, G loss: 1.711937\n",
      "Ep: 327, steps: 16, D loss: 0.256617, acc:  53%, G loss: 1.866567\n",
      "Ep: 327, steps: 17, D loss: 0.207763, acc:  72%, G loss: 1.823451\n",
      "Ep: 327, steps: 18, D loss: 0.241083, acc:  60%, G loss: 1.699550\n",
      "Ep: 327, steps: 19, D loss: 0.208741, acc:  67%, G loss: 1.717918\n",
      "Ep: 327, steps: 20, D loss: 0.183144, acc:  74%, G loss: 2.115387\n",
      "Ep: 327, steps: 21, D loss: 0.303475, acc:  33%, G loss: 1.611949\n",
      "Ep: 327, steps: 22, D loss: 0.200054, acc:  67%, G loss: 1.714208\n",
      "Ep: 327, steps: 23, D loss: 0.212290, acc:  66%, G loss: 2.126046\n",
      "Ep: 327, steps: 24, D loss: 0.175405, acc:  79%, G loss: 1.775589\n",
      "Ep: 327, steps: 25, D loss: 0.228035, acc:  61%, G loss: 1.804354\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 328, steps: 1, D loss: 0.229257, acc:  62%, G loss: 1.951701\n",
      "Ep: 328, steps: 2, D loss: 0.248332, acc:  55%, G loss: 1.609225\n",
      "Ep: 328, steps: 3, D loss: 0.151852, acc:  87%, G loss: 2.032781\n",
      "Ep: 328, steps: 4, D loss: 0.180894, acc:  84%, G loss: 1.734828\n",
      "Ep: 328, steps: 5, D loss: 0.269886, acc:  50%, G loss: 1.957953\n",
      "Ep: 328, steps: 6, D loss: 0.263349, acc:  51%, G loss: 1.734458\n",
      "Ep: 328, steps: 7, D loss: 0.391306, acc:  18%, G loss: 1.627265\n",
      "Ep: 328, steps: 8, D loss: 0.243841, acc:  56%, G loss: 1.914104\n",
      "Ep: 328, steps: 9, D loss: 0.213361, acc:  70%, G loss: 1.890186\n",
      "Ep: 328, steps: 10, D loss: 0.158480, acc:  88%, G loss: 1.673957\n",
      "Ep: 328, steps: 11, D loss: 0.221562, acc:  65%, G loss: 1.946848\n",
      "Saved Model\n",
      "Ep: 328, steps: 12, D loss: 0.319742, acc:  27%, G loss: 1.486193\n",
      "Ep: 328, steps: 13, D loss: 0.300845, acc:  33%, G loss: 1.522604\n",
      "Ep: 328, steps: 14, D loss: 0.225124, acc:  64%, G loss: 1.675431\n",
      "Ep: 328, steps: 15, D loss: 0.234753, acc:  60%, G loss: 1.759697\n",
      "Ep: 328, steps: 16, D loss: 0.184788, acc:  78%, G loss: 1.804309\n",
      "Ep: 328, steps: 17, D loss: 0.239612, acc:  62%, G loss: 1.628273\n",
      "Ep: 328, steps: 18, D loss: 0.210109, acc:  66%, G loss: 1.706217\n",
      "Ep: 328, steps: 19, D loss: 0.163568, acc:  82%, G loss: 2.060097\n",
      "Ep: 328, steps: 20, D loss: 0.310392, acc:  31%, G loss: 1.580208\n",
      "Ep: 328, steps: 21, D loss: 0.206876, acc:  65%, G loss: 1.695514\n",
      "Ep: 328, steps: 22, D loss: 0.180901, acc:  75%, G loss: 2.145517\n",
      "Ep: 328, steps: 23, D loss: 0.179426, acc:  77%, G loss: 1.867394\n",
      "Ep: 328, steps: 24, D loss: 0.213664, acc:  65%, G loss: 1.757818\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 329, steps: 1, D loss: 0.249627, acc:  56%, G loss: 2.002821\n",
      "Ep: 329, steps: 2, D loss: 0.261889, acc:  51%, G loss: 1.716065\n",
      "Ep: 329, steps: 3, D loss: 0.146731, acc:  87%, G loss: 2.045634\n",
      "Ep: 329, steps: 4, D loss: 0.179285, acc:  84%, G loss: 1.841890\n",
      "Ep: 329, steps: 5, D loss: 0.281904, acc:  49%, G loss: 1.905634\n",
      "Ep: 329, steps: 6, D loss: 0.257711, acc:  52%, G loss: 1.763351\n",
      "Ep: 329, steps: 7, D loss: 0.388365, acc:  17%, G loss: 1.539557\n",
      "Ep: 329, steps: 8, D loss: 0.240206, acc:  57%, G loss: 1.930540\n",
      "Ep: 329, steps: 9, D loss: 0.228182, acc:  65%, G loss: 1.820795\n",
      "Ep: 329, steps: 10, D loss: 0.157980, acc:  87%, G loss: 1.695893\n",
      "Ep: 329, steps: 11, D loss: 0.222857, acc:  62%, G loss: 1.977726\n",
      "Ep: 329, steps: 12, D loss: 0.336727, acc:  23%, G loss: 1.518032\n",
      "Ep: 329, steps: 13, D loss: 0.296438, acc:  35%, G loss: 1.550888\n",
      "Ep: 329, steps: 14, D loss: 0.293699, acc:  32%, G loss: 1.690468\n",
      "Ep: 329, steps: 15, D loss: 0.228617, acc:  64%, G loss: 1.708783\n",
      "Ep: 329, steps: 16, D loss: 0.238602, acc:  57%, G loss: 1.775909\n",
      "Ep: 329, steps: 17, D loss: 0.211331, acc:  69%, G loss: 1.748858\n",
      "Ep: 329, steps: 18, D loss: 0.246305, acc:  60%, G loss: 1.718353\n",
      "Ep: 329, steps: 19, D loss: 0.217023, acc:  66%, G loss: 1.756145\n",
      "Ep: 329, steps: 20, D loss: 0.177046, acc:  77%, G loss: 2.033091\n",
      "Ep: 329, steps: 21, D loss: 0.294735, acc:  34%, G loss: 1.520990\n",
      "Ep: 329, steps: 22, D loss: 0.213498, acc:  64%, G loss: 1.691256\n",
      "Ep: 329, steps: 23, D loss: 0.206799, acc:  67%, G loss: 2.161322\n",
      "Ep: 329, steps: 24, D loss: 0.177454, acc:  82%, G loss: 1.865818\n",
      "Ep: 329, steps: 25, D loss: 0.228153, acc:  62%, G loss: 1.944361\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 330, steps: 1, D loss: 0.248899, acc:  57%, G loss: 1.993236\n",
      "Ep: 330, steps: 2, D loss: 0.249106, acc:  54%, G loss: 1.631384\n",
      "Ep: 330, steps: 3, D loss: 0.161836, acc:  86%, G loss: 2.035597\n",
      "Ep: 330, steps: 4, D loss: 0.193664, acc:  80%, G loss: 1.792164\n",
      "Ep: 330, steps: 5, D loss: 0.255936, acc:  54%, G loss: 1.872801\n",
      "Ep: 330, steps: 6, D loss: 0.258550, acc:  50%, G loss: 1.725251\n",
      "Ep: 330, steps: 7, D loss: 0.361988, acc:  26%, G loss: 1.556668\n",
      "Ep: 330, steps: 8, D loss: 0.232446, acc:  59%, G loss: 1.810776\n",
      "Ep: 330, steps: 9, D loss: 0.206887, acc:  70%, G loss: 1.801312\n",
      "Ep: 330, steps: 10, D loss: 0.159469, acc:  88%, G loss: 1.725185\n",
      "Ep: 330, steps: 11, D loss: 0.227422, acc:  62%, G loss: 1.922640\n",
      "Ep: 330, steps: 12, D loss: 0.330651, acc:  25%, G loss: 1.486052\n",
      "Ep: 330, steps: 13, D loss: 0.301878, acc:  35%, G loss: 1.532019\n",
      "Ep: 330, steps: 14, D loss: 0.285892, acc:  38%, G loss: 1.598875\n",
      "Ep: 330, steps: 15, D loss: 0.235791, acc:  58%, G loss: 1.681911\n",
      "Ep: 330, steps: 16, D loss: 0.244014, acc:  56%, G loss: 1.725097\n",
      "Ep: 330, steps: 17, D loss: 0.221632, acc:  65%, G loss: 1.654853\n",
      "Ep: 330, steps: 18, D loss: 0.263012, acc:  53%, G loss: 1.743646\n",
      "Ep: 330, steps: 19, D loss: 0.208642, acc:  69%, G loss: 1.681911\n",
      "Ep: 330, steps: 20, D loss: 0.176342, acc:  77%, G loss: 1.955308\n",
      "Ep: 330, steps: 21, D loss: 0.309366, acc:  28%, G loss: 1.597359\n",
      "Ep: 330, steps: 22, D loss: 0.193315, acc:  68%, G loss: 1.672273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 330, steps: 23, D loss: 0.214517, acc:  66%, G loss: 2.189342\n",
      "Ep: 330, steps: 24, D loss: 0.178309, acc:  81%, G loss: 1.709535\n",
      "Ep: 330, steps: 25, D loss: 0.219723, acc:  62%, G loss: 2.334112\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 331, steps: 1, D loss: 0.227919, acc:  63%, G loss: 2.053600\n",
      "Ep: 331, steps: 2, D loss: 0.252577, acc:  52%, G loss: 1.632945\n",
      "Ep: 331, steps: 3, D loss: 0.154889, acc:  87%, G loss: 2.139429\n",
      "Ep: 331, steps: 4, D loss: 0.186597, acc:  81%, G loss: 1.799440\n",
      "Ep: 331, steps: 5, D loss: 0.241416, acc:  56%, G loss: 1.895544\n",
      "Ep: 331, steps: 6, D loss: 0.255609, acc:  52%, G loss: 1.710230\n",
      "Ep: 331, steps: 7, D loss: 0.350382, acc:  26%, G loss: 1.502707\n",
      "Ep: 331, steps: 8, D loss: 0.237044, acc:  59%, G loss: 1.848704\n",
      "Ep: 331, steps: 9, D loss: 0.247648, acc:  60%, G loss: 1.804060\n",
      "Saved Model\n",
      "Ep: 331, steps: 10, D loss: 0.162375, acc:  85%, G loss: 1.693158\n",
      "Ep: 331, steps: 11, D loss: 0.301184, acc:  30%, G loss: 1.503521\n",
      "Ep: 331, steps: 12, D loss: 0.266970, acc:  46%, G loss: 1.510188\n",
      "Ep: 331, steps: 13, D loss: 0.286027, acc:  40%, G loss: 1.652822\n",
      "Ep: 331, steps: 14, D loss: 0.258340, acc:  49%, G loss: 1.656246\n",
      "Ep: 331, steps: 15, D loss: 0.267794, acc:  50%, G loss: 1.721011\n",
      "Ep: 331, steps: 16, D loss: 0.228956, acc:  63%, G loss: 1.766801\n",
      "Ep: 331, steps: 17, D loss: 0.245690, acc:  57%, G loss: 1.674076\n",
      "Ep: 331, steps: 18, D loss: 0.220118, acc:  66%, G loss: 1.694595\n",
      "Ep: 331, steps: 19, D loss: 0.179402, acc:  78%, G loss: 2.025239\n",
      "Ep: 331, steps: 20, D loss: 0.277803, acc:  41%, G loss: 1.619552\n",
      "Ep: 331, steps: 21, D loss: 0.184580, acc:  71%, G loss: 1.623692\n",
      "Ep: 331, steps: 22, D loss: 0.200912, acc:  70%, G loss: 2.125064\n",
      "Ep: 331, steps: 23, D loss: 0.172259, acc:  83%, G loss: 1.821419\n",
      "Ep: 331, steps: 24, D loss: 0.207721, acc:  68%, G loss: 1.822401\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 332, steps: 1, D loss: 0.272232, acc:  52%, G loss: 2.122965\n",
      "Ep: 332, steps: 2, D loss: 0.250981, acc:  54%, G loss: 1.675636\n",
      "Ep: 332, steps: 3, D loss: 0.150859, acc:  88%, G loss: 1.986349\n",
      "Ep: 332, steps: 4, D loss: 0.192856, acc:  80%, G loss: 1.696625\n",
      "Ep: 332, steps: 5, D loss: 0.271825, acc:  51%, G loss: 1.818104\n",
      "Ep: 332, steps: 6, D loss: 0.258190, acc:  50%, G loss: 1.728775\n",
      "Ep: 332, steps: 7, D loss: 0.370676, acc:  19%, G loss: 1.454241\n",
      "Ep: 332, steps: 8, D loss: 0.232647, acc:  60%, G loss: 1.813468\n",
      "Ep: 332, steps: 9, D loss: 0.208877, acc:  69%, G loss: 1.800413\n",
      "Ep: 332, steps: 10, D loss: 0.154768, acc:  87%, G loss: 1.721939\n",
      "Ep: 332, steps: 11, D loss: 0.231107, acc:  59%, G loss: 1.941756\n",
      "Ep: 332, steps: 12, D loss: 0.322454, acc:  25%, G loss: 1.515098\n",
      "Ep: 332, steps: 13, D loss: 0.291037, acc:  38%, G loss: 1.570052\n",
      "Ep: 332, steps: 14, D loss: 0.285461, acc:  36%, G loss: 1.669877\n",
      "Ep: 332, steps: 15, D loss: 0.221596, acc:  66%, G loss: 1.642747\n",
      "Ep: 332, steps: 16, D loss: 0.259110, acc:  51%, G loss: 1.772040\n",
      "Ep: 332, steps: 17, D loss: 0.213595, acc:  71%, G loss: 1.763055\n",
      "Ep: 332, steps: 18, D loss: 0.244599, acc:  59%, G loss: 1.736819\n",
      "Ep: 332, steps: 19, D loss: 0.208699, acc:  67%, G loss: 1.765626\n",
      "Ep: 332, steps: 20, D loss: 0.165845, acc:  81%, G loss: 1.997117\n",
      "Ep: 332, steps: 21, D loss: 0.308964, acc:  29%, G loss: 1.881953\n",
      "Ep: 332, steps: 22, D loss: 0.197966, acc:  67%, G loss: 1.673556\n",
      "Ep: 332, steps: 23, D loss: 0.201409, acc:  71%, G loss: 2.146923\n",
      "Ep: 332, steps: 24, D loss: 0.183606, acc:  80%, G loss: 1.837823\n",
      "Ep: 332, steps: 25, D loss: 0.206923, acc:  68%, G loss: 1.792522\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 333, steps: 1, D loss: 0.235161, acc:  62%, G loss: 1.881318\n",
      "Ep: 333, steps: 2, D loss: 0.253027, acc:  54%, G loss: 1.653836\n",
      "Ep: 333, steps: 3, D loss: 0.168215, acc:  79%, G loss: 2.091182\n",
      "Ep: 333, steps: 4, D loss: 0.185489, acc:  81%, G loss: 1.851474\n",
      "Ep: 333, steps: 5, D loss: 0.259707, acc:  51%, G loss: 1.838567\n",
      "Ep: 333, steps: 6, D loss: 0.254551, acc:  51%, G loss: 1.746162\n",
      "Ep: 333, steps: 7, D loss: 0.376695, acc:  20%, G loss: 1.505427\n",
      "Ep: 333, steps: 8, D loss: 0.235828, acc:  58%, G loss: 1.891997\n",
      "Ep: 333, steps: 9, D loss: 0.205033, acc:  72%, G loss: 1.859762\n",
      "Ep: 333, steps: 10, D loss: 0.159108, acc:  86%, G loss: 1.745765\n",
      "Ep: 333, steps: 11, D loss: 0.231453, acc:  61%, G loss: 1.986323\n",
      "Ep: 333, steps: 12, D loss: 0.306954, acc:  30%, G loss: 1.514754\n",
      "Ep: 333, steps: 13, D loss: 0.288751, acc:  37%, G loss: 1.596361\n",
      "Ep: 333, steps: 14, D loss: 0.284663, acc:  38%, G loss: 1.655777\n",
      "Ep: 333, steps: 15, D loss: 0.239035, acc:  58%, G loss: 1.733700\n",
      "Ep: 333, steps: 16, D loss: 0.254551, acc:  53%, G loss: 1.836507\n",
      "Ep: 333, steps: 17, D loss: 0.196901, acc:  77%, G loss: 1.691235\n",
      "Ep: 333, steps: 18, D loss: 0.241780, acc:  61%, G loss: 1.694968\n",
      "Ep: 333, steps: 19, D loss: 0.199380, acc:  71%, G loss: 1.740316\n",
      "Ep: 333, steps: 20, D loss: 0.172267, acc:  79%, G loss: 2.000926\n",
      "Ep: 333, steps: 21, D loss: 0.254130, acc:  49%, G loss: 1.708668\n",
      "Ep: 333, steps: 22, D loss: 0.184118, acc:  72%, G loss: 1.864665\n",
      "Ep: 333, steps: 23, D loss: 0.203328, acc:  70%, G loss: 2.094615\n",
      "Ep: 333, steps: 24, D loss: 0.171763, acc:  78%, G loss: 1.879121\n",
      "Ep: 333, steps: 25, D loss: 0.205285, acc:  67%, G loss: 2.033852\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 334, steps: 1, D loss: 0.241449, acc:  58%, G loss: 1.900296\n",
      "Ep: 334, steps: 2, D loss: 0.260857, acc:  50%, G loss: 1.638187\n",
      "Ep: 334, steps: 3, D loss: 0.151450, acc:  88%, G loss: 2.036166\n",
      "Ep: 334, steps: 4, D loss: 0.186213, acc:  81%, G loss: 1.852428\n",
      "Ep: 334, steps: 5, D loss: 0.275117, acc:  49%, G loss: 1.858611\n",
      "Ep: 334, steps: 6, D loss: 0.259749, acc:  53%, G loss: 1.748865\n",
      "Ep: 334, steps: 7, D loss: 0.362913, acc:  22%, G loss: 1.504369\n",
      "Saved Model\n",
      "Ep: 334, steps: 8, D loss: 0.235990, acc:  59%, G loss: 1.857631\n",
      "Ep: 334, steps: 9, D loss: 0.178970, acc:  78%, G loss: 1.648000\n",
      "Ep: 334, steps: 10, D loss: 0.274348, acc:  47%, G loss: 1.829666\n",
      "Ep: 334, steps: 11, D loss: 0.304525, acc:  29%, G loss: 1.455796\n",
      "Ep: 334, steps: 12, D loss: 0.286555, acc:  37%, G loss: 1.530221\n",
      "Ep: 334, steps: 13, D loss: 0.289712, acc:  35%, G loss: 1.662557\n",
      "Ep: 334, steps: 14, D loss: 0.239925, acc:  58%, G loss: 1.703853\n",
      "Ep: 334, steps: 15, D loss: 0.276307, acc:  51%, G loss: 1.693345\n",
      "Ep: 334, steps: 16, D loss: 0.192554, acc:  75%, G loss: 1.913777\n",
      "Ep: 334, steps: 17, D loss: 0.247733, acc:  57%, G loss: 1.652205\n",
      "Ep: 334, steps: 18, D loss: 0.213938, acc:  67%, G loss: 1.689680\n",
      "Ep: 334, steps: 19, D loss: 0.187971, acc:  76%, G loss: 1.962648\n",
      "Ep: 334, steps: 20, D loss: 0.270025, acc:  43%, G loss: 1.558222\n",
      "Ep: 334, steps: 21, D loss: 0.197244, acc:  69%, G loss: 1.636450\n",
      "Ep: 334, steps: 22, D loss: 0.204119, acc:  69%, G loss: 2.120080\n",
      "Ep: 334, steps: 23, D loss: 0.181136, acc:  79%, G loss: 1.819322\n",
      "Ep: 334, steps: 24, D loss: 0.201149, acc:  70%, G loss: 1.806120\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 335, steps: 1, D loss: 0.261188, acc:  52%, G loss: 1.995893\n",
      "Ep: 335, steps: 2, D loss: 0.249844, acc:  55%, G loss: 1.683930\n",
      "Ep: 335, steps: 3, D loss: 0.165971, acc:  86%, G loss: 2.016914\n",
      "Ep: 335, steps: 4, D loss: 0.183452, acc:  81%, G loss: 1.767864\n",
      "Ep: 335, steps: 5, D loss: 0.285845, acc:  47%, G loss: 1.813919\n",
      "Ep: 335, steps: 6, D loss: 0.258643, acc:  51%, G loss: 1.714336\n",
      "Ep: 335, steps: 7, D loss: 0.371231, acc:  21%, G loss: 1.736269\n",
      "Ep: 335, steps: 8, D loss: 0.239069, acc:  57%, G loss: 2.081709\n",
      "Ep: 335, steps: 9, D loss: 0.215105, acc:  68%, G loss: 1.805392\n",
      "Ep: 335, steps: 10, D loss: 0.167656, acc:  84%, G loss: 1.812684\n",
      "Ep: 335, steps: 11, D loss: 0.222703, acc:  66%, G loss: 1.912389\n",
      "Ep: 335, steps: 12, D loss: 0.308436, acc:  30%, G loss: 1.489143\n",
      "Ep: 335, steps: 13, D loss: 0.282491, acc:  40%, G loss: 1.488142\n",
      "Ep: 335, steps: 14, D loss: 0.293779, acc:  35%, G loss: 1.571806\n",
      "Ep: 335, steps: 15, D loss: 0.239949, acc:  58%, G loss: 1.753744\n",
      "Ep: 335, steps: 16, D loss: 0.222042, acc:  63%, G loss: 1.741343\n",
      "Ep: 335, steps: 17, D loss: 0.195373, acc:  73%, G loss: 1.769632\n",
      "Ep: 335, steps: 18, D loss: 0.225727, acc:  64%, G loss: 1.732212\n",
      "Ep: 335, steps: 19, D loss: 0.202551, acc:  69%, G loss: 1.713474\n",
      "Ep: 335, steps: 20, D loss: 0.171427, acc:  79%, G loss: 1.932536\n",
      "Ep: 335, steps: 21, D loss: 0.268451, acc:  44%, G loss: 1.608468\n",
      "Ep: 335, steps: 22, D loss: 0.177554, acc:  75%, G loss: 1.689094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 335, steps: 23, D loss: 0.208904, acc:  68%, G loss: 2.141627\n",
      "Ep: 335, steps: 24, D loss: 0.177934, acc:  77%, G loss: 1.877687\n",
      "Ep: 335, steps: 25, D loss: 0.215066, acc:  64%, G loss: 1.780352\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 336, steps: 1, D loss: 0.228041, acc:  61%, G loss: 1.869554\n",
      "Ep: 336, steps: 2, D loss: 0.247084, acc:  55%, G loss: 1.626792\n",
      "Ep: 336, steps: 3, D loss: 0.157136, acc:  85%, G loss: 2.066080\n",
      "Ep: 336, steps: 4, D loss: 0.180212, acc:  85%, G loss: 1.785426\n",
      "Ep: 336, steps: 5, D loss: 0.270797, acc:  51%, G loss: 1.821417\n",
      "Ep: 336, steps: 6, D loss: 0.273043, acc:  49%, G loss: 1.728911\n",
      "Ep: 336, steps: 7, D loss: 0.359514, acc:  22%, G loss: 1.556492\n",
      "Ep: 336, steps: 8, D loss: 0.246216, acc:  56%, G loss: 1.987457\n",
      "Ep: 336, steps: 9, D loss: 0.221559, acc:  68%, G loss: 1.831226\n",
      "Ep: 336, steps: 10, D loss: 0.159657, acc:  86%, G loss: 1.722517\n",
      "Ep: 336, steps: 11, D loss: 0.223458, acc:  63%, G loss: 1.945627\n",
      "Ep: 336, steps: 12, D loss: 0.320332, acc:  27%, G loss: 1.491458\n",
      "Ep: 336, steps: 13, D loss: 0.292924, acc:  37%, G loss: 1.567012\n",
      "Ep: 336, steps: 14, D loss: 0.292599, acc:  35%, G loss: 1.605733\n",
      "Ep: 336, steps: 15, D loss: 0.232934, acc:  60%, G loss: 1.670012\n",
      "Ep: 336, steps: 16, D loss: 0.257992, acc:  55%, G loss: 1.817881\n",
      "Ep: 336, steps: 17, D loss: 0.212846, acc:  70%, G loss: 1.824999\n",
      "Ep: 336, steps: 18, D loss: 0.227470, acc:  64%, G loss: 1.693297\n",
      "Ep: 336, steps: 19, D loss: 0.206906, acc:  68%, G loss: 1.698341\n",
      "Ep: 336, steps: 20, D loss: 0.188719, acc:  72%, G loss: 1.978850\n",
      "Ep: 336, steps: 21, D loss: 0.305947, acc:  29%, G loss: 1.634485\n",
      "Ep: 336, steps: 22, D loss: 0.190880, acc:  70%, G loss: 1.708480\n",
      "Ep: 336, steps: 23, D loss: 0.202374, acc:  70%, G loss: 2.172948\n",
      "Ep: 336, steps: 24, D loss: 0.186348, acc:  80%, G loss: 1.898878\n",
      "Ep: 336, steps: 25, D loss: 0.215492, acc:  65%, G loss: 1.859615\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 337, steps: 1, D loss: 0.237138, acc:  61%, G loss: 1.904574\n",
      "Ep: 337, steps: 2, D loss: 0.244464, acc:  58%, G loss: 1.610439\n",
      "Ep: 337, steps: 3, D loss: 0.157444, acc:  86%, G loss: 2.036045\n",
      "Ep: 337, steps: 4, D loss: 0.189247, acc:  80%, G loss: 1.766100\n",
      "Ep: 337, steps: 5, D loss: 0.295303, acc:  47%, G loss: 2.213468\n",
      "Saved Model\n",
      "Ep: 337, steps: 6, D loss: 0.257978, acc:  51%, G loss: 1.745363\n",
      "Ep: 337, steps: 7, D loss: 0.254888, acc:  51%, G loss: 1.812209\n",
      "Ep: 337, steps: 8, D loss: 0.165915, acc:  83%, G loss: 1.922985\n",
      "Ep: 337, steps: 9, D loss: 0.127705, acc:  94%, G loss: 1.804000\n",
      "Ep: 337, steps: 10, D loss: 0.193420, acc:  74%, G loss: 2.027962\n",
      "Ep: 337, steps: 11, D loss: 0.391484, acc:  13%, G loss: 1.418729\n",
      "Ep: 337, steps: 12, D loss: 0.331165, acc:  25%, G loss: 1.514992\n",
      "Ep: 337, steps: 13, D loss: 0.333907, acc:  27%, G loss: 1.613985\n",
      "Ep: 337, steps: 14, D loss: 0.236080, acc:  59%, G loss: 1.760514\n",
      "Ep: 337, steps: 15, D loss: 0.226382, acc:  62%, G loss: 1.834871\n",
      "Ep: 337, steps: 16, D loss: 0.197943, acc:  74%, G loss: 1.768989\n",
      "Ep: 337, steps: 17, D loss: 0.252951, acc:  55%, G loss: 1.649853\n",
      "Ep: 337, steps: 18, D loss: 0.191814, acc:  72%, G loss: 1.706552\n",
      "Ep: 337, steps: 19, D loss: 0.178414, acc:  76%, G loss: 1.945252\n",
      "Ep: 337, steps: 20, D loss: 0.306491, acc:  32%, G loss: 1.640213\n",
      "Ep: 337, steps: 21, D loss: 0.202706, acc:  66%, G loss: 1.816022\n",
      "Ep: 337, steps: 22, D loss: 0.187439, acc:  75%, G loss: 2.226822\n",
      "Ep: 337, steps: 23, D loss: 0.168532, acc:  81%, G loss: 1.870626\n",
      "Ep: 337, steps: 24, D loss: 0.215687, acc:  65%, G loss: 2.025749\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 338, steps: 1, D loss: 0.238861, acc:  58%, G loss: 2.006467\n",
      "Ep: 338, steps: 2, D loss: 0.246584, acc:  55%, G loss: 1.666241\n",
      "Ep: 338, steps: 3, D loss: 0.149731, acc:  89%, G loss: 2.106416\n",
      "Ep: 338, steps: 4, D loss: 0.177512, acc:  83%, G loss: 1.906256\n",
      "Ep: 338, steps: 5, D loss: 0.236477, acc:  57%, G loss: 1.912519\n",
      "Ep: 338, steps: 6, D loss: 0.268478, acc:  50%, G loss: 1.695622\n",
      "Ep: 338, steps: 7, D loss: 0.401011, acc:  19%, G loss: 1.459839\n",
      "Ep: 338, steps: 8, D loss: 0.235314, acc:  59%, G loss: 1.998133\n",
      "Ep: 338, steps: 9, D loss: 0.262295, acc:  60%, G loss: 1.932358\n",
      "Ep: 338, steps: 10, D loss: 0.179935, acc:  79%, G loss: 1.734536\n",
      "Ep: 338, steps: 11, D loss: 0.230765, acc:  62%, G loss: 1.930602\n",
      "Ep: 338, steps: 12, D loss: 0.326756, acc:  26%, G loss: 1.445360\n",
      "Ep: 338, steps: 13, D loss: 0.294653, acc:  36%, G loss: 1.523075\n",
      "Ep: 338, steps: 14, D loss: 0.278827, acc:  40%, G loss: 1.629947\n",
      "Ep: 338, steps: 15, D loss: 0.224514, acc:  66%, G loss: 1.681168\n",
      "Ep: 338, steps: 16, D loss: 0.262734, acc:  53%, G loss: 1.727117\n",
      "Ep: 338, steps: 17, D loss: 0.213453, acc:  70%, G loss: 1.686201\n",
      "Ep: 338, steps: 18, D loss: 0.216096, acc:  68%, G loss: 1.702625\n",
      "Ep: 338, steps: 19, D loss: 0.214318, acc:  68%, G loss: 1.682635\n",
      "Ep: 338, steps: 20, D loss: 0.175417, acc:  76%, G loss: 1.911591\n",
      "Ep: 338, steps: 21, D loss: 0.302378, acc:  34%, G loss: 1.608382\n",
      "Ep: 338, steps: 22, D loss: 0.204809, acc:  67%, G loss: 1.827412\n",
      "Ep: 338, steps: 23, D loss: 0.208931, acc:  69%, G loss: 2.215334\n",
      "Ep: 338, steps: 24, D loss: 0.171279, acc:  82%, G loss: 1.970145\n",
      "Ep: 338, steps: 25, D loss: 0.226093, acc:  62%, G loss: 1.885036\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 339, steps: 1, D loss: 0.251741, acc:  56%, G loss: 1.858276\n",
      "Ep: 339, steps: 2, D loss: 0.253471, acc:  53%, G loss: 1.714172\n",
      "Ep: 339, steps: 3, D loss: 0.144545, acc:  89%, G loss: 2.084572\n",
      "Ep: 339, steps: 4, D loss: 0.185206, acc:  83%, G loss: 1.781022\n",
      "Ep: 339, steps: 5, D loss: 0.266809, acc:  49%, G loss: 1.911171\n",
      "Ep: 339, steps: 6, D loss: 0.272699, acc:  49%, G loss: 1.655997\n",
      "Ep: 339, steps: 7, D loss: 0.358114, acc:  23%, G loss: 1.770099\n",
      "Ep: 339, steps: 8, D loss: 0.240004, acc:  56%, G loss: 1.954195\n",
      "Ep: 339, steps: 9, D loss: 0.201220, acc:  73%, G loss: 1.820738\n",
      "Ep: 339, steps: 10, D loss: 0.153858, acc:  88%, G loss: 1.672951\n",
      "Ep: 339, steps: 11, D loss: 0.242437, acc:  56%, G loss: 1.891601\n",
      "Ep: 339, steps: 12, D loss: 0.327135, acc:  26%, G loss: 1.539818\n",
      "Ep: 339, steps: 13, D loss: 0.283567, acc:  42%, G loss: 1.522017\n",
      "Ep: 339, steps: 14, D loss: 0.279517, acc:  39%, G loss: 1.581078\n",
      "Ep: 339, steps: 15, D loss: 0.232115, acc:  63%, G loss: 1.725383\n",
      "Ep: 339, steps: 16, D loss: 0.261431, acc:  52%, G loss: 1.722940\n",
      "Ep: 339, steps: 17, D loss: 0.194336, acc:  76%, G loss: 1.815226\n",
      "Ep: 339, steps: 18, D loss: 0.264536, acc:  52%, G loss: 1.711939\n",
      "Ep: 339, steps: 19, D loss: 0.207538, acc:  68%, G loss: 1.719753\n",
      "Ep: 339, steps: 20, D loss: 0.173298, acc:  79%, G loss: 1.896892\n",
      "Ep: 339, steps: 21, D loss: 0.276717, acc:  41%, G loss: 1.629773\n",
      "Ep: 339, steps: 22, D loss: 0.183272, acc:  73%, G loss: 1.639525\n",
      "Ep: 339, steps: 23, D loss: 0.183174, acc:  76%, G loss: 2.105796\n",
      "Ep: 339, steps: 24, D loss: 0.163788, acc:  85%, G loss: 1.879169\n",
      "Ep: 339, steps: 25, D loss: 0.218984, acc:  63%, G loss: 1.957700\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 340, steps: 1, D loss: 0.240112, acc:  59%, G loss: 2.063140\n",
      "Ep: 340, steps: 2, D loss: 0.251662, acc:  54%, G loss: 1.711411\n",
      "Ep: 340, steps: 3, D loss: 0.157109, acc:  86%, G loss: 2.124886\n",
      "Saved Model\n",
      "Ep: 340, steps: 4, D loss: 0.194057, acc:  80%, G loss: 1.790514\n",
      "Ep: 340, steps: 5, D loss: 0.265228, acc:  49%, G loss: 1.641888\n",
      "Ep: 340, steps: 6, D loss: 0.376267, acc:  21%, G loss: 1.464545\n",
      "Ep: 340, steps: 7, D loss: 0.230156, acc:  61%, G loss: 1.948015\n",
      "Ep: 340, steps: 8, D loss: 0.200967, acc:  76%, G loss: 1.817080\n",
      "Ep: 340, steps: 9, D loss: 0.170160, acc:  82%, G loss: 1.816997\n",
      "Ep: 340, steps: 10, D loss: 0.228491, acc:  60%, G loss: 1.891434\n",
      "Ep: 340, steps: 11, D loss: 0.337826, acc:  24%, G loss: 1.539057\n",
      "Ep: 340, steps: 12, D loss: 0.310720, acc:  35%, G loss: 1.629034\n",
      "Ep: 340, steps: 13, D loss: 0.288842, acc:  39%, G loss: 1.582153\n",
      "Ep: 340, steps: 14, D loss: 0.220867, acc:  68%, G loss: 1.683826\n",
      "Ep: 340, steps: 15, D loss: 0.258371, acc:  54%, G loss: 1.757610\n",
      "Ep: 340, steps: 16, D loss: 0.207064, acc:  73%, G loss: 1.776853\n",
      "Ep: 340, steps: 17, D loss: 0.233234, acc:  63%, G loss: 1.674647\n",
      "Ep: 340, steps: 18, D loss: 0.201441, acc:  71%, G loss: 1.706349\n",
      "Ep: 340, steps: 19, D loss: 0.170980, acc:  80%, G loss: 1.980464\n",
      "Ep: 340, steps: 20, D loss: 0.294186, acc:  34%, G loss: 1.550207\n",
      "Ep: 340, steps: 21, D loss: 0.190913, acc:  69%, G loss: 1.719310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 340, steps: 22, D loss: 0.197125, acc:  71%, G loss: 2.121459\n",
      "Ep: 340, steps: 23, D loss: 0.178341, acc:  80%, G loss: 1.832105\n",
      "Ep: 340, steps: 24, D loss: 0.249453, acc:  55%, G loss: 2.013272\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 341, steps: 1, D loss: 0.228531, acc:  64%, G loss: 1.940894\n",
      "Ep: 341, steps: 2, D loss: 0.242893, acc:  58%, G loss: 1.765720\n",
      "Ep: 341, steps: 3, D loss: 0.149780, acc:  89%, G loss: 2.044058\n",
      "Ep: 341, steps: 4, D loss: 0.177944, acc:  84%, G loss: 1.793396\n",
      "Ep: 341, steps: 5, D loss: 0.281847, acc:  46%, G loss: 1.869236\n",
      "Ep: 341, steps: 6, D loss: 0.262334, acc:  49%, G loss: 1.647595\n",
      "Ep: 341, steps: 7, D loss: 0.367531, acc:  23%, G loss: 1.450157\n",
      "Ep: 341, steps: 8, D loss: 0.231211, acc:  59%, G loss: 1.951638\n",
      "Ep: 341, steps: 9, D loss: 0.206001, acc:  72%, G loss: 1.779644\n",
      "Ep: 341, steps: 10, D loss: 0.156833, acc:  88%, G loss: 1.732113\n",
      "Ep: 341, steps: 11, D loss: 0.226775, acc:  62%, G loss: 1.924014\n",
      "Ep: 341, steps: 12, D loss: 0.332170, acc:  25%, G loss: 1.501814\n",
      "Ep: 341, steps: 13, D loss: 0.288741, acc:  37%, G loss: 1.554206\n",
      "Ep: 341, steps: 14, D loss: 0.291542, acc:  37%, G loss: 1.543891\n",
      "Ep: 341, steps: 15, D loss: 0.239471, acc:  58%, G loss: 1.729477\n",
      "Ep: 341, steps: 16, D loss: 0.255296, acc:  51%, G loss: 1.774752\n",
      "Ep: 341, steps: 17, D loss: 0.194311, acc:  76%, G loss: 1.767021\n",
      "Ep: 341, steps: 18, D loss: 0.249169, acc:  57%, G loss: 1.708396\n",
      "Ep: 341, steps: 19, D loss: 0.202398, acc:  70%, G loss: 1.769355\n",
      "Ep: 341, steps: 20, D loss: 0.172855, acc:  78%, G loss: 2.109913\n",
      "Ep: 341, steps: 21, D loss: 0.308344, acc:  29%, G loss: 1.534423\n",
      "Ep: 341, steps: 22, D loss: 0.192955, acc:  67%, G loss: 1.693135\n",
      "Ep: 341, steps: 23, D loss: 0.188726, acc:  74%, G loss: 2.119355\n",
      "Ep: 341, steps: 24, D loss: 0.170912, acc:  81%, G loss: 1.843499\n",
      "Ep: 341, steps: 25, D loss: 0.201635, acc:  69%, G loss: 1.928504\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 342, steps: 1, D loss: 0.253365, acc:  54%, G loss: 1.943459\n",
      "Ep: 342, steps: 2, D loss: 0.255008, acc:  54%, G loss: 1.684060\n",
      "Ep: 342, steps: 3, D loss: 0.158737, acc:  85%, G loss: 2.093685\n",
      "Ep: 342, steps: 4, D loss: 0.189676, acc:  79%, G loss: 1.797778\n",
      "Ep: 342, steps: 5, D loss: 0.297289, acc:  45%, G loss: 2.003510\n",
      "Ep: 342, steps: 6, D loss: 0.258893, acc:  50%, G loss: 1.674427\n",
      "Ep: 342, steps: 7, D loss: 0.363673, acc:  24%, G loss: 1.431545\n",
      "Ep: 342, steps: 8, D loss: 0.237568, acc:  59%, G loss: 2.137242\n",
      "Ep: 342, steps: 9, D loss: 0.208059, acc:  71%, G loss: 1.763850\n",
      "Ep: 342, steps: 10, D loss: 0.155498, acc:  88%, G loss: 1.682806\n",
      "Ep: 342, steps: 11, D loss: 0.236094, acc:  59%, G loss: 1.988598\n",
      "Ep: 342, steps: 12, D loss: 0.314458, acc:  30%, G loss: 1.514362\n",
      "Ep: 342, steps: 13, D loss: 0.285223, acc:  40%, G loss: 1.538582\n",
      "Ep: 342, steps: 14, D loss: 0.301197, acc:  32%, G loss: 1.612329\n",
      "Ep: 342, steps: 15, D loss: 0.227342, acc:  62%, G loss: 1.655625\n",
      "Ep: 342, steps: 16, D loss: 0.261524, acc:  52%, G loss: 1.878075\n",
      "Ep: 342, steps: 17, D loss: 0.203558, acc:  73%, G loss: 1.767775\n",
      "Ep: 342, steps: 18, D loss: 0.217517, acc:  68%, G loss: 1.756082\n",
      "Ep: 342, steps: 19, D loss: 0.207039, acc:  68%, G loss: 1.785892\n",
      "Ep: 342, steps: 20, D loss: 0.164202, acc:  81%, G loss: 1.885817\n",
      "Ep: 342, steps: 21, D loss: 0.307407, acc:  30%, G loss: 1.535012\n",
      "Ep: 342, steps: 22, D loss: 0.186454, acc:  71%, G loss: 1.671096\n",
      "Ep: 342, steps: 23, D loss: 0.200064, acc:  72%, G loss: 2.159251\n",
      "Ep: 342, steps: 24, D loss: 0.173156, acc:  82%, G loss: 1.809471\n",
      "Ep: 342, steps: 25, D loss: 0.206729, acc:  68%, G loss: 1.941359\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 343, steps: 1, D loss: 0.252135, acc:  54%, G loss: 1.962959\n",
      "Saved Model\n",
      "Ep: 343, steps: 2, D loss: 0.249603, acc:  54%, G loss: 1.712226\n",
      "Ep: 343, steps: 3, D loss: 0.208458, acc:  75%, G loss: 1.721653\n",
      "Ep: 343, steps: 4, D loss: 0.255627, acc:  53%, G loss: 1.779003\n",
      "Ep: 343, steps: 5, D loss: 0.235473, acc:  55%, G loss: 1.690751\n",
      "Ep: 343, steps: 6, D loss: 0.344095, acc:  27%, G loss: 1.660112\n",
      "Ep: 343, steps: 7, D loss: 0.238052, acc:  59%, G loss: 1.895143\n",
      "Ep: 343, steps: 8, D loss: 0.233244, acc:  63%, G loss: 1.776845\n",
      "Ep: 343, steps: 9, D loss: 0.173747, acc:  82%, G loss: 1.725395\n",
      "Ep: 343, steps: 10, D loss: 0.218502, acc:  64%, G loss: 1.898309\n",
      "Ep: 343, steps: 11, D loss: 0.324566, acc:  25%, G loss: 1.531817\n",
      "Ep: 343, steps: 12, D loss: 0.290560, acc:  40%, G loss: 1.542604\n",
      "Ep: 343, steps: 13, D loss: 0.289222, acc:  36%, G loss: 1.651871\n",
      "Ep: 343, steps: 14, D loss: 0.227331, acc:  64%, G loss: 1.695253\n",
      "Ep: 343, steps: 15, D loss: 0.243652, acc:  57%, G loss: 1.809206\n",
      "Ep: 343, steps: 16, D loss: 0.214931, acc:  70%, G loss: 1.874374\n",
      "Ep: 343, steps: 17, D loss: 0.233877, acc:  64%, G loss: 1.777711\n",
      "Ep: 343, steps: 18, D loss: 0.209881, acc:  67%, G loss: 1.759949\n",
      "Ep: 343, steps: 19, D loss: 0.184944, acc:  75%, G loss: 1.952335\n",
      "Ep: 343, steps: 20, D loss: 0.277562, acc:  40%, G loss: 1.580282\n",
      "Ep: 343, steps: 21, D loss: 0.204269, acc:  66%, G loss: 1.743558\n",
      "Ep: 343, steps: 22, D loss: 0.195635, acc:  72%, G loss: 2.166510\n",
      "Ep: 343, steps: 23, D loss: 0.168926, acc:  82%, G loss: 1.841950\n",
      "Ep: 343, steps: 24, D loss: 0.226264, acc:  61%, G loss: 1.881140\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 344, steps: 1, D loss: 0.245194, acc:  58%, G loss: 1.983606\n",
      "Ep: 344, steps: 2, D loss: 0.251254, acc:  55%, G loss: 1.662565\n",
      "Ep: 344, steps: 3, D loss: 0.162750, acc:  82%, G loss: 2.026321\n",
      "Ep: 344, steps: 4, D loss: 0.198315, acc:  77%, G loss: 1.764437\n",
      "Ep: 344, steps: 5, D loss: 0.269384, acc:  49%, G loss: 1.877909\n",
      "Ep: 344, steps: 6, D loss: 0.256192, acc:  53%, G loss: 1.632141\n",
      "Ep: 344, steps: 7, D loss: 0.364496, acc:  23%, G loss: 1.490703\n",
      "Ep: 344, steps: 8, D loss: 0.234903, acc:  59%, G loss: 1.903224\n",
      "Ep: 344, steps: 9, D loss: 0.241260, acc:  60%, G loss: 1.786546\n",
      "Ep: 344, steps: 10, D loss: 0.165496, acc:  84%, G loss: 1.679666\n",
      "Ep: 344, steps: 11, D loss: 0.230792, acc:  58%, G loss: 1.943370\n",
      "Ep: 344, steps: 12, D loss: 0.325346, acc:  27%, G loss: 1.528344\n",
      "Ep: 344, steps: 13, D loss: 0.287588, acc:  42%, G loss: 1.472236\n",
      "Ep: 344, steps: 14, D loss: 0.284508, acc:  37%, G loss: 1.578511\n",
      "Ep: 344, steps: 15, D loss: 0.230915, acc:  62%, G loss: 1.693409\n",
      "Ep: 344, steps: 16, D loss: 0.251565, acc:  55%, G loss: 1.791534\n",
      "Ep: 344, steps: 17, D loss: 0.195527, acc:  75%, G loss: 1.750748\n",
      "Ep: 344, steps: 18, D loss: 0.239926, acc:  59%, G loss: 1.728904\n",
      "Ep: 344, steps: 19, D loss: 0.209879, acc:  67%, G loss: 1.730125\n",
      "Ep: 344, steps: 20, D loss: 0.175506, acc:  78%, G loss: 1.942673\n",
      "Ep: 344, steps: 21, D loss: 0.312166, acc:  28%, G loss: 1.625505\n",
      "Ep: 344, steps: 22, D loss: 0.199058, acc:  67%, G loss: 1.800359\n",
      "Ep: 344, steps: 23, D loss: 0.220314, acc:  65%, G loss: 2.139289\n",
      "Ep: 344, steps: 24, D loss: 0.188659, acc:  77%, G loss: 1.774575\n",
      "Ep: 344, steps: 25, D loss: 0.221013, acc:  65%, G loss: 1.773795\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 345, steps: 1, D loss: 0.240416, acc:  60%, G loss: 1.855330\n",
      "Ep: 345, steps: 2, D loss: 0.243794, acc:  57%, G loss: 1.683516\n",
      "Ep: 345, steps: 3, D loss: 0.169210, acc:  82%, G loss: 2.019766\n",
      "Ep: 345, steps: 4, D loss: 0.180186, acc:  84%, G loss: 1.759267\n",
      "Ep: 345, steps: 5, D loss: 0.264592, acc:  52%, G loss: 1.847770\n",
      "Ep: 345, steps: 6, D loss: 0.267356, acc:  49%, G loss: 1.655000\n",
      "Ep: 345, steps: 7, D loss: 0.371828, acc:  22%, G loss: 1.513576\n",
      "Ep: 345, steps: 8, D loss: 0.239722, acc:  58%, G loss: 1.966243\n",
      "Ep: 345, steps: 9, D loss: 0.223300, acc:  66%, G loss: 1.792990\n",
      "Ep: 345, steps: 10, D loss: 0.170793, acc:  83%, G loss: 1.646851\n",
      "Ep: 345, steps: 11, D loss: 0.217939, acc:  65%, G loss: 1.907637\n",
      "Ep: 345, steps: 12, D loss: 0.312751, acc:  31%, G loss: 1.493017\n",
      "Ep: 345, steps: 13, D loss: 0.292320, acc:  39%, G loss: 1.613308\n",
      "Ep: 345, steps: 14, D loss: 0.294580, acc:  34%, G loss: 1.628936\n",
      "Ep: 345, steps: 15, D loss: 0.221701, acc:  67%, G loss: 1.710200\n",
      "Ep: 345, steps: 16, D loss: 0.248054, acc:  57%, G loss: 1.717972\n",
      "Ep: 345, steps: 17, D loss: 0.204853, acc:  73%, G loss: 1.754019\n",
      "Ep: 345, steps: 18, D loss: 0.244933, acc:  57%, G loss: 1.753778\n",
      "Ep: 345, steps: 19, D loss: 0.201924, acc:  71%, G loss: 1.710254\n",
      "Ep: 345, steps: 20, D loss: 0.182963, acc:  75%, G loss: 2.073780\n",
      "Ep: 345, steps: 21, D loss: 0.279109, acc:  38%, G loss: 1.619349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 345, steps: 22, D loss: 0.203089, acc:  66%, G loss: 1.864933\n",
      "Ep: 345, steps: 23, D loss: 0.204224, acc:  69%, G loss: 2.174694\n",
      "Ep: 345, steps: 24, D loss: 0.175349, acc:  81%, G loss: 1.852503\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 345, steps: 25, D loss: 0.225693, acc:  63%, G loss: 1.891940\n",
      "Ep: 345, steps: 26, D loss: 0.261227, acc:  51%, G loss: 1.523900\n",
      "Ep: 345, steps: 27, D loss: 0.155309, acc:  83%, G loss: 2.123155\n",
      "Ep: 345, steps: 28, D loss: 0.208338, acc:  73%, G loss: 1.799430\n",
      "Ep: 345, steps: 29, D loss: 0.237645, acc:  57%, G loss: 1.920475\n",
      "Ep: 345, steps: 30, D loss: 0.262273, acc:  52%, G loss: 1.829901\n",
      "Ep: 345, steps: 31, D loss: 0.338727, acc:  25%, G loss: 1.654458\n",
      "Ep: 345, steps: 32, D loss: 0.215306, acc:  65%, G loss: 2.041219\n",
      "Ep: 345, steps: 33, D loss: 0.229453, acc:  63%, G loss: 1.836329\n",
      "Ep: 345, steps: 34, D loss: 0.164720, acc:  84%, G loss: 1.636072\n",
      "Ep: 345, steps: 35, D loss: 0.259371, acc:  50%, G loss: 1.982660\n",
      "Ep: 345, steps: 36, D loss: 0.317434, acc:  29%, G loss: 1.559512\n",
      "Ep: 345, steps: 37, D loss: 0.282316, acc:  41%, G loss: 1.532998\n",
      "Ep: 345, steps: 38, D loss: 0.274206, acc:  42%, G loss: 1.639276\n",
      "Ep: 345, steps: 39, D loss: 0.245004, acc:  56%, G loss: 1.688003\n",
      "Ep: 345, steps: 40, D loss: 0.231212, acc:  62%, G loss: 1.752789\n",
      "Ep: 345, steps: 41, D loss: 0.216250, acc:  69%, G loss: 1.745489\n",
      "Ep: 345, steps: 42, D loss: 0.249790, acc:  58%, G loss: 1.642425\n",
      "Ep: 345, steps: 43, D loss: 0.216166, acc:  66%, G loss: 1.667148\n",
      "Ep: 345, steps: 44, D loss: 0.166047, acc:  83%, G loss: 1.891848\n",
      "Ep: 345, steps: 45, D loss: 0.300355, acc:  31%, G loss: 1.513368\n",
      "Ep: 345, steps: 46, D loss: 0.192309, acc:  69%, G loss: 1.914087\n",
      "Ep: 345, steps: 47, D loss: 0.206500, acc:  68%, G loss: 2.149073\n",
      "Ep: 345, steps: 48, D loss: 0.180149, acc:  79%, G loss: 1.834872\n",
      "Ep: 345, steps: 49, D loss: 0.209443, acc:  68%, G loss: 1.830000\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 346, steps: 1, D loss: 0.270313, acc:  49%, G loss: 1.906856\n",
      "Ep: 346, steps: 2, D loss: 0.241422, acc:  58%, G loss: 1.719563\n",
      "Ep: 346, steps: 3, D loss: 0.172215, acc:  79%, G loss: 2.101532\n",
      "Ep: 346, steps: 4, D loss: 0.192772, acc:  80%, G loss: 1.784207\n",
      "Ep: 346, steps: 5, D loss: 0.270402, acc:  50%, G loss: 1.861778\n",
      "Ep: 346, steps: 6, D loss: 0.253284, acc:  51%, G loss: 1.767791\n",
      "Ep: 346, steps: 7, D loss: 0.376920, acc:  18%, G loss: 1.652478\n",
      "Ep: 346, steps: 8, D loss: 0.237181, acc:  60%, G loss: 1.872798\n",
      "Ep: 346, steps: 9, D loss: 0.204994, acc:  74%, G loss: 1.805687\n",
      "Ep: 346, steps: 10, D loss: 0.167403, acc:  84%, G loss: 1.675176\n",
      "Ep: 346, steps: 11, D loss: 0.241156, acc:  56%, G loss: 2.032190\n",
      "Ep: 346, steps: 12, D loss: 0.307913, acc:  32%, G loss: 1.486211\n",
      "Ep: 346, steps: 13, D loss: 0.270558, acc:  45%, G loss: 1.496331\n",
      "Ep: 346, steps: 14, D loss: 0.290930, acc:  36%, G loss: 1.540303\n",
      "Ep: 346, steps: 15, D loss: 0.221739, acc:  67%, G loss: 1.673027\n",
      "Ep: 346, steps: 16, D loss: 0.248993, acc:  55%, G loss: 1.761205\n",
      "Ep: 346, steps: 17, D loss: 0.220228, acc:  65%, G loss: 1.782775\n",
      "Ep: 346, steps: 18, D loss: 0.238025, acc:  61%, G loss: 1.695474\n",
      "Ep: 346, steps: 19, D loss: 0.197818, acc:  70%, G loss: 1.660950\n",
      "Ep: 346, steps: 20, D loss: 0.170955, acc:  79%, G loss: 1.958890\n",
      "Ep: 346, steps: 21, D loss: 0.295653, acc:  32%, G loss: 1.542000\n",
      "Ep: 346, steps: 22, D loss: 0.187569, acc:  71%, G loss: 1.856589\n",
      "Ep: 346, steps: 23, D loss: 0.197917, acc:  71%, G loss: 2.151896\n",
      "Ep: 346, steps: 24, D loss: 0.186072, acc:  76%, G loss: 1.838332\n",
      "Ep: 346, steps: 25, D loss: 0.209005, acc:  68%, G loss: 1.842736\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 347, steps: 1, D loss: 0.255885, acc:  53%, G loss: 1.948420\n",
      "Ep: 347, steps: 2, D loss: 0.267308, acc:  51%, G loss: 1.653942\n",
      "Ep: 347, steps: 3, D loss: 0.169109, acc:  82%, G loss: 2.128649\n",
      "Ep: 347, steps: 4, D loss: 0.192177, acc:  79%, G loss: 1.823792\n",
      "Ep: 347, steps: 5, D loss: 0.290078, acc:  48%, G loss: 2.019549\n",
      "Ep: 347, steps: 6, D loss: 0.252651, acc:  53%, G loss: 1.779122\n",
      "Ep: 347, steps: 7, D loss: 0.363932, acc:  24%, G loss: 1.581453\n",
      "Ep: 347, steps: 8, D loss: 0.236087, acc:  60%, G loss: 1.857890\n",
      "Ep: 347, steps: 9, D loss: 0.224137, acc:  65%, G loss: 1.835896\n",
      "Ep: 347, steps: 10, D loss: 0.173448, acc:  81%, G loss: 1.695719\n",
      "Ep: 347, steps: 11, D loss: 0.220239, acc:  63%, G loss: 1.916901\n",
      "Ep: 347, steps: 12, D loss: 0.313607, acc:  30%, G loss: 1.428653\n",
      "Ep: 347, steps: 13, D loss: 0.283899, acc:  41%, G loss: 1.461791\n",
      "Ep: 347, steps: 14, D loss: 0.287816, acc:  35%, G loss: 1.561783\n",
      "Ep: 347, steps: 15, D loss: 0.230381, acc:  61%, G loss: 1.662411\n",
      "Ep: 347, steps: 16, D loss: 0.237620, acc:  60%, G loss: 1.769732\n",
      "Ep: 347, steps: 17, D loss: 0.190823, acc:  75%, G loss: 1.698556\n",
      "Ep: 347, steps: 18, D loss: 0.234919, acc:  61%, G loss: 1.673046\n",
      "Ep: 347, steps: 19, D loss: 0.222361, acc:  62%, G loss: 1.672616\n",
      "Ep: 347, steps: 20, D loss: 0.175240, acc:  78%, G loss: 1.943703\n",
      "Ep: 347, steps: 21, D loss: 0.306084, acc:  29%, G loss: 1.524103\n",
      "Ep: 347, steps: 22, D loss: 0.201303, acc:  67%, G loss: 1.828671\n",
      "Saved Model\n",
      "Ep: 347, steps: 23, D loss: 0.211903, acc:  67%, G loss: 2.151842\n",
      "Ep: 347, steps: 24, D loss: 0.230150, acc:  63%, G loss: 1.857858\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 348, steps: 1, D loss: 0.231891, acc:  60%, G loss: 1.855761\n",
      "Ep: 348, steps: 2, D loss: 0.230762, acc:  64%, G loss: 1.675220\n",
      "Ep: 348, steps: 3, D loss: 0.167604, acc:  80%, G loss: 2.130627\n",
      "Ep: 348, steps: 4, D loss: 0.192428, acc:  79%, G loss: 1.868157\n",
      "Ep: 348, steps: 5, D loss: 0.253737, acc:  52%, G loss: 1.953912\n",
      "Ep: 348, steps: 6, D loss: 0.246424, acc:  53%, G loss: 1.683544\n",
      "Ep: 348, steps: 7, D loss: 0.345262, acc:  23%, G loss: 1.395063\n",
      "Ep: 348, steps: 8, D loss: 0.236347, acc:  59%, G loss: 1.909107\n",
      "Ep: 348, steps: 9, D loss: 0.205970, acc:  73%, G loss: 1.876666\n",
      "Ep: 348, steps: 10, D loss: 0.172617, acc:  84%, G loss: 1.751227\n",
      "Ep: 348, steps: 11, D loss: 0.216450, acc:  65%, G loss: 2.004894\n",
      "Ep: 348, steps: 12, D loss: 0.319272, acc:  29%, G loss: 1.471086\n",
      "Ep: 348, steps: 13, D loss: 0.280290, acc:  43%, G loss: 1.534286\n",
      "Ep: 348, steps: 14, D loss: 0.296583, acc:  33%, G loss: 1.550466\n",
      "Ep: 348, steps: 15, D loss: 0.236251, acc:  59%, G loss: 1.715876\n",
      "Ep: 348, steps: 16, D loss: 0.253322, acc:  54%, G loss: 1.733448\n",
      "Ep: 348, steps: 17, D loss: 0.209141, acc:  72%, G loss: 1.756115\n",
      "Ep: 348, steps: 18, D loss: 0.242141, acc:  60%, G loss: 1.638108\n",
      "Ep: 348, steps: 19, D loss: 0.207675, acc:  69%, G loss: 1.692049\n",
      "Ep: 348, steps: 20, D loss: 0.174762, acc:  78%, G loss: 2.023005\n",
      "Ep: 348, steps: 21, D loss: 0.298482, acc:  30%, G loss: 1.609304\n",
      "Ep: 348, steps: 22, D loss: 0.190362, acc:  68%, G loss: 1.867209\n",
      "Ep: 348, steps: 23, D loss: 0.211900, acc:  68%, G loss: 2.211755\n",
      "Ep: 348, steps: 24, D loss: 0.180913, acc:  78%, G loss: 1.846451\n",
      "Ep: 348, steps: 25, D loss: 0.228334, acc:  61%, G loss: 2.151183\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 349, steps: 1, D loss: 0.241324, acc:  59%, G loss: 1.869764\n",
      "Ep: 349, steps: 2, D loss: 0.246657, acc:  55%, G loss: 1.715086\n",
      "Ep: 349, steps: 3, D loss: 0.149504, acc:  87%, G loss: 2.180388\n",
      "Ep: 349, steps: 4, D loss: 0.184372, acc:  82%, G loss: 1.861200\n",
      "Ep: 349, steps: 5, D loss: 0.264984, acc:  53%, G loss: 1.904635\n",
      "Ep: 349, steps: 6, D loss: 0.253256, acc:  54%, G loss: 1.753074\n",
      "Ep: 349, steps: 7, D loss: 0.361962, acc:  21%, G loss: 1.451733\n",
      "Ep: 349, steps: 8, D loss: 0.233252, acc:  60%, G loss: 2.106877\n",
      "Ep: 349, steps: 9, D loss: 0.219937, acc:  68%, G loss: 1.742562\n",
      "Ep: 349, steps: 10, D loss: 0.158639, acc:  86%, G loss: 1.744618\n",
      "Ep: 349, steps: 11, D loss: 0.230759, acc:  59%, G loss: 1.976902\n",
      "Ep: 349, steps: 12, D loss: 0.323607, acc:  27%, G loss: 1.451636\n",
      "Ep: 349, steps: 13, D loss: 0.282467, acc:  41%, G loss: 1.535918\n",
      "Ep: 349, steps: 14, D loss: 0.281566, acc:  39%, G loss: 1.614776\n",
      "Ep: 349, steps: 15, D loss: 0.224180, acc:  65%, G loss: 1.670345\n",
      "Ep: 349, steps: 16, D loss: 0.246661, acc:  56%, G loss: 1.739605\n",
      "Ep: 349, steps: 17, D loss: 0.197486, acc:  75%, G loss: 1.688888\n",
      "Ep: 349, steps: 18, D loss: 0.236003, acc:  60%, G loss: 1.733814\n",
      "Ep: 349, steps: 19, D loss: 0.207039, acc:  69%, G loss: 1.686483\n",
      "Ep: 349, steps: 20, D loss: 0.160398, acc:  84%, G loss: 1.899626\n",
      "Ep: 349, steps: 21, D loss: 0.305990, acc:  30%, G loss: 1.487728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 349, steps: 22, D loss: 0.196632, acc:  70%, G loss: 1.672458\n",
      "Ep: 349, steps: 23, D loss: 0.208388, acc:  68%, G loss: 2.130849\n",
      "Ep: 349, steps: 24, D loss: 0.186516, acc:  77%, G loss: 1.747302\n",
      "Ep: 349, steps: 25, D loss: 0.216044, acc:  65%, G loss: 1.703093\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 350, steps: 1, D loss: 0.267945, acc:  50%, G loss: 1.968481\n",
      "Ep: 350, steps: 2, D loss: 0.242218, acc:  58%, G loss: 1.671769\n",
      "Ep: 350, steps: 3, D loss: 0.163713, acc:  85%, G loss: 2.076873\n",
      "Ep: 350, steps: 4, D loss: 0.189986, acc:  80%, G loss: 1.788496\n",
      "Ep: 350, steps: 5, D loss: 0.293555, acc:  46%, G loss: 1.879199\n",
      "Ep: 350, steps: 6, D loss: 0.257822, acc:  51%, G loss: 1.646295\n",
      "Ep: 350, steps: 7, D loss: 0.363227, acc:  20%, G loss: 1.683608\n",
      "Ep: 350, steps: 8, D loss: 0.239780, acc:  57%, G loss: 1.911482\n",
      "Ep: 350, steps: 9, D loss: 0.222411, acc:  66%, G loss: 1.763668\n",
      "Ep: 350, steps: 10, D loss: 0.172722, acc:  83%, G loss: 1.682286\n",
      "Ep: 350, steps: 11, D loss: 0.223681, acc:  63%, G loss: 1.937393\n",
      "Ep: 350, steps: 12, D loss: 0.322158, acc:  24%, G loss: 1.432113\n",
      "Ep: 350, steps: 13, D loss: 0.281683, acc:  43%, G loss: 1.497217\n",
      "Ep: 350, steps: 14, D loss: 0.287880, acc:  37%, G loss: 1.587162\n",
      "Ep: 350, steps: 15, D loss: 0.231778, acc:  61%, G loss: 1.714648\n",
      "Ep: 350, steps: 16, D loss: 0.231652, acc:  62%, G loss: 1.809017\n",
      "Ep: 350, steps: 17, D loss: 0.210117, acc:  71%, G loss: 1.664175\n",
      "Ep: 350, steps: 18, D loss: 0.242177, acc:  59%, G loss: 1.627635\n",
      "Ep: 350, steps: 19, D loss: 0.222317, acc:  64%, G loss: 1.635885\n",
      "Ep: 350, steps: 20, D loss: 0.184101, acc:  75%, G loss: 2.053440\n",
      "Saved Model\n",
      "Ep: 350, steps: 21, D loss: 0.296093, acc:  32%, G loss: 1.502549\n",
      "Ep: 350, steps: 22, D loss: 0.198373, acc:  72%, G loss: 2.016498\n",
      "Ep: 350, steps: 23, D loss: 0.189112, acc:  79%, G loss: 1.717538\n",
      "Ep: 350, steps: 24, D loss: 0.222205, acc:  63%, G loss: 1.786432\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 351, steps: 1, D loss: 0.225447, acc:  63%, G loss: 1.923001\n",
      "Ep: 351, steps: 2, D loss: 0.236449, acc:  61%, G loss: 1.700995\n",
      "Ep: 351, steps: 3, D loss: 0.170280, acc:  80%, G loss: 2.087226\n",
      "Ep: 351, steps: 4, D loss: 0.186313, acc:  80%, G loss: 1.871655\n",
      "Ep: 351, steps: 5, D loss: 0.269256, acc:  49%, G loss: 1.895949\n",
      "Ep: 351, steps: 6, D loss: 0.260303, acc:  51%, G loss: 1.784408\n",
      "Ep: 351, steps: 7, D loss: 0.367062, acc:  19%, G loss: 1.392689\n",
      "Ep: 351, steps: 8, D loss: 0.230911, acc:  59%, G loss: 1.835629\n",
      "Ep: 351, steps: 9, D loss: 0.215035, acc:  70%, G loss: 1.846413\n",
      "Ep: 351, steps: 10, D loss: 0.171418, acc:  85%, G loss: 1.781079\n",
      "Ep: 351, steps: 11, D loss: 0.212201, acc:  67%, G loss: 1.935386\n",
      "Ep: 351, steps: 12, D loss: 0.325253, acc:  24%, G loss: 1.462656\n",
      "Ep: 351, steps: 13, D loss: 0.295373, acc:  36%, G loss: 1.533087\n",
      "Ep: 351, steps: 14, D loss: 0.285920, acc:  34%, G loss: 1.657644\n",
      "Ep: 351, steps: 15, D loss: 0.232951, acc:  63%, G loss: 1.698997\n",
      "Ep: 351, steps: 16, D loss: 0.255316, acc:  54%, G loss: 1.713358\n",
      "Ep: 351, steps: 17, D loss: 0.207491, acc:  71%, G loss: 1.709443\n",
      "Ep: 351, steps: 18, D loss: 0.248111, acc:  56%, G loss: 1.636690\n",
      "Ep: 351, steps: 19, D loss: 0.201843, acc:  69%, G loss: 1.729200\n",
      "Ep: 351, steps: 20, D loss: 0.162227, acc:  82%, G loss: 2.063996\n",
      "Ep: 351, steps: 21, D loss: 0.308569, acc:  27%, G loss: 1.535457\n",
      "Ep: 351, steps: 22, D loss: 0.197325, acc:  68%, G loss: 1.720876\n",
      "Ep: 351, steps: 23, D loss: 0.216599, acc:  64%, G loss: 2.272923\n",
      "Ep: 351, steps: 24, D loss: 0.183153, acc:  79%, G loss: 1.750464\n",
      "Ep: 351, steps: 25, D loss: 0.224413, acc:  62%, G loss: 2.007915\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 352, steps: 1, D loss: 0.251869, acc:  54%, G loss: 1.936146\n",
      "Ep: 352, steps: 2, D loss: 0.245395, acc:  55%, G loss: 1.694080\n",
      "Ep: 352, steps: 3, D loss: 0.161050, acc:  84%, G loss: 2.140312\n",
      "Ep: 352, steps: 4, D loss: 0.190896, acc:  79%, G loss: 2.077666\n",
      "Ep: 352, steps: 5, D loss: 0.267014, acc:  50%, G loss: 2.096071\n",
      "Ep: 352, steps: 6, D loss: 0.251355, acc:  53%, G loss: 1.637088\n",
      "Ep: 352, steps: 7, D loss: 0.348289, acc:  27%, G loss: 1.438429\n",
      "Ep: 352, steps: 8, D loss: 0.229949, acc:  61%, G loss: 1.903946\n",
      "Ep: 352, steps: 9, D loss: 0.241984, acc:  59%, G loss: 1.822797\n",
      "Ep: 352, steps: 10, D loss: 0.171608, acc:  83%, G loss: 1.626209\n",
      "Ep: 352, steps: 11, D loss: 0.219795, acc:  63%, G loss: 1.891623\n",
      "Ep: 352, steps: 12, D loss: 0.316532, acc:  27%, G loss: 1.412219\n",
      "Ep: 352, steps: 13, D loss: 0.266735, acc:  47%, G loss: 1.503124\n",
      "Ep: 352, steps: 14, D loss: 0.278830, acc:  42%, G loss: 1.639480\n",
      "Ep: 352, steps: 15, D loss: 0.241893, acc:  55%, G loss: 1.683734\n",
      "Ep: 352, steps: 16, D loss: 0.241418, acc:  58%, G loss: 1.684252\n",
      "Ep: 352, steps: 17, D loss: 0.203946, acc:  73%, G loss: 1.771295\n",
      "Ep: 352, steps: 18, D loss: 0.259771, acc:  53%, G loss: 1.648009\n",
      "Ep: 352, steps: 19, D loss: 0.207714, acc:  69%, G loss: 1.687297\n",
      "Ep: 352, steps: 20, D loss: 0.171576, acc:  81%, G loss: 1.888148\n",
      "Ep: 352, steps: 21, D loss: 0.291483, acc:  37%, G loss: 1.529023\n",
      "Ep: 352, steps: 22, D loss: 0.206399, acc:  67%, G loss: 1.704278\n",
      "Ep: 352, steps: 23, D loss: 0.194226, acc:  73%, G loss: 2.043945\n",
      "Ep: 352, steps: 24, D loss: 0.190118, acc:  74%, G loss: 1.788514\n",
      "Ep: 352, steps: 25, D loss: 0.208591, acc:  67%, G loss: 1.715429\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 353, steps: 1, D loss: 0.263489, acc:  50%, G loss: 1.831394\n",
      "Ep: 353, steps: 2, D loss: 0.256585, acc:  54%, G loss: 1.605294\n",
      "Ep: 353, steps: 3, D loss: 0.156196, acc:  86%, G loss: 2.155954\n",
      "Ep: 353, steps: 4, D loss: 0.197131, acc:  77%, G loss: 2.025162\n",
      "Ep: 353, steps: 5, D loss: 0.271600, acc:  49%, G loss: 1.881736\n",
      "Ep: 353, steps: 6, D loss: 0.263912, acc:  51%, G loss: 1.622253\n",
      "Ep: 353, steps: 7, D loss: 0.341713, acc:  23%, G loss: 1.569743\n",
      "Ep: 353, steps: 8, D loss: 0.234326, acc:  60%, G loss: 1.915156\n",
      "Ep: 353, steps: 9, D loss: 0.220646, acc:  68%, G loss: 1.786467\n",
      "Ep: 353, steps: 10, D loss: 0.160837, acc:  86%, G loss: 1.712884\n",
      "Ep: 353, steps: 11, D loss: 0.225021, acc:  62%, G loss: 1.909210\n",
      "Ep: 353, steps: 12, D loss: 0.317412, acc:  26%, G loss: 1.457381\n",
      "Ep: 353, steps: 13, D loss: 0.286807, acc:  39%, G loss: 1.552177\n",
      "Ep: 353, steps: 14, D loss: 0.285107, acc:  37%, G loss: 1.649467\n",
      "Ep: 353, steps: 15, D loss: 0.241548, acc:  57%, G loss: 1.659433\n",
      "Ep: 353, steps: 16, D loss: 0.228604, acc:  62%, G loss: 1.734687\n",
      "Ep: 353, steps: 17, D loss: 0.207128, acc:  74%, G loss: 1.689323\n",
      "Ep: 353, steps: 18, D loss: 0.237888, acc:  61%, G loss: 1.719848\n",
      "Saved Model\n",
      "Ep: 353, steps: 19, D loss: 0.201581, acc:  69%, G loss: 1.725770\n",
      "Ep: 353, steps: 20, D loss: 0.304261, acc:  26%, G loss: 1.557140\n",
      "Ep: 353, steps: 21, D loss: 0.199265, acc:  68%, G loss: 1.817140\n",
      "Ep: 353, steps: 22, D loss: 0.237567, acc:  61%, G loss: 2.225810\n",
      "Ep: 353, steps: 23, D loss: 0.172128, acc:  82%, G loss: 1.774997\n",
      "Ep: 353, steps: 24, D loss: 0.220854, acc:  63%, G loss: 1.744418\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 354, steps: 1, D loss: 0.267968, acc:  50%, G loss: 1.864702\n",
      "Ep: 354, steps: 2, D loss: 0.265464, acc:  49%, G loss: 1.572075\n",
      "Ep: 354, steps: 3, D loss: 0.164277, acc:  82%, G loss: 2.057650\n",
      "Ep: 354, steps: 4, D loss: 0.189192, acc:  81%, G loss: 1.911481\n",
      "Ep: 354, steps: 5, D loss: 0.280929, acc:  47%, G loss: 1.825292\n",
      "Ep: 354, steps: 6, D loss: 0.255842, acc:  53%, G loss: 1.707146\n",
      "Ep: 354, steps: 7, D loss: 0.364374, acc:  18%, G loss: 1.484881\n",
      "Ep: 354, steps: 8, D loss: 0.237472, acc:  59%, G loss: 1.835723\n",
      "Ep: 354, steps: 9, D loss: 0.234736, acc:  62%, G loss: 1.762731\n",
      "Ep: 354, steps: 10, D loss: 0.172295, acc:  84%, G loss: 1.718395\n",
      "Ep: 354, steps: 11, D loss: 0.235437, acc:  57%, G loss: 1.963592\n",
      "Ep: 354, steps: 12, D loss: 0.312998, acc:  29%, G loss: 1.536152\n",
      "Ep: 354, steps: 13, D loss: 0.282036, acc:  39%, G loss: 1.633492\n",
      "Ep: 354, steps: 14, D loss: 0.282998, acc:  40%, G loss: 1.640185\n",
      "Ep: 354, steps: 15, D loss: 0.230903, acc:  61%, G loss: 1.652660\n",
      "Ep: 354, steps: 16, D loss: 0.239750, acc:  59%, G loss: 1.801866\n",
      "Ep: 354, steps: 17, D loss: 0.202685, acc:  73%, G loss: 1.678040\n",
      "Ep: 354, steps: 18, D loss: 0.227018, acc:  64%, G loss: 1.720631\n",
      "Ep: 354, steps: 19, D loss: 0.212660, acc:  67%, G loss: 1.717098\n",
      "Ep: 354, steps: 20, D loss: 0.172849, acc:  78%, G loss: 1.855865\n",
      "Ep: 354, steps: 21, D loss: 0.310147, acc:  29%, G loss: 1.529267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 354, steps: 22, D loss: 0.181526, acc:  72%, G loss: 1.742146\n",
      "Ep: 354, steps: 23, D loss: 0.197459, acc:  71%, G loss: 2.091035\n",
      "Ep: 354, steps: 24, D loss: 0.195370, acc:  75%, G loss: 1.791093\n",
      "Ep: 354, steps: 25, D loss: 0.226639, acc:  60%, G loss: 1.657837\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 355, steps: 1, D loss: 0.259455, acc:  52%, G loss: 1.850091\n",
      "Ep: 355, steps: 2, D loss: 0.248875, acc:  54%, G loss: 1.652919\n",
      "Ep: 355, steps: 3, D loss: 0.167609, acc:  81%, G loss: 2.132503\n",
      "Ep: 355, steps: 4, D loss: 0.189680, acc:  82%, G loss: 1.995603\n",
      "Ep: 355, steps: 5, D loss: 0.274101, acc:  48%, G loss: 1.772348\n",
      "Ep: 355, steps: 6, D loss: 0.243157, acc:  53%, G loss: 1.638934\n",
      "Ep: 355, steps: 7, D loss: 0.374476, acc:  20%, G loss: 1.449280\n",
      "Ep: 355, steps: 8, D loss: 0.239222, acc:  60%, G loss: 1.758159\n",
      "Ep: 355, steps: 9, D loss: 0.240160, acc:  58%, G loss: 1.760870\n",
      "Ep: 355, steps: 10, D loss: 0.175671, acc:  82%, G loss: 1.647187\n",
      "Ep: 355, steps: 11, D loss: 0.230784, acc:  59%, G loss: 1.952996\n",
      "Ep: 355, steps: 12, D loss: 0.309711, acc:  27%, G loss: 1.477534\n",
      "Ep: 355, steps: 13, D loss: 0.278995, acc:  40%, G loss: 1.544851\n",
      "Ep: 355, steps: 14, D loss: 0.283065, acc:  38%, G loss: 1.636375\n",
      "Ep: 355, steps: 15, D loss: 0.232965, acc:  62%, G loss: 1.684866\n",
      "Ep: 355, steps: 16, D loss: 0.243016, acc:  57%, G loss: 1.726671\n",
      "Ep: 355, steps: 17, D loss: 0.213026, acc:  69%, G loss: 1.825022\n",
      "Ep: 355, steps: 18, D loss: 0.242534, acc:  58%, G loss: 1.643617\n",
      "Ep: 355, steps: 19, D loss: 0.206248, acc:  69%, G loss: 1.716642\n",
      "Ep: 355, steps: 20, D loss: 0.191366, acc:  74%, G loss: 1.865269\n",
      "Ep: 355, steps: 21, D loss: 0.308875, acc:  27%, G loss: 1.676814\n",
      "Ep: 355, steps: 22, D loss: 0.208317, acc:  64%, G loss: 1.812721\n",
      "Ep: 355, steps: 23, D loss: 0.216993, acc:  68%, G loss: 2.111768\n",
      "Ep: 355, steps: 24, D loss: 0.179338, acc:  80%, G loss: 1.782596\n",
      "Ep: 355, steps: 25, D loss: 0.225172, acc:  63%, G loss: 1.771626\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 356, steps: 1, D loss: 0.258462, acc:  53%, G loss: 1.830767\n",
      "Ep: 356, steps: 2, D loss: 0.244123, acc:  56%, G loss: 1.664188\n",
      "Ep: 356, steps: 3, D loss: 0.170939, acc:  81%, G loss: 2.171296\n",
      "Ep: 356, steps: 4, D loss: 0.200488, acc:  76%, G loss: 2.018010\n",
      "Ep: 356, steps: 5, D loss: 0.275188, acc:  48%, G loss: 1.811832\n",
      "Ep: 356, steps: 6, D loss: 0.250020, acc:  53%, G loss: 1.652511\n",
      "Ep: 356, steps: 7, D loss: 0.342692, acc:  24%, G loss: 1.458458\n",
      "Ep: 356, steps: 8, D loss: 0.228598, acc:  63%, G loss: 1.860329\n",
      "Ep: 356, steps: 9, D loss: 0.213132, acc:  70%, G loss: 1.876543\n",
      "Ep: 356, steps: 10, D loss: 0.182768, acc:  77%, G loss: 1.688359\n",
      "Ep: 356, steps: 11, D loss: 0.224343, acc:  63%, G loss: 1.945879\n",
      "Ep: 356, steps: 12, D loss: 0.307937, acc:  31%, G loss: 1.490650\n",
      "Ep: 356, steps: 13, D loss: 0.280875, acc:  43%, G loss: 1.555876\n",
      "Ep: 356, steps: 14, D loss: 0.291382, acc:  35%, G loss: 1.671591\n",
      "Ep: 356, steps: 15, D loss: 0.239673, acc:  57%, G loss: 1.639094\n",
      "Ep: 356, steps: 16, D loss: 0.247264, acc:  57%, G loss: 1.769465\n",
      "Saved Model\n",
      "Ep: 356, steps: 17, D loss: 0.202561, acc:  72%, G loss: 1.769268\n",
      "Ep: 356, steps: 18, D loss: 0.201294, acc:  72%, G loss: 1.693183\n",
      "Ep: 356, steps: 19, D loss: 0.171953, acc:  80%, G loss: 1.881374\n",
      "Ep: 356, steps: 20, D loss: 0.284553, acc:  38%, G loss: 1.655939\n",
      "Ep: 356, steps: 21, D loss: 0.185916, acc:  74%, G loss: 1.810269\n",
      "Ep: 356, steps: 22, D loss: 0.200262, acc:  71%, G loss: 2.135853\n",
      "Ep: 356, steps: 23, D loss: 0.187326, acc:  76%, G loss: 1.856660\n",
      "Ep: 356, steps: 24, D loss: 0.245673, acc:  55%, G loss: 2.292996\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 357, steps: 1, D loss: 0.234955, acc:  61%, G loss: 1.846734\n",
      "Ep: 357, steps: 2, D loss: 0.259952, acc:  52%, G loss: 1.718216\n",
      "Ep: 357, steps: 3, D loss: 0.148816, acc:  85%, G loss: 2.158235\n",
      "Ep: 357, steps: 4, D loss: 0.199061, acc:  77%, G loss: 1.980866\n",
      "Ep: 357, steps: 5, D loss: 0.285567, acc:  47%, G loss: 1.912211\n",
      "Ep: 357, steps: 6, D loss: 0.241371, acc:  55%, G loss: 1.707785\n",
      "Ep: 357, steps: 7, D loss: 0.347873, acc:  25%, G loss: 1.616373\n",
      "Ep: 357, steps: 8, D loss: 0.230990, acc:  62%, G loss: 1.960335\n",
      "Ep: 357, steps: 9, D loss: 0.230410, acc:  63%, G loss: 1.773378\n",
      "Ep: 357, steps: 10, D loss: 0.181755, acc:  79%, G loss: 1.637303\n",
      "Ep: 357, steps: 11, D loss: 0.223824, acc:  63%, G loss: 1.974646\n",
      "Ep: 357, steps: 12, D loss: 0.315441, acc:  28%, G loss: 1.515138\n",
      "Ep: 357, steps: 13, D loss: 0.276317, acc:  44%, G loss: 1.503907\n",
      "Ep: 357, steps: 14, D loss: 0.269584, acc:  43%, G loss: 1.653770\n",
      "Ep: 357, steps: 15, D loss: 0.244324, acc:  54%, G loss: 1.659005\n",
      "Ep: 357, steps: 16, D loss: 0.241396, acc:  58%, G loss: 1.766520\n",
      "Ep: 357, steps: 17, D loss: 0.211819, acc:  69%, G loss: 1.644628\n",
      "Ep: 357, steps: 18, D loss: 0.247526, acc:  58%, G loss: 1.690796\n",
      "Ep: 357, steps: 19, D loss: 0.199077, acc:  70%, G loss: 1.627431\n",
      "Ep: 357, steps: 20, D loss: 0.174764, acc:  78%, G loss: 1.882696\n",
      "Ep: 357, steps: 21, D loss: 0.293572, acc:  36%, G loss: 1.545394\n",
      "Ep: 357, steps: 22, D loss: 0.184336, acc:  71%, G loss: 1.834285\n",
      "Ep: 357, steps: 23, D loss: 0.232917, acc:  61%, G loss: 2.145967\n",
      "Ep: 357, steps: 24, D loss: 0.199026, acc:  73%, G loss: 1.773321\n",
      "Ep: 357, steps: 25, D loss: 0.210878, acc:  69%, G loss: 1.923966\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 358, steps: 1, D loss: 0.284801, acc:  45%, G loss: 2.035338\n",
      "Ep: 358, steps: 2, D loss: 0.241599, acc:  56%, G loss: 1.719571\n",
      "Ep: 358, steps: 3, D loss: 0.169311, acc:  82%, G loss: 2.187721\n",
      "Ep: 358, steps: 4, D loss: 0.186486, acc:  81%, G loss: 1.955594\n",
      "Ep: 358, steps: 5, D loss: 0.268761, acc:  49%, G loss: 1.889168\n",
      "Ep: 358, steps: 6, D loss: 0.255170, acc:  53%, G loss: 1.611044\n",
      "Ep: 358, steps: 7, D loss: 0.352125, acc:  24%, G loss: 1.537491\n",
      "Ep: 358, steps: 8, D loss: 0.228344, acc:  62%, G loss: 1.911929\n",
      "Ep: 358, steps: 9, D loss: 0.220961, acc:  66%, G loss: 1.786419\n",
      "Ep: 358, steps: 10, D loss: 0.171015, acc:  83%, G loss: 1.741970\n",
      "Ep: 358, steps: 11, D loss: 0.233074, acc:  59%, G loss: 1.958959\n",
      "Ep: 358, steps: 12, D loss: 0.323225, acc:  24%, G loss: 1.444821\n",
      "Ep: 358, steps: 13, D loss: 0.277117, acc:  43%, G loss: 1.488780\n",
      "Ep: 358, steps: 14, D loss: 0.277348, acc:  40%, G loss: 1.633849\n",
      "Ep: 358, steps: 15, D loss: 0.230606, acc:  62%, G loss: 1.678875\n",
      "Ep: 358, steps: 16, D loss: 0.236158, acc:  60%, G loss: 1.802980\n",
      "Ep: 358, steps: 17, D loss: 0.210553, acc:  70%, G loss: 1.659160\n",
      "Ep: 358, steps: 18, D loss: 0.243239, acc:  58%, G loss: 1.650265\n",
      "Ep: 358, steps: 19, D loss: 0.224268, acc:  63%, G loss: 1.671205\n",
      "Ep: 358, steps: 20, D loss: 0.186237, acc:  74%, G loss: 2.048663\n",
      "Ep: 358, steps: 21, D loss: 0.274302, acc:  40%, G loss: 1.592983\n",
      "Ep: 358, steps: 22, D loss: 0.184994, acc:  72%, G loss: 1.807148\n",
      "Ep: 358, steps: 23, D loss: 0.204560, acc:  70%, G loss: 2.100513\n",
      "Ep: 358, steps: 24, D loss: 0.174402, acc:  79%, G loss: 1.745075\n",
      "Ep: 358, steps: 25, D loss: 0.211251, acc:  66%, G loss: 1.769940\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 359, steps: 1, D loss: 0.232590, acc:  60%, G loss: 1.900507\n",
      "Ep: 359, steps: 2, D loss: 0.241368, acc:  58%, G loss: 1.599701\n",
      "Ep: 359, steps: 3, D loss: 0.161506, acc:  83%, G loss: 2.189477\n",
      "Ep: 359, steps: 4, D loss: 0.187890, acc:  81%, G loss: 2.127473\n",
      "Ep: 359, steps: 5, D loss: 0.293542, acc:  46%, G loss: 1.815559\n",
      "Ep: 359, steps: 6, D loss: 0.248460, acc:  54%, G loss: 1.621275\n",
      "Ep: 359, steps: 7, D loss: 0.373753, acc:  19%, G loss: 1.430317\n",
      "Ep: 359, steps: 8, D loss: 0.239532, acc:  58%, G loss: 1.898430\n",
      "Ep: 359, steps: 9, D loss: 0.215117, acc:  69%, G loss: 1.805105\n",
      "Ep: 359, steps: 10, D loss: 0.168876, acc:  81%, G loss: 1.676201\n",
      "Ep: 359, steps: 11, D loss: 0.232574, acc:  59%, G loss: 1.879640\n",
      "Ep: 359, steps: 12, D loss: 0.325402, acc:  25%, G loss: 1.457002\n",
      "Ep: 359, steps: 13, D loss: 0.285550, acc:  41%, G loss: 1.533211\n",
      "Ep: 359, steps: 14, D loss: 0.293006, acc:  34%, G loss: 1.609520\n",
      "Saved Model\n",
      "Ep: 359, steps: 15, D loss: 0.219541, acc:  68%, G loss: 1.620624\n",
      "Ep: 359, steps: 16, D loss: 0.183497, acc:  81%, G loss: 1.742538\n",
      "Ep: 359, steps: 17, D loss: 0.256029, acc:  55%, G loss: 1.586071\n",
      "Ep: 359, steps: 18, D loss: 0.216471, acc:  66%, G loss: 1.634999\n",
      "Ep: 359, steps: 19, D loss: 0.160650, acc:  82%, G loss: 1.879005\n",
      "Ep: 359, steps: 20, D loss: 0.304272, acc:  32%, G loss: 1.764300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 359, steps: 21, D loss: 0.183179, acc:  73%, G loss: 1.789798\n",
      "Ep: 359, steps: 22, D loss: 0.181051, acc:  76%, G loss: 2.059718\n",
      "Ep: 359, steps: 23, D loss: 0.180582, acc:  79%, G loss: 1.805259\n",
      "Ep: 359, steps: 24, D loss: 0.223604, acc:  63%, G loss: 1.749848\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 360, steps: 1, D loss: 0.236883, acc:  59%, G loss: 1.845030\n",
      "Ep: 360, steps: 2, D loss: 0.257014, acc:  52%, G loss: 1.623824\n",
      "Ep: 360, steps: 3, D loss: 0.166617, acc:  83%, G loss: 2.104241\n",
      "Ep: 360, steps: 4, D loss: 0.194913, acc:  79%, G loss: 2.020439\n",
      "Ep: 360, steps: 5, D loss: 0.268031, acc:  52%, G loss: 1.835519\n",
      "Ep: 360, steps: 6, D loss: 0.246517, acc:  53%, G loss: 1.604367\n",
      "Ep: 360, steps: 7, D loss: 0.357191, acc:  23%, G loss: 1.543302\n",
      "Ep: 360, steps: 8, D loss: 0.237882, acc:  59%, G loss: 1.889931\n",
      "Ep: 360, steps: 9, D loss: 0.262432, acc:  54%, G loss: 1.839314\n",
      "Ep: 360, steps: 10, D loss: 0.171336, acc:  84%, G loss: 1.723808\n",
      "Ep: 360, steps: 11, D loss: 0.215961, acc:  64%, G loss: 1.939177\n",
      "Ep: 360, steps: 12, D loss: 0.309164, acc:  28%, G loss: 1.471349\n",
      "Ep: 360, steps: 13, D loss: 0.287126, acc:  39%, G loss: 1.526373\n",
      "Ep: 360, steps: 14, D loss: 0.283116, acc:  37%, G loss: 1.599255\n",
      "Ep: 360, steps: 15, D loss: 0.245857, acc:  55%, G loss: 1.702069\n",
      "Ep: 360, steps: 16, D loss: 0.239958, acc:  58%, G loss: 1.752169\n",
      "Ep: 360, steps: 17, D loss: 0.216567, acc:  69%, G loss: 1.729905\n",
      "Ep: 360, steps: 18, D loss: 0.223640, acc:  66%, G loss: 1.649245\n",
      "Ep: 360, steps: 19, D loss: 0.206952, acc:  70%, G loss: 1.636965\n",
      "Ep: 360, steps: 20, D loss: 0.173873, acc:  79%, G loss: 1.930728\n",
      "Ep: 360, steps: 21, D loss: 0.291069, acc:  39%, G loss: 1.594128\n",
      "Ep: 360, steps: 22, D loss: 0.180393, acc:  75%, G loss: 1.729576\n",
      "Ep: 360, steps: 23, D loss: 0.215535, acc:  66%, G loss: 2.042057\n",
      "Ep: 360, steps: 24, D loss: 0.192957, acc:  72%, G loss: 1.799470\n",
      "Ep: 360, steps: 25, D loss: 0.223399, acc:  63%, G loss: 1.823067\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 361, steps: 1, D loss: 0.252937, acc:  56%, G loss: 1.791176\n",
      "Ep: 361, steps: 2, D loss: 0.246308, acc:  58%, G loss: 1.657951\n",
      "Ep: 361, steps: 3, D loss: 0.164955, acc:  82%, G loss: 2.183214\n",
      "Ep: 361, steps: 4, D loss: 0.196462, acc:  78%, G loss: 2.178920\n",
      "Ep: 361, steps: 5, D loss: 0.270503, acc:  49%, G loss: 1.827585\n",
      "Ep: 361, steps: 6, D loss: 0.259823, acc:  52%, G loss: 1.734287\n",
      "Ep: 361, steps: 7, D loss: 0.347774, acc:  22%, G loss: 1.629111\n",
      "Ep: 361, steps: 8, D loss: 0.235712, acc:  59%, G loss: 1.924268\n",
      "Ep: 361, steps: 9, D loss: 0.204074, acc:  74%, G loss: 1.848230\n",
      "Ep: 361, steps: 10, D loss: 0.158280, acc:  88%, G loss: 1.674755\n",
      "Ep: 361, steps: 11, D loss: 0.226462, acc:  62%, G loss: 1.853616\n",
      "Ep: 361, steps: 12, D loss: 0.339163, acc:  21%, G loss: 1.427048\n",
      "Ep: 361, steps: 13, D loss: 0.285831, acc:  40%, G loss: 1.468248\n",
      "Ep: 361, steps: 14, D loss: 0.292626, acc:  35%, G loss: 1.579579\n",
      "Ep: 361, steps: 15, D loss: 0.248559, acc:  53%, G loss: 1.568496\n",
      "Ep: 361, steps: 16, D loss: 0.252959, acc:  55%, G loss: 1.704572\n",
      "Ep: 361, steps: 17, D loss: 0.208606, acc:  72%, G loss: 1.661014\n",
      "Ep: 361, steps: 18, D loss: 0.252968, acc:  54%, G loss: 1.668167\n",
      "Ep: 361, steps: 19, D loss: 0.211750, acc:  67%, G loss: 1.658483\n",
      "Ep: 361, steps: 20, D loss: 0.159437, acc:  82%, G loss: 1.846777\n",
      "Ep: 361, steps: 21, D loss: 0.291496, acc:  34%, G loss: 1.515258\n",
      "Ep: 361, steps: 22, D loss: 0.178541, acc:  74%, G loss: 1.687920\n",
      "Ep: 361, steps: 23, D loss: 0.222160, acc:  63%, G loss: 2.154522\n",
      "Ep: 361, steps: 24, D loss: 0.189568, acc:  76%, G loss: 1.731508\n",
      "Ep: 361, steps: 25, D loss: 0.217351, acc:  65%, G loss: 1.998697\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 362, steps: 1, D loss: 0.243881, acc:  59%, G loss: 1.818910\n",
      "Ep: 362, steps: 2, D loss: 0.250242, acc:  53%, G loss: 1.725814\n",
      "Ep: 362, steps: 3, D loss: 0.170966, acc:  77%, G loss: 2.165767\n",
      "Ep: 362, steps: 4, D loss: 0.195390, acc:  78%, G loss: 2.060775\n",
      "Ep: 362, steps: 5, D loss: 0.270914, acc:  48%, G loss: 1.928595\n",
      "Ep: 362, steps: 6, D loss: 0.246995, acc:  53%, G loss: 1.728965\n",
      "Ep: 362, steps: 7, D loss: 0.333077, acc:  26%, G loss: 1.601780\n",
      "Ep: 362, steps: 8, D loss: 0.226383, acc:  62%, G loss: 1.938221\n",
      "Ep: 362, steps: 9, D loss: 0.239779, acc:  60%, G loss: 1.767598\n",
      "Ep: 362, steps: 10, D loss: 0.190759, acc:  77%, G loss: 1.750392\n",
      "Ep: 362, steps: 11, D loss: 0.236213, acc:  57%, G loss: 1.947720\n",
      "Ep: 362, steps: 12, D loss: 0.309572, acc:  30%, G loss: 1.451917\n",
      "Saved Model\n",
      "Ep: 362, steps: 13, D loss: 0.294237, acc:  36%, G loss: 1.517096\n",
      "Ep: 362, steps: 14, D loss: 0.238287, acc:  60%, G loss: 1.637197\n",
      "Ep: 362, steps: 15, D loss: 0.255276, acc:  54%, G loss: 1.747499\n",
      "Ep: 362, steps: 16, D loss: 0.189282, acc:  79%, G loss: 1.781081\n",
      "Ep: 362, steps: 17, D loss: 0.227175, acc:  63%, G loss: 1.664558\n",
      "Ep: 362, steps: 18, D loss: 0.198986, acc:  71%, G loss: 1.723984\n",
      "Ep: 362, steps: 19, D loss: 0.170518, acc:  78%, G loss: 1.816776\n",
      "Ep: 362, steps: 20, D loss: 0.308547, acc:  29%, G loss: 1.516072\n",
      "Ep: 362, steps: 21, D loss: 0.193160, acc:  68%, G loss: 1.775048\n",
      "Ep: 362, steps: 22, D loss: 0.204039, acc:  69%, G loss: 2.120965\n",
      "Ep: 362, steps: 23, D loss: 0.178540, acc:  79%, G loss: 1.761816\n",
      "Ep: 362, steps: 24, D loss: 0.216425, acc:  64%, G loss: 1.801260\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 363, steps: 1, D loss: 0.260956, acc:  55%, G loss: 1.859337\n",
      "Ep: 363, steps: 2, D loss: 0.241780, acc:  56%, G loss: 1.697872\n",
      "Ep: 363, steps: 3, D loss: 0.170522, acc:  79%, G loss: 2.145318\n",
      "Ep: 363, steps: 4, D loss: 0.186122, acc:  82%, G loss: 2.062668\n",
      "Ep: 363, steps: 5, D loss: 0.293096, acc:  46%, G loss: 1.870060\n",
      "Ep: 363, steps: 6, D loss: 0.247746, acc:  52%, G loss: 1.706826\n",
      "Ep: 363, steps: 7, D loss: 0.356045, acc:  22%, G loss: 1.602202\n",
      "Ep: 363, steps: 8, D loss: 0.258095, acc:  52%, G loss: 1.893645\n",
      "Ep: 363, steps: 9, D loss: 0.219643, acc:  65%, G loss: 1.834221\n",
      "Ep: 363, steps: 10, D loss: 0.184642, acc:  80%, G loss: 1.667498\n",
      "Ep: 363, steps: 11, D loss: 0.229459, acc:  61%, G loss: 1.860496\n",
      "Ep: 363, steps: 12, D loss: 0.318291, acc:  26%, G loss: 1.444785\n",
      "Ep: 363, steps: 13, D loss: 0.285560, acc:  39%, G loss: 1.478888\n",
      "Ep: 363, steps: 14, D loss: 0.291337, acc:  34%, G loss: 1.532630\n",
      "Ep: 363, steps: 15, D loss: 0.227590, acc:  65%, G loss: 1.675382\n",
      "Ep: 363, steps: 16, D loss: 0.235802, acc:  60%, G loss: 1.693263\n",
      "Ep: 363, steps: 17, D loss: 0.197520, acc:  75%, G loss: 1.713138\n",
      "Ep: 363, steps: 18, D loss: 0.247796, acc:  57%, G loss: 1.631516\n",
      "Ep: 363, steps: 19, D loss: 0.204175, acc:  70%, G loss: 1.654665\n",
      "Ep: 363, steps: 20, D loss: 0.183664, acc:  76%, G loss: 1.857182\n",
      "Ep: 363, steps: 21, D loss: 0.299841, acc:  32%, G loss: 1.511315\n",
      "Ep: 363, steps: 22, D loss: 0.195297, acc:  70%, G loss: 1.785961\n",
      "Ep: 363, steps: 23, D loss: 0.204376, acc:  71%, G loss: 2.093655\n",
      "Ep: 363, steps: 24, D loss: 0.190093, acc:  75%, G loss: 1.721452\n",
      "Ep: 363, steps: 25, D loss: 0.218245, acc:  65%, G loss: 1.737153\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 364, steps: 1, D loss: 0.255188, acc:  53%, G loss: 1.874347\n",
      "Ep: 364, steps: 2, D loss: 0.255961, acc:  52%, G loss: 1.628978\n",
      "Ep: 364, steps: 3, D loss: 0.163605, acc:  82%, G loss: 2.129759\n",
      "Ep: 364, steps: 4, D loss: 0.194353, acc:  78%, G loss: 2.072813\n",
      "Ep: 364, steps: 5, D loss: 0.261409, acc:  52%, G loss: 2.004231\n",
      "Ep: 364, steps: 6, D loss: 0.257714, acc:  52%, G loss: 1.791716\n",
      "Ep: 364, steps: 7, D loss: 0.349550, acc:  22%, G loss: 1.634894\n",
      "Ep: 364, steps: 8, D loss: 0.235499, acc:  59%, G loss: 1.841356\n",
      "Ep: 364, steps: 9, D loss: 0.224684, acc:  65%, G loss: 1.725409\n",
      "Ep: 364, steps: 10, D loss: 0.175114, acc:  83%, G loss: 1.692251\n",
      "Ep: 364, steps: 11, D loss: 0.226789, acc:  64%, G loss: 1.902889\n",
      "Ep: 364, steps: 12, D loss: 0.311402, acc:  30%, G loss: 1.462773\n",
      "Ep: 364, steps: 13, D loss: 0.290743, acc:  38%, G loss: 1.460997\n",
      "Ep: 364, steps: 14, D loss: 0.278670, acc:  38%, G loss: 1.519975\n",
      "Ep: 364, steps: 15, D loss: 0.227097, acc:  65%, G loss: 1.648839\n",
      "Ep: 364, steps: 16, D loss: 0.244630, acc:  58%, G loss: 1.787266\n",
      "Ep: 364, steps: 17, D loss: 0.208399, acc:  71%, G loss: 1.793418\n",
      "Ep: 364, steps: 18, D loss: 0.233988, acc:  63%, G loss: 1.653814\n",
      "Ep: 364, steps: 19, D loss: 0.206849, acc:  67%, G loss: 1.691190\n",
      "Ep: 364, steps: 20, D loss: 0.199408, acc:  70%, G loss: 1.877756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 364, steps: 21, D loss: 0.305979, acc:  29%, G loss: 1.563941\n",
      "Ep: 364, steps: 22, D loss: 0.179549, acc:  75%, G loss: 1.757605\n",
      "Ep: 364, steps: 23, D loss: 0.214602, acc:  66%, G loss: 2.088444\n",
      "Ep: 364, steps: 24, D loss: 0.196427, acc:  76%, G loss: 1.858652\n",
      "Ep: 364, steps: 25, D loss: 0.220247, acc:  65%, G loss: 1.696049\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 365, steps: 1, D loss: 0.246231, acc:  59%, G loss: 1.783947\n",
      "Ep: 365, steps: 2, D loss: 0.246543, acc:  56%, G loss: 1.646739\n",
      "Ep: 365, steps: 3, D loss: 0.174654, acc:  79%, G loss: 2.106254\n",
      "Ep: 365, steps: 4, D loss: 0.183949, acc:  83%, G loss: 2.040904\n",
      "Ep: 365, steps: 5, D loss: 0.277348, acc:  49%, G loss: 1.913413\n",
      "Ep: 365, steps: 6, D loss: 0.250159, acc:  52%, G loss: 1.736851\n",
      "Ep: 365, steps: 7, D loss: 0.369017, acc:  20%, G loss: 1.639082\n",
      "Ep: 365, steps: 8, D loss: 0.237157, acc:  60%, G loss: 1.884453\n",
      "Ep: 365, steps: 9, D loss: 0.206390, acc:  71%, G loss: 1.771796\n",
      "Saved Model\n",
      "Ep: 365, steps: 10, D loss: 0.164496, acc:  87%, G loss: 1.622283\n",
      "Ep: 365, steps: 11, D loss: 0.323326, acc:  27%, G loss: 1.408989\n",
      "Ep: 365, steps: 12, D loss: 0.262826, acc:  47%, G loss: 1.627653\n",
      "Ep: 365, steps: 13, D loss: 0.266549, acc:  48%, G loss: 1.620692\n",
      "Ep: 365, steps: 14, D loss: 0.268893, acc:  45%, G loss: 1.661179\n",
      "Ep: 365, steps: 15, D loss: 0.249086, acc:  56%, G loss: 1.697974\n",
      "Ep: 365, steps: 16, D loss: 0.221834, acc:  67%, G loss: 1.812300\n",
      "Ep: 365, steps: 17, D loss: 0.249324, acc:  56%, G loss: 1.666223\n",
      "Ep: 365, steps: 18, D loss: 0.221454, acc:  66%, G loss: 1.699492\n",
      "Ep: 365, steps: 19, D loss: 0.174977, acc:  80%, G loss: 1.879094\n",
      "Ep: 365, steps: 20, D loss: 0.289897, acc:  32%, G loss: 1.775308\n",
      "Ep: 365, steps: 21, D loss: 0.177735, acc:  75%, G loss: 1.821886\n",
      "Ep: 365, steps: 22, D loss: 0.201569, acc:  71%, G loss: 2.089744\n",
      "Ep: 365, steps: 23, D loss: 0.180296, acc:  80%, G loss: 1.747501\n",
      "Ep: 365, steps: 24, D loss: 0.230287, acc:  61%, G loss: 1.824025\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 366, steps: 1, D loss: 0.262939, acc:  51%, G loss: 1.759666\n",
      "Ep: 366, steps: 2, D loss: 0.245341, acc:  57%, G loss: 1.628571\n",
      "Ep: 366, steps: 3, D loss: 0.165150, acc:  81%, G loss: 2.168366\n",
      "Ep: 366, steps: 4, D loss: 0.204205, acc:  76%, G loss: 2.007327\n",
      "Ep: 366, steps: 5, D loss: 0.270484, acc:  49%, G loss: 1.875230\n",
      "Ep: 366, steps: 6, D loss: 0.257085, acc:  51%, G loss: 1.654962\n",
      "Ep: 366, steps: 7, D loss: 0.336191, acc:  25%, G loss: 1.426212\n",
      "Ep: 366, steps: 8, D loss: 0.234416, acc:  59%, G loss: 1.881748\n",
      "Ep: 366, steps: 9, D loss: 0.241744, acc:  59%, G loss: 1.787947\n",
      "Ep: 366, steps: 10, D loss: 0.179469, acc:  81%, G loss: 1.709753\n",
      "Ep: 366, steps: 11, D loss: 0.240580, acc:  55%, G loss: 1.932934\n",
      "Ep: 366, steps: 12, D loss: 0.315577, acc:  25%, G loss: 1.456767\n",
      "Ep: 366, steps: 13, D loss: 0.287533, acc:  35%, G loss: 1.498007\n",
      "Ep: 366, steps: 14, D loss: 0.276307, acc:  40%, G loss: 1.544834\n",
      "Ep: 366, steps: 15, D loss: 0.227948, acc:  66%, G loss: 1.639703\n",
      "Ep: 366, steps: 16, D loss: 0.242788, acc:  57%, G loss: 1.668437\n",
      "Ep: 366, steps: 17, D loss: 0.200742, acc:  74%, G loss: 1.689867\n",
      "Ep: 366, steps: 18, D loss: 0.227215, acc:  64%, G loss: 1.645830\n",
      "Ep: 366, steps: 19, D loss: 0.204332, acc:  70%, G loss: 1.735977\n",
      "Ep: 366, steps: 20, D loss: 0.180337, acc:  77%, G loss: 1.841895\n",
      "Ep: 366, steps: 21, D loss: 0.290800, acc:  35%, G loss: 1.496915\n",
      "Ep: 366, steps: 22, D loss: 0.190505, acc:  69%, G loss: 1.692395\n",
      "Ep: 366, steps: 23, D loss: 0.202984, acc:  71%, G loss: 2.219713\n",
      "Ep: 366, steps: 24, D loss: 0.180971, acc:  79%, G loss: 1.730186\n",
      "Ep: 366, steps: 25, D loss: 0.232198, acc:  60%, G loss: 1.904090\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 367, steps: 1, D loss: 0.243860, acc:  56%, G loss: 1.874816\n",
      "Ep: 367, steps: 2, D loss: 0.253254, acc:  54%, G loss: 1.610550\n",
      "Ep: 367, steps: 3, D loss: 0.174141, acc:  79%, G loss: 2.104651\n",
      "Ep: 367, steps: 4, D loss: 0.197156, acc:  78%, G loss: 2.036062\n",
      "Ep: 367, steps: 5, D loss: 0.270510, acc:  49%, G loss: 1.801130\n",
      "Ep: 367, steps: 6, D loss: 0.248245, acc:  54%, G loss: 1.682301\n",
      "Ep: 367, steps: 7, D loss: 0.353501, acc:  21%, G loss: 1.480384\n",
      "Ep: 367, steps: 8, D loss: 0.227475, acc:  62%, G loss: 1.944903\n",
      "Ep: 367, steps: 9, D loss: 0.237547, acc:  61%, G loss: 1.842112\n",
      "Ep: 367, steps: 10, D loss: 0.175341, acc:  83%, G loss: 1.728701\n",
      "Ep: 367, steps: 11, D loss: 0.242895, acc:  56%, G loss: 1.914295\n",
      "Ep: 367, steps: 12, D loss: 0.307384, acc:  29%, G loss: 1.414351\n",
      "Ep: 367, steps: 13, D loss: 0.275040, acc:  43%, G loss: 1.452482\n",
      "Ep: 367, steps: 14, D loss: 0.283518, acc:  38%, G loss: 1.585884\n",
      "Ep: 367, steps: 15, D loss: 0.235553, acc:  60%, G loss: 1.630632\n",
      "Ep: 367, steps: 16, D loss: 0.231427, acc:  63%, G loss: 1.705652\n",
      "Ep: 367, steps: 17, D loss: 0.197606, acc:  76%, G loss: 1.744175\n",
      "Ep: 367, steps: 18, D loss: 0.256361, acc:  53%, G loss: 1.681680\n",
      "Ep: 367, steps: 19, D loss: 0.212978, acc:  66%, G loss: 1.675547\n",
      "Ep: 367, steps: 20, D loss: 0.187255, acc:  74%, G loss: 1.927810\n",
      "Ep: 367, steps: 21, D loss: 0.303102, acc:  29%, G loss: 1.713210\n",
      "Ep: 367, steps: 22, D loss: 0.175237, acc:  76%, G loss: 1.839580\n",
      "Ep: 367, steps: 23, D loss: 0.215057, acc:  65%, G loss: 2.066117\n",
      "Ep: 367, steps: 24, D loss: 0.186494, acc:  79%, G loss: 1.698483\n",
      "Ep: 367, steps: 25, D loss: 0.229337, acc:  59%, G loss: 1.808400\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 368, steps: 1, D loss: 0.266664, acc:  52%, G loss: 1.799510\n",
      "Ep: 368, steps: 2, D loss: 0.251103, acc:  53%, G loss: 1.603852\n",
      "Ep: 368, steps: 3, D loss: 0.166129, acc:  80%, G loss: 2.051225\n",
      "Ep: 368, steps: 4, D loss: 0.199675, acc:  78%, G loss: 1.969428\n",
      "Ep: 368, steps: 5, D loss: 0.268364, acc:  49%, G loss: 1.780015\n",
      "Ep: 368, steps: 6, D loss: 0.248952, acc:  55%, G loss: 1.731020\n",
      "Ep: 368, steps: 7, D loss: 0.349497, acc:  21%, G loss: 1.490433\n",
      "Saved Model\n",
      "Ep: 368, steps: 8, D loss: 0.227351, acc:  60%, G loss: 1.936080\n",
      "Ep: 368, steps: 9, D loss: 0.182120, acc:  81%, G loss: 1.641300\n",
      "Ep: 368, steps: 10, D loss: 0.242874, acc:  55%, G loss: 1.763402\n",
      "Ep: 368, steps: 11, D loss: 0.318247, acc:  27%, G loss: 1.516469\n",
      "Ep: 368, steps: 12, D loss: 0.281422, acc:  38%, G loss: 1.491180\n",
      "Ep: 368, steps: 13, D loss: 0.274815, acc:  42%, G loss: 1.546021\n",
      "Ep: 368, steps: 14, D loss: 0.234663, acc:  61%, G loss: 1.605351\n",
      "Ep: 368, steps: 15, D loss: 0.253018, acc:  53%, G loss: 1.706303\n",
      "Ep: 368, steps: 16, D loss: 0.189339, acc:  77%, G loss: 1.691642\n",
      "Ep: 368, steps: 17, D loss: 0.242172, acc:  59%, G loss: 1.649606\n",
      "Ep: 368, steps: 18, D loss: 0.219709, acc:  63%, G loss: 1.707952\n",
      "Ep: 368, steps: 19, D loss: 0.180130, acc:  78%, G loss: 1.839914\n",
      "Ep: 368, steps: 20, D loss: 0.266542, acc:  43%, G loss: 1.504348\n",
      "Ep: 368, steps: 21, D loss: 0.183206, acc:  74%, G loss: 1.732658\n",
      "Ep: 368, steps: 22, D loss: 0.211573, acc:  67%, G loss: 2.124175\n",
      "Ep: 368, steps: 23, D loss: 0.177946, acc:  79%, G loss: 1.744564\n",
      "Ep: 368, steps: 24, D loss: 0.235313, acc:  58%, G loss: 1.859505\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 369, steps: 1, D loss: 0.273964, acc:  47%, G loss: 1.874978\n",
      "Ep: 369, steps: 2, D loss: 0.262586, acc:  50%, G loss: 1.680592\n",
      "Ep: 369, steps: 3, D loss: 0.166707, acc:  81%, G loss: 2.182006\n",
      "Ep: 369, steps: 4, D loss: 0.199215, acc:  78%, G loss: 1.976436\n",
      "Ep: 369, steps: 5, D loss: 0.288869, acc:  45%, G loss: 1.889702\n",
      "Ep: 369, steps: 6, D loss: 0.249054, acc:  53%, G loss: 1.788631\n",
      "Ep: 369, steps: 7, D loss: 0.345709, acc:  22%, G loss: 1.531788\n",
      "Ep: 369, steps: 8, D loss: 0.236142, acc:  60%, G loss: 1.836516\n",
      "Ep: 369, steps: 9, D loss: 0.226125, acc:  66%, G loss: 1.737249\n",
      "Ep: 369, steps: 10, D loss: 0.193987, acc:  74%, G loss: 1.625860\n",
      "Ep: 369, steps: 11, D loss: 0.239243, acc:  56%, G loss: 1.911543\n",
      "Ep: 369, steps: 12, D loss: 0.312939, acc:  26%, G loss: 1.476244\n",
      "Ep: 369, steps: 13, D loss: 0.283629, acc:  39%, G loss: 1.462149\n",
      "Ep: 369, steps: 14, D loss: 0.277865, acc:  40%, G loss: 1.614629\n",
      "Ep: 369, steps: 15, D loss: 0.237400, acc:  59%, G loss: 1.673019\n",
      "Ep: 369, steps: 16, D loss: 0.242441, acc:  57%, G loss: 1.719253\n",
      "Ep: 369, steps: 17, D loss: 0.216004, acc:  70%, G loss: 1.669692\n",
      "Ep: 369, steps: 18, D loss: 0.237163, acc:  59%, G loss: 1.617064\n",
      "Ep: 369, steps: 19, D loss: 0.200959, acc:  69%, G loss: 1.624014\n",
      "Ep: 369, steps: 20, D loss: 0.188197, acc:  75%, G loss: 1.821457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 369, steps: 21, D loss: 0.287180, acc:  37%, G loss: 1.527471\n",
      "Ep: 369, steps: 22, D loss: 0.183554, acc:  74%, G loss: 1.732301\n",
      "Ep: 369, steps: 23, D loss: 0.204819, acc:  69%, G loss: 2.075236\n",
      "Ep: 369, steps: 24, D loss: 0.187948, acc:  76%, G loss: 1.766750\n",
      "Ep: 369, steps: 25, D loss: 0.238761, acc:  58%, G loss: 1.885454\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 370, steps: 1, D loss: 0.250083, acc:  56%, G loss: 1.816717\n",
      "Ep: 370, steps: 2, D loss: 0.264110, acc:  49%, G loss: 1.678200\n",
      "Ep: 370, steps: 3, D loss: 0.181963, acc:  77%, G loss: 2.054567\n",
      "Ep: 370, steps: 4, D loss: 0.200910, acc:  77%, G loss: 2.006516\n",
      "Ep: 370, steps: 5, D loss: 0.287340, acc:  45%, G loss: 1.876342\n",
      "Ep: 370, steps: 6, D loss: 0.254524, acc:  52%, G loss: 1.715479\n",
      "Ep: 370, steps: 7, D loss: 0.354629, acc:  20%, G loss: 1.609746\n",
      "Ep: 370, steps: 8, D loss: 0.227727, acc:  62%, G loss: 1.878019\n",
      "Ep: 370, steps: 9, D loss: 0.244171, acc:  59%, G loss: 1.830778\n",
      "Ep: 370, steps: 10, D loss: 0.179778, acc:  80%, G loss: 1.727077\n",
      "Ep: 370, steps: 11, D loss: 0.227636, acc:  62%, G loss: 1.956935\n",
      "Ep: 370, steps: 12, D loss: 0.312281, acc:  28%, G loss: 1.427062\n",
      "Ep: 370, steps: 13, D loss: 0.280565, acc:  40%, G loss: 1.442348\n",
      "Ep: 370, steps: 14, D loss: 0.278845, acc:  40%, G loss: 1.525359\n",
      "Ep: 370, steps: 15, D loss: 0.239709, acc:  58%, G loss: 1.666997\n",
      "Ep: 370, steps: 16, D loss: 0.244687, acc:  58%, G loss: 1.694082\n",
      "Ep: 370, steps: 17, D loss: 0.203898, acc:  72%, G loss: 1.630058\n",
      "Ep: 370, steps: 18, D loss: 0.240114, acc:  57%, G loss: 1.630461\n",
      "Ep: 370, steps: 19, D loss: 0.216802, acc:  67%, G loss: 1.612550\n",
      "Ep: 370, steps: 20, D loss: 0.180049, acc:  76%, G loss: 1.959502\n",
      "Ep: 370, steps: 21, D loss: 0.285083, acc:  37%, G loss: 1.496761\n",
      "Ep: 370, steps: 22, D loss: 0.186002, acc:  73%, G loss: 1.694286\n",
      "Ep: 370, steps: 23, D loss: 0.222532, acc:  63%, G loss: 2.023650\n",
      "Ep: 370, steps: 24, D loss: 0.190210, acc:  76%, G loss: 1.733840\n",
      "Ep: 370, steps: 25, D loss: 0.221062, acc:  63%, G loss: 1.652670\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 371, steps: 1, D loss: 0.278589, acc:  45%, G loss: 1.910542\n",
      "Ep: 371, steps: 2, D loss: 0.237786, acc:  60%, G loss: 1.680960\n",
      "Ep: 371, steps: 3, D loss: 0.169068, acc:  80%, G loss: 2.194209\n",
      "Ep: 371, steps: 4, D loss: 0.190769, acc:  82%, G loss: 1.973259\n",
      "Ep: 371, steps: 5, D loss: 0.280617, acc:  45%, G loss: 1.855115\n",
      "Saved Model\n",
      "Ep: 371, steps: 6, D loss: 0.252520, acc:  53%, G loss: 1.718940\n",
      "Ep: 371, steps: 7, D loss: 0.254937, acc:  54%, G loss: 1.743430\n",
      "Ep: 371, steps: 8, D loss: 0.182489, acc:  80%, G loss: 1.829826\n",
      "Ep: 371, steps: 9, D loss: 0.132014, acc:  94%, G loss: 1.862766\n",
      "Ep: 371, steps: 10, D loss: 0.197884, acc:  76%, G loss: 1.971664\n",
      "Ep: 371, steps: 11, D loss: 0.377669, acc:  16%, G loss: 1.434382\n",
      "Ep: 371, steps: 12, D loss: 0.324415, acc:  23%, G loss: 1.558923\n",
      "Ep: 371, steps: 13, D loss: 0.314802, acc:  31%, G loss: 1.554435\n",
      "Ep: 371, steps: 14, D loss: 0.236143, acc:  60%, G loss: 1.605675\n",
      "Ep: 371, steps: 15, D loss: 0.243369, acc:  58%, G loss: 1.691827\n",
      "Ep: 371, steps: 16, D loss: 0.204512, acc:  74%, G loss: 1.744199\n",
      "Ep: 371, steps: 17, D loss: 0.257058, acc:  53%, G loss: 1.580969\n",
      "Ep: 371, steps: 18, D loss: 0.192598, acc:  71%, G loss: 1.682359\n",
      "Ep: 371, steps: 19, D loss: 0.175028, acc:  79%, G loss: 1.813884\n",
      "Ep: 371, steps: 20, D loss: 0.313391, acc:  29%, G loss: 1.610038\n",
      "Ep: 371, steps: 21, D loss: 0.182434, acc:  71%, G loss: 1.787140\n",
      "Ep: 371, steps: 22, D loss: 0.204745, acc:  69%, G loss: 2.166579\n",
      "Ep: 371, steps: 23, D loss: 0.183379, acc:  76%, G loss: 1.692541\n",
      "Ep: 371, steps: 24, D loss: 0.231757, acc:  60%, G loss: 1.669643\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 372, steps: 1, D loss: 0.233394, acc:  60%, G loss: 1.817949\n",
      "Ep: 372, steps: 2, D loss: 0.253168, acc:  51%, G loss: 1.719774\n",
      "Ep: 372, steps: 3, D loss: 0.158981, acc:  83%, G loss: 2.181064\n",
      "Ep: 372, steps: 4, D loss: 0.173747, acc:  85%, G loss: 2.020563\n",
      "Ep: 372, steps: 5, D loss: 0.292778, acc:  42%, G loss: 1.883473\n",
      "Ep: 372, steps: 6, D loss: 0.249078, acc:  53%, G loss: 1.661306\n",
      "Ep: 372, steps: 7, D loss: 0.365383, acc:  19%, G loss: 1.541640\n",
      "Ep: 372, steps: 8, D loss: 0.242565, acc:  59%, G loss: 1.876558\n",
      "Ep: 372, steps: 9, D loss: 0.244207, acc:  59%, G loss: 1.773408\n",
      "Ep: 372, steps: 10, D loss: 0.175167, acc:  82%, G loss: 1.762588\n",
      "Ep: 372, steps: 11, D loss: 0.220875, acc:  65%, G loss: 1.879390\n",
      "Ep: 372, steps: 12, D loss: 0.326793, acc:  23%, G loss: 1.484700\n",
      "Ep: 372, steps: 13, D loss: 0.279470, acc:  38%, G loss: 1.451683\n",
      "Ep: 372, steps: 14, D loss: 0.283794, acc:  39%, G loss: 1.569006\n",
      "Ep: 372, steps: 15, D loss: 0.240042, acc:  59%, G loss: 1.651884\n",
      "Ep: 372, steps: 16, D loss: 0.254658, acc:  55%, G loss: 1.743999\n",
      "Ep: 372, steps: 17, D loss: 0.196329, acc:  78%, G loss: 1.633905\n",
      "Ep: 372, steps: 18, D loss: 0.247985, acc:  55%, G loss: 1.688881\n",
      "Ep: 372, steps: 19, D loss: 0.211602, acc:  67%, G loss: 1.629734\n",
      "Ep: 372, steps: 20, D loss: 0.172838, acc:  80%, G loss: 1.892201\n",
      "Ep: 372, steps: 21, D loss: 0.283275, acc:  38%, G loss: 1.474408\n",
      "Ep: 372, steps: 22, D loss: 0.182110, acc:  73%, G loss: 1.744462\n",
      "Ep: 372, steps: 23, D loss: 0.203118, acc:  72%, G loss: 2.061173\n",
      "Ep: 372, steps: 24, D loss: 0.193074, acc:  74%, G loss: 1.799272\n",
      "Ep: 372, steps: 25, D loss: 0.235528, acc:  61%, G loss: 1.874976\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 373, steps: 1, D loss: 0.240007, acc:  59%, G loss: 1.888903\n",
      "Ep: 373, steps: 2, D loss: 0.251324, acc:  55%, G loss: 1.653351\n",
      "Ep: 373, steps: 3, D loss: 0.150722, acc:  87%, G loss: 2.125417\n",
      "Ep: 373, steps: 4, D loss: 0.187599, acc:  79%, G loss: 1.957299\n",
      "Ep: 373, steps: 5, D loss: 0.248614, acc:  53%, G loss: 1.814166\n",
      "Ep: 373, steps: 6, D loss: 0.261144, acc:  51%, G loss: 1.641420\n",
      "Ep: 373, steps: 7, D loss: 0.343829, acc:  26%, G loss: 1.586128\n",
      "Ep: 373, steps: 8, D loss: 0.230117, acc:  62%, G loss: 1.815313\n",
      "Ep: 373, steps: 9, D loss: 0.249656, acc:  55%, G loss: 1.746825\n",
      "Ep: 373, steps: 10, D loss: 0.175039, acc:  81%, G loss: 1.712019\n",
      "Ep: 373, steps: 11, D loss: 0.230241, acc:  60%, G loss: 1.943064\n",
      "Ep: 373, steps: 12, D loss: 0.307877, acc:  31%, G loss: 1.481409\n",
      "Ep: 373, steps: 13, D loss: 0.276567, acc:  40%, G loss: 1.439612\n",
      "Ep: 373, steps: 14, D loss: 0.278066, acc:  39%, G loss: 1.508652\n",
      "Ep: 373, steps: 15, D loss: 0.238397, acc:  59%, G loss: 1.630259\n",
      "Ep: 373, steps: 16, D loss: 0.235184, acc:  61%, G loss: 1.694341\n",
      "Ep: 373, steps: 17, D loss: 0.190420, acc:  77%, G loss: 1.686995\n",
      "Ep: 373, steps: 18, D loss: 0.251782, acc:  56%, G loss: 1.614622\n",
      "Ep: 373, steps: 19, D loss: 0.211441, acc:  69%, G loss: 1.720550\n",
      "Ep: 373, steps: 20, D loss: 0.183990, acc:  76%, G loss: 1.844579\n",
      "Ep: 373, steps: 21, D loss: 0.294181, acc:  32%, G loss: 1.513695\n",
      "Ep: 373, steps: 22, D loss: 0.182570, acc:  73%, G loss: 1.848712\n",
      "Ep: 373, steps: 23, D loss: 0.218218, acc:  65%, G loss: 2.049051\n",
      "Ep: 373, steps: 24, D loss: 0.183004, acc:  79%, G loss: 1.678561\n",
      "Ep: 373, steps: 25, D loss: 0.211738, acc:  67%, G loss: 1.815973\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 374, steps: 1, D loss: 0.270695, acc:  50%, G loss: 1.912431\n",
      "Ep: 374, steps: 2, D loss: 0.236174, acc:  60%, G loss: 1.616493\n",
      "Ep: 374, steps: 3, D loss: 0.182442, acc:  75%, G loss: 2.067813\n",
      "Saved Model\n",
      "Ep: 374, steps: 4, D loss: 0.188716, acc:  81%, G loss: 2.034095\n",
      "Ep: 374, steps: 5, D loss: 0.259732, acc:  51%, G loss: 1.686325\n",
      "Ep: 374, steps: 6, D loss: 0.365860, acc:  18%, G loss: 1.450540\n",
      "Ep: 374, steps: 7, D loss: 0.230762, acc:  62%, G loss: 1.863844\n",
      "Ep: 374, steps: 8, D loss: 0.251019, acc:  61%, G loss: 2.029058\n",
      "Ep: 374, steps: 9, D loss: 0.173341, acc:  82%, G loss: 1.697977\n",
      "Ep: 374, steps: 10, D loss: 0.223800, acc:  63%, G loss: 1.952257\n",
      "Ep: 374, steps: 11, D loss: 0.321881, acc:  26%, G loss: 1.458513\n",
      "Ep: 374, steps: 12, D loss: 0.308355, acc:  29%, G loss: 1.451887\n",
      "Ep: 374, steps: 13, D loss: 0.282998, acc:  38%, G loss: 1.554221\n",
      "Ep: 374, steps: 14, D loss: 0.237574, acc:  59%, G loss: 1.670240\n",
      "Ep: 374, steps: 15, D loss: 0.244155, acc:  58%, G loss: 1.717826\n",
      "Ep: 374, steps: 16, D loss: 0.206894, acc:  74%, G loss: 1.677399\n",
      "Ep: 374, steps: 17, D loss: 0.222779, acc:  65%, G loss: 1.638478\n",
      "Ep: 374, steps: 18, D loss: 0.199994, acc:  71%, G loss: 1.695326\n",
      "Ep: 374, steps: 19, D loss: 0.166456, acc:  80%, G loss: 1.858184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 374, steps: 20, D loss: 0.296845, acc:  34%, G loss: 1.701411\n",
      "Ep: 374, steps: 21, D loss: 0.172045, acc:  77%, G loss: 1.890066\n",
      "Ep: 374, steps: 22, D loss: 0.234725, acc:  62%, G loss: 2.304727\n",
      "Ep: 374, steps: 23, D loss: 0.183363, acc:  76%, G loss: 1.729574\n",
      "Ep: 374, steps: 24, D loss: 0.210703, acc:  66%, G loss: 1.737951\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 375, steps: 1, D loss: 0.263434, acc:  53%, G loss: 1.799365\n",
      "Ep: 375, steps: 2, D loss: 0.248599, acc:  53%, G loss: 1.560372\n",
      "Ep: 375, steps: 3, D loss: 0.154606, acc:  85%, G loss: 2.055027\n",
      "Ep: 375, steps: 4, D loss: 0.195891, acc:  78%, G loss: 1.948743\n",
      "Ep: 375, steps: 5, D loss: 0.309201, acc:  39%, G loss: 1.746161\n",
      "Ep: 375, steps: 6, D loss: 0.260706, acc:  52%, G loss: 1.708732\n",
      "Ep: 375, steps: 7, D loss: 0.334174, acc:  25%, G loss: 1.536100\n",
      "Ep: 375, steps: 8, D loss: 0.235845, acc:  59%, G loss: 1.792715\n",
      "Ep: 375, steps: 9, D loss: 0.225198, acc:  66%, G loss: 1.742324\n",
      "Ep: 375, steps: 10, D loss: 0.192615, acc:  75%, G loss: 1.700596\n",
      "Ep: 375, steps: 11, D loss: 0.226169, acc:  61%, G loss: 1.974928\n",
      "Ep: 375, steps: 12, D loss: 0.316503, acc:  29%, G loss: 1.466340\n",
      "Ep: 375, steps: 13, D loss: 0.282521, acc:  42%, G loss: 1.469819\n",
      "Ep: 375, steps: 14, D loss: 0.280523, acc:  39%, G loss: 1.533529\n",
      "Ep: 375, steps: 15, D loss: 0.241736, acc:  57%, G loss: 1.592875\n",
      "Ep: 375, steps: 16, D loss: 0.239946, acc:  58%, G loss: 1.681095\n",
      "Ep: 375, steps: 17, D loss: 0.211856, acc:  70%, G loss: 1.648970\n",
      "Ep: 375, steps: 18, D loss: 0.245268, acc:  58%, G loss: 1.584860\n",
      "Ep: 375, steps: 19, D loss: 0.210812, acc:  69%, G loss: 1.657468\n",
      "Ep: 375, steps: 20, D loss: 0.174514, acc:  78%, G loss: 1.962804\n",
      "Ep: 375, steps: 21, D loss: 0.275157, acc:  40%, G loss: 1.633088\n",
      "Ep: 375, steps: 22, D loss: 0.183258, acc:  73%, G loss: 1.810791\n",
      "Ep: 375, steps: 23, D loss: 0.196273, acc:  73%, G loss: 2.045084\n",
      "Ep: 375, steps: 24, D loss: 0.186578, acc:  77%, G loss: 1.711312\n",
      "Ep: 375, steps: 25, D loss: 0.211711, acc:  66%, G loss: 1.786504\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 376, steps: 1, D loss: 0.262654, acc:  50%, G loss: 1.805949\n",
      "Ep: 376, steps: 2, D loss: 0.248007, acc:  55%, G loss: 1.689396\n",
      "Ep: 376, steps: 3, D loss: 0.161273, acc:  83%, G loss: 2.129292\n",
      "Ep: 376, steps: 4, D loss: 0.189224, acc:  80%, G loss: 1.938805\n",
      "Ep: 376, steps: 5, D loss: 0.301597, acc:  44%, G loss: 1.868850\n",
      "Ep: 376, steps: 6, D loss: 0.246209, acc:  53%, G loss: 1.700738\n",
      "Ep: 376, steps: 7, D loss: 0.341945, acc:  25%, G loss: 1.438891\n",
      "Ep: 376, steps: 8, D loss: 0.237150, acc:  58%, G loss: 1.809005\n",
      "Ep: 376, steps: 9, D loss: 0.218975, acc:  66%, G loss: 1.939608\n",
      "Ep: 376, steps: 10, D loss: 0.173145, acc:  83%, G loss: 1.704422\n",
      "Ep: 376, steps: 11, D loss: 0.221087, acc:  65%, G loss: 1.906785\n",
      "Ep: 376, steps: 12, D loss: 0.312224, acc:  29%, G loss: 1.440104\n",
      "Ep: 376, steps: 13, D loss: 0.286101, acc:  38%, G loss: 1.462820\n",
      "Ep: 376, steps: 14, D loss: 0.284481, acc:  37%, G loss: 1.526531\n",
      "Ep: 376, steps: 15, D loss: 0.222974, acc:  66%, G loss: 1.613615\n",
      "Ep: 376, steps: 16, D loss: 0.231283, acc:  62%, G loss: 1.728141\n",
      "Ep: 376, steps: 17, D loss: 0.200365, acc:  73%, G loss: 1.748872\n",
      "Ep: 376, steps: 18, D loss: 0.244028, acc:  59%, G loss: 1.606796\n",
      "Ep: 376, steps: 19, D loss: 0.211135, acc:  69%, G loss: 1.662749\n",
      "Ep: 376, steps: 20, D loss: 0.179162, acc:  79%, G loss: 1.894838\n",
      "Ep: 376, steps: 21, D loss: 0.288233, acc:  36%, G loss: 1.505382\n",
      "Ep: 376, steps: 22, D loss: 0.167833, acc:  78%, G loss: 1.753689\n",
      "Ep: 376, steps: 23, D loss: 0.216717, acc:  67%, G loss: 2.079508\n",
      "Ep: 376, steps: 24, D loss: 0.183526, acc:  77%, G loss: 1.704107\n",
      "Ep: 376, steps: 25, D loss: 0.222027, acc:  64%, G loss: 1.788801\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 377, steps: 1, D loss: 0.283613, acc:  47%, G loss: 1.734153\n",
      "Saved Model\n",
      "Ep: 377, steps: 2, D loss: 0.246638, acc:  54%, G loss: 1.693052\n",
      "Ep: 377, steps: 3, D loss: 0.216455, acc:  71%, G loss: 1.774894\n",
      "Ep: 377, steps: 4, D loss: 0.286050, acc:  43%, G loss: 1.756817\n",
      "Ep: 377, steps: 5, D loss: 0.234425, acc:  54%, G loss: 1.584583\n",
      "Ep: 377, steps: 6, D loss: 0.320197, acc:  30%, G loss: 1.489915\n",
      "Ep: 377, steps: 7, D loss: 0.239369, acc:  59%, G loss: 1.757078\n",
      "Ep: 377, steps: 8, D loss: 0.230592, acc:  62%, G loss: 1.731994\n",
      "Ep: 377, steps: 9, D loss: 0.189304, acc:  79%, G loss: 1.650904\n",
      "Ep: 377, steps: 10, D loss: 0.222890, acc:  63%, G loss: 1.866908\n",
      "Ep: 377, steps: 11, D loss: 0.302264, acc:  30%, G loss: 1.435274\n",
      "Ep: 377, steps: 12, D loss: 0.285896, acc:  36%, G loss: 1.415101\n",
      "Ep: 377, steps: 13, D loss: 0.285566, acc:  36%, G loss: 1.597842\n",
      "Ep: 377, steps: 14, D loss: 0.242710, acc:  56%, G loss: 1.720462\n",
      "Ep: 377, steps: 15, D loss: 0.250604, acc:  54%, G loss: 1.684234\n",
      "Ep: 377, steps: 16, D loss: 0.202519, acc:  73%, G loss: 1.699730\n",
      "Ep: 377, steps: 17, D loss: 0.246097, acc:  56%, G loss: 1.632548\n",
      "Ep: 377, steps: 18, D loss: 0.222212, acc:  65%, G loss: 1.642031\n",
      "Ep: 377, steps: 19, D loss: 0.173818, acc:  79%, G loss: 1.906251\n",
      "Ep: 377, steps: 20, D loss: 0.270967, acc:  42%, G loss: 1.530567\n",
      "Ep: 377, steps: 21, D loss: 0.174480, acc:  77%, G loss: 1.732433\n",
      "Ep: 377, steps: 22, D loss: 0.215009, acc:  67%, G loss: 2.036348\n",
      "Ep: 377, steps: 23, D loss: 0.188710, acc:  74%, G loss: 1.672045\n",
      "Ep: 377, steps: 24, D loss: 0.224453, acc:  61%, G loss: 1.770403\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 378, steps: 1, D loss: 0.248646, acc:  53%, G loss: 1.948577\n",
      "Ep: 378, steps: 2, D loss: 0.259665, acc:  51%, G loss: 1.693810\n",
      "Ep: 378, steps: 3, D loss: 0.171753, acc:  78%, G loss: 2.122277\n",
      "Ep: 378, steps: 4, D loss: 0.201831, acc:  76%, G loss: 1.989577\n",
      "Ep: 378, steps: 5, D loss: 0.301643, acc:  42%, G loss: 1.809938\n",
      "Ep: 378, steps: 6, D loss: 0.252198, acc:  52%, G loss: 1.690270\n",
      "Ep: 378, steps: 7, D loss: 0.345817, acc:  23%, G loss: 1.514622\n",
      "Ep: 378, steps: 8, D loss: 0.236263, acc:  57%, G loss: 1.921911\n",
      "Ep: 378, steps: 9, D loss: 0.217205, acc:  69%, G loss: 1.824063\n",
      "Ep: 378, steps: 10, D loss: 0.186585, acc:  76%, G loss: 1.698601\n",
      "Ep: 378, steps: 11, D loss: 0.235688, acc:  60%, G loss: 1.958492\n",
      "Ep: 378, steps: 12, D loss: 0.324737, acc:  25%, G loss: 1.429730\n",
      "Ep: 378, steps: 13, D loss: 0.290949, acc:  36%, G loss: 1.452465\n",
      "Ep: 378, steps: 14, D loss: 0.279325, acc:  39%, G loss: 1.548832\n",
      "Ep: 378, steps: 15, D loss: 0.226326, acc:  66%, G loss: 1.657729\n",
      "Ep: 378, steps: 16, D loss: 0.250418, acc:  55%, G loss: 1.752703\n",
      "Ep: 378, steps: 17, D loss: 0.194676, acc:  76%, G loss: 1.769491\n",
      "Ep: 378, steps: 18, D loss: 0.242283, acc:  59%, G loss: 1.573524\n",
      "Ep: 378, steps: 19, D loss: 0.212075, acc:  67%, G loss: 1.662376\n",
      "Ep: 378, steps: 20, D loss: 0.188334, acc:  74%, G loss: 1.906842\n",
      "Ep: 378, steps: 21, D loss: 0.306831, acc:  28%, G loss: 1.418958\n",
      "Ep: 378, steps: 22, D loss: 0.180678, acc:  75%, G loss: 1.796639\n",
      "Ep: 378, steps: 23, D loss: 0.208943, acc:  68%, G loss: 2.089160\n",
      "Ep: 378, steps: 24, D loss: 0.188201, acc:  75%, G loss: 1.659675\n",
      "Ep: 378, steps: 25, D loss: 0.227268, acc:  63%, G loss: 1.653888\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 379, steps: 1, D loss: 0.243791, acc:  57%, G loss: 1.794962\n",
      "Ep: 379, steps: 2, D loss: 0.257218, acc:  51%, G loss: 1.715762\n",
      "Ep: 379, steps: 3, D loss: 0.160712, acc:  84%, G loss: 2.165971\n",
      "Ep: 379, steps: 4, D loss: 0.186977, acc:  80%, G loss: 1.987001\n",
      "Ep: 379, steps: 5, D loss: 0.281552, acc:  46%, G loss: 1.809053\n",
      "Ep: 379, steps: 6, D loss: 0.243475, acc:  53%, G loss: 1.718084\n",
      "Ep: 379, steps: 7, D loss: 0.348130, acc:  24%, G loss: 1.571607\n",
      "Ep: 379, steps: 8, D loss: 0.233434, acc:  61%, G loss: 1.797794\n",
      "Ep: 379, steps: 9, D loss: 0.229011, acc:  62%, G loss: 1.782418\n",
      "Ep: 379, steps: 10, D loss: 0.173098, acc:  83%, G loss: 1.624483\n",
      "Ep: 379, steps: 11, D loss: 0.238249, acc:  56%, G loss: 1.910082\n",
      "Ep: 379, steps: 12, D loss: 0.309130, acc:  31%, G loss: 1.443785\n",
      "Ep: 379, steps: 13, D loss: 0.288386, acc:  34%, G loss: 1.440873\n",
      "Ep: 379, steps: 14, D loss: 0.282447, acc:  36%, G loss: 1.527571\n",
      "Ep: 379, steps: 15, D loss: 0.236551, acc:  60%, G loss: 1.704099\n",
      "Ep: 379, steps: 16, D loss: 0.241838, acc:  58%, G loss: 1.710985\n",
      "Ep: 379, steps: 17, D loss: 0.186688, acc:  80%, G loss: 1.632276\n",
      "Ep: 379, steps: 18, D loss: 0.260981, acc:  55%, G loss: 1.599339\n",
      "Ep: 379, steps: 19, D loss: 0.217979, acc:  67%, G loss: 1.653941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 379, steps: 20, D loss: 0.173166, acc:  81%, G loss: 1.809382\n",
      "Ep: 379, steps: 21, D loss: 0.295009, acc:  32%, G loss: 1.573778\n",
      "Ep: 379, steps: 22, D loss: 0.185914, acc:  73%, G loss: 1.792541\n",
      "Ep: 379, steps: 23, D loss: 0.216269, acc:  66%, G loss: 2.026660\n",
      "Ep: 379, steps: 24, D loss: 0.188462, acc:  77%, G loss: 1.651066\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 379, steps: 25, D loss: 0.240198, acc:  57%, G loss: 1.855624\n",
      "Ep: 379, steps: 26, D loss: 0.265964, acc:  48%, G loss: 1.484726\n",
      "Ep: 379, steps: 27, D loss: 0.159020, acc:  82%, G loss: 2.181908\n",
      "Ep: 379, steps: 28, D loss: 0.207603, acc:  73%, G loss: 1.894277\n",
      "Ep: 379, steps: 29, D loss: 0.252794, acc:  55%, G loss: 1.890931\n",
      "Ep: 379, steps: 30, D loss: 0.236173, acc:  56%, G loss: 1.698791\n",
      "Ep: 379, steps: 31, D loss: 0.329992, acc:  28%, G loss: 1.596381\n",
      "Ep: 379, steps: 32, D loss: 0.231641, acc:  60%, G loss: 1.794169\n",
      "Ep: 379, steps: 33, D loss: 0.241418, acc:  59%, G loss: 1.687008\n",
      "Ep: 379, steps: 34, D loss: 0.185421, acc:  77%, G loss: 1.617662\n",
      "Ep: 379, steps: 35, D loss: 0.244332, acc:  53%, G loss: 1.951695\n",
      "Ep: 379, steps: 36, D loss: 0.318977, acc:  28%, G loss: 1.577392\n",
      "Ep: 379, steps: 37, D loss: 0.274640, acc:  44%, G loss: 1.507124\n",
      "Ep: 379, steps: 38, D loss: 0.274526, acc:  40%, G loss: 1.558969\n",
      "Ep: 379, steps: 39, D loss: 0.225628, acc:  66%, G loss: 1.606058\n",
      "Ep: 379, steps: 40, D loss: 0.237570, acc:  60%, G loss: 1.725297\n",
      "Ep: 379, steps: 41, D loss: 0.212949, acc:  71%, G loss: 1.727173\n",
      "Ep: 379, steps: 42, D loss: 0.254605, acc:  54%, G loss: 1.595601\n",
      "Ep: 379, steps: 43, D loss: 0.222422, acc:  65%, G loss: 1.672070\n",
      "Ep: 379, steps: 44, D loss: 0.169145, acc:  81%, G loss: 1.948179\n",
      "Ep: 379, steps: 45, D loss: 0.294876, acc:  31%, G loss: 1.538944\n",
      "Ep: 379, steps: 46, D loss: 0.184917, acc:  74%, G loss: 1.789318\n",
      "Ep: 379, steps: 47, D loss: 0.219262, acc:  66%, G loss: 2.055712\n",
      "Ep: 379, steps: 48, D loss: 0.179970, acc:  80%, G loss: 1.670504\n",
      "Ep: 379, steps: 49, D loss: 0.234444, acc:  58%, G loss: 1.855491\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 380, steps: 1, D loss: 0.285231, acc:  46%, G loss: 1.784509\n",
      "Ep: 380, steps: 2, D loss: 0.249369, acc:  52%, G loss: 1.644862\n",
      "Ep: 380, steps: 3, D loss: 0.176224, acc:  79%, G loss: 2.058403\n",
      "Ep: 380, steps: 4, D loss: 0.197157, acc:  78%, G loss: 1.958774\n",
      "Ep: 380, steps: 5, D loss: 0.270032, acc:  49%, G loss: 1.838472\n",
      "Ep: 380, steps: 6, D loss: 0.247011, acc:  54%, G loss: 1.656021\n",
      "Ep: 380, steps: 7, D loss: 0.323663, acc:  26%, G loss: 1.631121\n",
      "Ep: 380, steps: 8, D loss: 0.220945, acc:  65%, G loss: 2.018775\n",
      "Ep: 380, steps: 9, D loss: 0.211181, acc:  71%, G loss: 1.830037\n",
      "Ep: 380, steps: 10, D loss: 0.170300, acc:  85%, G loss: 1.614786\n",
      "Ep: 380, steps: 11, D loss: 0.225325, acc:  63%, G loss: 1.902388\n",
      "Ep: 380, steps: 12, D loss: 0.308608, acc:  31%, G loss: 1.419986\n",
      "Ep: 380, steps: 13, D loss: 0.280810, acc:  42%, G loss: 1.490625\n",
      "Ep: 380, steps: 14, D loss: 0.286617, acc:  36%, G loss: 1.554907\n",
      "Ep: 380, steps: 15, D loss: 0.227387, acc:  66%, G loss: 1.639877\n",
      "Ep: 380, steps: 16, D loss: 0.232299, acc:  60%, G loss: 1.695733\n",
      "Ep: 380, steps: 17, D loss: 0.191538, acc:  77%, G loss: 1.681627\n",
      "Ep: 380, steps: 18, D loss: 0.262759, acc:  54%, G loss: 1.640756\n",
      "Ep: 380, steps: 19, D loss: 0.210651, acc:  67%, G loss: 1.713703\n",
      "Ep: 380, steps: 20, D loss: 0.170696, acc:  79%, G loss: 1.873639\n",
      "Ep: 380, steps: 21, D loss: 0.297392, acc:  33%, G loss: 1.615552\n",
      "Ep: 380, steps: 22, D loss: 0.171302, acc:  75%, G loss: 1.805222\n",
      "Ep: 380, steps: 23, D loss: 0.209570, acc:  68%, G loss: 2.116239\n",
      "Ep: 380, steps: 24, D loss: 0.187162, acc:  74%, G loss: 1.673385\n",
      "Ep: 380, steps: 25, D loss: 0.222240, acc:  63%, G loss: 1.842235\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 381, steps: 1, D loss: 0.275720, acc:  50%, G loss: 1.760702\n",
      "Ep: 381, steps: 2, D loss: 0.244761, acc:  56%, G loss: 1.670291\n",
      "Ep: 381, steps: 3, D loss: 0.164212, acc:  84%, G loss: 2.106953\n",
      "Ep: 381, steps: 4, D loss: 0.195190, acc:  78%, G loss: 1.968583\n",
      "Ep: 381, steps: 5, D loss: 0.284482, acc:  43%, G loss: 1.827757\n",
      "Ep: 381, steps: 6, D loss: 0.238876, acc:  56%, G loss: 1.714384\n",
      "Ep: 381, steps: 7, D loss: 0.357791, acc:  22%, G loss: 1.608729\n",
      "Ep: 381, steps: 8, D loss: 0.239405, acc:  58%, G loss: 1.825982\n",
      "Ep: 381, steps: 9, D loss: 0.221147, acc:  66%, G loss: 1.743562\n",
      "Ep: 381, steps: 10, D loss: 0.191071, acc:  76%, G loss: 1.655355\n",
      "Ep: 381, steps: 11, D loss: 0.224110, acc:  64%, G loss: 1.988942\n",
      "Ep: 381, steps: 12, D loss: 0.303005, acc:  32%, G loss: 1.391434\n",
      "Ep: 381, steps: 13, D loss: 0.284269, acc:  37%, G loss: 1.468306\n",
      "Ep: 381, steps: 14, D loss: 0.292507, acc:  33%, G loss: 1.544271\n",
      "Ep: 381, steps: 15, D loss: 0.222091, acc:  68%, G loss: 1.703779\n",
      "Ep: 381, steps: 16, D loss: 0.259427, acc:  53%, G loss: 1.784737\n",
      "Ep: 381, steps: 17, D loss: 0.210211, acc:  71%, G loss: 1.689225\n",
      "Ep: 381, steps: 18, D loss: 0.235075, acc:  61%, G loss: 1.664475\n",
      "Ep: 381, steps: 19, D loss: 0.208118, acc:  71%, G loss: 1.659547\n",
      "Ep: 381, steps: 20, D loss: 0.202401, acc:  71%, G loss: 1.784634\n",
      "Ep: 381, steps: 21, D loss: 0.297566, acc:  30%, G loss: 1.563015\n",
      "Ep: 381, steps: 22, D loss: 0.186082, acc:  74%, G loss: 1.729061\n",
      "Saved Model\n",
      "Ep: 381, steps: 23, D loss: 0.212084, acc:  70%, G loss: 2.077150\n",
      "Ep: 381, steps: 24, D loss: 0.256125, acc:  51%, G loss: 1.784347\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 382, steps: 1, D loss: 0.244115, acc:  58%, G loss: 1.807101\n",
      "Ep: 382, steps: 2, D loss: 0.225313, acc:  65%, G loss: 1.632095\n",
      "Ep: 382, steps: 3, D loss: 0.166376, acc:  81%, G loss: 2.038785\n",
      "Ep: 382, steps: 4, D loss: 0.196228, acc:  78%, G loss: 1.970399\n",
      "Ep: 382, steps: 5, D loss: 0.288269, acc:  44%, G loss: 1.800187\n",
      "Ep: 382, steps: 6, D loss: 0.256556, acc:  52%, G loss: 1.651041\n",
      "Ep: 382, steps: 7, D loss: 0.332716, acc:  23%, G loss: 1.476478\n",
      "Ep: 382, steps: 8, D loss: 0.226472, acc:  62%, G loss: 1.780800\n",
      "Ep: 382, steps: 9, D loss: 0.211363, acc:  71%, G loss: 1.704108\n",
      "Ep: 382, steps: 10, D loss: 0.174270, acc:  83%, G loss: 1.680551\n",
      "Ep: 382, steps: 11, D loss: 0.238151, acc:  56%, G loss: 1.892806\n",
      "Ep: 382, steps: 12, D loss: 0.304761, acc:  31%, G loss: 1.464258\n",
      "Ep: 382, steps: 13, D loss: 0.292415, acc:  35%, G loss: 1.436019\n",
      "Ep: 382, steps: 14, D loss: 0.281010, acc:  37%, G loss: 1.576540\n",
      "Ep: 382, steps: 15, D loss: 0.240723, acc:  57%, G loss: 1.723167\n",
      "Ep: 382, steps: 16, D loss: 0.258031, acc:  54%, G loss: 1.755294\n",
      "Ep: 382, steps: 17, D loss: 0.208705, acc:  73%, G loss: 1.696375\n",
      "Ep: 382, steps: 18, D loss: 0.239808, acc:  60%, G loss: 1.603461\n",
      "Ep: 382, steps: 19, D loss: 0.214335, acc:  67%, G loss: 1.677354\n",
      "Ep: 382, steps: 20, D loss: 0.172896, acc:  79%, G loss: 1.804485\n",
      "Ep: 382, steps: 21, D loss: 0.292126, acc:  31%, G loss: 1.552994\n",
      "Ep: 382, steps: 22, D loss: 0.170664, acc:  78%, G loss: 1.728435\n",
      "Ep: 382, steps: 23, D loss: 0.237010, acc:  60%, G loss: 2.066866\n",
      "Ep: 382, steps: 24, D loss: 0.188967, acc:  76%, G loss: 1.697936\n",
      "Ep: 382, steps: 25, D loss: 0.232264, acc:  60%, G loss: 1.640295\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 383, steps: 1, D loss: 0.283802, acc:  43%, G loss: 1.883323\n",
      "Ep: 383, steps: 2, D loss: 0.241469, acc:  57%, G loss: 1.661503\n",
      "Ep: 383, steps: 3, D loss: 0.155720, acc:  84%, G loss: 2.206433\n",
      "Ep: 383, steps: 4, D loss: 0.195603, acc:  78%, G loss: 2.063489\n",
      "Ep: 383, steps: 5, D loss: 0.272704, acc:  48%, G loss: 1.772173\n",
      "Ep: 383, steps: 6, D loss: 0.247071, acc:  55%, G loss: 1.726856\n",
      "Ep: 383, steps: 7, D loss: 0.341308, acc:  24%, G loss: 1.493759\n",
      "Ep: 383, steps: 8, D loss: 0.247711, acc:  57%, G loss: 1.842231\n",
      "Ep: 383, steps: 9, D loss: 0.237106, acc:  63%, G loss: 1.742216\n",
      "Ep: 383, steps: 10, D loss: 0.186070, acc:  79%, G loss: 1.733889\n",
      "Ep: 383, steps: 11, D loss: 0.232605, acc:  59%, G loss: 1.930569\n",
      "Ep: 383, steps: 12, D loss: 0.292666, acc:  34%, G loss: 1.460864\n",
      "Ep: 383, steps: 13, D loss: 0.285711, acc:  36%, G loss: 1.453543\n",
      "Ep: 383, steps: 14, D loss: 0.285639, acc:  35%, G loss: 1.566171\n",
      "Ep: 383, steps: 15, D loss: 0.227825, acc:  64%, G loss: 1.644764\n",
      "Ep: 383, steps: 16, D loss: 0.257739, acc:  53%, G loss: 1.748116\n",
      "Ep: 383, steps: 17, D loss: 0.219604, acc:  69%, G loss: 1.603849\n",
      "Ep: 383, steps: 18, D loss: 0.240481, acc:  60%, G loss: 1.629879\n",
      "Ep: 383, steps: 19, D loss: 0.229222, acc:  63%, G loss: 1.648297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 383, steps: 20, D loss: 0.177509, acc:  78%, G loss: 1.856086\n",
      "Ep: 383, steps: 21, D loss: 0.298788, acc:  30%, G loss: 1.688768\n",
      "Ep: 383, steps: 22, D loss: 0.171443, acc:  77%, G loss: 1.814170\n",
      "Ep: 383, steps: 23, D loss: 0.202825, acc:  70%, G loss: 2.073368\n",
      "Ep: 383, steps: 24, D loss: 0.191504, acc:  75%, G loss: 1.765713\n",
      "Ep: 383, steps: 25, D loss: 0.230700, acc:  60%, G loss: 1.769203\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 384, steps: 1, D loss: 0.245799, acc:  57%, G loss: 1.846182\n",
      "Ep: 384, steps: 2, D loss: 0.251239, acc:  53%, G loss: 1.675748\n",
      "Ep: 384, steps: 3, D loss: 0.163603, acc:  82%, G loss: 2.119467\n",
      "Ep: 384, steps: 4, D loss: 0.193341, acc:  78%, G loss: 1.916805\n",
      "Ep: 384, steps: 5, D loss: 0.282581, acc:  46%, G loss: 1.791892\n",
      "Ep: 384, steps: 6, D loss: 0.243541, acc:  56%, G loss: 1.763316\n",
      "Ep: 384, steps: 7, D loss: 0.325618, acc:  31%, G loss: 1.547329\n",
      "Ep: 384, steps: 8, D loss: 0.241882, acc:  58%, G loss: 1.801109\n",
      "Ep: 384, steps: 9, D loss: 0.218011, acc:  67%, G loss: 1.737002\n",
      "Ep: 384, steps: 10, D loss: 0.184835, acc:  75%, G loss: 1.627861\n",
      "Ep: 384, steps: 11, D loss: 0.217844, acc:  66%, G loss: 1.894774\n",
      "Ep: 384, steps: 12, D loss: 0.302192, acc:  31%, G loss: 1.436959\n",
      "Ep: 384, steps: 13, D loss: 0.283630, acc:  38%, G loss: 1.473798\n",
      "Ep: 384, steps: 14, D loss: 0.274669, acc:  41%, G loss: 1.551703\n",
      "Ep: 384, steps: 15, D loss: 0.245328, acc:  53%, G loss: 1.714079\n",
      "Ep: 384, steps: 16, D loss: 0.244903, acc:  59%, G loss: 1.869925\n",
      "Ep: 384, steps: 17, D loss: 0.199279, acc:  73%, G loss: 1.649201\n",
      "Ep: 384, steps: 18, D loss: 0.250504, acc:  54%, G loss: 1.641694\n",
      "Ep: 384, steps: 19, D loss: 0.218176, acc:  67%, G loss: 1.770647\n",
      "Saved Model\n",
      "Ep: 384, steps: 20, D loss: 0.174436, acc:  81%, G loss: 1.890584\n",
      "Ep: 384, steps: 21, D loss: 0.182171, acc:  76%, G loss: 1.677912\n",
      "Ep: 384, steps: 22, D loss: 0.229043, acc:  64%, G loss: 1.980868\n",
      "Ep: 384, steps: 23, D loss: 0.174480, acc:  79%, G loss: 1.715187\n",
      "Ep: 384, steps: 24, D loss: 0.227383, acc:  62%, G loss: 1.657537\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 385, steps: 1, D loss: 0.307801, acc:  38%, G loss: 1.823145\n",
      "Ep: 385, steps: 2, D loss: 0.231257, acc:  62%, G loss: 1.574607\n",
      "Ep: 385, steps: 3, D loss: 0.163021, acc:  85%, G loss: 2.099792\n",
      "Ep: 385, steps: 4, D loss: 0.182031, acc:  85%, G loss: 1.878542\n",
      "Ep: 385, steps: 5, D loss: 0.286080, acc:  44%, G loss: 1.749045\n",
      "Ep: 385, steps: 6, D loss: 0.251174, acc:  53%, G loss: 1.730989\n",
      "Ep: 385, steps: 7, D loss: 0.371254, acc:  16%, G loss: 1.430566\n",
      "Ep: 385, steps: 8, D loss: 0.247234, acc:  54%, G loss: 1.782881\n",
      "Ep: 385, steps: 9, D loss: 0.217395, acc:  69%, G loss: 1.852740\n",
      "Ep: 385, steps: 10, D loss: 0.186626, acc:  79%, G loss: 1.692592\n",
      "Ep: 385, steps: 11, D loss: 0.212888, acc:  67%, G loss: 1.901514\n",
      "Ep: 385, steps: 12, D loss: 0.295846, acc:  33%, G loss: 1.474060\n",
      "Ep: 385, steps: 13, D loss: 0.297080, acc:  32%, G loss: 1.490543\n",
      "Ep: 385, steps: 14, D loss: 0.281708, acc:  37%, G loss: 1.507456\n",
      "Ep: 385, steps: 15, D loss: 0.215798, acc:  70%, G loss: 1.588239\n",
      "Ep: 385, steps: 16, D loss: 0.252379, acc:  55%, G loss: 1.813645\n",
      "Ep: 385, steps: 17, D loss: 0.204558, acc:  72%, G loss: 1.680886\n",
      "Ep: 385, steps: 18, D loss: 0.226921, acc:  65%, G loss: 1.645857\n",
      "Ep: 385, steps: 19, D loss: 0.208089, acc:  68%, G loss: 1.698864\n",
      "Ep: 385, steps: 20, D loss: 0.172359, acc:  80%, G loss: 1.931855\n",
      "Ep: 385, steps: 21, D loss: 0.289627, acc:  36%, G loss: 1.538366\n",
      "Ep: 385, steps: 22, D loss: 0.186613, acc:  75%, G loss: 1.706599\n",
      "Ep: 385, steps: 23, D loss: 0.211221, acc:  68%, G loss: 2.019269\n",
      "Ep: 385, steps: 24, D loss: 0.180453, acc:  78%, G loss: 1.641360\n",
      "Ep: 385, steps: 25, D loss: 0.240797, acc:  58%, G loss: 1.656628\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 386, steps: 1, D loss: 0.251097, acc:  54%, G loss: 1.810489\n",
      "Ep: 386, steps: 2, D loss: 0.237736, acc:  57%, G loss: 1.615408\n",
      "Ep: 386, steps: 3, D loss: 0.155710, acc:  86%, G loss: 2.109269\n",
      "Ep: 386, steps: 4, D loss: 0.186700, acc:  80%, G loss: 1.970020\n",
      "Ep: 386, steps: 5, D loss: 0.293291, acc:  43%, G loss: 1.868836\n",
      "Ep: 386, steps: 6, D loss: 0.237331, acc:  57%, G loss: 1.662901\n",
      "Ep: 386, steps: 7, D loss: 0.374847, acc:  20%, G loss: 1.664754\n",
      "Ep: 386, steps: 8, D loss: 0.243675, acc:  58%, G loss: 1.829044\n",
      "Ep: 386, steps: 9, D loss: 0.223116, acc:  65%, G loss: 1.744564\n",
      "Ep: 386, steps: 10, D loss: 0.178458, acc:  81%, G loss: 1.625220\n",
      "Ep: 386, steps: 11, D loss: 0.224934, acc:  61%, G loss: 1.890333\n",
      "Ep: 386, steps: 12, D loss: 0.306862, acc:  30%, G loss: 1.525758\n",
      "Ep: 386, steps: 13, D loss: 0.280997, acc:  40%, G loss: 1.448508\n",
      "Ep: 386, steps: 14, D loss: 0.286832, acc:  36%, G loss: 1.469339\n",
      "Ep: 386, steps: 15, D loss: 0.230224, acc:  64%, G loss: 1.586965\n",
      "Ep: 386, steps: 16, D loss: 0.242780, acc:  57%, G loss: 1.700933\n",
      "Ep: 386, steps: 17, D loss: 0.203194, acc:  73%, G loss: 1.684396\n",
      "Ep: 386, steps: 18, D loss: 0.247159, acc:  56%, G loss: 1.624249\n",
      "Ep: 386, steps: 19, D loss: 0.214986, acc:  66%, G loss: 1.747481\n",
      "Ep: 386, steps: 20, D loss: 0.193138, acc:  73%, G loss: 1.863746\n",
      "Ep: 386, steps: 21, D loss: 0.288745, acc:  36%, G loss: 1.577288\n",
      "Ep: 386, steps: 22, D loss: 0.185609, acc:  71%, G loss: 1.658785\n",
      "Ep: 386, steps: 23, D loss: 0.227835, acc:  65%, G loss: 2.072861\n",
      "Ep: 386, steps: 24, D loss: 0.197956, acc:  73%, G loss: 1.652648\n",
      "Ep: 386, steps: 25, D loss: 0.243411, acc:  56%, G loss: 1.643392\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 387, steps: 1, D loss: 0.252592, acc:  56%, G loss: 1.708994\n",
      "Ep: 387, steps: 2, D loss: 0.249553, acc:  53%, G loss: 1.585220\n",
      "Ep: 387, steps: 3, D loss: 0.169232, acc:  80%, G loss: 2.095968\n",
      "Ep: 387, steps: 4, D loss: 0.198772, acc:  78%, G loss: 1.893745\n",
      "Ep: 387, steps: 5, D loss: 0.279933, acc:  45%, G loss: 1.788287\n",
      "Ep: 387, steps: 6, D loss: 0.242911, acc:  56%, G loss: 1.718869\n",
      "Ep: 387, steps: 7, D loss: 0.339036, acc:  23%, G loss: 1.450994\n",
      "Ep: 387, steps: 8, D loss: 0.229439, acc:  62%, G loss: 1.818760\n",
      "Ep: 387, steps: 9, D loss: 0.227260, acc:  66%, G loss: 1.753249\n",
      "Ep: 387, steps: 10, D loss: 0.177335, acc:  83%, G loss: 1.706310\n",
      "Ep: 387, steps: 11, D loss: 0.219919, acc:  62%, G loss: 1.882242\n",
      "Ep: 387, steps: 12, D loss: 0.300525, acc:  34%, G loss: 1.446590\n",
      "Ep: 387, steps: 13, D loss: 0.282531, acc:  37%, G loss: 1.467757\n",
      "Ep: 387, steps: 14, D loss: 0.280626, acc:  38%, G loss: 1.493907\n",
      "Ep: 387, steps: 15, D loss: 0.228402, acc:  63%, G loss: 1.593052\n",
      "Ep: 387, steps: 16, D loss: 0.233745, acc:  60%, G loss: 1.716037\n",
      "Ep: 387, steps: 17, D loss: 0.206634, acc:  74%, G loss: 1.667247\n",
      "Saved Model\n",
      "Ep: 387, steps: 18, D loss: 0.245867, acc:  56%, G loss: 1.687937\n",
      "Ep: 387, steps: 19, D loss: 0.179872, acc:  76%, G loss: 1.815955\n",
      "Ep: 387, steps: 20, D loss: 0.284986, acc:  36%, G loss: 1.674835\n",
      "Ep: 387, steps: 21, D loss: 0.170792, acc:  77%, G loss: 1.637983\n",
      "Ep: 387, steps: 22, D loss: 0.229758, acc:  61%, G loss: 2.045776\n",
      "Ep: 387, steps: 23, D loss: 0.189153, acc:  77%, G loss: 1.697521\n",
      "Ep: 387, steps: 24, D loss: 0.215553, acc:  66%, G loss: 1.651428\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 388, steps: 1, D loss: 0.299940, acc:  46%, G loss: 1.924207\n",
      "Ep: 388, steps: 2, D loss: 0.252939, acc:  53%, G loss: 1.545269\n",
      "Ep: 388, steps: 3, D loss: 0.177490, acc:  78%, G loss: 2.014011\n",
      "Ep: 388, steps: 4, D loss: 0.204528, acc:  75%, G loss: 1.860684\n",
      "Ep: 388, steps: 5, D loss: 0.266217, acc:  50%, G loss: 1.941279\n",
      "Ep: 388, steps: 6, D loss: 0.247767, acc:  52%, G loss: 1.780154\n",
      "Ep: 388, steps: 7, D loss: 0.311028, acc:  33%, G loss: 1.603526\n",
      "Ep: 388, steps: 8, D loss: 0.240948, acc:  57%, G loss: 1.829490\n",
      "Ep: 388, steps: 9, D loss: 0.223999, acc:  66%, G loss: 1.745296\n",
      "Ep: 388, steps: 10, D loss: 0.168651, acc:  86%, G loss: 1.672560\n",
      "Ep: 388, steps: 11, D loss: 0.225929, acc:  63%, G loss: 1.904755\n",
      "Ep: 388, steps: 12, D loss: 0.308507, acc:  29%, G loss: 1.400040\n",
      "Ep: 388, steps: 13, D loss: 0.288023, acc:  36%, G loss: 1.474490\n",
      "Ep: 388, steps: 14, D loss: 0.274774, acc:  44%, G loss: 1.495381\n",
      "Ep: 388, steps: 15, D loss: 0.242592, acc:  58%, G loss: 1.633587\n",
      "Ep: 388, steps: 16, D loss: 0.238492, acc:  60%, G loss: 1.693197\n",
      "Ep: 388, steps: 17, D loss: 0.201730, acc:  75%, G loss: 1.690951\n",
      "Ep: 388, steps: 18, D loss: 0.248755, acc:  57%, G loss: 1.595509\n",
      "Ep: 388, steps: 19, D loss: 0.205673, acc:  70%, G loss: 1.668867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 388, steps: 20, D loss: 0.185265, acc:  74%, G loss: 1.853173\n",
      "Ep: 388, steps: 21, D loss: 0.287454, acc:  37%, G loss: 1.646542\n",
      "Ep: 388, steps: 22, D loss: 0.160156, acc:  82%, G loss: 1.713807\n",
      "Ep: 388, steps: 23, D loss: 0.216868, acc:  68%, G loss: 2.017136\n",
      "Ep: 388, steps: 24, D loss: 0.187814, acc:  74%, G loss: 1.703111\n",
      "Ep: 388, steps: 25, D loss: 0.254999, acc:  54%, G loss: 1.948165\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 389, steps: 1, D loss: 0.218095, acc:  67%, G loss: 1.770745\n",
      "Ep: 389, steps: 2, D loss: 0.261414, acc:  49%, G loss: 1.630117\n",
      "Ep: 389, steps: 3, D loss: 0.149606, acc:  84%, G loss: 2.165126\n",
      "Ep: 389, steps: 4, D loss: 0.187762, acc:  80%, G loss: 1.961470\n",
      "Ep: 389, steps: 5, D loss: 0.273252, acc:  46%, G loss: 1.903595\n",
      "Ep: 389, steps: 6, D loss: 0.255175, acc:  52%, G loss: 1.669461\n",
      "Ep: 389, steps: 7, D loss: 0.351456, acc:  26%, G loss: 1.598369\n",
      "Ep: 389, steps: 8, D loss: 0.236035, acc:  59%, G loss: 1.972372\n",
      "Ep: 389, steps: 9, D loss: 0.238503, acc:  61%, G loss: 1.784564\n",
      "Ep: 389, steps: 10, D loss: 0.175506, acc:  82%, G loss: 1.686685\n",
      "Ep: 389, steps: 11, D loss: 0.232736, acc:  60%, G loss: 1.927210\n",
      "Ep: 389, steps: 12, D loss: 0.315794, acc:  29%, G loss: 1.392149\n",
      "Ep: 389, steps: 13, D loss: 0.284994, acc:  37%, G loss: 1.463233\n",
      "Ep: 389, steps: 14, D loss: 0.269815, acc:  41%, G loss: 1.522383\n",
      "Ep: 389, steps: 15, D loss: 0.242054, acc:  58%, G loss: 1.695521\n",
      "Ep: 389, steps: 16, D loss: 0.244258, acc:  57%, G loss: 1.687712\n",
      "Ep: 389, steps: 17, D loss: 0.198377, acc:  75%, G loss: 1.644439\n",
      "Ep: 389, steps: 18, D loss: 0.255907, acc:  53%, G loss: 1.593898\n",
      "Ep: 389, steps: 19, D loss: 0.213955, acc:  68%, G loss: 1.695294\n",
      "Ep: 389, steps: 20, D loss: 0.189221, acc:  75%, G loss: 1.906695\n",
      "Ep: 389, steps: 21, D loss: 0.279148, acc:  37%, G loss: 1.477584\n",
      "Ep: 389, steps: 22, D loss: 0.188160, acc:  72%, G loss: 1.669693\n",
      "Ep: 389, steps: 23, D loss: 0.220330, acc:  65%, G loss: 1.985526\n",
      "Ep: 389, steps: 24, D loss: 0.200246, acc:  71%, G loss: 1.660027\n",
      "Ep: 389, steps: 25, D loss: 0.217786, acc:  66%, G loss: 1.620814\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 390, steps: 1, D loss: 0.262491, acc:  51%, G loss: 1.856240\n",
      "Ep: 390, steps: 2, D loss: 0.241547, acc:  57%, G loss: 1.600979\n",
      "Ep: 390, steps: 3, D loss: 0.159490, acc:  83%, G loss: 2.132791\n",
      "Ep: 390, steps: 4, D loss: 0.192742, acc:  80%, G loss: 2.005771\n",
      "Ep: 390, steps: 5, D loss: 0.296139, acc:  41%, G loss: 2.049575\n",
      "Ep: 390, steps: 6, D loss: 0.244334, acc:  57%, G loss: 1.665250\n",
      "Ep: 390, steps: 7, D loss: 0.318630, acc:  27%, G loss: 1.496586\n",
      "Ep: 390, steps: 8, D loss: 0.242120, acc:  58%, G loss: 1.831365\n",
      "Ep: 390, steps: 9, D loss: 0.238773, acc:  60%, G loss: 1.725346\n",
      "Ep: 390, steps: 10, D loss: 0.173431, acc:  86%, G loss: 1.699743\n",
      "Ep: 390, steps: 11, D loss: 0.230135, acc:  59%, G loss: 1.881962\n",
      "Ep: 390, steps: 12, D loss: 0.315724, acc:  26%, G loss: 1.380821\n",
      "Ep: 390, steps: 13, D loss: 0.292014, acc:  32%, G loss: 1.394251\n",
      "Ep: 390, steps: 14, D loss: 0.289429, acc:  35%, G loss: 1.449922\n",
      "Ep: 390, steps: 15, D loss: 0.232294, acc:  63%, G loss: 1.633908\n",
      "Saved Model\n",
      "Ep: 390, steps: 16, D loss: 0.236297, acc:  60%, G loss: 1.707276\n",
      "Ep: 390, steps: 17, D loss: 0.225747, acc:  67%, G loss: 1.667529\n",
      "Ep: 390, steps: 18, D loss: 0.237319, acc:  60%, G loss: 1.651283\n",
      "Ep: 390, steps: 19, D loss: 0.167005, acc:  80%, G loss: 1.797317\n",
      "Ep: 390, steps: 20, D loss: 0.300671, acc:  31%, G loss: 1.563436\n",
      "Ep: 390, steps: 21, D loss: 0.185278, acc:  73%, G loss: 1.710209\n",
      "Ep: 390, steps: 22, D loss: 0.247841, acc:  56%, G loss: 2.028816\n",
      "Ep: 390, steps: 23, D loss: 0.184671, acc:  78%, G loss: 1.703047\n",
      "Ep: 390, steps: 24, D loss: 0.233458, acc:  59%, G loss: 1.834343\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 391, steps: 1, D loss: 0.232205, acc:  61%, G loss: 1.806165\n",
      "Ep: 391, steps: 2, D loss: 0.244796, acc:  56%, G loss: 1.605393\n",
      "Ep: 391, steps: 3, D loss: 0.163320, acc:  81%, G loss: 2.157052\n",
      "Ep: 391, steps: 4, D loss: 0.183560, acc:  82%, G loss: 1.967643\n",
      "Ep: 391, steps: 5, D loss: 0.282580, acc:  48%, G loss: 1.788667\n",
      "Ep: 391, steps: 6, D loss: 0.251578, acc:  53%, G loss: 1.769723\n",
      "Ep: 391, steps: 7, D loss: 0.353493, acc:  25%, G loss: 1.547264\n",
      "Ep: 391, steps: 8, D loss: 0.243239, acc:  57%, G loss: 1.758168\n",
      "Ep: 391, steps: 9, D loss: 0.234557, acc:  63%, G loss: 1.768549\n",
      "Ep: 391, steps: 10, D loss: 0.183371, acc:  79%, G loss: 1.644443\n",
      "Ep: 391, steps: 11, D loss: 0.223613, acc:  63%, G loss: 1.874170\n",
      "Ep: 391, steps: 12, D loss: 0.323699, acc:  25%, G loss: 1.430920\n",
      "Ep: 391, steps: 13, D loss: 0.279327, acc:  41%, G loss: 1.445527\n",
      "Ep: 391, steps: 14, D loss: 0.283633, acc:  37%, G loss: 1.514500\n",
      "Ep: 391, steps: 15, D loss: 0.240614, acc:  57%, G loss: 1.699833\n",
      "Ep: 391, steps: 16, D loss: 0.243695, acc:  56%, G loss: 1.695248\n",
      "Ep: 391, steps: 17, D loss: 0.203987, acc:  75%, G loss: 1.705666\n",
      "Ep: 391, steps: 18, D loss: 0.241046, acc:  58%, G loss: 1.577279\n",
      "Ep: 391, steps: 19, D loss: 0.201331, acc:  71%, G loss: 1.676429\n",
      "Ep: 391, steps: 20, D loss: 0.171970, acc:  79%, G loss: 1.857142\n",
      "Ep: 391, steps: 21, D loss: 0.296711, acc:  29%, G loss: 1.456227\n",
      "Ep: 391, steps: 22, D loss: 0.169672, acc:  79%, G loss: 1.781378\n",
      "Ep: 391, steps: 23, D loss: 0.198582, acc:  72%, G loss: 2.004003\n",
      "Ep: 391, steps: 24, D loss: 0.182591, acc:  78%, G loss: 1.669438\n",
      "Ep: 391, steps: 25, D loss: 0.230641, acc:  62%, G loss: 1.760520\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 392, steps: 1, D loss: 0.247111, acc:  57%, G loss: 1.740101\n",
      "Ep: 392, steps: 2, D loss: 0.248542, acc:  55%, G loss: 1.596479\n",
      "Ep: 392, steps: 3, D loss: 0.156798, acc:  84%, G loss: 2.069214\n",
      "Ep: 392, steps: 4, D loss: 0.183806, acc:  83%, G loss: 1.896356\n",
      "Ep: 392, steps: 5, D loss: 0.291555, acc:  45%, G loss: 1.917035\n",
      "Ep: 392, steps: 6, D loss: 0.242975, acc:  54%, G loss: 1.725426\n",
      "Ep: 392, steps: 7, D loss: 0.356764, acc:  24%, G loss: 1.565564\n",
      "Ep: 392, steps: 8, D loss: 0.236184, acc:  60%, G loss: 1.821486\n",
      "Ep: 392, steps: 9, D loss: 0.225001, acc:  65%, G loss: 1.716294\n",
      "Ep: 392, steps: 10, D loss: 0.182892, acc:  78%, G loss: 1.669014\n",
      "Ep: 392, steps: 11, D loss: 0.223258, acc:  64%, G loss: 1.918154\n",
      "Ep: 392, steps: 12, D loss: 0.318599, acc:  27%, G loss: 1.380668\n",
      "Ep: 392, steps: 13, D loss: 0.281903, acc:  40%, G loss: 1.467131\n",
      "Ep: 392, steps: 14, D loss: 0.283986, acc:  38%, G loss: 1.529863\n",
      "Ep: 392, steps: 15, D loss: 0.239679, acc:  58%, G loss: 1.744375\n",
      "Ep: 392, steps: 16, D loss: 0.242233, acc:  57%, G loss: 1.667922\n",
      "Ep: 392, steps: 17, D loss: 0.206175, acc:  71%, G loss: 1.662579\n",
      "Ep: 392, steps: 18, D loss: 0.247469, acc:  55%, G loss: 1.668230\n",
      "Ep: 392, steps: 19, D loss: 0.198095, acc:  73%, G loss: 1.627574\n",
      "Ep: 392, steps: 20, D loss: 0.177101, acc:  81%, G loss: 1.911825\n",
      "Ep: 392, steps: 21, D loss: 0.291954, acc:  34%, G loss: 1.584766\n",
      "Ep: 392, steps: 22, D loss: 0.181857, acc:  73%, G loss: 1.781088\n",
      "Ep: 392, steps: 23, D loss: 0.211948, acc:  67%, G loss: 2.063082\n",
      "Ep: 392, steps: 24, D loss: 0.189151, acc:  75%, G loss: 1.700709\n",
      "Ep: 392, steps: 25, D loss: 0.233662, acc:  59%, G loss: 1.600494\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 393, steps: 1, D loss: 0.266823, acc:  50%, G loss: 1.705870\n",
      "Ep: 393, steps: 2, D loss: 0.255056, acc:  49%, G loss: 1.582344\n",
      "Ep: 393, steps: 3, D loss: 0.166627, acc:  81%, G loss: 2.127314\n",
      "Ep: 393, steps: 4, D loss: 0.197705, acc:  78%, G loss: 1.963575\n",
      "Ep: 393, steps: 5, D loss: 0.257939, acc:  51%, G loss: 1.878658\n",
      "Ep: 393, steps: 6, D loss: 0.237918, acc:  56%, G loss: 1.702013\n",
      "Ep: 393, steps: 7, D loss: 0.334815, acc:  26%, G loss: 1.429974\n",
      "Ep: 393, steps: 8, D loss: 0.232866, acc:  60%, G loss: 1.831753\n",
      "Ep: 393, steps: 9, D loss: 0.240558, acc:  60%, G loss: 1.750793\n",
      "Ep: 393, steps: 10, D loss: 0.184565, acc:  79%, G loss: 1.661352\n",
      "Ep: 393, steps: 11, D loss: 0.218515, acc:  65%, G loss: 1.869354\n",
      "Ep: 393, steps: 12, D loss: 0.305487, acc:  31%, G loss: 1.437338\n",
      "Ep: 393, steps: 13, D loss: 0.279298, acc:  39%, G loss: 1.471797\n",
      "Saved Model\n",
      "Ep: 393, steps: 14, D loss: 0.282862, acc:  36%, G loss: 1.550605\n",
      "Ep: 393, steps: 15, D loss: 0.235724, acc:  60%, G loss: 1.711467\n",
      "Ep: 393, steps: 16, D loss: 0.215034, acc:  69%, G loss: 1.625960\n",
      "Ep: 393, steps: 17, D loss: 0.270927, acc:  49%, G loss: 1.570771\n",
      "Ep: 393, steps: 18, D loss: 0.204559, acc:  70%, G loss: 1.607123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 393, steps: 19, D loss: 0.164647, acc:  82%, G loss: 1.808540\n",
      "Ep: 393, steps: 20, D loss: 0.280520, acc:  38%, G loss: 1.576262\n",
      "Ep: 393, steps: 21, D loss: 0.172569, acc:  77%, G loss: 1.801311\n",
      "Ep: 393, steps: 22, D loss: 0.227266, acc:  64%, G loss: 2.072932\n",
      "Ep: 393, steps: 23, D loss: 0.189545, acc:  77%, G loss: 1.635679\n",
      "Ep: 393, steps: 24, D loss: 0.248532, acc:  57%, G loss: 1.616998\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 394, steps: 1, D loss: 0.268564, acc:  50%, G loss: 1.754314\n",
      "Ep: 394, steps: 2, D loss: 0.243744, acc:  56%, G loss: 1.544172\n",
      "Ep: 394, steps: 3, D loss: 0.165243, acc:  81%, G loss: 2.076340\n",
      "Ep: 394, steps: 4, D loss: 0.191969, acc:  81%, G loss: 1.940595\n",
      "Ep: 394, steps: 5, D loss: 0.261321, acc:  51%, G loss: 1.818173\n",
      "Ep: 394, steps: 6, D loss: 0.249507, acc:  54%, G loss: 1.735987\n",
      "Ep: 394, steps: 7, D loss: 0.359104, acc:  21%, G loss: 1.549952\n",
      "Ep: 394, steps: 8, D loss: 0.239625, acc:  58%, G loss: 1.882024\n",
      "Ep: 394, steps: 9, D loss: 0.228004, acc:  64%, G loss: 1.703613\n",
      "Ep: 394, steps: 10, D loss: 0.187804, acc:  78%, G loss: 1.720051\n",
      "Ep: 394, steps: 11, D loss: 0.213503, acc:  66%, G loss: 1.910334\n",
      "Ep: 394, steps: 12, D loss: 0.321380, acc:  28%, G loss: 1.434418\n",
      "Ep: 394, steps: 13, D loss: 0.278927, acc:  40%, G loss: 1.428273\n",
      "Ep: 394, steps: 14, D loss: 0.286629, acc:  38%, G loss: 1.529548\n",
      "Ep: 394, steps: 15, D loss: 0.246947, acc:  56%, G loss: 1.646086\n",
      "Ep: 394, steps: 16, D loss: 0.245381, acc:  58%, G loss: 1.695506\n",
      "Ep: 394, steps: 17, D loss: 0.198629, acc:  74%, G loss: 1.712240\n",
      "Ep: 394, steps: 18, D loss: 0.240679, acc:  58%, G loss: 1.582045\n",
      "Ep: 394, steps: 19, D loss: 0.211289, acc:  70%, G loss: 1.665603\n",
      "Ep: 394, steps: 20, D loss: 0.202839, acc:  72%, G loss: 1.986232\n",
      "Ep: 394, steps: 21, D loss: 0.292035, acc:  31%, G loss: 1.563502\n",
      "Ep: 394, steps: 22, D loss: 0.186457, acc:  75%, G loss: 1.741524\n",
      "Ep: 394, steps: 23, D loss: 0.226767, acc:  66%, G loss: 1.960503\n",
      "Ep: 394, steps: 24, D loss: 0.182437, acc:  77%, G loss: 1.655414\n",
      "Ep: 394, steps: 25, D loss: 0.227836, acc:  62%, G loss: 1.560961\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 395, steps: 1, D loss: 0.259170, acc:  53%, G loss: 1.705135\n",
      "Ep: 395, steps: 2, D loss: 0.248560, acc:  52%, G loss: 1.602164\n",
      "Ep: 395, steps: 3, D loss: 0.170367, acc:  78%, G loss: 2.079370\n",
      "Ep: 395, steps: 4, D loss: 0.189770, acc:  79%, G loss: 1.990043\n",
      "Ep: 395, steps: 5, D loss: 0.274950, acc:  46%, G loss: 1.839722\n",
      "Ep: 395, steps: 6, D loss: 0.253086, acc:  52%, G loss: 1.725655\n",
      "Ep: 395, steps: 7, D loss: 0.334376, acc:  25%, G loss: 1.702419\n",
      "Ep: 395, steps: 8, D loss: 0.223338, acc:  65%, G loss: 1.958746\n",
      "Ep: 395, steps: 9, D loss: 0.214116, acc:  70%, G loss: 1.732122\n",
      "Ep: 395, steps: 10, D loss: 0.166132, acc:  87%, G loss: 1.647853\n",
      "Ep: 395, steps: 11, D loss: 0.235223, acc:  60%, G loss: 1.826646\n",
      "Ep: 395, steps: 12, D loss: 0.303315, acc:  32%, G loss: 1.427356\n",
      "Ep: 395, steps: 13, D loss: 0.285747, acc:  38%, G loss: 1.433307\n",
      "Ep: 395, steps: 14, D loss: 0.273961, acc:  39%, G loss: 1.494082\n",
      "Ep: 395, steps: 15, D loss: 0.240506, acc:  58%, G loss: 1.658407\n",
      "Ep: 395, steps: 16, D loss: 0.224449, acc:  66%, G loss: 1.700452\n",
      "Ep: 395, steps: 17, D loss: 0.189648, acc:  78%, G loss: 1.688291\n",
      "Ep: 395, steps: 18, D loss: 0.257727, acc:  54%, G loss: 1.616897\n",
      "Ep: 395, steps: 19, D loss: 0.208713, acc:  68%, G loss: 1.668792\n",
      "Ep: 395, steps: 20, D loss: 0.150822, acc:  84%, G loss: 1.909620\n",
      "Ep: 395, steps: 21, D loss: 0.280600, acc:  38%, G loss: 1.499254\n",
      "Ep: 395, steps: 22, D loss: 0.167027, acc:  78%, G loss: 1.617189\n",
      "Ep: 395, steps: 23, D loss: 0.202790, acc:  71%, G loss: 2.027618\n",
      "Ep: 395, steps: 24, D loss: 0.190781, acc:  74%, G loss: 1.730412\n",
      "Ep: 395, steps: 25, D loss: 0.241979, acc:  57%, G loss: 1.639464\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 396, steps: 1, D loss: 0.261664, acc:  52%, G loss: 1.810848\n",
      "Ep: 396, steps: 2, D loss: 0.244281, acc:  56%, G loss: 1.649473\n",
      "Ep: 396, steps: 3, D loss: 0.164903, acc:  82%, G loss: 2.140602\n",
      "Ep: 396, steps: 4, D loss: 0.199135, acc:  77%, G loss: 1.984389\n",
      "Ep: 396, steps: 5, D loss: 0.255997, acc:  52%, G loss: 1.818271\n",
      "Ep: 396, steps: 6, D loss: 0.249880, acc:  53%, G loss: 1.725662\n",
      "Ep: 396, steps: 7, D loss: 0.330569, acc:  29%, G loss: 1.581828\n",
      "Ep: 396, steps: 8, D loss: 0.230591, acc:  62%, G loss: 1.928833\n",
      "Ep: 396, steps: 9, D loss: 0.260457, acc:  55%, G loss: 1.774762\n",
      "Ep: 396, steps: 10, D loss: 0.184125, acc:  76%, G loss: 1.644380\n",
      "Ep: 396, steps: 11, D loss: 0.222773, acc:  62%, G loss: 1.909338\n",
      "Saved Model\n",
      "Ep: 396, steps: 12, D loss: 0.312900, acc:  29%, G loss: 1.468278\n",
      "Ep: 396, steps: 13, D loss: 0.296469, acc:  34%, G loss: 1.544781\n",
      "Ep: 396, steps: 14, D loss: 0.224913, acc:  66%, G loss: 1.773399\n",
      "Ep: 396, steps: 15, D loss: 0.236756, acc:  60%, G loss: 1.674187\n",
      "Ep: 396, steps: 16, D loss: 0.195761, acc:  77%, G loss: 1.648592\n",
      "Ep: 396, steps: 17, D loss: 0.224928, acc:  66%, G loss: 1.566713\n",
      "Ep: 396, steps: 18, D loss: 0.202029, acc:  71%, G loss: 1.660003\n",
      "Ep: 396, steps: 19, D loss: 0.165415, acc:  82%, G loss: 1.882193\n",
      "Ep: 396, steps: 20, D loss: 0.295034, acc:  32%, G loss: 1.469969\n",
      "Ep: 396, steps: 21, D loss: 0.181582, acc:  74%, G loss: 1.744975\n",
      "Ep: 396, steps: 22, D loss: 0.208587, acc:  68%, G loss: 2.035228\n",
      "Ep: 396, steps: 23, D loss: 0.184890, acc:  76%, G loss: 1.737534\n",
      "Ep: 396, steps: 24, D loss: 0.206706, acc:  68%, G loss: 1.648730\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 397, steps: 1, D loss: 0.294029, acc:  49%, G loss: 1.817074\n",
      "Ep: 397, steps: 2, D loss: 0.255848, acc:  51%, G loss: 1.634633\n",
      "Ep: 397, steps: 3, D loss: 0.160358, acc:  83%, G loss: 2.109728\n",
      "Ep: 397, steps: 4, D loss: 0.184140, acc:  80%, G loss: 1.971550\n",
      "Ep: 397, steps: 5, D loss: 0.292196, acc:  44%, G loss: 1.773701\n",
      "Ep: 397, steps: 6, D loss: 0.245354, acc:  57%, G loss: 1.722191\n",
      "Ep: 397, steps: 7, D loss: 0.345046, acc:  23%, G loss: 1.697455\n",
      "Ep: 397, steps: 8, D loss: 0.240507, acc:  56%, G loss: 1.778851\n",
      "Ep: 397, steps: 9, D loss: 0.206708, acc:  72%, G loss: 1.744984\n",
      "Ep: 397, steps: 10, D loss: 0.182025, acc:  79%, G loss: 1.722687\n",
      "Ep: 397, steps: 11, D loss: 0.229434, acc:  63%, G loss: 1.824592\n",
      "Ep: 397, steps: 12, D loss: 0.323184, acc:  25%, G loss: 1.407811\n",
      "Ep: 397, steps: 13, D loss: 0.292560, acc:  35%, G loss: 1.477115\n",
      "Ep: 397, steps: 14, D loss: 0.283995, acc:  36%, G loss: 1.542967\n",
      "Ep: 397, steps: 15, D loss: 0.231460, acc:  62%, G loss: 1.630491\n",
      "Ep: 397, steps: 16, D loss: 0.244496, acc:  58%, G loss: 1.707101\n",
      "Ep: 397, steps: 17, D loss: 0.196190, acc:  76%, G loss: 1.613636\n",
      "Ep: 397, steps: 18, D loss: 0.250814, acc:  56%, G loss: 1.582951\n",
      "Ep: 397, steps: 19, D loss: 0.206335, acc:  69%, G loss: 1.679536\n",
      "Ep: 397, steps: 20, D loss: 0.173573, acc:  77%, G loss: 1.910851\n",
      "Ep: 397, steps: 21, D loss: 0.286746, acc:  38%, G loss: 1.495015\n",
      "Ep: 397, steps: 22, D loss: 0.153464, acc:  84%, G loss: 1.804758\n",
      "Ep: 397, steps: 23, D loss: 0.222509, acc:  67%, G loss: 2.081690\n",
      "Ep: 397, steps: 24, D loss: 0.197046, acc:  72%, G loss: 1.723245\n",
      "Ep: 397, steps: 25, D loss: 0.222414, acc:  63%, G loss: 1.619782\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 398, steps: 1, D loss: 0.264147, acc:  52%, G loss: 1.819452\n",
      "Ep: 398, steps: 2, D loss: 0.247540, acc:  54%, G loss: 1.635473\n",
      "Ep: 398, steps: 3, D loss: 0.165763, acc:  82%, G loss: 2.038126\n",
      "Ep: 398, steps: 4, D loss: 0.189883, acc:  79%, G loss: 1.926477\n",
      "Ep: 398, steps: 5, D loss: 0.286319, acc:  47%, G loss: 1.834318\n",
      "Ep: 398, steps: 6, D loss: 0.266147, acc:  51%, G loss: 1.804700\n",
      "Ep: 398, steps: 7, D loss: 0.340396, acc:  25%, G loss: 1.534061\n",
      "Ep: 398, steps: 8, D loss: 0.232269, acc:  60%, G loss: 1.868967\n",
      "Ep: 398, steps: 9, D loss: 0.236472, acc:  61%, G loss: 1.693609\n",
      "Ep: 398, steps: 10, D loss: 0.181869, acc:  81%, G loss: 1.640334\n",
      "Ep: 398, steps: 11, D loss: 0.218462, acc:  65%, G loss: 1.876841\n",
      "Ep: 398, steps: 12, D loss: 0.312173, acc:  29%, G loss: 1.354984\n",
      "Ep: 398, steps: 13, D loss: 0.284877, acc:  37%, G loss: 1.427099\n",
      "Ep: 398, steps: 14, D loss: 0.274424, acc:  41%, G loss: 1.523797\n",
      "Ep: 398, steps: 15, D loss: 0.248056, acc:  55%, G loss: 1.590557\n",
      "Ep: 398, steps: 16, D loss: 0.235106, acc:  61%, G loss: 1.715763\n",
      "Ep: 398, steps: 17, D loss: 0.206289, acc:  75%, G loss: 1.598268\n",
      "Ep: 398, steps: 18, D loss: 0.237773, acc:  62%, G loss: 1.616252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 398, steps: 19, D loss: 0.204212, acc:  71%, G loss: 1.631540\n",
      "Ep: 398, steps: 20, D loss: 0.168859, acc:  79%, G loss: 1.790440\n",
      "Ep: 398, steps: 21, D loss: 0.295749, acc:  32%, G loss: 1.559643\n",
      "Ep: 398, steps: 22, D loss: 0.160295, acc:  82%, G loss: 1.774103\n",
      "Ep: 398, steps: 23, D loss: 0.220628, acc:  66%, G loss: 2.046819\n",
      "Ep: 398, steps: 24, D loss: 0.199984, acc:  72%, G loss: 1.798666\n",
      "Ep: 398, steps: 25, D loss: 0.247568, acc:  56%, G loss: 1.729312\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 399, steps: 1, D loss: 0.254559, acc:  55%, G loss: 1.724889\n",
      "Ep: 399, steps: 2, D loss: 0.239193, acc:  57%, G loss: 1.596755\n",
      "Ep: 399, steps: 3, D loss: 0.163450, acc:  83%, G loss: 2.093541\n",
      "Ep: 399, steps: 4, D loss: 0.189549, acc:  81%, G loss: 1.860011\n",
      "Ep: 399, steps: 5, D loss: 0.263800, acc:  50%, G loss: 1.908503\n",
      "Ep: 399, steps: 6, D loss: 0.233785, acc:  56%, G loss: 1.739206\n",
      "Ep: 399, steps: 7, D loss: 0.328716, acc:  30%, G loss: 1.787219\n",
      "Ep: 399, steps: 8, D loss: 0.243263, acc:  59%, G loss: 1.863268\n",
      "Ep: 399, steps: 9, D loss: 0.230648, acc:  63%, G loss: 1.955391\n",
      "Saved Model\n",
      "Ep: 399, steps: 10, D loss: 0.181746, acc:  81%, G loss: 1.692439\n",
      "Ep: 399, steps: 11, D loss: 0.286310, acc:  40%, G loss: 1.435706\n",
      "Ep: 399, steps: 12, D loss: 0.268512, acc:  45%, G loss: 1.484835\n",
      "Ep: 399, steps: 13, D loss: 0.270697, acc:  42%, G loss: 1.525035\n",
      "Ep: 399, steps: 14, D loss: 0.256069, acc:  50%, G loss: 1.638111\n",
      "Ep: 399, steps: 15, D loss: 0.248927, acc:  57%, G loss: 1.701236\n",
      "Ep: 399, steps: 16, D loss: 0.205071, acc:  71%, G loss: 1.667438\n",
      "Ep: 399, steps: 17, D loss: 0.268402, acc:  50%, G loss: 1.606447\n",
      "Ep: 399, steps: 18, D loss: 0.219189, acc:  66%, G loss: 1.656259\n",
      "Ep: 399, steps: 19, D loss: 0.179062, acc:  76%, G loss: 1.770039\n",
      "Ep: 399, steps: 20, D loss: 0.277791, acc:  38%, G loss: 1.518726\n",
      "Ep: 399, steps: 21, D loss: 0.161173, acc:  81%, G loss: 1.723168\n",
      "Ep: 399, steps: 22, D loss: 0.205343, acc:  69%, G loss: 2.023331\n",
      "Ep: 399, steps: 23, D loss: 0.189971, acc:  78%, G loss: 1.705702\n",
      "Ep: 399, steps: 24, D loss: 0.243080, acc:  56%, G loss: 2.115555\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 400, steps: 1, D loss: 0.250995, acc:  56%, G loss: 1.817832\n",
      "Ep: 400, steps: 2, D loss: 0.245459, acc:  55%, G loss: 1.606622\n",
      "Ep: 400, steps: 3, D loss: 0.159367, acc:  84%, G loss: 2.093135\n",
      "Ep: 400, steps: 4, D loss: 0.184993, acc:  82%, G loss: 1.897599\n",
      "Ep: 400, steps: 5, D loss: 0.269094, acc:  51%, G loss: 1.931159\n",
      "Ep: 400, steps: 6, D loss: 0.239737, acc:  56%, G loss: 1.763785\n",
      "Ep: 400, steps: 7, D loss: 0.330362, acc:  28%, G loss: 1.620362\n",
      "Ep: 400, steps: 8, D loss: 0.233654, acc:  60%, G loss: 1.872690\n",
      "Ep: 400, steps: 9, D loss: 0.234527, acc:  61%, G loss: 1.662000\n",
      "Ep: 400, steps: 10, D loss: 0.169834, acc:  84%, G loss: 1.642834\n",
      "Ep: 400, steps: 11, D loss: 0.252113, acc:  51%, G loss: 1.849401\n",
      "Ep: 400, steps: 12, D loss: 0.304217, acc:  32%, G loss: 1.488941\n",
      "Ep: 400, steps: 13, D loss: 0.278281, acc:  42%, G loss: 1.461570\n",
      "Ep: 400, steps: 14, D loss: 0.270055, acc:  43%, G loss: 1.560444\n",
      "Ep: 400, steps: 15, D loss: 0.237033, acc:  60%, G loss: 1.671635\n",
      "Ep: 400, steps: 16, D loss: 0.236501, acc:  60%, G loss: 1.709320\n",
      "Ep: 400, steps: 17, D loss: 0.196496, acc:  74%, G loss: 1.671963\n",
      "Ep: 400, steps: 18, D loss: 0.257793, acc:  54%, G loss: 1.673554\n",
      "Ep: 400, steps: 19, D loss: 0.210354, acc:  69%, G loss: 1.619384\n",
      "Ep: 400, steps: 20, D loss: 0.184836, acc:  75%, G loss: 1.877609\n",
      "Ep: 400, steps: 21, D loss: 0.284408, acc:  37%, G loss: 1.506247\n",
      "Ep: 400, steps: 22, D loss: 0.162339, acc:  80%, G loss: 1.911068\n",
      "Ep: 400, steps: 23, D loss: 0.226013, acc:  63%, G loss: 2.139177\n",
      "Ep: 400, steps: 24, D loss: 0.185388, acc:  78%, G loss: 1.657974\n",
      "Ep: 400, steps: 25, D loss: 0.205752, acc:  67%, G loss: 1.583375\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 401, steps: 1, D loss: 0.295728, acc:  41%, G loss: 1.845792\n",
      "Ep: 401, steps: 2, D loss: 0.245650, acc:  56%, G loss: 1.649098\n",
      "Ep: 401, steps: 3, D loss: 0.174065, acc:  78%, G loss: 2.119605\n",
      "Ep: 401, steps: 4, D loss: 0.198088, acc:  78%, G loss: 1.991017\n",
      "Ep: 401, steps: 5, D loss: 0.272395, acc:  47%, G loss: 1.801769\n",
      "Ep: 401, steps: 6, D loss: 0.244376, acc:  55%, G loss: 1.615296\n",
      "Ep: 401, steps: 7, D loss: 0.356158, acc:  22%, G loss: 1.405786\n",
      "Ep: 401, steps: 8, D loss: 0.241420, acc:  57%, G loss: 1.860615\n",
      "Ep: 401, steps: 9, D loss: 0.218307, acc:  68%, G loss: 1.705331\n",
      "Ep: 401, steps: 10, D loss: 0.175033, acc:  84%, G loss: 1.638450\n",
      "Ep: 401, steps: 11, D loss: 0.218189, acc:  65%, G loss: 1.809558\n",
      "Ep: 401, steps: 12, D loss: 0.301747, acc:  33%, G loss: 1.382712\n",
      "Ep: 401, steps: 13, D loss: 0.299165, acc:  30%, G loss: 1.449802\n",
      "Ep: 401, steps: 14, D loss: 0.282685, acc:  37%, G loss: 1.536892\n",
      "Ep: 401, steps: 15, D loss: 0.235884, acc:  61%, G loss: 1.722668\n",
      "Ep: 401, steps: 16, D loss: 0.238138, acc:  58%, G loss: 1.732964\n",
      "Ep: 401, steps: 17, D loss: 0.197087, acc:  76%, G loss: 1.658941\n",
      "Ep: 401, steps: 18, D loss: 0.256680, acc:  56%, G loss: 1.598057\n",
      "Ep: 401, steps: 19, D loss: 0.207709, acc:  69%, G loss: 1.661802\n",
      "Ep: 401, steps: 20, D loss: 0.181226, acc:  76%, G loss: 1.818068\n",
      "Ep: 401, steps: 21, D loss: 0.301304, acc:  30%, G loss: 1.462832\n",
      "Ep: 401, steps: 22, D loss: 0.167784, acc:  79%, G loss: 1.732058\n",
      "Ep: 401, steps: 23, D loss: 0.231964, acc:  63%, G loss: 2.080053\n",
      "Ep: 401, steps: 24, D loss: 0.199018, acc:  73%, G loss: 1.727044\n",
      "Ep: 401, steps: 25, D loss: 0.236467, acc:  58%, G loss: 1.840560\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 402, steps: 1, D loss: 0.248102, acc:  56%, G loss: 1.752306\n",
      "Ep: 402, steps: 2, D loss: 0.241525, acc:  55%, G loss: 1.617797\n",
      "Ep: 402, steps: 3, D loss: 0.155594, acc:  86%, G loss: 2.112196\n",
      "Ep: 402, steps: 4, D loss: 0.193033, acc:  79%, G loss: 1.870153\n",
      "Ep: 402, steps: 5, D loss: 0.276994, acc:  49%, G loss: 1.786343\n",
      "Ep: 402, steps: 6, D loss: 0.243561, acc:  55%, G loss: 1.740403\n",
      "Ep: 402, steps: 7, D loss: 0.346293, acc:  27%, G loss: 1.631655\n",
      "Saved Model\n",
      "Ep: 402, steps: 8, D loss: 0.240872, acc:  57%, G loss: 1.882190\n",
      "Ep: 402, steps: 9, D loss: 0.191659, acc:  77%, G loss: 1.673733\n",
      "Ep: 402, steps: 10, D loss: 0.261279, acc:  49%, G loss: 1.741046\n",
      "Ep: 402, steps: 11, D loss: 0.305029, acc:  32%, G loss: 1.369432\n",
      "Ep: 402, steps: 12, D loss: 0.273949, acc:  43%, G loss: 1.477706\n",
      "Ep: 402, steps: 13, D loss: 0.267493, acc:  45%, G loss: 1.566451\n",
      "Ep: 402, steps: 14, D loss: 0.248291, acc:  56%, G loss: 1.736385\n",
      "Ep: 402, steps: 15, D loss: 0.256745, acc:  54%, G loss: 1.666547\n",
      "Ep: 402, steps: 16, D loss: 0.189732, acc:  79%, G loss: 1.737630\n",
      "Ep: 402, steps: 17, D loss: 0.246680, acc:  57%, G loss: 1.584559\n",
      "Ep: 402, steps: 18, D loss: 0.216368, acc:  68%, G loss: 1.661007\n",
      "Ep: 402, steps: 19, D loss: 0.191584, acc:  72%, G loss: 1.961701\n",
      "Ep: 402, steps: 20, D loss: 0.272763, acc:  38%, G loss: 1.601406\n",
      "Ep: 402, steps: 21, D loss: 0.159498, acc:  84%, G loss: 1.692340\n",
      "Ep: 402, steps: 22, D loss: 0.213888, acc:  66%, G loss: 1.956737\n",
      "Ep: 402, steps: 23, D loss: 0.182286, acc:  79%, G loss: 1.638551\n",
      "Ep: 402, steps: 24, D loss: 0.237191, acc:  59%, G loss: 1.613018\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 403, steps: 1, D loss: 0.286908, acc:  45%, G loss: 1.783057\n",
      "Ep: 403, steps: 2, D loss: 0.234984, acc:  58%, G loss: 1.571362\n",
      "Ep: 403, steps: 3, D loss: 0.165335, acc:  83%, G loss: 2.045446\n",
      "Ep: 403, steps: 4, D loss: 0.192465, acc:  79%, G loss: 1.909546\n",
      "Ep: 403, steps: 5, D loss: 0.295241, acc:  42%, G loss: 1.717921\n",
      "Ep: 403, steps: 6, D loss: 0.247583, acc:  55%, G loss: 1.663791\n",
      "Ep: 403, steps: 7, D loss: 0.307376, acc:  32%, G loss: 1.486953\n",
      "Ep: 403, steps: 8, D loss: 0.245521, acc:  58%, G loss: 1.751923\n",
      "Ep: 403, steps: 9, D loss: 0.233619, acc:  63%, G loss: 1.788341\n",
      "Ep: 403, steps: 10, D loss: 0.184856, acc:  81%, G loss: 1.631805\n",
      "Ep: 403, steps: 11, D loss: 0.223455, acc:  64%, G loss: 1.875236\n",
      "Ep: 403, steps: 12, D loss: 0.315816, acc:  28%, G loss: 1.445491\n",
      "Ep: 403, steps: 13, D loss: 0.287903, acc:  34%, G loss: 1.449559\n",
      "Ep: 403, steps: 14, D loss: 0.285481, acc:  37%, G loss: 1.562304\n",
      "Ep: 403, steps: 15, D loss: 0.245758, acc:  55%, G loss: 1.657532\n",
      "Ep: 403, steps: 16, D loss: 0.241947, acc:  56%, G loss: 1.654363\n",
      "Ep: 403, steps: 17, D loss: 0.203040, acc:  75%, G loss: 1.646863\n",
      "Ep: 403, steps: 18, D loss: 0.230806, acc:  62%, G loss: 1.557106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 403, steps: 19, D loss: 0.201881, acc:  70%, G loss: 1.658722\n",
      "Ep: 403, steps: 20, D loss: 0.174151, acc:  78%, G loss: 1.829315\n",
      "Ep: 403, steps: 21, D loss: 0.290393, acc:  33%, G loss: 1.574926\n",
      "Ep: 403, steps: 22, D loss: 0.176895, acc:  76%, G loss: 1.691041\n",
      "Ep: 403, steps: 23, D loss: 0.203979, acc:  69%, G loss: 2.015684\n",
      "Ep: 403, steps: 24, D loss: 0.198174, acc:  74%, G loss: 1.677419\n",
      "Ep: 403, steps: 25, D loss: 0.236560, acc:  59%, G loss: 1.661270\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 404, steps: 1, D loss: 0.257214, acc:  52%, G loss: 1.906443\n",
      "Ep: 404, steps: 2, D loss: 0.245180, acc:  56%, G loss: 1.636480\n",
      "Ep: 404, steps: 3, D loss: 0.166016, acc:  82%, G loss: 2.096506\n",
      "Ep: 404, steps: 4, D loss: 0.186715, acc:  80%, G loss: 1.939858\n",
      "Ep: 404, steps: 5, D loss: 0.278233, acc:  44%, G loss: 1.921085\n",
      "Ep: 404, steps: 6, D loss: 0.232141, acc:  56%, G loss: 1.664195\n",
      "Ep: 404, steps: 7, D loss: 0.355436, acc:  21%, G loss: 1.582748\n",
      "Ep: 404, steps: 8, D loss: 0.234435, acc:  59%, G loss: 1.797501\n",
      "Ep: 404, steps: 9, D loss: 0.242693, acc:  59%, G loss: 1.677362\n",
      "Ep: 404, steps: 10, D loss: 0.183850, acc:  78%, G loss: 1.644150\n",
      "Ep: 404, steps: 11, D loss: 0.219126, acc:  66%, G loss: 1.805352\n",
      "Ep: 404, steps: 12, D loss: 0.302770, acc:  33%, G loss: 1.389731\n",
      "Ep: 404, steps: 13, D loss: 0.277308, acc:  39%, G loss: 1.418625\n",
      "Ep: 404, steps: 14, D loss: 0.276719, acc:  41%, G loss: 1.546728\n",
      "Ep: 404, steps: 15, D loss: 0.234873, acc:  60%, G loss: 1.663752\n",
      "Ep: 404, steps: 16, D loss: 0.237068, acc:  61%, G loss: 1.685848\n",
      "Ep: 404, steps: 17, D loss: 0.198322, acc:  75%, G loss: 1.692720\n",
      "Ep: 404, steps: 18, D loss: 0.241805, acc:  59%, G loss: 1.618311\n",
      "Ep: 404, steps: 19, D loss: 0.205504, acc:  71%, G loss: 1.599738\n",
      "Ep: 404, steps: 20, D loss: 0.179751, acc:  77%, G loss: 1.834282\n",
      "Ep: 404, steps: 21, D loss: 0.304206, acc:  32%, G loss: 1.483950\n",
      "Ep: 404, steps: 22, D loss: 0.177058, acc:  78%, G loss: 1.763534\n",
      "Ep: 404, steps: 23, D loss: 0.219275, acc:  67%, G loss: 2.035137\n",
      "Ep: 404, steps: 24, D loss: 0.200185, acc:  72%, G loss: 1.662542\n",
      "Ep: 404, steps: 25, D loss: 0.254247, acc:  55%, G loss: 1.815032\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 405, steps: 1, D loss: 0.240574, acc:  59%, G loss: 1.771998\n",
      "Ep: 405, steps: 2, D loss: 0.239488, acc:  56%, G loss: 1.637588\n",
      "Ep: 405, steps: 3, D loss: 0.165638, acc:  82%, G loss: 2.156595\n",
      "Ep: 405, steps: 4, D loss: 0.192998, acc:  78%, G loss: 2.009119\n",
      "Ep: 405, steps: 5, D loss: 0.256646, acc:  51%, G loss: 1.730422\n",
      "Saved Model\n",
      "Ep: 405, steps: 6, D loss: 0.246410, acc:  54%, G loss: 1.692266\n",
      "Ep: 405, steps: 7, D loss: 0.254940, acc:  50%, G loss: 1.757204\n",
      "Ep: 405, steps: 8, D loss: 0.204472, acc:  72%, G loss: 1.863147\n",
      "Ep: 405, steps: 9, D loss: 0.154581, acc:  88%, G loss: 1.894147\n",
      "Ep: 405, steps: 10, D loss: 0.190382, acc:  76%, G loss: 2.023639\n",
      "Ep: 405, steps: 11, D loss: 0.364458, acc:  18%, G loss: 1.337300\n",
      "Ep: 405, steps: 12, D loss: 0.316119, acc:  27%, G loss: 1.462933\n",
      "Ep: 405, steps: 13, D loss: 0.304002, acc:  35%, G loss: 1.596486\n",
      "Ep: 405, steps: 14, D loss: 0.222735, acc:  65%, G loss: 1.737608\n",
      "Ep: 405, steps: 15, D loss: 0.243666, acc:  57%, G loss: 1.704562\n",
      "Ep: 405, steps: 16, D loss: 0.199555, acc:  75%, G loss: 1.678041\n",
      "Ep: 405, steps: 17, D loss: 0.252369, acc:  54%, G loss: 1.607643\n",
      "Ep: 405, steps: 18, D loss: 0.192057, acc:  72%, G loss: 1.694532\n",
      "Ep: 405, steps: 19, D loss: 0.177487, acc:  76%, G loss: 1.834416\n",
      "Ep: 405, steps: 20, D loss: 0.304350, acc:  32%, G loss: 1.487695\n",
      "Ep: 405, steps: 21, D loss: 0.168570, acc:  77%, G loss: 1.757251\n",
      "Ep: 405, steps: 22, D loss: 0.214221, acc:  67%, G loss: 2.155075\n",
      "Ep: 405, steps: 23, D loss: 0.190135, acc:  72%, G loss: 1.656527\n",
      "Ep: 405, steps: 24, D loss: 0.222887, acc:  62%, G loss: 1.891634\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 406, steps: 1, D loss: 0.270463, acc:  51%, G loss: 1.945714\n",
      "Ep: 406, steps: 2, D loss: 0.245095, acc:  56%, G loss: 1.656292\n",
      "Ep: 406, steps: 3, D loss: 0.153154, acc:  85%, G loss: 2.121530\n",
      "Ep: 406, steps: 4, D loss: 0.176635, acc:  83%, G loss: 1.935459\n",
      "Ep: 406, steps: 5, D loss: 0.279911, acc:  45%, G loss: 1.791610\n",
      "Ep: 406, steps: 6, D loss: 0.256581, acc:  53%, G loss: 1.643326\n",
      "Ep: 406, steps: 7, D loss: 0.357708, acc:  23%, G loss: 1.576215\n",
      "Ep: 406, steps: 8, D loss: 0.233429, acc:  60%, G loss: 1.814969\n",
      "Ep: 406, steps: 9, D loss: 0.221477, acc:  67%, G loss: 1.737112\n",
      "Ep: 406, steps: 10, D loss: 0.176477, acc:  80%, G loss: 1.626494\n",
      "Ep: 406, steps: 11, D loss: 0.231168, acc:  61%, G loss: 1.853185\n",
      "Ep: 406, steps: 12, D loss: 0.313031, acc:  29%, G loss: 1.353958\n",
      "Ep: 406, steps: 13, D loss: 0.288922, acc:  35%, G loss: 1.410551\n",
      "Ep: 406, steps: 14, D loss: 0.278975, acc:  38%, G loss: 1.482973\n",
      "Ep: 406, steps: 15, D loss: 0.240597, acc:  58%, G loss: 1.605581\n",
      "Ep: 406, steps: 16, D loss: 0.254934, acc:  55%, G loss: 1.658689\n",
      "Ep: 406, steps: 17, D loss: 0.197394, acc:  77%, G loss: 1.742113\n",
      "Ep: 406, steps: 18, D loss: 0.246515, acc:  58%, G loss: 1.586034\n",
      "Ep: 406, steps: 19, D loss: 0.206481, acc:  70%, G loss: 1.617875\n",
      "Ep: 406, steps: 20, D loss: 0.183941, acc:  75%, G loss: 1.784639\n",
      "Ep: 406, steps: 21, D loss: 0.287939, acc:  35%, G loss: 1.525942\n",
      "Ep: 406, steps: 22, D loss: 0.173367, acc:  80%, G loss: 1.672238\n",
      "Ep: 406, steps: 23, D loss: 0.222594, acc:  63%, G loss: 1.976061\n",
      "Ep: 406, steps: 24, D loss: 0.206254, acc:  69%, G loss: 1.664150\n",
      "Ep: 406, steps: 25, D loss: 0.231332, acc:  62%, G loss: 1.715805\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 407, steps: 1, D loss: 0.236270, acc:  59%, G loss: 1.830404\n",
      "Ep: 407, steps: 2, D loss: 0.244671, acc:  56%, G loss: 1.598486\n",
      "Ep: 407, steps: 3, D loss: 0.155954, acc:  83%, G loss: 2.009928\n",
      "Ep: 407, steps: 4, D loss: 0.179774, acc:  82%, G loss: 1.923956\n",
      "Ep: 407, steps: 5, D loss: 0.271989, acc:  48%, G loss: 1.784053\n",
      "Ep: 407, steps: 6, D loss: 0.248319, acc:  53%, G loss: 1.750809\n",
      "Ep: 407, steps: 7, D loss: 0.322720, acc:  31%, G loss: 1.484296\n",
      "Ep: 407, steps: 8, D loss: 0.242930, acc:  57%, G loss: 1.755695\n",
      "Ep: 407, steps: 9, D loss: 0.243015, acc:  60%, G loss: 1.715134\n",
      "Ep: 407, steps: 10, D loss: 0.165924, acc:  87%, G loss: 1.630616\n",
      "Ep: 407, steps: 11, D loss: 0.231170, acc:  62%, G loss: 1.859655\n",
      "Ep: 407, steps: 12, D loss: 0.293366, acc:  36%, G loss: 1.386787\n",
      "Ep: 407, steps: 13, D loss: 0.280171, acc:  39%, G loss: 1.379592\n",
      "Ep: 407, steps: 14, D loss: 0.267247, acc:  44%, G loss: 1.551915\n",
      "Ep: 407, steps: 15, D loss: 0.247532, acc:  55%, G loss: 1.618630\n",
      "Ep: 407, steps: 16, D loss: 0.236054, acc:  60%, G loss: 1.701316\n",
      "Ep: 407, steps: 17, D loss: 0.195157, acc:  75%, G loss: 1.612180\n",
      "Ep: 407, steps: 18, D loss: 0.255166, acc:  54%, G loss: 1.584399\n",
      "Ep: 407, steps: 19, D loss: 0.206494, acc:  69%, G loss: 1.616866\n",
      "Ep: 407, steps: 20, D loss: 0.171001, acc:  79%, G loss: 1.879634\n",
      "Ep: 407, steps: 21, D loss: 0.281381, acc:  38%, G loss: 1.510774\n",
      "Ep: 407, steps: 22, D loss: 0.149283, acc:  85%, G loss: 1.750720\n",
      "Ep: 407, steps: 23, D loss: 0.221297, acc:  66%, G loss: 1.990610\n",
      "Ep: 407, steps: 24, D loss: 0.187994, acc:  76%, G loss: 1.636012\n",
      "Ep: 407, steps: 25, D loss: 0.213306, acc:  66%, G loss: 1.672425\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 408, steps: 1, D loss: 0.271170, acc:  52%, G loss: 1.736609\n",
      "Ep: 408, steps: 2, D loss: 0.250334, acc:  52%, G loss: 1.543553\n",
      "Ep: 408, steps: 3, D loss: 0.160984, acc:  83%, G loss: 2.028098\n",
      "Saved Model\n",
      "Ep: 408, steps: 4, D loss: 0.192704, acc:  79%, G loss: 1.899130\n",
      "Ep: 408, steps: 5, D loss: 0.240916, acc:  53%, G loss: 1.813481\n",
      "Ep: 408, steps: 6, D loss: 0.368667, acc:  19%, G loss: 1.462572\n",
      "Ep: 408, steps: 7, D loss: 0.245740, acc:  59%, G loss: 1.762975\n",
      "Ep: 408, steps: 8, D loss: 0.205211, acc:  74%, G loss: 1.772167\n",
      "Ep: 408, steps: 9, D loss: 0.172663, acc:  82%, G loss: 1.689459\n",
      "Ep: 408, steps: 10, D loss: 0.202343, acc:  72%, G loss: 1.870456\n",
      "Ep: 408, steps: 11, D loss: 0.311261, acc:  29%, G loss: 1.408657\n",
      "Ep: 408, steps: 12, D loss: 0.296376, acc:  32%, G loss: 1.401724\n",
      "Ep: 408, steps: 13, D loss: 0.279117, acc:  41%, G loss: 1.552046\n",
      "Ep: 408, steps: 14, D loss: 0.229651, acc:  61%, G loss: 1.638988\n",
      "Ep: 408, steps: 15, D loss: 0.250521, acc:  57%, G loss: 1.744200\n",
      "Ep: 408, steps: 16, D loss: 0.201865, acc:  75%, G loss: 1.691323\n",
      "Ep: 408, steps: 17, D loss: 0.251992, acc:  56%, G loss: 1.629330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 408, steps: 18, D loss: 0.203254, acc:  71%, G loss: 1.680211\n",
      "Ep: 408, steps: 19, D loss: 0.170848, acc:  79%, G loss: 1.859107\n",
      "Ep: 408, steps: 20, D loss: 0.297464, acc:  34%, G loss: 1.503667\n",
      "Ep: 408, steps: 21, D loss: 0.168188, acc:  80%, G loss: 1.689573\n",
      "Ep: 408, steps: 22, D loss: 0.211016, acc:  68%, G loss: 2.013900\n",
      "Ep: 408, steps: 23, D loss: 0.197980, acc:  72%, G loss: 1.666250\n",
      "Ep: 408, steps: 24, D loss: 0.243836, acc:  55%, G loss: 1.580925\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 409, steps: 1, D loss: 0.254278, acc:  54%, G loss: 1.727893\n",
      "Ep: 409, steps: 2, D loss: 0.242405, acc:  57%, G loss: 1.623639\n",
      "Ep: 409, steps: 3, D loss: 0.162575, acc:  81%, G loss: 2.051832\n",
      "Ep: 409, steps: 4, D loss: 0.194570, acc:  79%, G loss: 1.948223\n",
      "Ep: 409, steps: 5, D loss: 0.285283, acc:  44%, G loss: 1.767782\n",
      "Ep: 409, steps: 6, D loss: 0.250977, acc:  52%, G loss: 1.705276\n",
      "Ep: 409, steps: 7, D loss: 0.359090, acc:  21%, G loss: 1.542898\n",
      "Ep: 409, steps: 8, D loss: 0.231324, acc:  61%, G loss: 1.751746\n",
      "Ep: 409, steps: 9, D loss: 0.229553, acc:  63%, G loss: 1.699986\n",
      "Ep: 409, steps: 10, D loss: 0.186534, acc:  78%, G loss: 1.714074\n",
      "Ep: 409, steps: 11, D loss: 0.218529, acc:  65%, G loss: 1.905698\n",
      "Ep: 409, steps: 12, D loss: 0.314300, acc:  29%, G loss: 1.449549\n",
      "Ep: 409, steps: 13, D loss: 0.289190, acc:  36%, G loss: 1.421402\n",
      "Ep: 409, steps: 14, D loss: 0.283872, acc:  35%, G loss: 1.599883\n",
      "Ep: 409, steps: 15, D loss: 0.243023, acc:  57%, G loss: 1.711847\n",
      "Ep: 409, steps: 16, D loss: 0.258228, acc:  51%, G loss: 1.653299\n",
      "Ep: 409, steps: 17, D loss: 0.212581, acc:  71%, G loss: 1.684604\n",
      "Ep: 409, steps: 18, D loss: 0.242903, acc:  59%, G loss: 1.649886\n",
      "Ep: 409, steps: 19, D loss: 0.204473, acc:  71%, G loss: 1.625551\n",
      "Ep: 409, steps: 20, D loss: 0.194219, acc:  73%, G loss: 1.840687\n",
      "Ep: 409, steps: 21, D loss: 0.296596, acc:  29%, G loss: 1.514708\n",
      "Ep: 409, steps: 22, D loss: 0.156882, acc:  83%, G loss: 1.812780\n",
      "Ep: 409, steps: 23, D loss: 0.224418, acc:  64%, G loss: 2.040626\n",
      "Ep: 409, steps: 24, D loss: 0.192658, acc:  75%, G loss: 1.648511\n",
      "Ep: 409, steps: 25, D loss: 0.247548, acc:  56%, G loss: 1.828648\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 410, steps: 1, D loss: 0.248027, acc:  57%, G loss: 1.733291\n",
      "Ep: 410, steps: 2, D loss: 0.239797, acc:  57%, G loss: 1.565785\n",
      "Ep: 410, steps: 3, D loss: 0.181364, acc:  75%, G loss: 2.062390\n",
      "Ep: 410, steps: 4, D loss: 0.191188, acc:  80%, G loss: 1.926048\n",
      "Ep: 410, steps: 5, D loss: 0.272543, acc:  45%, G loss: 1.772732\n",
      "Ep: 410, steps: 6, D loss: 0.233912, acc:  56%, G loss: 1.723602\n",
      "Ep: 410, steps: 7, D loss: 0.314721, acc:  31%, G loss: 1.514799\n",
      "Ep: 410, steps: 8, D loss: 0.237202, acc:  59%, G loss: 1.814644\n",
      "Ep: 410, steps: 9, D loss: 0.246399, acc:  60%, G loss: 1.771729\n",
      "Ep: 410, steps: 10, D loss: 0.193300, acc:  75%, G loss: 1.668330\n",
      "Ep: 410, steps: 11, D loss: 0.221044, acc:  64%, G loss: 1.932193\n",
      "Ep: 410, steps: 12, D loss: 0.306051, acc:  30%, G loss: 1.409818\n",
      "Ep: 410, steps: 13, D loss: 0.282662, acc:  39%, G loss: 1.480039\n",
      "Ep: 410, steps: 14, D loss: 0.275455, acc:  42%, G loss: 1.627977\n",
      "Ep: 410, steps: 15, D loss: 0.232708, acc:  62%, G loss: 1.601364\n",
      "Ep: 410, steps: 16, D loss: 0.245690, acc:  56%, G loss: 1.657495\n",
      "Ep: 410, steps: 17, D loss: 0.203940, acc:  73%, G loss: 1.721058\n",
      "Ep: 410, steps: 18, D loss: 0.251903, acc:  56%, G loss: 1.565381\n",
      "Ep: 410, steps: 19, D loss: 0.210964, acc:  68%, G loss: 1.664650\n",
      "Ep: 410, steps: 20, D loss: 0.194567, acc:  70%, G loss: 1.896211\n",
      "Ep: 410, steps: 21, D loss: 0.273093, acc:  42%, G loss: 1.518609\n",
      "Ep: 410, steps: 22, D loss: 0.168435, acc:  80%, G loss: 1.706861\n",
      "Ep: 410, steps: 23, D loss: 0.206294, acc:  70%, G loss: 2.007563\n",
      "Ep: 410, steps: 24, D loss: 0.183336, acc:  78%, G loss: 1.679316\n",
      "Ep: 410, steps: 25, D loss: 0.213031, acc:  66%, G loss: 1.641341\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 411, steps: 1, D loss: 0.275620, acc:  49%, G loss: 1.740589\n",
      "Saved Model\n",
      "Ep: 411, steps: 2, D loss: 0.236756, acc:  59%, G loss: 1.625923\n",
      "Ep: 411, steps: 3, D loss: 0.190218, acc:  81%, G loss: 1.708722\n",
      "Ep: 411, steps: 4, D loss: 0.292270, acc:  40%, G loss: 1.782477\n",
      "Ep: 411, steps: 5, D loss: 0.227943, acc:  58%, G loss: 1.636241\n",
      "Ep: 411, steps: 6, D loss: 0.306325, acc:  34%, G loss: 1.467138\n",
      "Ep: 411, steps: 7, D loss: 0.251304, acc:  51%, G loss: 1.740406\n",
      "Ep: 411, steps: 8, D loss: 0.224020, acc:  67%, G loss: 1.746241\n",
      "Ep: 411, steps: 9, D loss: 0.188759, acc:  78%, G loss: 1.645150\n",
      "Ep: 411, steps: 10, D loss: 0.226729, acc:  61%, G loss: 1.847354\n",
      "Ep: 411, steps: 11, D loss: 0.309665, acc:  29%, G loss: 1.435018\n",
      "Ep: 411, steps: 12, D loss: 0.284022, acc:  37%, G loss: 1.486050\n",
      "Ep: 411, steps: 13, D loss: 0.280437, acc:  40%, G loss: 1.580402\n",
      "Ep: 411, steps: 14, D loss: 0.237695, acc:  59%, G loss: 1.639306\n",
      "Ep: 411, steps: 15, D loss: 0.245911, acc:  57%, G loss: 1.671106\n",
      "Ep: 411, steps: 16, D loss: 0.212632, acc:  71%, G loss: 1.680250\n",
      "Ep: 411, steps: 17, D loss: 0.241916, acc:  59%, G loss: 1.607551\n",
      "Ep: 411, steps: 18, D loss: 0.199565, acc:  71%, G loss: 1.706372\n",
      "Ep: 411, steps: 19, D loss: 0.180100, acc:  77%, G loss: 1.817738\n",
      "Ep: 411, steps: 20, D loss: 0.292240, acc:  33%, G loss: 1.558300\n",
      "Ep: 411, steps: 21, D loss: 0.165894, acc:  78%, G loss: 1.825501\n",
      "Ep: 411, steps: 22, D loss: 0.205003, acc:  70%, G loss: 2.019883\n",
      "Ep: 411, steps: 23, D loss: 0.201835, acc:  70%, G loss: 1.674698\n",
      "Ep: 411, steps: 24, D loss: 0.247004, acc:  55%, G loss: 1.600344\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 412, steps: 1, D loss: 0.275396, acc:  46%, G loss: 1.835495\n",
      "Ep: 412, steps: 2, D loss: 0.259081, acc:  50%, G loss: 1.591929\n",
      "Ep: 412, steps: 3, D loss: 0.171623, acc:  78%, G loss: 2.082235\n",
      "Ep: 412, steps: 4, D loss: 0.200801, acc:  77%, G loss: 1.919801\n",
      "Ep: 412, steps: 5, D loss: 0.276877, acc:  45%, G loss: 1.774588\n",
      "Ep: 412, steps: 6, D loss: 0.251770, acc:  53%, G loss: 1.664344\n",
      "Ep: 412, steps: 7, D loss: 0.347611, acc:  26%, G loss: 1.532264\n",
      "Ep: 412, steps: 8, D loss: 0.239471, acc:  60%, G loss: 1.820956\n",
      "Ep: 412, steps: 9, D loss: 0.238578, acc:  60%, G loss: 1.818858\n",
      "Ep: 412, steps: 10, D loss: 0.187938, acc:  77%, G loss: 1.606802\n",
      "Ep: 412, steps: 11, D loss: 0.216162, acc:  66%, G loss: 1.806394\n",
      "Ep: 412, steps: 12, D loss: 0.297760, acc:  33%, G loss: 1.412872\n",
      "Ep: 412, steps: 13, D loss: 0.282689, acc:  39%, G loss: 1.500680\n",
      "Ep: 412, steps: 14, D loss: 0.261813, acc:  45%, G loss: 1.600259\n",
      "Ep: 412, steps: 15, D loss: 0.250627, acc:  55%, G loss: 1.688535\n",
      "Ep: 412, steps: 16, D loss: 0.241294, acc:  59%, G loss: 1.701833\n",
      "Ep: 412, steps: 17, D loss: 0.203348, acc:  73%, G loss: 1.660646\n",
      "Ep: 412, steps: 18, D loss: 0.239939, acc:  60%, G loss: 1.595245\n",
      "Ep: 412, steps: 19, D loss: 0.233468, acc:  62%, G loss: 1.658957\n",
      "Ep: 412, steps: 20, D loss: 0.197057, acc:  72%, G loss: 1.920772\n",
      "Ep: 412, steps: 21, D loss: 0.287200, acc:  36%, G loss: 1.445947\n",
      "Ep: 412, steps: 22, D loss: 0.186584, acc:  76%, G loss: 1.599753\n",
      "Ep: 412, steps: 23, D loss: 0.214142, acc:  67%, G loss: 1.956415\n",
      "Ep: 412, steps: 24, D loss: 0.196709, acc:  75%, G loss: 1.782548\n",
      "Ep: 412, steps: 25, D loss: 0.240904, acc:  57%, G loss: 1.561990\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 413, steps: 1, D loss: 0.237405, acc:  60%, G loss: 1.788330\n",
      "Ep: 413, steps: 2, D loss: 0.255666, acc:  52%, G loss: 1.575068\n",
      "Ep: 413, steps: 3, D loss: 0.162553, acc:  82%, G loss: 2.049983\n",
      "Ep: 413, steps: 4, D loss: 0.196052, acc:  78%, G loss: 1.927358\n",
      "Ep: 413, steps: 5, D loss: 0.284087, acc:  44%, G loss: 1.780605\n",
      "Ep: 413, steps: 6, D loss: 0.245596, acc:  55%, G loss: 1.642694\n",
      "Ep: 413, steps: 7, D loss: 0.348809, acc:  23%, G loss: 1.671324\n",
      "Ep: 413, steps: 8, D loss: 0.243523, acc:  58%, G loss: 1.833499\n",
      "Ep: 413, steps: 9, D loss: 0.234957, acc:  62%, G loss: 1.691700\n",
      "Ep: 413, steps: 10, D loss: 0.183672, acc:  81%, G loss: 1.629592\n",
      "Ep: 413, steps: 11, D loss: 0.235616, acc:  58%, G loss: 1.834636\n",
      "Ep: 413, steps: 12, D loss: 0.292707, acc:  34%, G loss: 1.414494\n",
      "Ep: 413, steps: 13, D loss: 0.283314, acc:  37%, G loss: 1.430643\n",
      "Ep: 413, steps: 14, D loss: 0.273230, acc:  40%, G loss: 1.612216\n",
      "Ep: 413, steps: 15, D loss: 0.226592, acc:  65%, G loss: 1.689539\n",
      "Ep: 413, steps: 16, D loss: 0.242758, acc:  56%, G loss: 1.682265\n",
      "Ep: 413, steps: 17, D loss: 0.193730, acc:  77%, G loss: 1.732757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 413, steps: 18, D loss: 0.257476, acc:  54%, G loss: 1.631680\n",
      "Ep: 413, steps: 19, D loss: 0.200951, acc:  72%, G loss: 1.671311\n",
      "Ep: 413, steps: 20, D loss: 0.169862, acc:  81%, G loss: 1.805228\n",
      "Ep: 413, steps: 21, D loss: 0.297838, acc:  31%, G loss: 1.537860\n",
      "Ep: 413, steps: 22, D loss: 0.166162, acc:  81%, G loss: 1.708648\n",
      "Ep: 413, steps: 23, D loss: 0.218416, acc:  67%, G loss: 1.962492\n",
      "Ep: 413, steps: 24, D loss: 0.190498, acc:  75%, G loss: 1.654884\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 413, steps: 25, D loss: 0.251178, acc:  56%, G loss: 1.561946\n",
      "Ep: 413, steps: 26, D loss: 0.251604, acc:  55%, G loss: 1.487636\n",
      "Ep: 413, steps: 27, D loss: 0.157809, acc:  82%, G loss: 2.071743\n",
      "Ep: 413, steps: 28, D loss: 0.196787, acc:  76%, G loss: 1.850665\n",
      "Ep: 413, steps: 29, D loss: 0.250621, acc:  55%, G loss: 1.927674\n",
      "Ep: 413, steps: 30, D loss: 0.244685, acc:  56%, G loss: 1.634861\n",
      "Ep: 413, steps: 31, D loss: 0.332057, acc:  30%, G loss: 1.532040\n",
      "Ep: 413, steps: 32, D loss: 0.231238, acc:  60%, G loss: 1.909576\n",
      "Ep: 413, steps: 33, D loss: 0.237830, acc:  60%, G loss: 1.755943\n",
      "Ep: 413, steps: 34, D loss: 0.180592, acc:  81%, G loss: 1.669364\n",
      "Ep: 413, steps: 35, D loss: 0.236376, acc:  56%, G loss: 1.860367\n",
      "Ep: 413, steps: 36, D loss: 0.323838, acc:  27%, G loss: 1.546805\n",
      "Ep: 413, steps: 37, D loss: 0.287382, acc:  40%, G loss: 1.470066\n",
      "Ep: 413, steps: 38, D loss: 0.271648, acc:  40%, G loss: 1.557724\n",
      "Ep: 413, steps: 39, D loss: 0.244404, acc:  54%, G loss: 1.696059\n",
      "Ep: 413, steps: 40, D loss: 0.247187, acc:  58%, G loss: 1.712377\n",
      "Ep: 413, steps: 41, D loss: 0.205234, acc:  74%, G loss: 1.663163\n",
      "Ep: 413, steps: 42, D loss: 0.244886, acc:  57%, G loss: 1.634014\n",
      "Ep: 413, steps: 43, D loss: 0.215344, acc:  68%, G loss: 1.634943\n",
      "Ep: 413, steps: 44, D loss: 0.179590, acc:  77%, G loss: 1.858118\n",
      "Ep: 413, steps: 45, D loss: 0.274699, acc:  39%, G loss: 1.612015\n",
      "Ep: 413, steps: 46, D loss: 0.171033, acc:  79%, G loss: 1.689447\n",
      "Ep: 413, steps: 47, D loss: 0.214488, acc:  68%, G loss: 2.002568\n",
      "Ep: 413, steps: 48, D loss: 0.179597, acc:  79%, G loss: 1.632691\n",
      "Ep: 413, steps: 49, D loss: 0.239908, acc:  59%, G loss: 1.649773\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 414, steps: 1, D loss: 0.281523, acc:  44%, G loss: 1.689104\n",
      "Ep: 414, steps: 2, D loss: 0.258714, acc:  51%, G loss: 1.625326\n",
      "Ep: 414, steps: 3, D loss: 0.168922, acc:  81%, G loss: 2.065645\n",
      "Ep: 414, steps: 4, D loss: 0.191130, acc:  80%, G loss: 1.852193\n",
      "Ep: 414, steps: 5, D loss: 0.275698, acc:  45%, G loss: 1.755872\n",
      "Ep: 414, steps: 6, D loss: 0.245891, acc:  53%, G loss: 1.691571\n",
      "Ep: 414, steps: 7, D loss: 0.337098, acc:  26%, G loss: 1.465223\n",
      "Ep: 414, steps: 8, D loss: 0.237942, acc:  59%, G loss: 1.837952\n",
      "Ep: 414, steps: 9, D loss: 0.222892, acc:  66%, G loss: 1.776037\n",
      "Ep: 414, steps: 10, D loss: 0.182125, acc:  80%, G loss: 1.698013\n",
      "Ep: 414, steps: 11, D loss: 0.226739, acc:  61%, G loss: 1.894400\n",
      "Ep: 414, steps: 12, D loss: 0.313940, acc:  29%, G loss: 1.437555\n",
      "Ep: 414, steps: 13, D loss: 0.299129, acc:  32%, G loss: 1.438049\n",
      "Ep: 414, steps: 14, D loss: 0.288373, acc:  34%, G loss: 1.551645\n",
      "Ep: 414, steps: 15, D loss: 0.239518, acc:  60%, G loss: 1.611570\n",
      "Ep: 414, steps: 16, D loss: 0.239750, acc:  58%, G loss: 1.711513\n",
      "Ep: 414, steps: 17, D loss: 0.201018, acc:  73%, G loss: 1.645538\n",
      "Ep: 414, steps: 18, D loss: 0.254892, acc:  54%, G loss: 1.638959\n",
      "Ep: 414, steps: 19, D loss: 0.219950, acc:  66%, G loss: 1.610817\n",
      "Ep: 414, steps: 20, D loss: 0.191773, acc:  73%, G loss: 1.851257\n",
      "Ep: 414, steps: 21, D loss: 0.263228, acc:  44%, G loss: 1.459816\n",
      "Ep: 414, steps: 22, D loss: 0.171234, acc:  80%, G loss: 1.682312\n",
      "Ep: 414, steps: 23, D loss: 0.231288, acc:  62%, G loss: 2.013554\n",
      "Ep: 414, steps: 24, D loss: 0.195320, acc:  72%, G loss: 1.645611\n",
      "Ep: 414, steps: 25, D loss: 0.255861, acc:  54%, G loss: 1.865421\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 415, steps: 1, D loss: 0.232647, acc:  62%, G loss: 1.885161\n",
      "Ep: 415, steps: 2, D loss: 0.242188, acc:  57%, G loss: 1.638986\n",
      "Ep: 415, steps: 3, D loss: 0.166018, acc:  80%, G loss: 2.166264\n",
      "Ep: 415, steps: 4, D loss: 0.188993, acc:  80%, G loss: 1.893714\n",
      "Ep: 415, steps: 5, D loss: 0.254925, acc:  52%, G loss: 1.790560\n",
      "Ep: 415, steps: 6, D loss: 0.243175, acc:  55%, G loss: 1.657485\n",
      "Ep: 415, steps: 7, D loss: 0.327062, acc:  28%, G loss: 1.541653\n",
      "Ep: 415, steps: 8, D loss: 0.227917, acc:  62%, G loss: 1.904012\n",
      "Ep: 415, steps: 9, D loss: 0.256560, acc:  54%, G loss: 1.776539\n",
      "Ep: 415, steps: 10, D loss: 0.183612, acc:  80%, G loss: 1.648121\n",
      "Ep: 415, steps: 11, D loss: 0.230931, acc:  61%, G loss: 1.869079\n",
      "Ep: 415, steps: 12, D loss: 0.293497, acc:  35%, G loss: 1.424148\n",
      "Ep: 415, steps: 13, D loss: 0.286918, acc:  36%, G loss: 1.471758\n",
      "Ep: 415, steps: 14, D loss: 0.267821, acc:  45%, G loss: 1.535612\n",
      "Ep: 415, steps: 15, D loss: 0.239192, acc:  60%, G loss: 1.603252\n",
      "Ep: 415, steps: 16, D loss: 0.248233, acc:  55%, G loss: 1.658186\n",
      "Ep: 415, steps: 17, D loss: 0.198060, acc:  75%, G loss: 1.580524\n",
      "Ep: 415, steps: 18, D loss: 0.248609, acc:  55%, G loss: 1.661834\n",
      "Ep: 415, steps: 19, D loss: 0.204311, acc:  71%, G loss: 1.647889\n",
      "Ep: 415, steps: 20, D loss: 0.197454, acc:  68%, G loss: 1.882576\n",
      "Ep: 415, steps: 21, D loss: 0.278719, acc:  36%, G loss: 1.458140\n",
      "Ep: 415, steps: 22, D loss: 0.170369, acc:  80%, G loss: 1.724418\n",
      "Saved Model\n",
      "Ep: 415, steps: 23, D loss: 0.212794, acc:  67%, G loss: 2.011124\n",
      "Ep: 415, steps: 24, D loss: 0.224887, acc:  62%, G loss: 1.682465\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 416, steps: 1, D loss: 0.277030, acc:  52%, G loss: 1.890554\n",
      "Ep: 416, steps: 2, D loss: 0.236970, acc:  60%, G loss: 1.577577\n",
      "Ep: 416, steps: 3, D loss: 0.189992, acc:  73%, G loss: 2.013324\n",
      "Ep: 416, steps: 4, D loss: 0.181061, acc:  83%, G loss: 1.827819\n",
      "Ep: 416, steps: 5, D loss: 0.291381, acc:  44%, G loss: 1.765853\n",
      "Ep: 416, steps: 6, D loss: 0.238305, acc:  59%, G loss: 1.695903\n",
      "Ep: 416, steps: 7, D loss: 0.326670, acc:  28%, G loss: 1.538718\n",
      "Ep: 416, steps: 8, D loss: 0.240483, acc:  59%, G loss: 1.754820\n",
      "Ep: 416, steps: 9, D loss: 0.220984, acc:  67%, G loss: 1.772119\n",
      "Ep: 416, steps: 10, D loss: 0.179484, acc:  80%, G loss: 1.634349\n",
      "Ep: 416, steps: 11, D loss: 0.222076, acc:  65%, G loss: 1.792783\n",
      "Ep: 416, steps: 12, D loss: 0.301924, acc:  33%, G loss: 1.515446\n",
      "Ep: 416, steps: 13, D loss: 0.285976, acc:  36%, G loss: 1.481969\n",
      "Ep: 416, steps: 14, D loss: 0.281095, acc:  37%, G loss: 1.576307\n",
      "Ep: 416, steps: 15, D loss: 0.247268, acc:  54%, G loss: 1.629308\n",
      "Ep: 416, steps: 16, D loss: 0.247113, acc:  56%, G loss: 1.623328\n",
      "Ep: 416, steps: 17, D loss: 0.200180, acc:  74%, G loss: 1.668813\n",
      "Ep: 416, steps: 18, D loss: 0.244812, acc:  58%, G loss: 1.638515\n",
      "Ep: 416, steps: 19, D loss: 0.197595, acc:  72%, G loss: 1.722170\n",
      "Ep: 416, steps: 20, D loss: 0.182553, acc:  74%, G loss: 1.843405\n",
      "Ep: 416, steps: 21, D loss: 0.283152, acc:  35%, G loss: 1.531459\n",
      "Ep: 416, steps: 22, D loss: 0.149856, acc:  84%, G loss: 1.713996\n",
      "Ep: 416, steps: 23, D loss: 0.214906, acc:  67%, G loss: 2.039688\n",
      "Ep: 416, steps: 24, D loss: 0.204268, acc:  70%, G loss: 1.618824\n",
      "Ep: 416, steps: 25, D loss: 0.220348, acc:  63%, G loss: 1.736415\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 417, steps: 1, D loss: 0.257326, acc:  54%, G loss: 1.806115\n",
      "Ep: 417, steps: 2, D loss: 0.244607, acc:  56%, G loss: 1.588953\n",
      "Ep: 417, steps: 3, D loss: 0.159941, acc:  82%, G loss: 2.083277\n",
      "Ep: 417, steps: 4, D loss: 0.197703, acc:  74%, G loss: 1.934256\n",
      "Ep: 417, steps: 5, D loss: 0.294102, acc:  43%, G loss: 1.780787\n",
      "Ep: 417, steps: 6, D loss: 0.246223, acc:  55%, G loss: 1.709460\n",
      "Ep: 417, steps: 7, D loss: 0.340737, acc:  23%, G loss: 1.568830\n",
      "Ep: 417, steps: 8, D loss: 0.239502, acc:  56%, G loss: 1.855427\n",
      "Ep: 417, steps: 9, D loss: 0.235577, acc:  61%, G loss: 1.723528\n",
      "Ep: 417, steps: 10, D loss: 0.185381, acc:  79%, G loss: 1.647544\n",
      "Ep: 417, steps: 11, D loss: 0.229380, acc:  59%, G loss: 1.850880\n",
      "Ep: 417, steps: 12, D loss: 0.309355, acc:  29%, G loss: 1.460864\n",
      "Ep: 417, steps: 13, D loss: 0.291696, acc:  35%, G loss: 1.483686\n",
      "Ep: 417, steps: 14, D loss: 0.276586, acc:  40%, G loss: 1.566939\n",
      "Ep: 417, steps: 15, D loss: 0.246555, acc:  56%, G loss: 1.621192\n",
      "Ep: 417, steps: 16, D loss: 0.244414, acc:  59%, G loss: 1.651778\n",
      "Ep: 417, steps: 17, D loss: 0.213056, acc:  71%, G loss: 1.629504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 417, steps: 18, D loss: 0.255351, acc:  54%, G loss: 1.645581\n",
      "Ep: 417, steps: 19, D loss: 0.217679, acc:  65%, G loss: 1.649154\n",
      "Ep: 417, steps: 20, D loss: 0.180388, acc:  76%, G loss: 1.775698\n",
      "Ep: 417, steps: 21, D loss: 0.284095, acc:  37%, G loss: 1.533098\n",
      "Ep: 417, steps: 22, D loss: 0.171328, acc:  77%, G loss: 1.800797\n",
      "Ep: 417, steps: 23, D loss: 0.206647, acc:  71%, G loss: 2.014577\n",
      "Ep: 417, steps: 24, D loss: 0.195681, acc:  74%, G loss: 1.649891\n",
      "Ep: 417, steps: 25, D loss: 0.216660, acc:  66%, G loss: 1.632607\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 418, steps: 1, D loss: 0.259326, acc:  52%, G loss: 1.761852\n",
      "Ep: 418, steps: 2, D loss: 0.257406, acc:  50%, G loss: 1.615165\n",
      "Ep: 418, steps: 3, D loss: 0.167930, acc:  80%, G loss: 2.054494\n",
      "Ep: 418, steps: 4, D loss: 0.191209, acc:  79%, G loss: 1.912126\n",
      "Ep: 418, steps: 5, D loss: 0.275249, acc:  45%, G loss: 1.744935\n",
      "Ep: 418, steps: 6, D loss: 0.242222, acc:  55%, G loss: 1.658490\n",
      "Ep: 418, steps: 7, D loss: 0.356203, acc:  21%, G loss: 1.514140\n",
      "Ep: 418, steps: 8, D loss: 0.246648, acc:  55%, G loss: 1.766952\n",
      "Ep: 418, steps: 9, D loss: 0.213244, acc:  70%, G loss: 1.724612\n",
      "Ep: 418, steps: 10, D loss: 0.191784, acc:  75%, G loss: 1.621592\n",
      "Ep: 418, steps: 11, D loss: 0.225839, acc:  63%, G loss: 1.864489\n",
      "Ep: 418, steps: 12, D loss: 0.314187, acc:  28%, G loss: 1.450074\n",
      "Ep: 418, steps: 13, D loss: 0.289599, acc:  36%, G loss: 1.437235\n",
      "Ep: 418, steps: 14, D loss: 0.283253, acc:  37%, G loss: 1.608039\n",
      "Ep: 418, steps: 15, D loss: 0.229345, acc:  62%, G loss: 1.618954\n",
      "Ep: 418, steps: 16, D loss: 0.239030, acc:  57%, G loss: 1.660288\n",
      "Ep: 418, steps: 17, D loss: 0.206319, acc:  72%, G loss: 1.677853\n",
      "Ep: 418, steps: 18, D loss: 0.243026, acc:  58%, G loss: 1.652050\n",
      "Ep: 418, steps: 19, D loss: 0.208715, acc:  69%, G loss: 1.678887\n",
      "Ep: 418, steps: 20, D loss: 0.189442, acc:  74%, G loss: 1.773017\n",
      "Saved Model\n",
      "Ep: 418, steps: 21, D loss: 0.271745, acc:  39%, G loss: 1.505597\n",
      "Ep: 418, steps: 22, D loss: 0.232279, acc:  61%, G loss: 1.959363\n",
      "Ep: 418, steps: 23, D loss: 0.192808, acc:  78%, G loss: 1.597640\n",
      "Ep: 418, steps: 24, D loss: 0.235142, acc:  59%, G loss: 1.602040\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 419, steps: 1, D loss: 0.235181, acc:  60%, G loss: 1.789029\n",
      "Ep: 419, steps: 2, D loss: 0.247891, acc:  55%, G loss: 1.643788\n",
      "Ep: 419, steps: 3, D loss: 0.170552, acc:  79%, G loss: 2.004909\n",
      "Ep: 419, steps: 4, D loss: 0.188965, acc:  82%, G loss: 1.854302\n",
      "Ep: 419, steps: 5, D loss: 0.291361, acc:  44%, G loss: 1.841641\n",
      "Ep: 419, steps: 6, D loss: 0.238244, acc:  54%, G loss: 1.659503\n",
      "Ep: 419, steps: 7, D loss: 0.318079, acc:  32%, G loss: 1.522126\n",
      "Ep: 419, steps: 8, D loss: 0.229912, acc:  61%, G loss: 1.806928\n",
      "Ep: 419, steps: 9, D loss: 0.230641, acc:  63%, G loss: 1.716095\n",
      "Ep: 419, steps: 10, D loss: 0.185565, acc:  78%, G loss: 1.613074\n",
      "Ep: 419, steps: 11, D loss: 0.216428, acc:  65%, G loss: 1.806954\n",
      "Ep: 419, steps: 12, D loss: 0.307238, acc:  31%, G loss: 1.421904\n",
      "Ep: 419, steps: 13, D loss: 0.284643, acc:  39%, G loss: 1.451902\n",
      "Ep: 419, steps: 14, D loss: 0.269440, acc:  42%, G loss: 1.534828\n",
      "Ep: 419, steps: 15, D loss: 0.249273, acc:  54%, G loss: 1.631627\n",
      "Ep: 419, steps: 16, D loss: 0.240780, acc:  58%, G loss: 1.691496\n",
      "Ep: 419, steps: 17, D loss: 0.213229, acc:  70%, G loss: 1.624518\n",
      "Ep: 419, steps: 18, D loss: 0.256286, acc:  55%, G loss: 1.786147\n",
      "Ep: 419, steps: 19, D loss: 0.211302, acc:  69%, G loss: 1.683382\n",
      "Ep: 419, steps: 20, D loss: 0.183358, acc:  75%, G loss: 1.866756\n",
      "Ep: 419, steps: 21, D loss: 0.281296, acc:  36%, G loss: 1.545586\n",
      "Ep: 419, steps: 22, D loss: 0.176705, acc:  77%, G loss: 1.691782\n",
      "Ep: 419, steps: 23, D loss: 0.210004, acc:  67%, G loss: 1.869353\n",
      "Ep: 419, steps: 24, D loss: 0.197537, acc:  72%, G loss: 1.601293\n",
      "Ep: 419, steps: 25, D loss: 0.236557, acc:  61%, G loss: 1.670159\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 420, steps: 1, D loss: 0.256812, acc:  54%, G loss: 1.744666\n",
      "Ep: 420, steps: 2, D loss: 0.244664, acc:  56%, G loss: 1.621097\n",
      "Ep: 420, steps: 3, D loss: 0.179900, acc:  76%, G loss: 2.120928\n",
      "Ep: 420, steps: 4, D loss: 0.189640, acc:  80%, G loss: 1.923747\n",
      "Ep: 420, steps: 5, D loss: 0.272999, acc:  47%, G loss: 1.772008\n",
      "Ep: 420, steps: 6, D loss: 0.237871, acc:  57%, G loss: 1.689335\n",
      "Ep: 420, steps: 7, D loss: 0.340239, acc:  26%, G loss: 1.480307\n",
      "Ep: 420, steps: 8, D loss: 0.240676, acc:  57%, G loss: 1.843067\n",
      "Ep: 420, steps: 9, D loss: 0.231257, acc:  63%, G loss: 1.708296\n",
      "Ep: 420, steps: 10, D loss: 0.184660, acc:  78%, G loss: 1.604182\n",
      "Ep: 420, steps: 11, D loss: 0.228374, acc:  61%, G loss: 1.822667\n",
      "Ep: 420, steps: 12, D loss: 0.295272, acc:  35%, G loss: 1.422197\n",
      "Ep: 420, steps: 13, D loss: 0.281593, acc:  38%, G loss: 1.477799\n",
      "Ep: 420, steps: 14, D loss: 0.286543, acc:  38%, G loss: 1.664735\n",
      "Ep: 420, steps: 15, D loss: 0.237624, acc:  60%, G loss: 1.659528\n",
      "Ep: 420, steps: 16, D loss: 0.257972, acc:  53%, G loss: 1.762180\n",
      "Ep: 420, steps: 17, D loss: 0.196150, acc:  77%, G loss: 1.634279\n",
      "Ep: 420, steps: 18, D loss: 0.255854, acc:  51%, G loss: 1.664041\n",
      "Ep: 420, steps: 19, D loss: 0.209955, acc:  69%, G loss: 1.627766\n",
      "Ep: 420, steps: 20, D loss: 0.183529, acc:  75%, G loss: 1.857176\n",
      "Ep: 420, steps: 21, D loss: 0.280255, acc:  39%, G loss: 1.464017\n",
      "Ep: 420, steps: 22, D loss: 0.168526, acc:  77%, G loss: 1.679825\n",
      "Ep: 420, steps: 23, D loss: 0.219129, acc:  66%, G loss: 1.987284\n",
      "Ep: 420, steps: 24, D loss: 0.189272, acc:  76%, G loss: 1.622408\n",
      "Ep: 420, steps: 25, D loss: 0.229874, acc:  59%, G loss: 1.628070\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 421, steps: 1, D loss: 0.255960, acc:  55%, G loss: 1.707225\n",
      "Ep: 421, steps: 2, D loss: 0.253643, acc:  54%, G loss: 1.602977\n",
      "Ep: 421, steps: 3, D loss: 0.160023, acc:  81%, G loss: 2.050081\n",
      "Ep: 421, steps: 4, D loss: 0.196779, acc:  79%, G loss: 1.889855\n",
      "Ep: 421, steps: 5, D loss: 0.258666, acc:  51%, G loss: 1.825458\n",
      "Ep: 421, steps: 6, D loss: 0.249781, acc:  53%, G loss: 1.694206\n",
      "Ep: 421, steps: 7, D loss: 0.344524, acc:  24%, G loss: 1.504393\n",
      "Ep: 421, steps: 8, D loss: 0.236272, acc:  60%, G loss: 1.787995\n",
      "Ep: 421, steps: 9, D loss: 0.232917, acc:  64%, G loss: 1.727808\n",
      "Ep: 421, steps: 10, D loss: 0.185040, acc:  79%, G loss: 1.644446\n",
      "Ep: 421, steps: 11, D loss: 0.213337, acc:  67%, G loss: 1.910810\n",
      "Ep: 421, steps: 12, D loss: 0.308121, acc:  31%, G loss: 1.480354\n",
      "Ep: 421, steps: 13, D loss: 0.289322, acc:  37%, G loss: 1.565016\n",
      "Ep: 421, steps: 14, D loss: 0.280966, acc:  40%, G loss: 1.612625\n",
      "Ep: 421, steps: 15, D loss: 0.225923, acc:  64%, G loss: 1.657541\n",
      "Ep: 421, steps: 16, D loss: 0.246522, acc:  56%, G loss: 1.689578\n",
      "Ep: 421, steps: 17, D loss: 0.206514, acc:  73%, G loss: 1.602852\n",
      "Ep: 421, steps: 18, D loss: 0.269856, acc:  50%, G loss: 1.669271\n",
      "Saved Model\n",
      "Ep: 421, steps: 19, D loss: 0.216831, acc:  66%, G loss: 1.630793\n",
      "Ep: 421, steps: 20, D loss: 0.272300, acc:  38%, G loss: 1.455977\n",
      "Ep: 421, steps: 21, D loss: 0.200128, acc:  69%, G loss: 1.660198\n",
      "Ep: 421, steps: 22, D loss: 0.233493, acc:  62%, G loss: 1.978509\n",
      "Ep: 421, steps: 23, D loss: 0.194642, acc:  75%, G loss: 1.612869\n",
      "Ep: 421, steps: 24, D loss: 0.244273, acc:  56%, G loss: 1.695678\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 422, steps: 1, D loss: 0.259498, acc:  55%, G loss: 1.698435\n",
      "Ep: 422, steps: 2, D loss: 0.245286, acc:  56%, G loss: 1.570741\n",
      "Ep: 422, steps: 3, D loss: 0.178065, acc:  76%, G loss: 1.907345\n",
      "Ep: 422, steps: 4, D loss: 0.207095, acc:  74%, G loss: 1.840693\n",
      "Ep: 422, steps: 5, D loss: 0.272616, acc:  49%, G loss: 1.782677\n",
      "Ep: 422, steps: 6, D loss: 0.242853, acc:  54%, G loss: 1.621961\n",
      "Ep: 422, steps: 7, D loss: 0.310107, acc:  31%, G loss: 1.542457\n",
      "Ep: 422, steps: 8, D loss: 0.232533, acc:  60%, G loss: 1.799685\n",
      "Ep: 422, steps: 9, D loss: 0.206008, acc:  73%, G loss: 1.692348\n",
      "Ep: 422, steps: 10, D loss: 0.169822, acc:  86%, G loss: 1.629555\n",
      "Ep: 422, steps: 11, D loss: 0.216895, acc:  63%, G loss: 1.938522\n",
      "Ep: 422, steps: 12, D loss: 0.306467, acc:  32%, G loss: 1.376333\n",
      "Ep: 422, steps: 13, D loss: 0.272924, acc:  43%, G loss: 1.492458\n",
      "Ep: 422, steps: 14, D loss: 0.278534, acc:  38%, G loss: 1.562095\n",
      "Ep: 422, steps: 15, D loss: 0.253621, acc:  52%, G loss: 1.609773\n",
      "Ep: 422, steps: 16, D loss: 0.235762, acc:  59%, G loss: 1.680979\n",
      "Ep: 422, steps: 17, D loss: 0.211482, acc:  70%, G loss: 1.658740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 422, steps: 18, D loss: 0.262497, acc:  52%, G loss: 1.692464\n",
      "Ep: 422, steps: 19, D loss: 0.202949, acc:  71%, G loss: 1.694282\n",
      "Ep: 422, steps: 20, D loss: 0.181054, acc:  76%, G loss: 1.754454\n",
      "Ep: 422, steps: 21, D loss: 0.279544, acc:  37%, G loss: 1.521724\n",
      "Ep: 422, steps: 22, D loss: 0.157820, acc:  78%, G loss: 1.798706\n",
      "Ep: 422, steps: 23, D loss: 0.240356, acc:  61%, G loss: 2.079901\n",
      "Ep: 422, steps: 24, D loss: 0.198321, acc:  71%, G loss: 1.664306\n",
      "Ep: 422, steps: 25, D loss: 0.243813, acc:  57%, G loss: 1.730376\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 423, steps: 1, D loss: 0.255624, acc:  54%, G loss: 1.837623\n",
      "Ep: 423, steps: 2, D loss: 0.252492, acc:  53%, G loss: 1.597966\n",
      "Ep: 423, steps: 3, D loss: 0.168056, acc:  83%, G loss: 2.044130\n",
      "Ep: 423, steps: 4, D loss: 0.189881, acc:  79%, G loss: 1.919770\n",
      "Ep: 423, steps: 5, D loss: 0.274797, acc:  47%, G loss: 1.830416\n",
      "Ep: 423, steps: 6, D loss: 0.236597, acc:  57%, G loss: 1.660329\n",
      "Ep: 423, steps: 7, D loss: 0.359145, acc:  25%, G loss: 1.652233\n",
      "Ep: 423, steps: 8, D loss: 0.229094, acc:  62%, G loss: 1.825392\n",
      "Ep: 423, steps: 9, D loss: 0.238252, acc:  59%, G loss: 1.760804\n",
      "Ep: 423, steps: 10, D loss: 0.187538, acc:  76%, G loss: 1.642956\n",
      "Ep: 423, steps: 11, D loss: 0.216332, acc:  64%, G loss: 1.911386\n",
      "Ep: 423, steps: 12, D loss: 0.309972, acc:  30%, G loss: 1.432630\n",
      "Ep: 423, steps: 13, D loss: 0.283851, acc:  40%, G loss: 1.495706\n",
      "Ep: 423, steps: 14, D loss: 0.273731, acc:  41%, G loss: 1.574877\n",
      "Ep: 423, steps: 15, D loss: 0.240667, acc:  60%, G loss: 1.683599\n",
      "Ep: 423, steps: 16, D loss: 0.245161, acc:  56%, G loss: 1.653266\n",
      "Ep: 423, steps: 17, D loss: 0.209525, acc:  71%, G loss: 1.727383\n",
      "Ep: 423, steps: 18, D loss: 0.243863, acc:  58%, G loss: 1.676426\n",
      "Ep: 423, steps: 19, D loss: 0.221135, acc:  65%, G loss: 1.649504\n",
      "Ep: 423, steps: 20, D loss: 0.200261, acc:  71%, G loss: 1.775101\n",
      "Ep: 423, steps: 21, D loss: 0.270236, acc:  41%, G loss: 1.550287\n",
      "Ep: 423, steps: 22, D loss: 0.183676, acc:  76%, G loss: 1.687326\n",
      "Ep: 423, steps: 23, D loss: 0.236306, acc:  62%, G loss: 1.972057\n",
      "Ep: 423, steps: 24, D loss: 0.194160, acc:  73%, G loss: 1.643359\n",
      "Ep: 423, steps: 25, D loss: 0.225118, acc:  62%, G loss: 1.638706\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 424, steps: 1, D loss: 0.270257, acc:  49%, G loss: 1.801812\n",
      "Ep: 424, steps: 2, D loss: 0.237655, acc:  58%, G loss: 1.617097\n",
      "Ep: 424, steps: 3, D loss: 0.169158, acc:  81%, G loss: 2.003393\n",
      "Ep: 424, steps: 4, D loss: 0.194868, acc:  76%, G loss: 1.851178\n",
      "Ep: 424, steps: 5, D loss: 0.284342, acc:  44%, G loss: 1.815028\n",
      "Ep: 424, steps: 6, D loss: 0.247533, acc:  56%, G loss: 1.674660\n",
      "Ep: 424, steps: 7, D loss: 0.340310, acc:  24%, G loss: 1.540467\n",
      "Ep: 424, steps: 8, D loss: 0.235333, acc:  62%, G loss: 1.785438\n",
      "Ep: 424, steps: 9, D loss: 0.215996, acc:  70%, G loss: 1.678338\n",
      "Ep: 424, steps: 10, D loss: 0.190288, acc:  76%, G loss: 1.680093\n",
      "Ep: 424, steps: 11, D loss: 0.223856, acc:  62%, G loss: 1.872024\n",
      "Ep: 424, steps: 12, D loss: 0.305847, acc:  29%, G loss: 1.437729\n",
      "Ep: 424, steps: 13, D loss: 0.290110, acc:  34%, G loss: 1.401487\n",
      "Ep: 424, steps: 14, D loss: 0.271367, acc:  41%, G loss: 1.514433\n",
      "Ep: 424, steps: 15, D loss: 0.227746, acc:  66%, G loss: 1.614607\n",
      "Ep: 424, steps: 16, D loss: 0.245005, acc:  55%, G loss: 1.638435\n",
      "Saved Model\n",
      "Ep: 424, steps: 17, D loss: 0.191301, acc:  77%, G loss: 1.626802\n",
      "Ep: 424, steps: 18, D loss: 0.212146, acc:  67%, G loss: 1.623015\n",
      "Ep: 424, steps: 19, D loss: 0.206507, acc:  66%, G loss: 1.882230\n",
      "Ep: 424, steps: 20, D loss: 0.271376, acc:  43%, G loss: 1.604785\n",
      "Ep: 424, steps: 21, D loss: 0.146780, acc:  88%, G loss: 1.775111\n",
      "Ep: 424, steps: 22, D loss: 0.208470, acc:  70%, G loss: 2.028846\n",
      "Ep: 424, steps: 23, D loss: 0.189390, acc:  79%, G loss: 1.627762\n",
      "Ep: 424, steps: 24, D loss: 0.244615, acc:  56%, G loss: 1.673852\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 425, steps: 1, D loss: 0.265385, acc:  51%, G loss: 1.766592\n",
      "Ep: 425, steps: 2, D loss: 0.252187, acc:  53%, G loss: 1.579282\n",
      "Ep: 425, steps: 3, D loss: 0.175624, acc:  76%, G loss: 2.056721\n",
      "Ep: 425, steps: 4, D loss: 0.190891, acc:  80%, G loss: 1.757385\n",
      "Ep: 425, steps: 5, D loss: 0.275736, acc:  47%, G loss: 1.831794\n",
      "Ep: 425, steps: 6, D loss: 0.236993, acc:  56%, G loss: 1.681248\n",
      "Ep: 425, steps: 7, D loss: 0.354736, acc:  21%, G loss: 1.547772\n",
      "Ep: 425, steps: 8, D loss: 0.230342, acc:  62%, G loss: 1.785400\n",
      "Ep: 425, steps: 9, D loss: 0.215434, acc:  67%, G loss: 1.703504\n",
      "Ep: 425, steps: 10, D loss: 0.184023, acc:  79%, G loss: 1.744583\n",
      "Ep: 425, steps: 11, D loss: 0.229589, acc:  59%, G loss: 1.952212\n",
      "Ep: 425, steps: 12, D loss: 0.304506, acc:  31%, G loss: 1.421029\n",
      "Ep: 425, steps: 13, D loss: 0.281507, acc:  40%, G loss: 1.453188\n",
      "Ep: 425, steps: 14, D loss: 0.269433, acc:  43%, G loss: 1.573972\n",
      "Ep: 425, steps: 15, D loss: 0.242113, acc:  57%, G loss: 1.567717\n",
      "Ep: 425, steps: 16, D loss: 0.237002, acc:  58%, G loss: 1.641224\n",
      "Ep: 425, steps: 17, D loss: 0.210541, acc:  71%, G loss: 1.597795\n",
      "Ep: 425, steps: 18, D loss: 0.254318, acc:  55%, G loss: 1.698913\n",
      "Ep: 425, steps: 19, D loss: 0.210684, acc:  69%, G loss: 1.624875\n",
      "Ep: 425, steps: 20, D loss: 0.174174, acc:  80%, G loss: 1.824204\n",
      "Ep: 425, steps: 21, D loss: 0.280086, acc:  37%, G loss: 1.535043\n",
      "Ep: 425, steps: 22, D loss: 0.165492, acc:  82%, G loss: 1.725557\n",
      "Ep: 425, steps: 23, D loss: 0.233052, acc:  62%, G loss: 2.031974\n",
      "Ep: 425, steps: 24, D loss: 0.195529, acc:  73%, G loss: 1.633760\n",
      "Ep: 425, steps: 25, D loss: 0.248298, acc:  56%, G loss: 1.649526\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 426, steps: 1, D loss: 0.252344, acc:  55%, G loss: 1.756905\n",
      "Ep: 426, steps: 2, D loss: 0.240468, acc:  57%, G loss: 1.635072\n",
      "Ep: 426, steps: 3, D loss: 0.176604, acc:  78%, G loss: 2.007043\n",
      "Ep: 426, steps: 4, D loss: 0.188430, acc:  81%, G loss: 1.799802\n",
      "Ep: 426, steps: 5, D loss: 0.278654, acc:  47%, G loss: 1.782850\n",
      "Ep: 426, steps: 6, D loss: 0.249253, acc:  54%, G loss: 1.654765\n",
      "Ep: 426, steps: 7, D loss: 0.322196, acc:  28%, G loss: 1.369655\n",
      "Ep: 426, steps: 8, D loss: 0.235169, acc:  60%, G loss: 1.807525\n",
      "Ep: 426, steps: 9, D loss: 0.222948, acc:  66%, G loss: 1.683933\n",
      "Ep: 426, steps: 10, D loss: 0.174440, acc:  83%, G loss: 1.820830\n",
      "Ep: 426, steps: 11, D loss: 0.234834, acc:  58%, G loss: 1.942045\n",
      "Ep: 426, steps: 12, D loss: 0.303440, acc:  32%, G loss: 1.483133\n",
      "Ep: 426, steps: 13, D loss: 0.282926, acc:  39%, G loss: 1.439216\n",
      "Ep: 426, steps: 14, D loss: 0.280372, acc:  38%, G loss: 1.501959\n",
      "Ep: 426, steps: 15, D loss: 0.245887, acc:  56%, G loss: 1.613480\n",
      "Ep: 426, steps: 16, D loss: 0.241331, acc:  58%, G loss: 1.667124\n",
      "Ep: 426, steps: 17, D loss: 0.199645, acc:  75%, G loss: 1.661719\n",
      "Ep: 426, steps: 18, D loss: 0.258898, acc:  54%, G loss: 1.644495\n",
      "Ep: 426, steps: 19, D loss: 0.211571, acc:  68%, G loss: 1.693646\n",
      "Ep: 426, steps: 20, D loss: 0.175887, acc:  76%, G loss: 1.833718\n",
      "Ep: 426, steps: 21, D loss: 0.287052, acc:  36%, G loss: 1.572310\n",
      "Ep: 426, steps: 22, D loss: 0.145787, acc:  84%, G loss: 1.831936\n",
      "Ep: 426, steps: 23, D loss: 0.251786, acc:  57%, G loss: 2.079982\n",
      "Ep: 426, steps: 24, D loss: 0.184874, acc:  78%, G loss: 1.681309\n",
      "Ep: 426, steps: 25, D loss: 0.282395, acc:  47%, G loss: 2.063404\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 427, steps: 1, D loss: 0.249886, acc:  56%, G loss: 1.899831\n",
      "Ep: 427, steps: 2, D loss: 0.245106, acc:  55%, G loss: 1.645902\n",
      "Ep: 427, steps: 3, D loss: 0.154993, acc:  83%, G loss: 2.095308\n",
      "Ep: 427, steps: 4, D loss: 0.189578, acc:  81%, G loss: 1.767391\n",
      "Ep: 427, steps: 5, D loss: 0.234511, acc:  57%, G loss: 2.033427\n",
      "Ep: 427, steps: 6, D loss: 0.245556, acc:  54%, G loss: 1.648557\n",
      "Ep: 427, steps: 7, D loss: 0.304746, acc:  37%, G loss: 1.558657\n",
      "Ep: 427, steps: 8, D loss: 0.228066, acc:  62%, G loss: 1.808583\n",
      "Ep: 427, steps: 9, D loss: 0.240441, acc:  62%, G loss: 1.678505\n",
      "Ep: 427, steps: 10, D loss: 0.172408, acc:  82%, G loss: 1.668830\n",
      "Ep: 427, steps: 11, D loss: 0.221005, acc:  64%, G loss: 1.939989\n",
      "Ep: 427, steps: 12, D loss: 0.300912, acc:  32%, G loss: 1.365323\n",
      "Ep: 427, steps: 13, D loss: 0.282206, acc:  39%, G loss: 1.459007\n",
      "Ep: 427, steps: 14, D loss: 0.253485, acc:  51%, G loss: 1.562481\n",
      "Saved Model\n",
      "Ep: 427, steps: 15, D loss: 0.255700, acc:  52%, G loss: 1.564013\n",
      "Ep: 427, steps: 16, D loss: 0.197156, acc:  74%, G loss: 1.715813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 427, steps: 17, D loss: 0.261446, acc:  52%, G loss: 1.711574\n",
      "Ep: 427, steps: 18, D loss: 0.212272, acc:  68%, G loss: 1.663350\n",
      "Ep: 427, steps: 19, D loss: 0.183661, acc:  78%, G loss: 1.781303\n",
      "Ep: 427, steps: 20, D loss: 0.282329, acc:  40%, G loss: 1.507636\n",
      "Ep: 427, steps: 21, D loss: 0.173787, acc:  78%, G loss: 1.795483\n",
      "Ep: 427, steps: 22, D loss: 0.211849, acc:  69%, G loss: 1.932992\n",
      "Ep: 427, steps: 23, D loss: 0.196191, acc:  73%, G loss: 1.657292\n",
      "Ep: 427, steps: 24, D loss: 0.199501, acc:  70%, G loss: 1.718195\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 428, steps: 1, D loss: 0.265961, acc:  54%, G loss: 1.754735\n",
      "Ep: 428, steps: 2, D loss: 0.245187, acc:  56%, G loss: 1.638432\n",
      "Ep: 428, steps: 3, D loss: 0.173690, acc:  82%, G loss: 2.032992\n",
      "Ep: 428, steps: 4, D loss: 0.193541, acc:  78%, G loss: 1.827941\n",
      "Ep: 428, steps: 5, D loss: 0.268145, acc:  54%, G loss: 1.736102\n",
      "Ep: 428, steps: 6, D loss: 0.253072, acc:  53%, G loss: 1.602525\n",
      "Ep: 428, steps: 7, D loss: 0.316325, acc:  31%, G loss: 1.440102\n",
      "Ep: 428, steps: 8, D loss: 0.248066, acc:  54%, G loss: 1.858286\n",
      "Ep: 428, steps: 9, D loss: 0.231416, acc:  65%, G loss: 1.773066\n",
      "Ep: 428, steps: 10, D loss: 0.174360, acc:  84%, G loss: 1.668968\n",
      "Ep: 428, steps: 11, D loss: 0.223498, acc:  63%, G loss: 1.884683\n",
      "Ep: 428, steps: 12, D loss: 0.288696, acc:  38%, G loss: 1.403654\n",
      "Ep: 428, steps: 13, D loss: 0.281114, acc:  38%, G loss: 1.487925\n",
      "Ep: 428, steps: 14, D loss: 0.275856, acc:  40%, G loss: 1.539527\n",
      "Ep: 428, steps: 15, D loss: 0.252024, acc:  52%, G loss: 1.604181\n",
      "Ep: 428, steps: 16, D loss: 0.240878, acc:  57%, G loss: 1.662243\n",
      "Ep: 428, steps: 17, D loss: 0.219230, acc:  68%, G loss: 1.695073\n",
      "Ep: 428, steps: 18, D loss: 0.233819, acc:  62%, G loss: 1.660526\n",
      "Ep: 428, steps: 19, D loss: 0.215124, acc:  66%, G loss: 1.610309\n",
      "Ep: 428, steps: 20, D loss: 0.178479, acc:  76%, G loss: 1.796222\n",
      "Ep: 428, steps: 21, D loss: 0.278632, acc:  38%, G loss: 1.470427\n",
      "Ep: 428, steps: 22, D loss: 0.163263, acc:  80%, G loss: 1.722433\n",
      "Ep: 428, steps: 23, D loss: 0.251410, acc:  58%, G loss: 2.073810\n",
      "Ep: 428, steps: 24, D loss: 0.202058, acc:  71%, G loss: 1.685669\n",
      "Ep: 428, steps: 25, D loss: 0.200080, acc:  72%, G loss: 1.918007\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 429, steps: 1, D loss: 0.251099, acc:  55%, G loss: 1.674828\n",
      "Ep: 429, steps: 2, D loss: 0.243836, acc:  55%, G loss: 1.518087\n",
      "Ep: 429, steps: 3, D loss: 0.168243, acc:  80%, G loss: 2.016769\n",
      "Ep: 429, steps: 4, D loss: 0.193709, acc:  79%, G loss: 1.879763\n",
      "Ep: 429, steps: 5, D loss: 0.290537, acc:  44%, G loss: 1.683244\n",
      "Ep: 429, steps: 6, D loss: 0.242170, acc:  54%, G loss: 1.577187\n",
      "Ep: 429, steps: 7, D loss: 0.335007, acc:  26%, G loss: 1.524801\n",
      "Ep: 429, steps: 8, D loss: 0.233467, acc:  62%, G loss: 2.033991\n",
      "Ep: 429, steps: 9, D loss: 0.239261, acc:  59%, G loss: 1.682479\n",
      "Ep: 429, steps: 10, D loss: 0.182173, acc:  82%, G loss: 1.637978\n",
      "Ep: 429, steps: 11, D loss: 0.229517, acc:  60%, G loss: 1.856934\n",
      "Ep: 429, steps: 12, D loss: 0.303233, acc:  31%, G loss: 1.349171\n",
      "Ep: 429, steps: 13, D loss: 0.286059, acc:  37%, G loss: 1.453008\n",
      "Ep: 429, steps: 14, D loss: 0.271317, acc:  43%, G loss: 1.571073\n",
      "Ep: 429, steps: 15, D loss: 0.236081, acc:  60%, G loss: 1.626757\n",
      "Ep: 429, steps: 16, D loss: 0.240160, acc:  58%, G loss: 1.710347\n",
      "Ep: 429, steps: 17, D loss: 0.202094, acc:  73%, G loss: 1.649992\n",
      "Ep: 429, steps: 18, D loss: 0.245893, acc:  57%, G loss: 1.705146\n",
      "Ep: 429, steps: 19, D loss: 0.212215, acc:  68%, G loss: 1.629521\n",
      "Ep: 429, steps: 20, D loss: 0.189985, acc:  73%, G loss: 1.818295\n",
      "Ep: 429, steps: 21, D loss: 0.278585, acc:  40%, G loss: 1.418117\n",
      "Ep: 429, steps: 22, D loss: 0.167798, acc:  80%, G loss: 1.754133\n",
      "Ep: 429, steps: 23, D loss: 0.224253, acc:  64%, G loss: 2.050767\n",
      "Ep: 429, steps: 24, D loss: 0.181839, acc:  78%, G loss: 1.646093\n",
      "Ep: 429, steps: 25, D loss: 0.207874, acc:  68%, G loss: 1.724021\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 430, steps: 1, D loss: 0.263941, acc:  51%, G loss: 1.859347\n",
      "Ep: 430, steps: 2, D loss: 0.251912, acc:  53%, G loss: 1.626353\n",
      "Ep: 430, steps: 3, D loss: 0.171765, acc:  80%, G loss: 2.040606\n",
      "Ep: 430, steps: 4, D loss: 0.194788, acc:  78%, G loss: 1.858170\n",
      "Ep: 430, steps: 5, D loss: 0.283409, acc:  45%, G loss: 1.709356\n",
      "Ep: 430, steps: 6, D loss: 0.244169, acc:  55%, G loss: 1.598918\n",
      "Ep: 430, steps: 7, D loss: 0.352955, acc:  22%, G loss: 1.511890\n",
      "Ep: 430, steps: 8, D loss: 0.238838, acc:  60%, G loss: 1.909891\n",
      "Ep: 430, steps: 9, D loss: 0.227445, acc:  65%, G loss: 1.773433\n",
      "Ep: 430, steps: 10, D loss: 0.184770, acc:  79%, G loss: 1.622038\n",
      "Ep: 430, steps: 11, D loss: 0.228950, acc:  60%, G loss: 1.837269\n",
      "Ep: 430, steps: 12, D loss: 0.318668, acc:  27%, G loss: 1.417438\n",
      "Saved Model\n",
      "Ep: 430, steps: 13, D loss: 0.275177, acc:  43%, G loss: 1.503485\n",
      "Ep: 430, steps: 14, D loss: 0.231000, acc:  62%, G loss: 1.614704\n",
      "Ep: 430, steps: 15, D loss: 0.247073, acc:  55%, G loss: 1.653725\n",
      "Ep: 430, steps: 16, D loss: 0.204871, acc:  73%, G loss: 1.627835\n",
      "Ep: 430, steps: 17, D loss: 0.218297, acc:  68%, G loss: 1.723374\n",
      "Ep: 430, steps: 18, D loss: 0.201620, acc:  69%, G loss: 1.726747\n",
      "Ep: 430, steps: 19, D loss: 0.194214, acc:  69%, G loss: 1.843246\n",
      "Ep: 430, steps: 20, D loss: 0.297584, acc:  32%, G loss: 1.440543\n",
      "Ep: 430, steps: 21, D loss: 0.167815, acc:  81%, G loss: 1.770427\n",
      "Ep: 430, steps: 22, D loss: 0.216433, acc:  66%, G loss: 1.967842\n",
      "Ep: 430, steps: 23, D loss: 0.198316, acc:  70%, G loss: 1.673144\n",
      "Ep: 430, steps: 24, D loss: 0.209152, acc:  69%, G loss: 1.658095\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 431, steps: 1, D loss: 0.261623, acc:  53%, G loss: 1.790095\n",
      "Ep: 431, steps: 2, D loss: 0.244956, acc:  55%, G loss: 1.593470\n",
      "Ep: 431, steps: 3, D loss: 0.159283, acc:  83%, G loss: 2.030123\n",
      "Ep: 431, steps: 4, D loss: 0.180700, acc:  83%, G loss: 1.881323\n",
      "Ep: 431, steps: 5, D loss: 0.270862, acc:  50%, G loss: 1.720253\n",
      "Ep: 431, steps: 6, D loss: 0.252269, acc:  53%, G loss: 1.613014\n",
      "Ep: 431, steps: 7, D loss: 0.395492, acc:  19%, G loss: 1.415522\n",
      "Ep: 431, steps: 8, D loss: 0.252229, acc:  54%, G loss: 1.882288\n",
      "Ep: 431, steps: 9, D loss: 0.218915, acc:  68%, G loss: 1.714252\n",
      "Ep: 431, steps: 10, D loss: 0.186735, acc:  79%, G loss: 1.599601\n",
      "Ep: 431, steps: 11, D loss: 0.220130, acc:  64%, G loss: 1.851131\n",
      "Ep: 431, steps: 12, D loss: 0.299754, acc:  34%, G loss: 1.391817\n",
      "Ep: 431, steps: 13, D loss: 0.281958, acc:  39%, G loss: 1.424873\n",
      "Ep: 431, steps: 14, D loss: 0.277401, acc:  40%, G loss: 1.553377\n",
      "Ep: 431, steps: 15, D loss: 0.243532, acc:  58%, G loss: 1.559216\n",
      "Ep: 431, steps: 16, D loss: 0.238017, acc:  58%, G loss: 1.666329\n",
      "Ep: 431, steps: 17, D loss: 0.203385, acc:  73%, G loss: 1.668742\n",
      "Ep: 431, steps: 18, D loss: 0.250128, acc:  56%, G loss: 1.677567\n",
      "Ep: 431, steps: 19, D loss: 0.221110, acc:  65%, G loss: 1.618879\n",
      "Ep: 431, steps: 20, D loss: 0.184719, acc:  75%, G loss: 1.911172\n",
      "Ep: 431, steps: 21, D loss: 0.277752, acc:  39%, G loss: 1.436810\n",
      "Ep: 431, steps: 22, D loss: 0.167460, acc:  81%, G loss: 1.734737\n",
      "Ep: 431, steps: 23, D loss: 0.225361, acc:  64%, G loss: 2.039509\n",
      "Ep: 431, steps: 24, D loss: 0.202546, acc:  69%, G loss: 1.669908\n",
      "Ep: 431, steps: 25, D loss: 0.219393, acc:  64%, G loss: 1.837167\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 432, steps: 1, D loss: 0.243291, acc:  56%, G loss: 1.774742\n",
      "Ep: 432, steps: 2, D loss: 0.252872, acc:  52%, G loss: 1.603920\n",
      "Ep: 432, steps: 3, D loss: 0.161552, acc:  85%, G loss: 1.996142\n",
      "Ep: 432, steps: 4, D loss: 0.194241, acc:  77%, G loss: 1.851597\n",
      "Ep: 432, steps: 5, D loss: 0.289909, acc:  48%, G loss: 1.800082\n",
      "Ep: 432, steps: 6, D loss: 0.261420, acc:  50%, G loss: 1.609842\n",
      "Ep: 432, steps: 7, D loss: 0.341713, acc:  26%, G loss: 1.436406\n",
      "Ep: 432, steps: 8, D loss: 0.222192, acc:  62%, G loss: 1.792403\n",
      "Ep: 432, steps: 9, D loss: 0.242959, acc:  58%, G loss: 1.673974\n",
      "Ep: 432, steps: 10, D loss: 0.183648, acc:  80%, G loss: 1.574109\n",
      "Ep: 432, steps: 11, D loss: 0.226014, acc:  60%, G loss: 1.896802\n",
      "Ep: 432, steps: 12, D loss: 0.304509, acc:  31%, G loss: 1.391163\n",
      "Ep: 432, steps: 13, D loss: 0.295085, acc:  34%, G loss: 1.544342\n",
      "Ep: 432, steps: 14, D loss: 0.269592, acc:  41%, G loss: 1.562520\n",
      "Ep: 432, steps: 15, D loss: 0.242673, acc:  56%, G loss: 1.564825\n",
      "Ep: 432, steps: 16, D loss: 0.256335, acc:  53%, G loss: 1.631902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 432, steps: 17, D loss: 0.219262, acc:  67%, G loss: 1.631234\n",
      "Ep: 432, steps: 18, D loss: 0.242687, acc:  58%, G loss: 1.694128\n",
      "Ep: 432, steps: 19, D loss: 0.214024, acc:  67%, G loss: 1.670297\n",
      "Ep: 432, steps: 20, D loss: 0.195723, acc:  72%, G loss: 1.771915\n",
      "Ep: 432, steps: 21, D loss: 0.278459, acc:  37%, G loss: 1.488986\n",
      "Ep: 432, steps: 22, D loss: 0.174769, acc:  79%, G loss: 1.731026\n",
      "Ep: 432, steps: 23, D loss: 0.227646, acc:  63%, G loss: 1.960385\n",
      "Ep: 432, steps: 24, D loss: 0.180544, acc:  79%, G loss: 1.640911\n",
      "Ep: 432, steps: 25, D loss: 0.234516, acc:  59%, G loss: 1.680614\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 433, steps: 1, D loss: 0.268228, acc:  53%, G loss: 1.825250\n",
      "Ep: 433, steps: 2, D loss: 0.253039, acc:  52%, G loss: 1.608058\n",
      "Ep: 433, steps: 3, D loss: 0.168315, acc:  81%, G loss: 2.040499\n",
      "Ep: 433, steps: 4, D loss: 0.190554, acc:  80%, G loss: 1.833536\n",
      "Ep: 433, steps: 5, D loss: 0.275944, acc:  45%, G loss: 1.692241\n",
      "Ep: 433, steps: 6, D loss: 0.244594, acc:  55%, G loss: 1.637237\n",
      "Ep: 433, steps: 7, D loss: 0.342509, acc:  23%, G loss: 1.581148\n",
      "Ep: 433, steps: 8, D loss: 0.238414, acc:  58%, G loss: 1.810462\n",
      "Ep: 433, steps: 9, D loss: 0.231981, acc:  63%, G loss: 1.677356\n",
      "Ep: 433, steps: 10, D loss: 0.168923, acc:  85%, G loss: 1.594676\n",
      "Saved Model\n",
      "Ep: 433, steps: 11, D loss: 0.238149, acc:  57%, G loss: 1.850001\n",
      "Ep: 433, steps: 12, D loss: 0.292780, acc:  34%, G loss: 1.432753\n",
      "Ep: 433, steps: 13, D loss: 0.283326, acc:  38%, G loss: 1.612031\n",
      "Ep: 433, steps: 14, D loss: 0.252848, acc:  52%, G loss: 1.632912\n",
      "Ep: 433, steps: 15, D loss: 0.237744, acc:  59%, G loss: 1.684824\n",
      "Ep: 433, steps: 16, D loss: 0.198853, acc:  75%, G loss: 1.690037\n",
      "Ep: 433, steps: 17, D loss: 0.235188, acc:  61%, G loss: 1.752373\n",
      "Ep: 433, steps: 18, D loss: 0.205499, acc:  69%, G loss: 1.678697\n",
      "Ep: 433, steps: 19, D loss: 0.168036, acc:  78%, G loss: 1.764784\n",
      "Ep: 433, steps: 20, D loss: 0.292311, acc:  36%, G loss: 1.477581\n",
      "Ep: 433, steps: 21, D loss: 0.151553, acc:  83%, G loss: 1.806271\n",
      "Ep: 433, steps: 22, D loss: 0.204969, acc:  71%, G loss: 2.046426\n",
      "Ep: 433, steps: 23, D loss: 0.194709, acc:  70%, G loss: 1.635326\n",
      "Ep: 433, steps: 24, D loss: 0.245097, acc:  55%, G loss: 1.693083\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 434, steps: 1, D loss: 0.253414, acc:  56%, G loss: 1.802140\n",
      "Ep: 434, steps: 2, D loss: 0.252400, acc:  52%, G loss: 1.603995\n",
      "Ep: 434, steps: 3, D loss: 0.177196, acc:  78%, G loss: 1.991310\n",
      "Ep: 434, steps: 4, D loss: 0.192686, acc:  79%, G loss: 1.809476\n",
      "Ep: 434, steps: 5, D loss: 0.270357, acc:  49%, G loss: 1.796619\n",
      "Ep: 434, steps: 6, D loss: 0.252661, acc:  53%, G loss: 1.642267\n",
      "Ep: 434, steps: 7, D loss: 0.363545, acc:  19%, G loss: 1.489221\n",
      "Ep: 434, steps: 8, D loss: 0.226820, acc:  61%, G loss: 1.787157\n",
      "Ep: 434, steps: 9, D loss: 0.228495, acc:  63%, G loss: 1.665949\n",
      "Ep: 434, steps: 10, D loss: 0.188096, acc:  79%, G loss: 1.672441\n",
      "Ep: 434, steps: 11, D loss: 0.218445, acc:  65%, G loss: 1.884685\n",
      "Ep: 434, steps: 12, D loss: 0.322726, acc:  27%, G loss: 1.407865\n",
      "Ep: 434, steps: 13, D loss: 0.294910, acc:  33%, G loss: 1.534655\n",
      "Ep: 434, steps: 14, D loss: 0.282450, acc:  39%, G loss: 1.572130\n",
      "Ep: 434, steps: 15, D loss: 0.240693, acc:  57%, G loss: 1.691082\n",
      "Ep: 434, steps: 16, D loss: 0.239411, acc:  57%, G loss: 1.635700\n",
      "Ep: 434, steps: 17, D loss: 0.197107, acc:  77%, G loss: 1.620858\n",
      "Ep: 434, steps: 18, D loss: 0.256867, acc:  54%, G loss: 1.774755\n",
      "Ep: 434, steps: 19, D loss: 0.224385, acc:  64%, G loss: 1.655818\n",
      "Ep: 434, steps: 20, D loss: 0.208324, acc:  68%, G loss: 1.853512\n",
      "Ep: 434, steps: 21, D loss: 0.294021, acc:  28%, G loss: 1.496893\n",
      "Ep: 434, steps: 22, D loss: 0.176862, acc:  79%, G loss: 1.802710\n",
      "Ep: 434, steps: 23, D loss: 0.218330, acc:  66%, G loss: 1.978049\n",
      "Ep: 434, steps: 24, D loss: 0.192464, acc:  76%, G loss: 1.664399\n",
      "Ep: 434, steps: 25, D loss: 0.239679, acc:  58%, G loss: 1.629586\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 435, steps: 1, D loss: 0.252842, acc:  54%, G loss: 1.821451\n",
      "Ep: 435, steps: 2, D loss: 0.248064, acc:  56%, G loss: 1.540002\n",
      "Ep: 435, steps: 3, D loss: 0.165299, acc:  81%, G loss: 1.988186\n",
      "Ep: 435, steps: 4, D loss: 0.194079, acc:  79%, G loss: 1.767288\n",
      "Ep: 435, steps: 5, D loss: 0.277910, acc:  49%, G loss: 1.743507\n",
      "Ep: 435, steps: 6, D loss: 0.241741, acc:  55%, G loss: 1.628542\n",
      "Ep: 435, steps: 7, D loss: 0.308425, acc:  31%, G loss: 1.489587\n",
      "Ep: 435, steps: 8, D loss: 0.236336, acc:  59%, G loss: 1.740680\n",
      "Ep: 435, steps: 9, D loss: 0.213789, acc:  70%, G loss: 1.668575\n",
      "Ep: 435, steps: 10, D loss: 0.184936, acc:  78%, G loss: 1.630047\n",
      "Ep: 435, steps: 11, D loss: 0.242726, acc:  54%, G loss: 1.911955\n",
      "Ep: 435, steps: 12, D loss: 0.318960, acc:  26%, G loss: 1.420309\n",
      "Ep: 435, steps: 13, D loss: 0.283040, acc:  39%, G loss: 1.412544\n",
      "Ep: 435, steps: 14, D loss: 0.271994, acc:  43%, G loss: 1.525357\n",
      "Ep: 435, steps: 15, D loss: 0.246172, acc:  55%, G loss: 1.671527\n",
      "Ep: 435, steps: 16, D loss: 0.248588, acc:  57%, G loss: 1.665387\n",
      "Ep: 435, steps: 17, D loss: 0.208190, acc:  71%, G loss: 1.638140\n",
      "Ep: 435, steps: 18, D loss: 0.251570, acc:  55%, G loss: 1.672720\n",
      "Ep: 435, steps: 19, D loss: 0.219703, acc:  65%, G loss: 1.636828\n",
      "Ep: 435, steps: 20, D loss: 0.177615, acc:  78%, G loss: 1.787552\n",
      "Ep: 435, steps: 21, D loss: 0.278599, acc:  38%, G loss: 1.456422\n",
      "Ep: 435, steps: 22, D loss: 0.162672, acc:  79%, G loss: 1.854596\n",
      "Ep: 435, steps: 23, D loss: 0.213888, acc:  67%, G loss: 1.996005\n",
      "Ep: 435, steps: 24, D loss: 0.209375, acc:  67%, G loss: 1.749104\n",
      "Ep: 435, steps: 25, D loss: 0.243816, acc:  56%, G loss: 1.611586\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 436, steps: 1, D loss: 0.251341, acc:  56%, G loss: 1.736004\n",
      "Ep: 436, steps: 2, D loss: 0.259956, acc:  48%, G loss: 1.562346\n",
      "Ep: 436, steps: 3, D loss: 0.168184, acc:  82%, G loss: 2.031397\n",
      "Ep: 436, steps: 4, D loss: 0.196496, acc:  77%, G loss: 1.917385\n",
      "Ep: 436, steps: 5, D loss: 0.271528, acc:  50%, G loss: 1.793103\n",
      "Ep: 436, steps: 6, D loss: 0.244106, acc:  56%, G loss: 1.615500\n",
      "Ep: 436, steps: 7, D loss: 0.344768, acc:  24%, G loss: 1.450921\n",
      "Ep: 436, steps: 8, D loss: 0.233776, acc:  60%, G loss: 1.706736\n",
      "Saved Model\n",
      "Ep: 436, steps: 9, D loss: 0.230185, acc:  63%, G loss: 1.644003\n",
      "Ep: 436, steps: 10, D loss: 0.234119, acc:  58%, G loss: 1.778167\n",
      "Ep: 436, steps: 11, D loss: 0.286558, acc:  37%, G loss: 1.404099\n",
      "Ep: 436, steps: 12, D loss: 0.283357, acc:  35%, G loss: 1.405001\n",
      "Ep: 436, steps: 13, D loss: 0.268017, acc:  45%, G loss: 1.503801\n",
      "Ep: 436, steps: 14, D loss: 0.261572, acc:  48%, G loss: 1.633872\n",
      "Ep: 436, steps: 15, D loss: 0.251917, acc:  54%, G loss: 1.702907\n",
      "Ep: 436, steps: 16, D loss: 0.220939, acc:  68%, G loss: 1.590958\n",
      "Ep: 436, steps: 17, D loss: 0.250145, acc:  57%, G loss: 1.590228\n",
      "Ep: 436, steps: 18, D loss: 0.232037, acc:  62%, G loss: 1.676267\n",
      "Ep: 436, steps: 19, D loss: 0.190971, acc:  76%, G loss: 1.786485\n",
      "Ep: 436, steps: 20, D loss: 0.273716, acc:  40%, G loss: 1.513870\n",
      "Ep: 436, steps: 21, D loss: 0.174139, acc:  78%, G loss: 1.666413\n",
      "Ep: 436, steps: 22, D loss: 0.219381, acc:  67%, G loss: 1.926670\n",
      "Ep: 436, steps: 23, D loss: 0.188562, acc:  77%, G loss: 1.579565\n",
      "Ep: 436, steps: 24, D loss: 0.242023, acc:  58%, G loss: 1.674723\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 437, steps: 1, D loss: 0.244035, acc:  59%, G loss: 1.754404\n",
      "Ep: 437, steps: 2, D loss: 0.250333, acc:  53%, G loss: 1.546167\n",
      "Ep: 437, steps: 3, D loss: 0.172172, acc:  77%, G loss: 2.011786\n",
      "Ep: 437, steps: 4, D loss: 0.205406, acc:  74%, G loss: 1.869581\n",
      "Ep: 437, steps: 5, D loss: 0.267267, acc:  50%, G loss: 1.861269\n",
      "Ep: 437, steps: 6, D loss: 0.230597, acc:  59%, G loss: 1.595585\n",
      "Ep: 437, steps: 7, D loss: 0.336879, acc:  25%, G loss: 1.499214\n",
      "Ep: 437, steps: 8, D loss: 0.228394, acc:  64%, G loss: 1.770775\n",
      "Ep: 437, steps: 9, D loss: 0.231994, acc:  61%, G loss: 1.683223\n",
      "Ep: 437, steps: 10, D loss: 0.182547, acc:  78%, G loss: 1.660777\n",
      "Ep: 437, steps: 11, D loss: 0.230075, acc:  61%, G loss: 1.841777\n",
      "Ep: 437, steps: 12, D loss: 0.294602, acc:  36%, G loss: 1.425542\n",
      "Ep: 437, steps: 13, D loss: 0.277278, acc:  40%, G loss: 1.434640\n",
      "Ep: 437, steps: 14, D loss: 0.284406, acc:  37%, G loss: 1.515375\n",
      "Ep: 437, steps: 15, D loss: 0.257097, acc:  53%, G loss: 1.584324\n",
      "Ep: 437, steps: 16, D loss: 0.241319, acc:  57%, G loss: 1.601224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 437, steps: 17, D loss: 0.199119, acc:  72%, G loss: 1.601150\n",
      "Ep: 437, steps: 18, D loss: 0.257226, acc:  55%, G loss: 1.722569\n",
      "Ep: 437, steps: 19, D loss: 0.218136, acc:  64%, G loss: 1.690883\n",
      "Ep: 437, steps: 20, D loss: 0.193144, acc:  73%, G loss: 1.812373\n",
      "Ep: 437, steps: 21, D loss: 0.261514, acc:  45%, G loss: 1.494238\n",
      "Ep: 437, steps: 22, D loss: 0.153330, acc:  84%, G loss: 1.739022\n",
      "Ep: 437, steps: 23, D loss: 0.220916, acc:  65%, G loss: 1.965338\n",
      "Ep: 437, steps: 24, D loss: 0.198960, acc:  71%, G loss: 1.652791\n",
      "Ep: 437, steps: 25, D loss: 0.252641, acc:  54%, G loss: 1.686551\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 438, steps: 1, D loss: 0.274023, acc:  50%, G loss: 1.800588\n",
      "Ep: 438, steps: 2, D loss: 0.253277, acc:  53%, G loss: 1.563789\n",
      "Ep: 438, steps: 3, D loss: 0.170788, acc:  83%, G loss: 1.978846\n",
      "Ep: 438, steps: 4, D loss: 0.193686, acc:  79%, G loss: 1.786174\n",
      "Ep: 438, steps: 5, D loss: 0.287507, acc:  45%, G loss: 1.799621\n",
      "Ep: 438, steps: 6, D loss: 0.231565, acc:  57%, G loss: 1.707341\n",
      "Ep: 438, steps: 7, D loss: 0.316090, acc:  28%, G loss: 1.436702\n",
      "Ep: 438, steps: 8, D loss: 0.230543, acc:  62%, G loss: 1.835518\n",
      "Ep: 438, steps: 9, D loss: 0.226008, acc:  64%, G loss: 1.684104\n",
      "Ep: 438, steps: 10, D loss: 0.182955, acc:  82%, G loss: 1.619725\n",
      "Ep: 438, steps: 11, D loss: 0.240299, acc:  57%, G loss: 1.856586\n",
      "Ep: 438, steps: 12, D loss: 0.299213, acc:  31%, G loss: 1.451141\n",
      "Ep: 438, steps: 13, D loss: 0.282620, acc:  37%, G loss: 1.450982\n",
      "Ep: 438, steps: 14, D loss: 0.274357, acc:  42%, G loss: 1.505475\n",
      "Ep: 438, steps: 15, D loss: 0.245165, acc:  58%, G loss: 1.604232\n",
      "Ep: 438, steps: 16, D loss: 0.248775, acc:  55%, G loss: 1.649928\n",
      "Ep: 438, steps: 17, D loss: 0.205003, acc:  73%, G loss: 1.612995\n",
      "Ep: 438, steps: 18, D loss: 0.246691, acc:  59%, G loss: 1.724799\n",
      "Ep: 438, steps: 19, D loss: 0.200322, acc:  70%, G loss: 1.637517\n",
      "Ep: 438, steps: 20, D loss: 0.184736, acc:  74%, G loss: 1.814808\n",
      "Ep: 438, steps: 21, D loss: 0.284075, acc:  35%, G loss: 1.598390\n",
      "Ep: 438, steps: 22, D loss: 0.156295, acc:  83%, G loss: 1.767967\n",
      "Ep: 438, steps: 23, D loss: 0.229048, acc:  65%, G loss: 1.982845\n",
      "Ep: 438, steps: 24, D loss: 0.191599, acc:  77%, G loss: 1.651405\n",
      "Ep: 438, steps: 25, D loss: 0.245099, acc:  54%, G loss: 1.703925\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 439, steps: 1, D loss: 0.236296, acc:  61%, G loss: 1.782448\n",
      "Ep: 439, steps: 2, D loss: 0.243157, acc:  56%, G loss: 1.552311\n",
      "Ep: 439, steps: 3, D loss: 0.166505, acc:  82%, G loss: 2.036200\n",
      "Ep: 439, steps: 4, D loss: 0.201270, acc:  74%, G loss: 1.863503\n",
      "Ep: 439, steps: 5, D loss: 0.273294, acc:  47%, G loss: 1.744781\n",
      "Ep: 439, steps: 6, D loss: 0.241552, acc:  56%, G loss: 1.663770\n",
      "Saved Model\n",
      "Ep: 439, steps: 7, D loss: 0.342211, acc:  26%, G loss: 1.584468\n",
      "Ep: 439, steps: 8, D loss: 0.211631, acc:  69%, G loss: 1.770202\n",
      "Ep: 439, steps: 9, D loss: 0.187180, acc:  76%, G loss: 1.742608\n",
      "Ep: 439, steps: 10, D loss: 0.218206, acc:  63%, G loss: 1.902707\n",
      "Ep: 439, steps: 11, D loss: 0.299366, acc:  32%, G loss: 1.430843\n",
      "Ep: 439, steps: 12, D loss: 0.294782, acc:  34%, G loss: 1.458024\n",
      "Ep: 439, steps: 13, D loss: 0.280838, acc:  37%, G loss: 1.515497\n",
      "Ep: 439, steps: 14, D loss: 0.252733, acc:  54%, G loss: 1.623742\n",
      "Ep: 439, steps: 15, D loss: 0.242403, acc:  57%, G loss: 1.615386\n",
      "Ep: 439, steps: 16, D loss: 0.193620, acc:  76%, G loss: 1.663306\n",
      "Ep: 439, steps: 17, D loss: 0.258486, acc:  53%, G loss: 1.728333\n",
      "Ep: 439, steps: 18, D loss: 0.215278, acc:  68%, G loss: 1.675257\n",
      "Ep: 439, steps: 19, D loss: 0.201415, acc:  72%, G loss: 1.730048\n",
      "Ep: 439, steps: 20, D loss: 0.257085, acc:  47%, G loss: 1.594974\n",
      "Ep: 439, steps: 21, D loss: 0.180757, acc:  76%, G loss: 1.743909\n",
      "Ep: 439, steps: 22, D loss: 0.219712, acc:  67%, G loss: 1.972008\n",
      "Ep: 439, steps: 23, D loss: 0.188830, acc:  76%, G loss: 1.557378\n",
      "Ep: 439, steps: 24, D loss: 0.232131, acc:  60%, G loss: 1.604397\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 440, steps: 1, D loss: 0.264162, acc:  51%, G loss: 1.810518\n",
      "Ep: 440, steps: 2, D loss: 0.251822, acc:  54%, G loss: 1.575327\n",
      "Ep: 440, steps: 3, D loss: 0.170065, acc:  79%, G loss: 2.066258\n",
      "Ep: 440, steps: 4, D loss: 0.180891, acc:  83%, G loss: 1.881493\n",
      "Ep: 440, steps: 5, D loss: 0.290481, acc:  45%, G loss: 1.820517\n",
      "Ep: 440, steps: 6, D loss: 0.254094, acc:  52%, G loss: 1.674524\n",
      "Ep: 440, steps: 7, D loss: 0.329373, acc:  28%, G loss: 1.513606\n",
      "Ep: 440, steps: 8, D loss: 0.239174, acc:  60%, G loss: 1.758209\n",
      "Ep: 440, steps: 9, D loss: 0.213887, acc:  70%, G loss: 1.755729\n",
      "Ep: 440, steps: 10, D loss: 0.182521, acc:  81%, G loss: 1.722016\n",
      "Ep: 440, steps: 11, D loss: 0.233806, acc:  60%, G loss: 1.930104\n",
      "Ep: 440, steps: 12, D loss: 0.298235, acc:  32%, G loss: 1.421454\n",
      "Ep: 440, steps: 13, D loss: 0.277318, acc:  38%, G loss: 1.438845\n",
      "Ep: 440, steps: 14, D loss: 0.273330, acc:  42%, G loss: 1.487428\n",
      "Ep: 440, steps: 15, D loss: 0.238345, acc:  60%, G loss: 1.597868\n",
      "Ep: 440, steps: 16, D loss: 0.244757, acc:  57%, G loss: 1.737023\n",
      "Ep: 440, steps: 17, D loss: 0.221734, acc:  69%, G loss: 1.669922\n",
      "Ep: 440, steps: 18, D loss: 0.242525, acc:  58%, G loss: 1.613174\n",
      "Ep: 440, steps: 19, D loss: 0.214727, acc:  67%, G loss: 1.695674\n",
      "Ep: 440, steps: 20, D loss: 0.184455, acc:  74%, G loss: 1.817891\n",
      "Ep: 440, steps: 21, D loss: 0.261709, acc:  44%, G loss: 1.510547\n",
      "Ep: 440, steps: 22, D loss: 0.150525, acc:  84%, G loss: 1.715750\n",
      "Ep: 440, steps: 23, D loss: 0.219110, acc:  65%, G loss: 1.988884\n",
      "Ep: 440, steps: 24, D loss: 0.209968, acc:  68%, G loss: 1.691847\n",
      "Ep: 440, steps: 25, D loss: 0.239262, acc:  58%, G loss: 1.876786\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 441, steps: 1, D loss: 0.229925, acc:  63%, G loss: 1.687524\n",
      "Ep: 441, steps: 2, D loss: 0.257022, acc:  51%, G loss: 1.543596\n",
      "Ep: 441, steps: 3, D loss: 0.177641, acc:  77%, G loss: 2.032976\n",
      "Ep: 441, steps: 4, D loss: 0.197424, acc:  77%, G loss: 1.891126\n",
      "Ep: 441, steps: 5, D loss: 0.271517, acc:  47%, G loss: 1.804549\n",
      "Ep: 441, steps: 6, D loss: 0.251322, acc:  55%, G loss: 1.686135\n",
      "Ep: 441, steps: 7, D loss: 0.387917, acc:  19%, G loss: 1.531980\n",
      "Ep: 441, steps: 8, D loss: 0.235608, acc:  60%, G loss: 1.787266\n",
      "Ep: 441, steps: 9, D loss: 0.223108, acc:  67%, G loss: 1.714102\n",
      "Ep: 441, steps: 10, D loss: 0.190999, acc:  74%, G loss: 1.743610\n",
      "Ep: 441, steps: 11, D loss: 0.217039, acc:  64%, G loss: 1.887089\n",
      "Ep: 441, steps: 12, D loss: 0.314445, acc:  31%, G loss: 1.412014\n",
      "Ep: 441, steps: 13, D loss: 0.278217, acc:  42%, G loss: 1.409788\n",
      "Ep: 441, steps: 14, D loss: 0.287139, acc:  36%, G loss: 1.513342\n",
      "Ep: 441, steps: 15, D loss: 0.235816, acc:  62%, G loss: 1.670245\n",
      "Ep: 441, steps: 16, D loss: 0.234923, acc:  59%, G loss: 1.653111\n",
      "Ep: 441, steps: 17, D loss: 0.198412, acc:  76%, G loss: 1.629369\n",
      "Ep: 441, steps: 18, D loss: 0.253851, acc:  54%, G loss: 1.611080\n",
      "Ep: 441, steps: 19, D loss: 0.207010, acc:  70%, G loss: 1.659812\n",
      "Ep: 441, steps: 20, D loss: 0.208352, acc:  67%, G loss: 1.773885\n",
      "Ep: 441, steps: 21, D loss: 0.283818, acc:  36%, G loss: 1.448263\n",
      "Ep: 441, steps: 22, D loss: 0.184718, acc:  75%, G loss: 1.649439\n",
      "Ep: 441, steps: 23, D loss: 0.224212, acc:  65%, G loss: 2.062095\n",
      "Ep: 441, steps: 24, D loss: 0.188347, acc:  76%, G loss: 1.660047\n",
      "Ep: 441, steps: 25, D loss: 0.256284, acc:  56%, G loss: 1.921891\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 442, steps: 1, D loss: 0.234370, acc:  61%, G loss: 1.846061\n",
      "Ep: 442, steps: 2, D loss: 0.246537, acc:  55%, G loss: 1.554520\n",
      "Ep: 442, steps: 3, D loss: 0.164155, acc:  82%, G loss: 2.026296\n",
      "Ep: 442, steps: 4, D loss: 0.189249, acc:  79%, G loss: 1.844307\n",
      "Saved Model\n",
      "Ep: 442, steps: 5, D loss: 0.268009, acc:  50%, G loss: 1.788972\n",
      "Ep: 442, steps: 6, D loss: 0.293094, acc:  36%, G loss: 1.504211\n",
      "Ep: 442, steps: 7, D loss: 0.212504, acc:  66%, G loss: 1.839203\n",
      "Ep: 442, steps: 8, D loss: 0.256131, acc:  52%, G loss: 1.675306\n",
      "Ep: 442, steps: 9, D loss: 0.170000, acc:  85%, G loss: 1.703399\n",
      "Ep: 442, steps: 10, D loss: 0.224002, acc:  62%, G loss: 1.832589\n",
      "Ep: 442, steps: 11, D loss: 0.294983, acc:  37%, G loss: 1.354850\n",
      "Ep: 442, steps: 12, D loss: 0.287956, acc:  38%, G loss: 1.411693\n",
      "Ep: 442, steps: 13, D loss: 0.284342, acc:  36%, G loss: 1.552409\n",
      "Ep: 442, steps: 14, D loss: 0.261200, acc:  49%, G loss: 1.667403\n",
      "Ep: 442, steps: 15, D loss: 0.252554, acc:  52%, G loss: 1.651201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 442, steps: 16, D loss: 0.215336, acc:  68%, G loss: 1.649612\n",
      "Ep: 442, steps: 17, D loss: 0.256683, acc:  55%, G loss: 1.634199\n",
      "Ep: 442, steps: 18, D loss: 0.215997, acc:  67%, G loss: 1.609338\n",
      "Ep: 442, steps: 19, D loss: 0.165656, acc:  81%, G loss: 1.857911\n",
      "Ep: 442, steps: 20, D loss: 0.285702, acc:  33%, G loss: 1.489332\n",
      "Ep: 442, steps: 21, D loss: 0.154983, acc:  81%, G loss: 1.702643\n",
      "Ep: 442, steps: 22, D loss: 0.221088, acc:  65%, G loss: 1.936610\n",
      "Ep: 442, steps: 23, D loss: 0.196610, acc:  76%, G loss: 1.636398\n",
      "Ep: 442, steps: 24, D loss: 0.219016, acc:  67%, G loss: 1.647312\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 443, steps: 1, D loss: 0.280753, acc:  47%, G loss: 1.874643\n",
      "Ep: 443, steps: 2, D loss: 0.251099, acc:  55%, G loss: 1.595801\n",
      "Ep: 443, steps: 3, D loss: 0.170536, acc:  82%, G loss: 2.048064\n",
      "Ep: 443, steps: 4, D loss: 0.190701, acc:  78%, G loss: 1.954000\n",
      "Ep: 443, steps: 5, D loss: 0.292689, acc:  44%, G loss: 1.727561\n",
      "Ep: 443, steps: 6, D loss: 0.250075, acc:  53%, G loss: 1.647663\n",
      "Ep: 443, steps: 7, D loss: 0.306910, acc:  34%, G loss: 1.464142\n",
      "Ep: 443, steps: 8, D loss: 0.252695, acc:  54%, G loss: 1.775177\n",
      "Ep: 443, steps: 9, D loss: 0.227726, acc:  66%, G loss: 1.725876\n",
      "Ep: 443, steps: 10, D loss: 0.174800, acc:  82%, G loss: 1.739186\n",
      "Ep: 443, steps: 11, D loss: 0.218150, acc:  64%, G loss: 1.872722\n",
      "Ep: 443, steps: 12, D loss: 0.291810, acc:  34%, G loss: 1.414843\n",
      "Ep: 443, steps: 13, D loss: 0.277479, acc:  40%, G loss: 1.387320\n",
      "Ep: 443, steps: 14, D loss: 0.280664, acc:  40%, G loss: 1.576629\n",
      "Ep: 443, steps: 15, D loss: 0.244069, acc:  57%, G loss: 1.576406\n",
      "Ep: 443, steps: 16, D loss: 0.246258, acc:  56%, G loss: 1.628072\n",
      "Ep: 443, steps: 17, D loss: 0.213274, acc:  70%, G loss: 1.659104\n",
      "Ep: 443, steps: 18, D loss: 0.239195, acc:  61%, G loss: 1.749472\n",
      "Ep: 443, steps: 19, D loss: 0.207904, acc:  70%, G loss: 1.625893\n",
      "Ep: 443, steps: 20, D loss: 0.168294, acc:  79%, G loss: 1.723603\n",
      "Ep: 443, steps: 21, D loss: 0.280119, acc:  36%, G loss: 1.510080\n",
      "Ep: 443, steps: 22, D loss: 0.163611, acc:  79%, G loss: 1.801295\n",
      "Ep: 443, steps: 23, D loss: 0.223702, acc:  65%, G loss: 2.006100\n",
      "Ep: 443, steps: 24, D loss: 0.205967, acc:  69%, G loss: 1.722571\n",
      "Ep: 443, steps: 25, D loss: 0.222920, acc:  63%, G loss: 1.609709\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 444, steps: 1, D loss: 0.250710, acc:  55%, G loss: 1.862155\n",
      "Ep: 444, steps: 2, D loss: 0.250876, acc:  55%, G loss: 1.585880\n",
      "Ep: 444, steps: 3, D loss: 0.169062, acc:  76%, G loss: 2.088379\n",
      "Ep: 444, steps: 4, D loss: 0.194552, acc:  76%, G loss: 1.957955\n",
      "Ep: 444, steps: 5, D loss: 0.269822, acc:  47%, G loss: 1.758342\n",
      "Ep: 444, steps: 6, D loss: 0.255663, acc:  52%, G loss: 1.702180\n",
      "Ep: 444, steps: 7, D loss: 0.329365, acc:  26%, G loss: 1.494782\n",
      "Ep: 444, steps: 8, D loss: 0.245308, acc:  58%, G loss: 1.808970\n",
      "Ep: 444, steps: 9, D loss: 0.207762, acc:  72%, G loss: 1.715734\n",
      "Ep: 444, steps: 10, D loss: 0.193885, acc:  74%, G loss: 1.863199\n",
      "Ep: 444, steps: 11, D loss: 0.226170, acc:  62%, G loss: 1.860277\n",
      "Ep: 444, steps: 12, D loss: 0.299907, acc:  35%, G loss: 1.368190\n",
      "Ep: 444, steps: 13, D loss: 0.284081, acc:  39%, G loss: 1.411253\n",
      "Ep: 444, steps: 14, D loss: 0.278387, acc:  40%, G loss: 1.515889\n",
      "Ep: 444, steps: 15, D loss: 0.233958, acc:  62%, G loss: 1.587299\n",
      "Ep: 444, steps: 16, D loss: 0.258585, acc:  52%, G loss: 1.624615\n",
      "Ep: 444, steps: 17, D loss: 0.206246, acc:  72%, G loss: 1.615807\n",
      "Ep: 444, steps: 18, D loss: 0.237683, acc:  60%, G loss: 1.650531\n",
      "Ep: 444, steps: 19, D loss: 0.213437, acc:  68%, G loss: 1.679867\n",
      "Ep: 444, steps: 20, D loss: 0.177724, acc:  76%, G loss: 1.735585\n",
      "Ep: 444, steps: 21, D loss: 0.271607, acc:  38%, G loss: 1.557506\n",
      "Ep: 444, steps: 22, D loss: 0.156280, acc:  82%, G loss: 1.685087\n",
      "Ep: 444, steps: 23, D loss: 0.224700, acc:  65%, G loss: 1.976974\n",
      "Ep: 444, steps: 24, D loss: 0.206005, acc:  69%, G loss: 1.708079\n",
      "Ep: 444, steps: 25, D loss: 0.232140, acc:  60%, G loss: 1.862693\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 445, steps: 1, D loss: 0.229043, acc:  64%, G loss: 1.777864\n",
      "Ep: 445, steps: 2, D loss: 0.252031, acc:  53%, G loss: 1.605066\n",
      "Saved Model\n",
      "Ep: 445, steps: 3, D loss: 0.158173, acc:  86%, G loss: 2.040892\n",
      "Ep: 445, steps: 4, D loss: 0.274046, acc:  49%, G loss: 1.766107\n",
      "Ep: 445, steps: 5, D loss: 0.244611, acc:  58%, G loss: 1.650383\n",
      "Ep: 445, steps: 6, D loss: 0.305221, acc:  34%, G loss: 1.712594\n",
      "Ep: 445, steps: 7, D loss: 0.227477, acc:  65%, G loss: 1.807474\n",
      "Ep: 445, steps: 8, D loss: 0.246847, acc:  57%, G loss: 1.723908\n",
      "Ep: 445, steps: 9, D loss: 0.176812, acc:  81%, G loss: 1.704493\n",
      "Ep: 445, steps: 10, D loss: 0.229528, acc:  60%, G loss: 1.821045\n",
      "Ep: 445, steps: 11, D loss: 0.298533, acc:  34%, G loss: 1.396023\n",
      "Ep: 445, steps: 12, D loss: 0.265980, acc:  45%, G loss: 1.388861\n",
      "Ep: 445, steps: 13, D loss: 0.261205, acc:  49%, G loss: 1.589690\n",
      "Ep: 445, steps: 14, D loss: 0.255958, acc:  51%, G loss: 1.637722\n",
      "Ep: 445, steps: 15, D loss: 0.243821, acc:  56%, G loss: 1.690047\n",
      "Ep: 445, steps: 16, D loss: 0.217888, acc:  67%, G loss: 1.625747\n",
      "Ep: 445, steps: 17, D loss: 0.248970, acc:  57%, G loss: 1.757840\n",
      "Ep: 445, steps: 18, D loss: 0.228664, acc:  63%, G loss: 1.670097\n",
      "Ep: 445, steps: 19, D loss: 0.215178, acc:  64%, G loss: 1.793188\n",
      "Ep: 445, steps: 20, D loss: 0.275587, acc:  40%, G loss: 1.518880\n",
      "Ep: 445, steps: 21, D loss: 0.165835, acc:  80%, G loss: 1.725171\n",
      "Ep: 445, steps: 22, D loss: 0.234119, acc:  61%, G loss: 1.939914\n",
      "Ep: 445, steps: 23, D loss: 0.205489, acc:  71%, G loss: 1.663924\n",
      "Ep: 445, steps: 24, D loss: 0.243621, acc:  56%, G loss: 1.650295\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 446, steps: 1, D loss: 0.259329, acc:  53%, G loss: 1.802922\n",
      "Ep: 446, steps: 2, D loss: 0.248859, acc:  55%, G loss: 1.521727\n",
      "Ep: 446, steps: 3, D loss: 0.176194, acc:  78%, G loss: 1.976491\n",
      "Ep: 446, steps: 4, D loss: 0.201736, acc:  76%, G loss: 1.806194\n",
      "Ep: 446, steps: 5, D loss: 0.258988, acc:  55%, G loss: 1.789368\n",
      "Ep: 446, steps: 6, D loss: 0.252768, acc:  53%, G loss: 1.648144\n",
      "Ep: 446, steps: 7, D loss: 0.345191, acc:  23%, G loss: 1.444691\n",
      "Ep: 446, steps: 8, D loss: 0.233061, acc:  59%, G loss: 1.771616\n",
      "Ep: 446, steps: 9, D loss: 0.226353, acc:  65%, G loss: 1.682687\n",
      "Ep: 446, steps: 10, D loss: 0.180298, acc:  78%, G loss: 1.662687\n",
      "Ep: 446, steps: 11, D loss: 0.233714, acc:  56%, G loss: 1.834554\n",
      "Ep: 446, steps: 12, D loss: 0.292191, acc:  35%, G loss: 1.351432\n",
      "Ep: 446, steps: 13, D loss: 0.279925, acc:  39%, G loss: 1.427857\n",
      "Ep: 446, steps: 14, D loss: 0.272414, acc:  41%, G loss: 1.602932\n",
      "Ep: 446, steps: 15, D loss: 0.234049, acc:  62%, G loss: 1.611732\n",
      "Ep: 446, steps: 16, D loss: 0.252346, acc:  54%, G loss: 1.655742\n",
      "Ep: 446, steps: 17, D loss: 0.215876, acc:  70%, G loss: 1.674823\n",
      "Ep: 446, steps: 18, D loss: 0.248946, acc:  57%, G loss: 1.738834\n",
      "Ep: 446, steps: 19, D loss: 0.209710, acc:  68%, G loss: 1.635050\n",
      "Ep: 446, steps: 20, D loss: 0.191204, acc:  71%, G loss: 1.840805\n",
      "Ep: 446, steps: 21, D loss: 0.272448, acc:  41%, G loss: 1.658325\n",
      "Ep: 446, steps: 22, D loss: 0.156423, acc:  83%, G loss: 1.721103\n",
      "Ep: 446, steps: 23, D loss: 0.223170, acc:  64%, G loss: 1.985489\n",
      "Ep: 446, steps: 24, D loss: 0.211444, acc:  68%, G loss: 1.730896\n",
      "Ep: 446, steps: 25, D loss: 0.229321, acc:  61%, G loss: 1.876703\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 447, steps: 1, D loss: 0.252780, acc:  55%, G loss: 1.895862\n",
      "Ep: 447, steps: 2, D loss: 0.242788, acc:  55%, G loss: 1.514343\n",
      "Ep: 447, steps: 3, D loss: 0.164776, acc:  80%, G loss: 1.976629\n",
      "Ep: 447, steps: 4, D loss: 0.195199, acc:  77%, G loss: 1.893155\n",
      "Ep: 447, steps: 5, D loss: 0.282978, acc:  46%, G loss: 1.801079\n",
      "Ep: 447, steps: 6, D loss: 0.244131, acc:  55%, G loss: 1.641233\n",
      "Ep: 447, steps: 7, D loss: 0.352422, acc:  22%, G loss: 1.680672\n",
      "Ep: 447, steps: 8, D loss: 0.226508, acc:  64%, G loss: 1.751090\n",
      "Ep: 447, steps: 9, D loss: 0.219540, acc:  68%, G loss: 1.645667\n",
      "Ep: 447, steps: 10, D loss: 0.178174, acc:  81%, G loss: 1.780578\n",
      "Ep: 447, steps: 11, D loss: 0.224759, acc:  62%, G loss: 1.867568\n",
      "Ep: 447, steps: 12, D loss: 0.293746, acc:  35%, G loss: 1.356191\n",
      "Ep: 447, steps: 13, D loss: 0.283048, acc:  39%, G loss: 1.381385\n",
      "Ep: 447, steps: 14, D loss: 0.272503, acc:  44%, G loss: 1.570824\n",
      "Ep: 447, steps: 15, D loss: 0.235867, acc:  61%, G loss: 1.635551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 447, steps: 16, D loss: 0.235353, acc:  61%, G loss: 1.670512\n",
      "Ep: 447, steps: 17, D loss: 0.208788, acc:  72%, G loss: 1.702794\n",
      "Ep: 447, steps: 18, D loss: 0.258559, acc:  55%, G loss: 1.603337\n",
      "Ep: 447, steps: 19, D loss: 0.211602, acc:  68%, G loss: 1.653443\n",
      "Ep: 447, steps: 20, D loss: 0.188155, acc:  74%, G loss: 1.805140\n",
      "Ep: 447, steps: 21, D loss: 0.285940, acc:  37%, G loss: 1.522555\n",
      "Ep: 447, steps: 22, D loss: 0.176973, acc:  77%, G loss: 1.655268\n",
      "Ep: 447, steps: 23, D loss: 0.238752, acc:  61%, G loss: 1.921346\n",
      "Ep: 447, steps: 24, D loss: 0.194429, acc:  74%, G loss: 1.680886\n",
      "Ep: 447, steps: 25, D loss: 0.229255, acc:  61%, G loss: 1.794115\n",
      "Data exhausted, Re Initialize\n",
      "Saved Model\n",
      "Ep: 448, steps: 1, D loss: 0.253219, acc:  55%, G loss: 1.823250\n",
      "Ep: 448, steps: 2, D loss: 0.169777, acc:  77%, G loss: 2.007382\n",
      "Ep: 448, steps: 3, D loss: 0.212636, acc:  70%, G loss: 1.708421\n",
      "Ep: 448, steps: 4, D loss: 0.258400, acc:  53%, G loss: 1.777602\n",
      "Ep: 448, steps: 5, D loss: 0.264425, acc:  52%, G loss: 1.638842\n",
      "Ep: 448, steps: 6, D loss: 0.311196, acc:  34%, G loss: 1.470295\n",
      "Ep: 448, steps: 7, D loss: 0.236404, acc:  60%, G loss: 1.705969\n",
      "Ep: 448, steps: 8, D loss: 0.224254, acc:  68%, G loss: 1.645218\n",
      "Ep: 448, steps: 9, D loss: 0.182698, acc:  78%, G loss: 1.647775\n",
      "Ep: 448, steps: 10, D loss: 0.250215, acc:  52%, G loss: 1.803830\n",
      "Ep: 448, steps: 11, D loss: 0.293648, acc:  33%, G loss: 1.381608\n",
      "Ep: 448, steps: 12, D loss: 0.282062, acc:  38%, G loss: 1.433407\n",
      "Ep: 448, steps: 13, D loss: 0.284445, acc:  35%, G loss: 1.562351\n",
      "Ep: 448, steps: 14, D loss: 0.257377, acc:  48%, G loss: 1.631793\n",
      "Ep: 448, steps: 15, D loss: 0.258183, acc:  54%, G loss: 1.748148\n",
      "Ep: 448, steps: 16, D loss: 0.207466, acc:  73%, G loss: 1.623062\n",
      "Ep: 448, steps: 17, D loss: 0.241439, acc:  59%, G loss: 1.691491\n",
      "Ep: 448, steps: 18, D loss: 0.220838, acc:  64%, G loss: 1.647167\n",
      "Ep: 448, steps: 19, D loss: 0.180571, acc:  76%, G loss: 1.786262\n",
      "Ep: 448, steps: 20, D loss: 0.280348, acc:  36%, G loss: 1.441150\n",
      "Ep: 448, steps: 21, D loss: 0.162480, acc:  82%, G loss: 1.700827\n",
      "Ep: 448, steps: 22, D loss: 0.241277, acc:  61%, G loss: 1.975487\n",
      "Ep: 448, steps: 23, D loss: 0.200611, acc:  73%, G loss: 1.656528\n",
      "Ep: 448, steps: 24, D loss: 0.227967, acc:  60%, G loss: 1.662898\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 449, steps: 1, D loss: 0.244713, acc:  58%, G loss: 1.967223\n",
      "Ep: 449, steps: 2, D loss: 0.249999, acc:  54%, G loss: 1.571234\n",
      "Ep: 449, steps: 3, D loss: 0.169156, acc:  81%, G loss: 2.048845\n",
      "Ep: 449, steps: 4, D loss: 0.184988, acc:  82%, G loss: 1.791426\n",
      "Ep: 449, steps: 5, D loss: 0.273776, acc:  51%, G loss: 1.782687\n",
      "Ep: 449, steps: 6, D loss: 0.260559, acc:  52%, G loss: 1.575290\n",
      "Ep: 449, steps: 7, D loss: 0.377712, acc:  18%, G loss: 1.548057\n",
      "Ep: 449, steps: 8, D loss: 0.238077, acc:  56%, G loss: 1.675438\n",
      "Ep: 449, steps: 9, D loss: 0.223436, acc:  66%, G loss: 1.744405\n",
      "Ep: 449, steps: 10, D loss: 0.184366, acc:  78%, G loss: 1.626169\n",
      "Ep: 449, steps: 11, D loss: 0.233606, acc:  59%, G loss: 1.843959\n",
      "Ep: 449, steps: 12, D loss: 0.286965, acc:  37%, G loss: 1.440119\n",
      "Ep: 449, steps: 13, D loss: 0.288005, acc:  38%, G loss: 1.403404\n",
      "Ep: 449, steps: 14, D loss: 0.282825, acc:  39%, G loss: 1.567229\n",
      "Ep: 449, steps: 15, D loss: 0.255598, acc:  52%, G loss: 1.628947\n",
      "Ep: 449, steps: 16, D loss: 0.237699, acc:  59%, G loss: 1.652313\n",
      "Ep: 449, steps: 17, D loss: 0.206015, acc:  73%, G loss: 1.655561\n",
      "Ep: 449, steps: 18, D loss: 0.245900, acc:  56%, G loss: 1.743031\n",
      "Ep: 449, steps: 19, D loss: 0.220821, acc:  66%, G loss: 1.710294\n",
      "Ep: 449, steps: 20, D loss: 0.190026, acc:  71%, G loss: 1.766026\n",
      "Ep: 449, steps: 21, D loss: 0.284168, acc:  36%, G loss: 1.444850\n",
      "Ep: 449, steps: 22, D loss: 0.167041, acc:  82%, G loss: 1.680493\n",
      "Ep: 449, steps: 23, D loss: 0.232759, acc:  62%, G loss: 1.967563\n",
      "Ep: 449, steps: 24, D loss: 0.214744, acc:  67%, G loss: 1.672727\n",
      "Ep: 449, steps: 25, D loss: 0.246338, acc:  56%, G loss: 1.633937\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 450, steps: 1, D loss: 0.243765, acc:  59%, G loss: 1.818243\n",
      "Ep: 450, steps: 2, D loss: 0.256693, acc:  52%, G loss: 1.550485\n",
      "Ep: 450, steps: 3, D loss: 0.163878, acc:  83%, G loss: 1.989188\n",
      "Ep: 450, steps: 4, D loss: 0.200532, acc:  76%, G loss: 1.862913\n",
      "Ep: 450, steps: 5, D loss: 0.259371, acc:  53%, G loss: 1.742163\n",
      "Ep: 450, steps: 6, D loss: 0.238307, acc:  55%, G loss: 1.637115\n",
      "Ep: 450, steps: 7, D loss: 0.346396, acc:  24%, G loss: 1.606359\n",
      "Ep: 450, steps: 8, D loss: 0.250496, acc:  54%, G loss: 1.791523\n",
      "Ep: 450, steps: 9, D loss: 0.227204, acc:  64%, G loss: 1.680472\n",
      "Ep: 450, steps: 10, D loss: 0.182070, acc:  78%, G loss: 1.702360\n",
      "Ep: 450, steps: 11, D loss: 0.224722, acc:  62%, G loss: 1.783520\n",
      "Ep: 450, steps: 12, D loss: 0.300547, acc:  33%, G loss: 1.353169\n",
      "Ep: 450, steps: 13, D loss: 0.285036, acc:  37%, G loss: 1.421139\n",
      "Ep: 450, steps: 14, D loss: 0.278094, acc:  37%, G loss: 1.570509\n",
      "Ep: 450, steps: 15, D loss: 0.232141, acc:  62%, G loss: 1.576743\n",
      "Ep: 450, steps: 16, D loss: 0.240842, acc:  58%, G loss: 1.619447\n",
      "Ep: 450, steps: 17, D loss: 0.211762, acc:  70%, G loss: 1.742584\n",
      "Ep: 450, steps: 18, D loss: 0.256535, acc:  54%, G loss: 1.657917\n",
      "Ep: 450, steps: 19, D loss: 0.235706, acc:  62%, G loss: 1.635456\n",
      "Ep: 450, steps: 20, D loss: 0.195414, acc:  71%, G loss: 1.798522\n",
      "Ep: 450, steps: 21, D loss: 0.276976, acc:  38%, G loss: 1.546564\n",
      "Ep: 450, steps: 22, D loss: 0.183707, acc:  77%, G loss: 1.620982\n",
      "Ep: 450, steps: 23, D loss: 0.255146, acc:  55%, G loss: 1.847253\n",
      "Saved Model\n",
      "Ep: 450, steps: 24, D loss: 0.212881, acc:  69%, G loss: 1.748351\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 451, steps: 1, D loss: 0.241939, acc:  59%, G loss: 1.862278\n",
      "Ep: 451, steps: 2, D loss: 0.229784, acc:  62%, G loss: 1.552719\n",
      "Ep: 451, steps: 3, D loss: 0.163267, acc:  79%, G loss: 2.035249\n",
      "Ep: 451, steps: 4, D loss: 0.177516, acc:  83%, G loss: 1.953730\n",
      "Ep: 451, steps: 5, D loss: 0.298726, acc:  42%, G loss: 1.706149\n",
      "Ep: 451, steps: 6, D loss: 0.248285, acc:  54%, G loss: 1.703307\n",
      "Ep: 451, steps: 7, D loss: 0.328691, acc:  28%, G loss: 1.439091\n",
      "Ep: 451, steps: 8, D loss: 0.240162, acc:  58%, G loss: 1.753063\n",
      "Ep: 451, steps: 9, D loss: 0.222744, acc:  67%, G loss: 1.637530\n",
      "Ep: 451, steps: 10, D loss: 0.182998, acc:  79%, G loss: 1.714602\n",
      "Ep: 451, steps: 11, D loss: 0.227291, acc:  60%, G loss: 1.857212\n",
      "Ep: 451, steps: 12, D loss: 0.293104, acc:  34%, G loss: 1.375153\n",
      "Ep: 451, steps: 13, D loss: 0.283555, acc:  38%, G loss: 1.437932\n",
      "Ep: 451, steps: 14, D loss: 0.282351, acc:  38%, G loss: 1.516924\n",
      "Ep: 451, steps: 15, D loss: 0.252173, acc:  53%, G loss: 1.609620\n",
      "Ep: 451, steps: 16, D loss: 0.238790, acc:  59%, G loss: 1.686954\n",
      "Ep: 451, steps: 17, D loss: 0.204729, acc:  74%, G loss: 1.654534\n",
      "Ep: 451, steps: 18, D loss: 0.242247, acc:  57%, G loss: 1.647418\n",
      "Ep: 451, steps: 19, D loss: 0.210987, acc:  68%, G loss: 1.601078\n",
      "Ep: 451, steps: 20, D loss: 0.171542, acc:  78%, G loss: 1.845130\n",
      "Ep: 451, steps: 21, D loss: 0.266683, acc:  43%, G loss: 1.622151\n",
      "Ep: 451, steps: 22, D loss: 0.147868, acc:  83%, G loss: 1.635121\n",
      "Ep: 451, steps: 23, D loss: 0.212350, acc:  69%, G loss: 1.932103\n",
      "Ep: 451, steps: 24, D loss: 0.200089, acc:  72%, G loss: 1.641257\n",
      "Ep: 451, steps: 25, D loss: 0.266190, acc:  51%, G loss: 1.868935\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 452, steps: 1, D loss: 0.257672, acc:  53%, G loss: 1.919958\n",
      "Ep: 452, steps: 2, D loss: 0.261461, acc:  51%, G loss: 1.566061\n",
      "Ep: 452, steps: 3, D loss: 0.180526, acc:  74%, G loss: 1.956033\n",
      "Ep: 452, steps: 4, D loss: 0.189208, acc:  81%, G loss: 1.834847\n",
      "Ep: 452, steps: 5, D loss: 0.290744, acc:  39%, G loss: 1.782445\n",
      "Ep: 452, steps: 6, D loss: 0.251016, acc:  54%, G loss: 1.625846\n",
      "Ep: 452, steps: 7, D loss: 0.366983, acc:  22%, G loss: 1.559517\n",
      "Ep: 452, steps: 8, D loss: 0.228308, acc:  64%, G loss: 1.772402\n",
      "Ep: 452, steps: 9, D loss: 0.226064, acc:  65%, G loss: 1.711772\n",
      "Ep: 452, steps: 10, D loss: 0.186294, acc:  79%, G loss: 1.621801\n",
      "Ep: 452, steps: 11, D loss: 0.219100, acc:  65%, G loss: 1.863793\n",
      "Ep: 452, steps: 12, D loss: 0.293542, acc:  35%, G loss: 1.388886\n",
      "Ep: 452, steps: 13, D loss: 0.276579, acc:  41%, G loss: 1.395533\n",
      "Ep: 452, steps: 14, D loss: 0.270286, acc:  45%, G loss: 1.579808\n",
      "Ep: 452, steps: 15, D loss: 0.236778, acc:  61%, G loss: 1.640771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 452, steps: 16, D loss: 0.227632, acc:  63%, G loss: 1.681421\n",
      "Ep: 452, steps: 17, D loss: 0.192632, acc:  76%, G loss: 1.597809\n",
      "Ep: 452, steps: 18, D loss: 0.268739, acc:  49%, G loss: 1.675431\n",
      "Ep: 452, steps: 19, D loss: 0.212949, acc:  68%, G loss: 1.638093\n",
      "Ep: 452, steps: 20, D loss: 0.207325, acc:  68%, G loss: 1.734269\n",
      "Ep: 452, steps: 21, D loss: 0.278375, acc:  36%, G loss: 1.465798\n",
      "Ep: 452, steps: 22, D loss: 0.175651, acc:  78%, G loss: 1.617986\n",
      "Ep: 452, steps: 23, D loss: 0.238963, acc:  60%, G loss: 1.929337\n",
      "Ep: 452, steps: 24, D loss: 0.194875, acc:  74%, G loss: 1.715194\n",
      "Ep: 452, steps: 25, D loss: 0.263166, acc:  53%, G loss: 1.824390\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 453, steps: 1, D loss: 0.237025, acc:  58%, G loss: 1.861171\n",
      "Ep: 453, steps: 2, D loss: 0.238177, acc:  58%, G loss: 1.560579\n",
      "Ep: 453, steps: 3, D loss: 0.155199, acc:  86%, G loss: 1.966638\n",
      "Ep: 453, steps: 4, D loss: 0.188570, acc:  80%, G loss: 1.834935\n",
      "Ep: 453, steps: 5, D loss: 0.276057, acc:  48%, G loss: 1.794368\n",
      "Ep: 453, steps: 6, D loss: 0.275751, acc:  49%, G loss: 1.614870\n",
      "Ep: 453, steps: 7, D loss: 0.320531, acc:  31%, G loss: 1.483018\n",
      "Ep: 453, steps: 8, D loss: 0.225784, acc:  63%, G loss: 1.709059\n",
      "Ep: 453, steps: 9, D loss: 0.221336, acc:  69%, G loss: 1.729350\n",
      "Ep: 453, steps: 10, D loss: 0.192856, acc:  75%, G loss: 1.646304\n",
      "Ep: 453, steps: 11, D loss: 0.219885, acc:  63%, G loss: 1.810713\n",
      "Ep: 453, steps: 12, D loss: 0.291856, acc:  36%, G loss: 1.340525\n",
      "Ep: 453, steps: 13, D loss: 0.276982, acc:  41%, G loss: 1.412598\n",
      "Ep: 453, steps: 14, D loss: 0.268294, acc:  41%, G loss: 1.554763\n",
      "Ep: 453, steps: 15, D loss: 0.251007, acc:  53%, G loss: 1.581119\n",
      "Ep: 453, steps: 16, D loss: 0.239513, acc:  60%, G loss: 1.655253\n",
      "Ep: 453, steps: 17, D loss: 0.205216, acc:  72%, G loss: 1.606406\n",
      "Ep: 453, steps: 18, D loss: 0.251624, acc:  56%, G loss: 1.677607\n",
      "Ep: 453, steps: 19, D loss: 0.220911, acc:  65%, G loss: 1.675965\n",
      "Ep: 453, steps: 20, D loss: 0.180011, acc:  75%, G loss: 1.887294\n",
      "Ep: 453, steps: 21, D loss: 0.286295, acc:  35%, G loss: 1.677228\n",
      "Saved Model\n",
      "Ep: 453, steps: 22, D loss: 0.154770, acc:  84%, G loss: 1.713799\n",
      "Ep: 453, steps: 23, D loss: 0.222655, acc:  65%, G loss: 1.625121\n",
      "Ep: 453, steps: 24, D loss: 0.246770, acc:  55%, G loss: 1.598429\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 454, steps: 1, D loss: 0.269910, acc:  50%, G loss: 1.736217\n",
      "Ep: 454, steps: 2, D loss: 0.237974, acc:  59%, G loss: 1.515296\n",
      "Ep: 454, steps: 3, D loss: 0.175493, acc:  77%, G loss: 1.978199\n",
      "Ep: 454, steps: 4, D loss: 0.195292, acc:  79%, G loss: 1.746654\n",
      "Ep: 454, steps: 5, D loss: 0.287748, acc:  45%, G loss: 1.723979\n",
      "Ep: 454, steps: 6, D loss: 0.225521, acc:  58%, G loss: 1.618037\n",
      "Ep: 454, steps: 7, D loss: 0.347415, acc:  27%, G loss: 1.582171\n",
      "Ep: 454, steps: 8, D loss: 0.241470, acc:  59%, G loss: 1.791844\n",
      "Ep: 454, steps: 9, D loss: 0.257555, acc:  51%, G loss: 1.625238\n",
      "Ep: 454, steps: 10, D loss: 0.185505, acc:  78%, G loss: 1.625155\n",
      "Ep: 454, steps: 11, D loss: 0.225251, acc:  61%, G loss: 1.773731\n",
      "Ep: 454, steps: 12, D loss: 0.300313, acc:  32%, G loss: 1.384490\n",
      "Ep: 454, steps: 13, D loss: 0.289005, acc:  35%, G loss: 1.449987\n",
      "Ep: 454, steps: 14, D loss: 0.278863, acc:  39%, G loss: 1.552863\n",
      "Ep: 454, steps: 15, D loss: 0.225390, acc:  67%, G loss: 1.660856\n",
      "Ep: 454, steps: 16, D loss: 0.236176, acc:  61%, G loss: 1.634500\n",
      "Ep: 454, steps: 17, D loss: 0.189320, acc:  78%, G loss: 1.639096\n",
      "Ep: 454, steps: 18, D loss: 0.241475, acc:  59%, G loss: 1.665120\n",
      "Ep: 454, steps: 19, D loss: 0.214186, acc:  67%, G loss: 1.595078\n",
      "Ep: 454, steps: 20, D loss: 0.206072, acc:  70%, G loss: 1.889327\n",
      "Ep: 454, steps: 21, D loss: 0.278261, acc:  39%, G loss: 1.602975\n",
      "Ep: 454, steps: 22, D loss: 0.178893, acc:  79%, G loss: 1.631863\n",
      "Ep: 454, steps: 23, D loss: 0.243035, acc:  60%, G loss: 1.891739\n",
      "Ep: 454, steps: 24, D loss: 0.205535, acc:  70%, G loss: 1.744272\n",
      "Ep: 454, steps: 25, D loss: 0.237136, acc:  59%, G loss: 1.743197\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 455, steps: 1, D loss: 0.248116, acc:  57%, G loss: 1.737281\n",
      "Ep: 455, steps: 2, D loss: 0.250444, acc:  54%, G loss: 1.581920\n",
      "Ep: 455, steps: 3, D loss: 0.170939, acc:  81%, G loss: 2.065283\n",
      "Ep: 455, steps: 4, D loss: 0.194206, acc:  78%, G loss: 1.825935\n",
      "Ep: 455, steps: 5, D loss: 0.276772, acc:  48%, G loss: 1.712629\n",
      "Ep: 455, steps: 6, D loss: 0.250745, acc:  54%, G loss: 1.630112\n",
      "Ep: 455, steps: 7, D loss: 0.307692, acc:  32%, G loss: 1.418137\n",
      "Ep: 455, steps: 8, D loss: 0.232289, acc:  62%, G loss: 1.818064\n",
      "Ep: 455, steps: 9, D loss: 0.221589, acc:  69%, G loss: 1.668919\n",
      "Ep: 455, steps: 10, D loss: 0.174765, acc:  84%, G loss: 1.666196\n",
      "Ep: 455, steps: 11, D loss: 0.219361, acc:  65%, G loss: 1.863576\n",
      "Ep: 455, steps: 12, D loss: 0.300828, acc:  33%, G loss: 1.469109\n",
      "Ep: 455, steps: 13, D loss: 0.283332, acc:  38%, G loss: 1.473414\n",
      "Ep: 455, steps: 14, D loss: 0.264947, acc:  44%, G loss: 1.545825\n",
      "Ep: 455, steps: 15, D loss: 0.248324, acc:  55%, G loss: 1.663301\n",
      "Ep: 455, steps: 16, D loss: 0.248034, acc:  56%, G loss: 1.704494\n",
      "Ep: 455, steps: 17, D loss: 0.216934, acc:  68%, G loss: 1.621582\n",
      "Ep: 455, steps: 18, D loss: 0.236856, acc:  62%, G loss: 1.663730\n",
      "Ep: 455, steps: 19, D loss: 0.206204, acc:  68%, G loss: 1.639755\n",
      "Ep: 455, steps: 20, D loss: 0.189103, acc:  71%, G loss: 1.822340\n",
      "Ep: 455, steps: 21, D loss: 0.286642, acc:  34%, G loss: 1.597285\n",
      "Ep: 455, steps: 22, D loss: 0.163992, acc:  79%, G loss: 1.624442\n",
      "Ep: 455, steps: 23, D loss: 0.244111, acc:  58%, G loss: 1.911274\n",
      "Ep: 455, steps: 24, D loss: 0.192581, acc:  75%, G loss: 1.636705\n",
      "Ep: 455, steps: 25, D loss: 0.272630, acc:  51%, G loss: 1.720460\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 456, steps: 1, D loss: 0.267163, acc:  52%, G loss: 1.769261\n",
      "Ep: 456, steps: 2, D loss: 0.244715, acc:  55%, G loss: 1.541593\n",
      "Ep: 456, steps: 3, D loss: 0.171602, acc:  81%, G loss: 1.938317\n",
      "Ep: 456, steps: 4, D loss: 0.198715, acc:  79%, G loss: 1.829662\n",
      "Ep: 456, steps: 5, D loss: 0.284544, acc:  45%, G loss: 1.685751\n",
      "Ep: 456, steps: 6, D loss: 0.247128, acc:  54%, G loss: 1.587511\n",
      "Ep: 456, steps: 7, D loss: 0.340075, acc:  23%, G loss: 1.594764\n",
      "Ep: 456, steps: 8, D loss: 0.228859, acc:  61%, G loss: 1.882979\n",
      "Ep: 456, steps: 9, D loss: 0.223356, acc:  65%, G loss: 1.676890\n",
      "Ep: 456, steps: 10, D loss: 0.186398, acc:  79%, G loss: 1.600871\n",
      "Ep: 456, steps: 11, D loss: 0.231394, acc:  59%, G loss: 1.779269\n",
      "Ep: 456, steps: 12, D loss: 0.298584, acc:  32%, G loss: 1.432999\n",
      "Ep: 456, steps: 13, D loss: 0.279795, acc:  38%, G loss: 1.442966\n",
      "Ep: 456, steps: 14, D loss: 0.271033, acc:  43%, G loss: 1.545002\n",
      "Ep: 456, steps: 15, D loss: 0.247584, acc:  55%, G loss: 1.585735\n",
      "Ep: 456, steps: 16, D loss: 0.249713, acc:  58%, G loss: 1.632373\n",
      "Ep: 456, steps: 17, D loss: 0.198967, acc:  75%, G loss: 1.610082\n",
      "Ep: 456, steps: 18, D loss: 0.261934, acc:  53%, G loss: 1.731686\n",
      "Ep: 456, steps: 19, D loss: 0.218123, acc:  67%, G loss: 1.670337\n",
      "Saved Model\n",
      "Ep: 456, steps: 20, D loss: 0.188273, acc:  73%, G loss: 1.813781\n",
      "Ep: 456, steps: 21, D loss: 0.173347, acc:  78%, G loss: 1.645403\n",
      "Ep: 456, steps: 22, D loss: 0.230251, acc:  63%, G loss: 1.966934\n",
      "Ep: 456, steps: 23, D loss: 0.199920, acc:  73%, G loss: 1.632011\n",
      "Ep: 456, steps: 24, D loss: 0.241984, acc:  57%, G loss: 1.646323\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 457, steps: 1, D loss: 0.212732, acc:  70%, G loss: 1.805035\n",
      "Ep: 457, steps: 2, D loss: 0.245869, acc:  56%, G loss: 1.542326\n",
      "Ep: 457, steps: 3, D loss: 0.165413, acc:  82%, G loss: 1.994448\n",
      "Ep: 457, steps: 4, D loss: 0.169955, acc:  86%, G loss: 1.936916\n",
      "Ep: 457, steps: 5, D loss: 0.287960, acc:  45%, G loss: 1.757164\n",
      "Ep: 457, steps: 6, D loss: 0.254287, acc:  54%, G loss: 1.631776\n",
      "Ep: 457, steps: 7, D loss: 0.335254, acc:  27%, G loss: 1.538563\n",
      "Ep: 457, steps: 8, D loss: 0.228481, acc:  61%, G loss: 1.986296\n",
      "Ep: 457, steps: 9, D loss: 0.246661, acc:  59%, G loss: 1.601472\n",
      "Ep: 457, steps: 10, D loss: 0.179643, acc:  79%, G loss: 1.606912\n",
      "Ep: 457, steps: 11, D loss: 0.235346, acc:  60%, G loss: 1.815356\n",
      "Ep: 457, steps: 12, D loss: 0.310842, acc:  30%, G loss: 1.430406\n",
      "Ep: 457, steps: 13, D loss: 0.292521, acc:  36%, G loss: 1.417278\n",
      "Ep: 457, steps: 14, D loss: 0.273866, acc:  41%, G loss: 1.519982\n",
      "Ep: 457, steps: 15, D loss: 0.228172, acc:  66%, G loss: 1.627044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 457, steps: 16, D loss: 0.243818, acc:  57%, G loss: 1.766574\n",
      "Ep: 457, steps: 17, D loss: 0.201961, acc:  73%, G loss: 1.577614\n",
      "Ep: 457, steps: 18, D loss: 0.241820, acc:  58%, G loss: 1.633173\n",
      "Ep: 457, steps: 19, D loss: 0.211143, acc:  68%, G loss: 1.664390\n",
      "Ep: 457, steps: 20, D loss: 0.203857, acc:  66%, G loss: 1.969279\n",
      "Ep: 457, steps: 21, D loss: 0.291719, acc:  35%, G loss: 1.539936\n",
      "Ep: 457, steps: 22, D loss: 0.170161, acc:  80%, G loss: 1.849490\n",
      "Ep: 457, steps: 23, D loss: 0.237512, acc:  60%, G loss: 1.944924\n",
      "Ep: 457, steps: 24, D loss: 0.207817, acc:  69%, G loss: 1.562057\n",
      "Ep: 457, steps: 25, D loss: 0.254866, acc:  54%, G loss: 1.676345\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 458, steps: 1, D loss: 0.261125, acc:  52%, G loss: 1.833855\n",
      "Ep: 458, steps: 2, D loss: 0.245515, acc:  55%, G loss: 1.538786\n",
      "Ep: 458, steps: 3, D loss: 0.182967, acc:  78%, G loss: 1.916049\n",
      "Ep: 458, steps: 4, D loss: 0.196670, acc:  79%, G loss: 1.834075\n",
      "Ep: 458, steps: 5, D loss: 0.283373, acc:  44%, G loss: 1.840828\n",
      "Ep: 458, steps: 6, D loss: 0.243142, acc:  56%, G loss: 1.560266\n",
      "Ep: 458, steps: 7, D loss: 0.338419, acc:  25%, G loss: 1.684612\n",
      "Ep: 458, steps: 8, D loss: 0.233538, acc:  61%, G loss: 1.869585\n",
      "Ep: 458, steps: 9, D loss: 0.231698, acc:  65%, G loss: 1.636875\n",
      "Ep: 458, steps: 10, D loss: 0.195626, acc:  74%, G loss: 1.571068\n",
      "Ep: 458, steps: 11, D loss: 0.231625, acc:  61%, G loss: 1.790445\n",
      "Ep: 458, steps: 12, D loss: 0.288502, acc:  37%, G loss: 1.398649\n",
      "Ep: 458, steps: 13, D loss: 0.282220, acc:  37%, G loss: 1.434887\n",
      "Ep: 458, steps: 14, D loss: 0.281211, acc:  37%, G loss: 1.514217\n",
      "Ep: 458, steps: 15, D loss: 0.250286, acc:  55%, G loss: 1.547709\n",
      "Ep: 458, steps: 16, D loss: 0.235279, acc:  60%, G loss: 1.658216\n",
      "Ep: 458, steps: 17, D loss: 0.216569, acc:  68%, G loss: 1.700778\n",
      "Ep: 458, steps: 18, D loss: 0.253705, acc:  56%, G loss: 1.586507\n",
      "Ep: 458, steps: 19, D loss: 0.214276, acc:  66%, G loss: 1.598457\n",
      "Ep: 458, steps: 20, D loss: 0.176935, acc:  76%, G loss: 1.807441\n",
      "Ep: 458, steps: 21, D loss: 0.267474, acc:  42%, G loss: 1.527243\n",
      "Ep: 458, steps: 22, D loss: 0.158527, acc:  82%, G loss: 1.777326\n",
      "Ep: 458, steps: 23, D loss: 0.230548, acc:  62%, G loss: 1.939207\n",
      "Ep: 458, steps: 24, D loss: 0.205493, acc:  70%, G loss: 1.607357\n",
      "Ep: 458, steps: 25, D loss: 0.259623, acc:  53%, G loss: 1.684490\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 459, steps: 1, D loss: 0.265524, acc:  52%, G loss: 1.949829\n",
      "Ep: 459, steps: 2, D loss: 0.238642, acc:  58%, G loss: 1.544116\n",
      "Ep: 459, steps: 3, D loss: 0.174752, acc:  81%, G loss: 1.984156\n",
      "Ep: 459, steps: 4, D loss: 0.189052, acc:  81%, G loss: 1.780961\n",
      "Ep: 459, steps: 5, D loss: 0.256232, acc:  53%, G loss: 1.717200\n",
      "Ep: 459, steps: 6, D loss: 0.256114, acc:  52%, G loss: 1.646665\n",
      "Ep: 459, steps: 7, D loss: 0.353985, acc:  24%, G loss: 1.506340\n",
      "Ep: 459, steps: 8, D loss: 0.248107, acc:  54%, G loss: 1.840235\n",
      "Ep: 459, steps: 9, D loss: 0.229843, acc:  64%, G loss: 1.699144\n",
      "Ep: 459, steps: 10, D loss: 0.181783, acc:  80%, G loss: 1.625452\n",
      "Ep: 459, steps: 11, D loss: 0.241152, acc:  54%, G loss: 1.829109\n",
      "Ep: 459, steps: 12, D loss: 0.295025, acc:  33%, G loss: 1.387312\n",
      "Ep: 459, steps: 13, D loss: 0.288898, acc:  33%, G loss: 1.445926\n",
      "Ep: 459, steps: 14, D loss: 0.283163, acc:  37%, G loss: 1.512345\n",
      "Ep: 459, steps: 15, D loss: 0.242698, acc:  57%, G loss: 1.598112\n",
      "Ep: 459, steps: 16, D loss: 0.249520, acc:  56%, G loss: 1.636818\n",
      "Ep: 459, steps: 17, D loss: 0.203296, acc:  74%, G loss: 1.675929\n",
      "Saved Model\n",
      "Ep: 459, steps: 18, D loss: 0.244299, acc:  58%, G loss: 1.662260\n",
      "Ep: 459, steps: 19, D loss: 0.188076, acc:  74%, G loss: 1.783042\n",
      "Ep: 459, steps: 20, D loss: 0.257807, acc:  47%, G loss: 1.483712\n",
      "Ep: 459, steps: 21, D loss: 0.176933, acc:  79%, G loss: 1.533004\n",
      "Ep: 459, steps: 22, D loss: 0.248494, acc:  56%, G loss: 1.846686\n",
      "Ep: 459, steps: 23, D loss: 0.197030, acc:  74%, G loss: 1.614550\n",
      "Ep: 459, steps: 24, D loss: 0.238035, acc:  59%, G loss: 1.591184\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 460, steps: 1, D loss: 0.252448, acc:  55%, G loss: 1.790613\n",
      "Ep: 460, steps: 2, D loss: 0.261778, acc:  49%, G loss: 1.616359\n",
      "Ep: 460, steps: 3, D loss: 0.168121, acc:  79%, G loss: 1.993398\n",
      "Ep: 460, steps: 4, D loss: 0.204785, acc:  76%, G loss: 1.825713\n",
      "Ep: 460, steps: 5, D loss: 0.277335, acc:  46%, G loss: 1.780843\n",
      "Ep: 460, steps: 6, D loss: 0.253105, acc:  54%, G loss: 1.619637\n",
      "Ep: 460, steps: 7, D loss: 0.296703, acc:  35%, G loss: 1.422055\n",
      "Ep: 460, steps: 8, D loss: 0.230586, acc:  62%, G loss: 1.706574\n",
      "Ep: 460, steps: 9, D loss: 0.217198, acc:  69%, G loss: 1.629974\n",
      "Ep: 460, steps: 10, D loss: 0.173149, acc:  84%, G loss: 1.555646\n",
      "Ep: 460, steps: 11, D loss: 0.223737, acc:  63%, G loss: 1.816212\n",
      "Ep: 460, steps: 12, D loss: 0.302136, acc:  31%, G loss: 1.452186\n",
      "Ep: 460, steps: 13, D loss: 0.279356, acc:  37%, G loss: 1.441046\n",
      "Ep: 460, steps: 14, D loss: 0.277155, acc:  41%, G loss: 1.537995\n",
      "Ep: 460, steps: 15, D loss: 0.259605, acc:  48%, G loss: 1.551010\n",
      "Ep: 460, steps: 16, D loss: 0.254612, acc:  55%, G loss: 1.649300\n",
      "Ep: 460, steps: 17, D loss: 0.219144, acc:  66%, G loss: 1.625199\n",
      "Ep: 460, steps: 18, D loss: 0.255980, acc:  53%, G loss: 1.526648\n",
      "Ep: 460, steps: 19, D loss: 0.209207, acc:  68%, G loss: 1.664098\n",
      "Ep: 460, steps: 20, D loss: 0.175388, acc:  76%, G loss: 1.773648\n",
      "Ep: 460, steps: 21, D loss: 0.279538, acc:  37%, G loss: 1.467254\n",
      "Ep: 460, steps: 22, D loss: 0.147325, acc:  85%, G loss: 1.632981\n",
      "Ep: 460, steps: 23, D loss: 0.251961, acc:  56%, G loss: 1.985988\n",
      "Ep: 460, steps: 24, D loss: 0.209690, acc:  71%, G loss: 1.671790\n",
      "Ep: 460, steps: 25, D loss: 0.262700, acc:  50%, G loss: 1.831503\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 461, steps: 1, D loss: 0.229471, acc:  65%, G loss: 1.853896\n",
      "Ep: 461, steps: 2, D loss: 0.240163, acc:  56%, G loss: 1.579118\n",
      "Ep: 461, steps: 3, D loss: 0.159616, acc:  84%, G loss: 2.050129\n",
      "Ep: 461, steps: 4, D loss: 0.183483, acc:  83%, G loss: 1.858990\n",
      "Ep: 461, steps: 5, D loss: 0.270606, acc:  49%, G loss: 1.771321\n",
      "Ep: 461, steps: 6, D loss: 0.239088, acc:  55%, G loss: 1.673583\n",
      "Ep: 461, steps: 7, D loss: 0.351091, acc:  24%, G loss: 1.548976\n",
      "Ep: 461, steps: 8, D loss: 0.221026, acc:  65%, G loss: 1.784218\n",
      "Ep: 461, steps: 9, D loss: 0.233302, acc:  62%, G loss: 1.636496\n",
      "Ep: 461, steps: 10, D loss: 0.193516, acc:  76%, G loss: 1.633832\n",
      "Ep: 461, steps: 11, D loss: 0.254290, acc:  50%, G loss: 1.836997\n",
      "Ep: 461, steps: 12, D loss: 0.296317, acc:  33%, G loss: 1.376771\n",
      "Ep: 461, steps: 13, D loss: 0.280953, acc:  39%, G loss: 1.451785\n",
      "Ep: 461, steps: 14, D loss: 0.281085, acc:  37%, G loss: 1.597372\n",
      "Ep: 461, steps: 15, D loss: 0.228698, acc:  64%, G loss: 1.769949\n",
      "Ep: 461, steps: 16, D loss: 0.253452, acc:  53%, G loss: 1.666095\n",
      "Ep: 461, steps: 17, D loss: 0.190437, acc:  77%, G loss: 1.675286\n",
      "Ep: 461, steps: 18, D loss: 0.255830, acc:  53%, G loss: 1.677131\n",
      "Ep: 461, steps: 19, D loss: 0.221515, acc:  66%, G loss: 1.618944\n",
      "Ep: 461, steps: 20, D loss: 0.193839, acc:  74%, G loss: 1.765022\n",
      "Ep: 461, steps: 21, D loss: 0.272289, acc:  39%, G loss: 1.446898\n",
      "Ep: 461, steps: 22, D loss: 0.167093, acc:  81%, G loss: 1.673687\n",
      "Ep: 461, steps: 23, D loss: 0.229200, acc:  62%, G loss: 1.885381\n",
      "Ep: 461, steps: 24, D loss: 0.204967, acc:  70%, G loss: 1.767542\n",
      "Ep: 461, steps: 25, D loss: 0.222922, acc:  64%, G loss: 1.699480\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 462, steps: 1, D loss: 0.264750, acc:  52%, G loss: 1.785284\n",
      "Ep: 462, steps: 2, D loss: 0.240793, acc:  57%, G loss: 1.581173\n",
      "Ep: 462, steps: 3, D loss: 0.177523, acc:  79%, G loss: 2.048496\n",
      "Ep: 462, steps: 4, D loss: 0.186484, acc:  80%, G loss: 1.854860\n",
      "Ep: 462, steps: 5, D loss: 0.290307, acc:  46%, G loss: 1.785276\n",
      "Ep: 462, steps: 6, D loss: 0.257944, acc:  53%, G loss: 1.684557\n",
      "Ep: 462, steps: 7, D loss: 0.317220, acc:  29%, G loss: 1.508102\n",
      "Ep: 462, steps: 8, D loss: 0.226527, acc:  62%, G loss: 1.798161\n",
      "Ep: 462, steps: 9, D loss: 0.213921, acc:  72%, G loss: 1.639650\n",
      "Ep: 462, steps: 10, D loss: 0.189235, acc:  77%, G loss: 1.635526\n",
      "Ep: 462, steps: 11, D loss: 0.253313, acc:  51%, G loss: 1.871838\n",
      "Ep: 462, steps: 12, D loss: 0.282821, acc:  38%, G loss: 1.448132\n",
      "Ep: 462, steps: 13, D loss: 0.285592, acc:  36%, G loss: 1.459782\n",
      "Ep: 462, steps: 14, D loss: 0.278582, acc:  37%, G loss: 1.550130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 462, steps: 15, D loss: 0.230000, acc:  62%, G loss: 1.607321\n",
      "Saved Model\n",
      "Ep: 462, steps: 16, D loss: 0.234526, acc:  60%, G loss: 1.678109\n",
      "Ep: 462, steps: 17, D loss: 0.236666, acc:  60%, G loss: 1.656321\n",
      "Ep: 462, steps: 18, D loss: 0.216538, acc:  66%, G loss: 1.637995\n",
      "Ep: 462, steps: 19, D loss: 0.178194, acc:  75%, G loss: 1.823639\n",
      "Ep: 462, steps: 20, D loss: 0.279278, acc:  39%, G loss: 1.495848\n",
      "Ep: 462, steps: 21, D loss: 0.163230, acc:  79%, G loss: 1.717405\n",
      "Ep: 462, steps: 22, D loss: 0.233826, acc:  60%, G loss: 1.941806\n",
      "Ep: 462, steps: 23, D loss: 0.199468, acc:  73%, G loss: 1.639436\n",
      "Ep: 462, steps: 24, D loss: 0.238879, acc:  56%, G loss: 1.560930\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 463, steps: 1, D loss: 0.242261, acc:  60%, G loss: 1.749651\n",
      "Ep: 463, steps: 2, D loss: 0.250070, acc:  54%, G loss: 1.540435\n",
      "Ep: 463, steps: 3, D loss: 0.171149, acc:  80%, G loss: 2.092189\n",
      "Ep: 463, steps: 4, D loss: 0.195140, acc:  79%, G loss: 1.924327\n",
      "Ep: 463, steps: 5, D loss: 0.271999, acc:  51%, G loss: 1.691905\n",
      "Ep: 463, steps: 6, D loss: 0.244064, acc:  56%, G loss: 1.621157\n",
      "Ep: 463, steps: 7, D loss: 0.356845, acc:  23%, G loss: 1.487981\n",
      "Ep: 463, steps: 8, D loss: 0.221313, acc:  63%, G loss: 1.725761\n",
      "Ep: 463, steps: 9, D loss: 0.227842, acc:  65%, G loss: 1.698585\n",
      "Ep: 463, steps: 10, D loss: 0.187854, acc:  77%, G loss: 1.654042\n",
      "Ep: 463, steps: 11, D loss: 0.210913, acc:  67%, G loss: 1.861851\n",
      "Ep: 463, steps: 12, D loss: 0.316750, acc:  27%, G loss: 1.337078\n",
      "Ep: 463, steps: 13, D loss: 0.286562, acc:  37%, G loss: 1.444025\n",
      "Ep: 463, steps: 14, D loss: 0.274893, acc:  39%, G loss: 1.583836\n",
      "Ep: 463, steps: 15, D loss: 0.247775, acc:  56%, G loss: 1.618904\n",
      "Ep: 463, steps: 16, D loss: 0.258306, acc:  54%, G loss: 1.654233\n",
      "Ep: 463, steps: 17, D loss: 0.216931, acc:  68%, G loss: 1.604036\n",
      "Ep: 463, steps: 18, D loss: 0.238728, acc:  59%, G loss: 1.647226\n",
      "Ep: 463, steps: 19, D loss: 0.214633, acc:  67%, G loss: 1.641757\n",
      "Ep: 463, steps: 20, D loss: 0.193107, acc:  74%, G loss: 1.774181\n",
      "Ep: 463, steps: 21, D loss: 0.283171, acc:  35%, G loss: 1.625900\n",
      "Ep: 463, steps: 22, D loss: 0.180051, acc:  78%, G loss: 1.725916\n",
      "Ep: 463, steps: 23, D loss: 0.217038, acc:  66%, G loss: 1.950226\n",
      "Ep: 463, steps: 24, D loss: 0.201607, acc:  71%, G loss: 1.633315\n",
      "Ep: 463, steps: 25, D loss: 0.266119, acc:  52%, G loss: 1.654233\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 464, steps: 1, D loss: 0.254146, acc:  54%, G loss: 1.734488\n",
      "Ep: 464, steps: 2, D loss: 0.248080, acc:  54%, G loss: 1.522471\n",
      "Ep: 464, steps: 3, D loss: 0.168802, acc:  82%, G loss: 2.041691\n",
      "Ep: 464, steps: 4, D loss: 0.197932, acc:  78%, G loss: 1.873587\n",
      "Ep: 464, steps: 5, D loss: 0.277397, acc:  45%, G loss: 1.728392\n",
      "Ep: 464, steps: 6, D loss: 0.245997, acc:  56%, G loss: 1.609406\n",
      "Ep: 464, steps: 7, D loss: 0.312559, acc:  29%, G loss: 1.561069\n",
      "Ep: 464, steps: 8, D loss: 0.231791, acc:  62%, G loss: 1.734957\n",
      "Ep: 464, steps: 9, D loss: 0.211717, acc:  73%, G loss: 1.669969\n",
      "Ep: 464, steps: 10, D loss: 0.181139, acc:  81%, G loss: 1.607258\n",
      "Ep: 464, steps: 11, D loss: 0.234010, acc:  58%, G loss: 1.784359\n",
      "Ep: 464, steps: 12, D loss: 0.297654, acc:  34%, G loss: 1.409380\n",
      "Ep: 464, steps: 13, D loss: 0.281213, acc:  39%, G loss: 1.533936\n",
      "Ep: 464, steps: 14, D loss: 0.280460, acc:  38%, G loss: 1.476754\n",
      "Ep: 464, steps: 15, D loss: 0.251865, acc:  52%, G loss: 1.593687\n",
      "Ep: 464, steps: 16, D loss: 0.239610, acc:  60%, G loss: 1.646517\n",
      "Ep: 464, steps: 17, D loss: 0.228603, acc:  64%, G loss: 1.617891\n",
      "Ep: 464, steps: 18, D loss: 0.231986, acc:  61%, G loss: 1.589327\n",
      "Ep: 464, steps: 19, D loss: 0.214128, acc:  68%, G loss: 1.633007\n",
      "Ep: 464, steps: 20, D loss: 0.166535, acc:  81%, G loss: 1.842377\n",
      "Ep: 464, steps: 21, D loss: 0.281829, acc:  38%, G loss: 1.548112\n",
      "Ep: 464, steps: 22, D loss: 0.153938, acc:  83%, G loss: 1.654428\n",
      "Ep: 464, steps: 23, D loss: 0.248198, acc:  59%, G loss: 2.022404\n",
      "Ep: 464, steps: 24, D loss: 0.206518, acc:  70%, G loss: 1.608333\n",
      "Ep: 464, steps: 25, D loss: 0.265573, acc:  48%, G loss: 1.556455\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 465, steps: 1, D loss: 0.249673, acc:  58%, G loss: 1.620387\n",
      "Ep: 465, steps: 2, D loss: 0.256371, acc:  51%, G loss: 1.489670\n",
      "Ep: 465, steps: 3, D loss: 0.167582, acc:  81%, G loss: 2.071741\n",
      "Ep: 465, steps: 4, D loss: 0.198563, acc:  76%, G loss: 1.906545\n",
      "Ep: 465, steps: 5, D loss: 0.266216, acc:  50%, G loss: 1.712832\n",
      "Ep: 465, steps: 6, D loss: 0.247753, acc:  53%, G loss: 1.633628\n",
      "Ep: 465, steps: 7, D loss: 0.327915, acc:  29%, G loss: 1.485149\n",
      "Ep: 465, steps: 8, D loss: 0.227412, acc:  64%, G loss: 1.715028\n",
      "Ep: 465, steps: 9, D loss: 0.228888, acc:  65%, G loss: 1.692325\n",
      "Ep: 465, steps: 10, D loss: 0.191520, acc:  77%, G loss: 1.686696\n",
      "Ep: 465, steps: 11, D loss: 0.239478, acc:  57%, G loss: 1.865707\n",
      "Ep: 465, steps: 12, D loss: 0.297544, acc:  33%, G loss: 1.357651\n",
      "Ep: 465, steps: 13, D loss: 0.291315, acc:  32%, G loss: 1.453732\n",
      "Saved Model\n",
      "Ep: 465, steps: 14, D loss: 0.278420, acc:  38%, G loss: 1.529161\n",
      "Ep: 465, steps: 15, D loss: 0.234664, acc:  61%, G loss: 1.722997\n",
      "Ep: 465, steps: 16, D loss: 0.213103, acc:  69%, G loss: 1.576840\n",
      "Ep: 465, steps: 17, D loss: 0.265986, acc:  51%, G loss: 1.531769\n",
      "Ep: 465, steps: 18, D loss: 0.223543, acc:  63%, G loss: 1.608994\n",
      "Ep: 465, steps: 19, D loss: 0.172761, acc:  79%, G loss: 1.842853\n",
      "Ep: 465, steps: 20, D loss: 0.273652, acc:  41%, G loss: 1.513089\n",
      "Ep: 465, steps: 21, D loss: 0.150070, acc:  84%, G loss: 1.704377\n",
      "Ep: 465, steps: 22, D loss: 0.210067, acc:  69%, G loss: 1.877882\n",
      "Ep: 465, steps: 23, D loss: 0.202951, acc:  74%, G loss: 1.683685\n",
      "Ep: 465, steps: 24, D loss: 0.253770, acc:  52%, G loss: 1.776239\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 466, steps: 1, D loss: 0.258564, acc:  53%, G loss: 1.724595\n",
      "Ep: 466, steps: 2, D loss: 0.249169, acc:  55%, G loss: 1.601047\n",
      "Ep: 466, steps: 3, D loss: 0.164703, acc:  81%, G loss: 2.057884\n",
      "Ep: 466, steps: 4, D loss: 0.185628, acc:  82%, G loss: 1.811972\n",
      "Ep: 466, steps: 5, D loss: 0.287430, acc:  47%, G loss: 1.727886\n",
      "Ep: 466, steps: 6, D loss: 0.239421, acc:  55%, G loss: 1.645904\n",
      "Ep: 466, steps: 7, D loss: 0.346417, acc:  23%, G loss: 1.503526\n",
      "Ep: 466, steps: 8, D loss: 0.236735, acc:  59%, G loss: 1.735736\n",
      "Ep: 466, steps: 9, D loss: 0.236290, acc:  61%, G loss: 1.646403\n",
      "Ep: 466, steps: 10, D loss: 0.195456, acc:  76%, G loss: 1.629350\n",
      "Ep: 466, steps: 11, D loss: 0.229657, acc:  60%, G loss: 1.828899\n",
      "Ep: 466, steps: 12, D loss: 0.317876, acc:  27%, G loss: 1.410721\n",
      "Ep: 466, steps: 13, D loss: 0.271070, acc:  43%, G loss: 1.479644\n",
      "Ep: 466, steps: 14, D loss: 0.274137, acc:  43%, G loss: 1.555860\n",
      "Ep: 466, steps: 15, D loss: 0.244333, acc:  56%, G loss: 1.558458\n",
      "Ep: 466, steps: 16, D loss: 0.241947, acc:  57%, G loss: 1.615168\n",
      "Ep: 466, steps: 17, D loss: 0.209496, acc:  71%, G loss: 1.545545\n",
      "Ep: 466, steps: 18, D loss: 0.255861, acc:  53%, G loss: 1.600937\n",
      "Ep: 466, steps: 19, D loss: 0.219967, acc:  67%, G loss: 1.654011\n",
      "Ep: 466, steps: 20, D loss: 0.197966, acc:  72%, G loss: 1.795446\n",
      "Ep: 466, steps: 21, D loss: 0.272299, acc:  39%, G loss: 1.448011\n",
      "Ep: 466, steps: 22, D loss: 0.172971, acc:  80%, G loss: 1.585887\n",
      "Ep: 466, steps: 23, D loss: 0.230314, acc:  62%, G loss: 1.902870\n",
      "Ep: 466, steps: 24, D loss: 0.209715, acc:  70%, G loss: 1.655622\n",
      "Ep: 466, steps: 25, D loss: 0.257857, acc:  53%, G loss: 1.664426\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 467, steps: 1, D loss: 0.234813, acc:  62%, G loss: 1.681751\n",
      "Ep: 467, steps: 2, D loss: 0.253120, acc:  53%, G loss: 1.537828\n",
      "Ep: 467, steps: 3, D loss: 0.158700, acc:  86%, G loss: 1.997587\n",
      "Ep: 467, steps: 4, D loss: 0.194160, acc:  78%, G loss: 1.872826\n",
      "Ep: 467, steps: 5, D loss: 0.262224, acc:  53%, G loss: 1.692231\n",
      "Ep: 467, steps: 6, D loss: 0.228939, acc:  60%, G loss: 1.658056\n",
      "Ep: 467, steps: 7, D loss: 0.316303, acc:  30%, G loss: 1.562611\n",
      "Ep: 467, steps: 8, D loss: 0.239048, acc:  58%, G loss: 1.693171\n",
      "Ep: 467, steps: 9, D loss: 0.226465, acc:  64%, G loss: 1.759064\n",
      "Ep: 467, steps: 10, D loss: 0.175919, acc:  82%, G loss: 1.667419\n",
      "Ep: 467, steps: 11, D loss: 0.221690, acc:  63%, G loss: 1.806830\n",
      "Ep: 467, steps: 12, D loss: 0.291452, acc:  36%, G loss: 1.388466\n",
      "Ep: 467, steps: 13, D loss: 0.291177, acc:  35%, G loss: 1.517386\n",
      "Ep: 467, steps: 14, D loss: 0.276811, acc:  40%, G loss: 1.606249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 467, steps: 15, D loss: 0.245155, acc:  57%, G loss: 1.603487\n",
      "Ep: 467, steps: 16, D loss: 0.240870, acc:  59%, G loss: 1.615872\n",
      "Ep: 467, steps: 17, D loss: 0.208753, acc:  71%, G loss: 1.601293\n",
      "Ep: 467, steps: 18, D loss: 0.248864, acc:  55%, G loss: 1.596582\n",
      "Ep: 467, steps: 19, D loss: 0.212955, acc:  66%, G loss: 1.618049\n",
      "Ep: 467, steps: 20, D loss: 0.172597, acc:  77%, G loss: 1.809519\n",
      "Ep: 467, steps: 21, D loss: 0.268952, acc:  44%, G loss: 1.534826\n",
      "Ep: 467, steps: 22, D loss: 0.153092, acc:  83%, G loss: 1.637952\n",
      "Ep: 467, steps: 23, D loss: 0.230304, acc:  61%, G loss: 1.962634\n",
      "Ep: 467, steps: 24, D loss: 0.203325, acc:  70%, G loss: 1.564915\n",
      "Ep: 467, steps: 25, D loss: 0.279084, acc:  48%, G loss: 1.681823\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 468, steps: 1, D loss: 0.241933, acc:  59%, G loss: 1.723236\n",
      "Ep: 468, steps: 2, D loss: 0.249124, acc:  53%, G loss: 1.470498\n",
      "Ep: 468, steps: 3, D loss: 0.169011, acc:  80%, G loss: 1.997012\n",
      "Ep: 468, steps: 4, D loss: 0.192433, acc:  79%, G loss: 1.746436\n",
      "Ep: 468, steps: 5, D loss: 0.271300, acc:  48%, G loss: 1.759348\n",
      "Ep: 468, steps: 6, D loss: 0.242484, acc:  57%, G loss: 1.688020\n",
      "Ep: 468, steps: 7, D loss: 0.318358, acc:  30%, G loss: 1.551840\n",
      "Ep: 468, steps: 8, D loss: 0.225840, acc:  62%, G loss: 1.722954\n",
      "Ep: 468, steps: 9, D loss: 0.218063, acc:  69%, G loss: 1.632119\n",
      "Ep: 468, steps: 10, D loss: 0.192874, acc:  73%, G loss: 1.619101\n",
      "Ep: 468, steps: 11, D loss: 0.259796, acc:  48%, G loss: 1.951754\n",
      "Saved Model\n",
      "Ep: 468, steps: 12, D loss: 0.306315, acc:  28%, G loss: 1.414866\n",
      "Ep: 468, steps: 13, D loss: 0.286272, acc:  37%, G loss: 1.512541\n",
      "Ep: 468, steps: 14, D loss: 0.232067, acc:  63%, G loss: 1.623833\n",
      "Ep: 468, steps: 15, D loss: 0.244842, acc:  56%, G loss: 1.622751\n",
      "Ep: 468, steps: 16, D loss: 0.210349, acc:  71%, G loss: 1.679879\n",
      "Ep: 468, steps: 17, D loss: 0.237107, acc:  61%, G loss: 1.568640\n",
      "Ep: 468, steps: 18, D loss: 0.207388, acc:  69%, G loss: 1.666264\n",
      "Ep: 468, steps: 19, D loss: 0.190123, acc:  72%, G loss: 1.844597\n",
      "Ep: 468, steps: 20, D loss: 0.282551, acc:  38%, G loss: 1.502830\n",
      "Ep: 468, steps: 21, D loss: 0.172362, acc:  79%, G loss: 1.751418\n",
      "Ep: 468, steps: 22, D loss: 0.244047, acc:  57%, G loss: 2.040339\n",
      "Ep: 468, steps: 23, D loss: 0.200600, acc:  72%, G loss: 1.611284\n",
      "Ep: 468, steps: 24, D loss: 0.258220, acc:  51%, G loss: 1.613293\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 469, steps: 1, D loss: 0.235006, acc:  61%, G loss: 1.748772\n",
      "Ep: 469, steps: 2, D loss: 0.240332, acc:  57%, G loss: 1.528331\n",
      "Ep: 469, steps: 3, D loss: 0.166079, acc:  81%, G loss: 2.059830\n",
      "Ep: 469, steps: 4, D loss: 0.181550, acc:  82%, G loss: 1.893485\n",
      "Ep: 469, steps: 5, D loss: 0.285450, acc:  46%, G loss: 1.771446\n",
      "Ep: 469, steps: 6, D loss: 0.263866, acc:  51%, G loss: 1.656170\n",
      "Ep: 469, steps: 7, D loss: 0.327949, acc:  27%, G loss: 1.499514\n",
      "Ep: 469, steps: 8, D loss: 0.232096, acc:  59%, G loss: 1.793733\n",
      "Ep: 469, steps: 9, D loss: 0.216816, acc:  69%, G loss: 1.685463\n",
      "Ep: 469, steps: 10, D loss: 0.175123, acc:  82%, G loss: 1.626249\n",
      "Ep: 469, steps: 11, D loss: 0.226140, acc:  62%, G loss: 1.725138\n",
      "Ep: 469, steps: 12, D loss: 0.302206, acc:  32%, G loss: 1.422841\n",
      "Ep: 469, steps: 13, D loss: 0.295686, acc:  34%, G loss: 1.494219\n",
      "Ep: 469, steps: 14, D loss: 0.272138, acc:  44%, G loss: 1.520721\n",
      "Ep: 469, steps: 15, D loss: 0.238048, acc:  58%, G loss: 1.563750\n",
      "Ep: 469, steps: 16, D loss: 0.245692, acc:  55%, G loss: 1.663446\n",
      "Ep: 469, steps: 17, D loss: 0.204152, acc:  74%, G loss: 1.709818\n",
      "Ep: 469, steps: 18, D loss: 0.250659, acc:  54%, G loss: 1.597054\n",
      "Ep: 469, steps: 19, D loss: 0.219803, acc:  66%, G loss: 1.634700\n",
      "Ep: 469, steps: 20, D loss: 0.183978, acc:  74%, G loss: 1.881838\n",
      "Ep: 469, steps: 21, D loss: 0.272858, acc:  40%, G loss: 1.480755\n",
      "Ep: 469, steps: 22, D loss: 0.156053, acc:  81%, G loss: 1.631419\n",
      "Ep: 469, steps: 23, D loss: 0.234677, acc:  61%, G loss: 1.908900\n",
      "Ep: 469, steps: 24, D loss: 0.212238, acc:  68%, G loss: 1.700342\n",
      "Ep: 469, steps: 25, D loss: 0.260653, acc:  53%, G loss: 1.541965\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 470, steps: 1, D loss: 0.247529, acc:  58%, G loss: 1.696888\n",
      "Ep: 470, steps: 2, D loss: 0.257339, acc:  51%, G loss: 1.584416\n",
      "Ep: 470, steps: 3, D loss: 0.168125, acc:  82%, G loss: 1.990263\n",
      "Ep: 470, steps: 4, D loss: 0.192938, acc:  78%, G loss: 1.797409\n",
      "Ep: 470, steps: 5, D loss: 0.288350, acc:  45%, G loss: 1.835474\n",
      "Ep: 470, steps: 6, D loss: 0.257020, acc:  54%, G loss: 1.622122\n",
      "Ep: 470, steps: 7, D loss: 0.336253, acc:  25%, G loss: 1.532074\n",
      "Ep: 470, steps: 8, D loss: 0.225527, acc:  63%, G loss: 1.747765\n",
      "Ep: 470, steps: 9, D loss: 0.252097, acc:  55%, G loss: 1.711708\n",
      "Ep: 470, steps: 10, D loss: 0.186093, acc:  78%, G loss: 1.634553\n",
      "Ep: 470, steps: 11, D loss: 0.234698, acc:  58%, G loss: 1.859653\n",
      "Ep: 470, steps: 12, D loss: 0.304464, acc:  29%, G loss: 1.441839\n",
      "Ep: 470, steps: 13, D loss: 0.287130, acc:  36%, G loss: 1.451668\n",
      "Ep: 470, steps: 14, D loss: 0.268804, acc:  42%, G loss: 1.486034\n",
      "Ep: 470, steps: 15, D loss: 0.255156, acc:  50%, G loss: 1.549548\n",
      "Ep: 470, steps: 16, D loss: 0.229877, acc:  62%, G loss: 1.645000\n",
      "Ep: 470, steps: 17, D loss: 0.200752, acc:  75%, G loss: 1.570583\n",
      "Ep: 470, steps: 18, D loss: 0.243251, acc:  58%, G loss: 1.601704\n",
      "Ep: 470, steps: 19, D loss: 0.226048, acc:  65%, G loss: 1.651514\n",
      "Ep: 470, steps: 20, D loss: 0.189280, acc:  74%, G loss: 1.772607\n",
      "Ep: 470, steps: 21, D loss: 0.268770, acc:  40%, G loss: 1.507506\n",
      "Ep: 470, steps: 22, D loss: 0.170290, acc:  79%, G loss: 1.681429\n",
      "Ep: 470, steps: 23, D loss: 0.235270, acc:  60%, G loss: 1.925812\n",
      "Ep: 470, steps: 24, D loss: 0.209152, acc:  69%, G loss: 1.619949\n",
      "Ep: 470, steps: 25, D loss: 0.259575, acc:  52%, G loss: 1.597812\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 471, steps: 1, D loss: 0.247922, acc:  59%, G loss: 1.725321\n",
      "Ep: 471, steps: 2, D loss: 0.246505, acc:  55%, G loss: 1.554348\n",
      "Ep: 471, steps: 3, D loss: 0.159336, acc:  85%, G loss: 1.947988\n",
      "Ep: 471, steps: 4, D loss: 0.191513, acc:  79%, G loss: 1.874027\n",
      "Ep: 471, steps: 5, D loss: 0.272778, acc:  47%, G loss: 1.744455\n",
      "Ep: 471, steps: 6, D loss: 0.230966, acc:  58%, G loss: 1.691671\n",
      "Ep: 471, steps: 7, D loss: 0.346021, acc:  24%, G loss: 1.519220\n",
      "Ep: 471, steps: 8, D loss: 0.233175, acc:  61%, G loss: 1.739330\n",
      "Ep: 471, steps: 9, D loss: 0.223400, acc:  66%, G loss: 1.671257\n",
      "Saved Model\n",
      "Ep: 471, steps: 10, D loss: 0.185487, acc:  79%, G loss: 1.617061\n",
      "Ep: 471, steps: 11, D loss: 0.284194, acc:  37%, G loss: 1.506307\n",
      "Ep: 471, steps: 12, D loss: 0.256087, acc:  49%, G loss: 1.445125\n",
      "Ep: 471, steps: 13, D loss: 0.257908, acc:  51%, G loss: 1.545726\n",
      "Ep: 471, steps: 14, D loss: 0.277120, acc:  40%, G loss: 1.522697\n",
      "Ep: 471, steps: 15, D loss: 0.264639, acc:  50%, G loss: 1.637969\n",
      "Ep: 471, steps: 16, D loss: 0.215475, acc:  68%, G loss: 1.654925\n",
      "Ep: 471, steps: 17, D loss: 0.259965, acc:  52%, G loss: 1.547854\n",
      "Ep: 471, steps: 18, D loss: 0.222314, acc:  64%, G loss: 1.576696\n",
      "Ep: 471, steps: 19, D loss: 0.184666, acc:  76%, G loss: 1.843610\n",
      "Ep: 471, steps: 20, D loss: 0.270445, acc:  40%, G loss: 1.453370\n",
      "Ep: 471, steps: 21, D loss: 0.163310, acc:  82%, G loss: 1.597238\n",
      "Ep: 471, steps: 22, D loss: 0.239696, acc:  59%, G loss: 1.926569\n",
      "Ep: 471, steps: 23, D loss: 0.196771, acc:  74%, G loss: 1.657192\n",
      "Ep: 471, steps: 24, D loss: 0.250580, acc:  56%, G loss: 1.733062\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 472, steps: 1, D loss: 0.258368, acc:  53%, G loss: 1.736624\n",
      "Ep: 472, steps: 2, D loss: 0.246291, acc:  54%, G loss: 1.534040\n",
      "Ep: 472, steps: 3, D loss: 0.166543, acc:  81%, G loss: 2.003994\n",
      "Ep: 472, steps: 4, D loss: 0.192188, acc:  79%, G loss: 1.861474\n",
      "Ep: 472, steps: 5, D loss: 0.284633, acc:  44%, G loss: 1.800319\n",
      "Ep: 472, steps: 6, D loss: 0.234391, acc:  57%, G loss: 1.669842\n",
      "Ep: 472, steps: 7, D loss: 0.310956, acc:  30%, G loss: 1.555810\n",
      "Ep: 472, steps: 8, D loss: 0.239997, acc:  58%, G loss: 1.777141\n",
      "Ep: 472, steps: 9, D loss: 0.231587, acc:  62%, G loss: 1.683451\n",
      "Ep: 472, steps: 10, D loss: 0.186127, acc:  77%, G loss: 1.647742\n",
      "Ep: 472, steps: 11, D loss: 0.250867, acc:  51%, G loss: 1.814853\n",
      "Ep: 472, steps: 12, D loss: 0.289056, acc:  39%, G loss: 1.319718\n",
      "Ep: 472, steps: 13, D loss: 0.285359, acc:  36%, G loss: 1.404305\n",
      "Ep: 472, steps: 14, D loss: 0.275095, acc:  40%, G loss: 1.513548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 472, steps: 15, D loss: 0.257786, acc:  49%, G loss: 1.632542\n",
      "Ep: 472, steps: 16, D loss: 0.253634, acc:  54%, G loss: 1.624338\n",
      "Ep: 472, steps: 17, D loss: 0.206589, acc:  73%, G loss: 1.707027\n",
      "Ep: 472, steps: 18, D loss: 0.241188, acc:  59%, G loss: 1.661556\n",
      "Ep: 472, steps: 19, D loss: 0.216037, acc:  67%, G loss: 1.657474\n",
      "Ep: 472, steps: 20, D loss: 0.185617, acc:  74%, G loss: 1.732391\n",
      "Ep: 472, steps: 21, D loss: 0.274500, acc:  39%, G loss: 1.414739\n",
      "Ep: 472, steps: 22, D loss: 0.152261, acc:  84%, G loss: 1.600649\n",
      "Ep: 472, steps: 23, D loss: 0.240079, acc:  57%, G loss: 1.919253\n",
      "Ep: 472, steps: 24, D loss: 0.199956, acc:  74%, G loss: 1.604882\n",
      "Ep: 472, steps: 25, D loss: 0.260964, acc:  52%, G loss: 1.719548\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 473, steps: 1, D loss: 0.237951, acc:  61%, G loss: 1.745846\n",
      "Ep: 473, steps: 2, D loss: 0.255564, acc:  52%, G loss: 1.557348\n",
      "Ep: 473, steps: 3, D loss: 0.179517, acc:  78%, G loss: 2.007381\n",
      "Ep: 473, steps: 4, D loss: 0.193509, acc:  79%, G loss: 1.805391\n",
      "Ep: 473, steps: 5, D loss: 0.263587, acc:  54%, G loss: 1.740791\n",
      "Ep: 473, steps: 6, D loss: 0.250119, acc:  54%, G loss: 1.712781\n",
      "Ep: 473, steps: 7, D loss: 0.334792, acc:  27%, G loss: 1.559178\n",
      "Ep: 473, steps: 8, D loss: 0.222698, acc:  63%, G loss: 1.853865\n",
      "Ep: 473, steps: 9, D loss: 0.239604, acc:  61%, G loss: 1.694366\n",
      "Ep: 473, steps: 10, D loss: 0.186133, acc:  79%, G loss: 1.584739\n",
      "Ep: 473, steps: 11, D loss: 0.241464, acc:  55%, G loss: 1.798490\n",
      "Ep: 473, steps: 12, D loss: 0.287004, acc:  35%, G loss: 1.388930\n",
      "Ep: 473, steps: 13, D loss: 0.287653, acc:  35%, G loss: 1.446678\n",
      "Ep: 473, steps: 14, D loss: 0.270931, acc:  42%, G loss: 1.533000\n",
      "Ep: 473, steps: 15, D loss: 0.242390, acc:  58%, G loss: 1.604767\n",
      "Ep: 473, steps: 16, D loss: 0.245772, acc:  55%, G loss: 1.663250\n",
      "Ep: 473, steps: 17, D loss: 0.213346, acc:  71%, G loss: 1.562024\n",
      "Ep: 473, steps: 18, D loss: 0.246627, acc:  57%, G loss: 1.576176\n",
      "Ep: 473, steps: 19, D loss: 0.218621, acc:  67%, G loss: 1.611480\n",
      "Ep: 473, steps: 20, D loss: 0.192873, acc:  72%, G loss: 1.743209\n",
      "Ep: 473, steps: 21, D loss: 0.260545, acc:  45%, G loss: 1.484120\n",
      "Ep: 473, steps: 22, D loss: 0.167521, acc:  80%, G loss: 1.714849\n",
      "Ep: 473, steps: 23, D loss: 0.220642, acc:  65%, G loss: 1.892907\n",
      "Ep: 473, steps: 24, D loss: 0.207632, acc:  69%, G loss: 1.590982\n",
      "Ep: 473, steps: 25, D loss: 0.264759, acc:  49%, G loss: 1.790353\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 474, steps: 1, D loss: 0.252747, acc:  57%, G loss: 1.782354\n",
      "Ep: 474, steps: 2, D loss: 0.245445, acc:  56%, G loss: 1.575402\n",
      "Ep: 474, steps: 3, D loss: 0.165664, acc:  82%, G loss: 1.977506\n",
      "Ep: 474, steps: 4, D loss: 0.184526, acc:  81%, G loss: 1.909351\n",
      "Ep: 474, steps: 5, D loss: 0.269679, acc:  48%, G loss: 1.778244\n",
      "Ep: 474, steps: 6, D loss: 0.237194, acc:  57%, G loss: 1.825817\n",
      "Ep: 474, steps: 7, D loss: 0.346503, acc:  24%, G loss: 1.514449\n",
      "Saved Model\n",
      "Ep: 474, steps: 8, D loss: 0.244249, acc:  55%, G loss: 1.777449\n",
      "Ep: 474, steps: 9, D loss: 0.200242, acc:  72%, G loss: 1.604834\n",
      "Ep: 474, steps: 10, D loss: 0.278092, acc:  41%, G loss: 1.732469\n",
      "Ep: 474, steps: 11, D loss: 0.273503, acc:  43%, G loss: 1.387381\n",
      "Ep: 474, steps: 12, D loss: 0.281854, acc:  36%, G loss: 1.430811\n",
      "Ep: 474, steps: 13, D loss: 0.275418, acc:  38%, G loss: 1.535410\n",
      "Ep: 474, steps: 14, D loss: 0.253599, acc:  51%, G loss: 1.641662\n",
      "Ep: 474, steps: 15, D loss: 0.259247, acc:  51%, G loss: 1.636208\n",
      "Ep: 474, steps: 16, D loss: 0.196852, acc:  76%, G loss: 1.659088\n",
      "Ep: 474, steps: 17, D loss: 0.252645, acc:  54%, G loss: 1.591133\n",
      "Ep: 474, steps: 18, D loss: 0.234249, acc:  63%, G loss: 1.630325\n",
      "Ep: 474, steps: 19, D loss: 0.203971, acc:  67%, G loss: 1.748363\n",
      "Ep: 474, steps: 20, D loss: 0.277674, acc:  36%, G loss: 1.597105\n",
      "Ep: 474, steps: 21, D loss: 0.171945, acc:  79%, G loss: 1.596457\n",
      "Ep: 474, steps: 22, D loss: 0.229775, acc:  61%, G loss: 1.926762\n",
      "Ep: 474, steps: 23, D loss: 0.201705, acc:  73%, G loss: 1.604790\n",
      "Ep: 474, steps: 24, D loss: 0.251023, acc:  53%, G loss: 1.738619\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 475, steps: 1, D loss: 0.246183, acc:  59%, G loss: 1.742557\n",
      "Ep: 475, steps: 2, D loss: 0.252604, acc:  53%, G loss: 1.532607\n",
      "Ep: 475, steps: 3, D loss: 0.171974, acc:  81%, G loss: 1.963264\n",
      "Ep: 475, steps: 4, D loss: 0.192727, acc:  79%, G loss: 1.795229\n",
      "Ep: 475, steps: 5, D loss: 0.275963, acc:  49%, G loss: 1.732083\n",
      "Ep: 475, steps: 6, D loss: 0.251940, acc:  52%, G loss: 1.638473\n",
      "Ep: 475, steps: 7, D loss: 0.336856, acc:  27%, G loss: 1.600221\n",
      "Ep: 475, steps: 8, D loss: 0.226181, acc:  63%, G loss: 1.780519\n",
      "Ep: 475, steps: 9, D loss: 0.239408, acc:  61%, G loss: 1.672657\n",
      "Ep: 475, steps: 10, D loss: 0.192428, acc:  79%, G loss: 1.677344\n",
      "Ep: 475, steps: 11, D loss: 0.219540, acc:  63%, G loss: 1.790670\n",
      "Ep: 475, steps: 12, D loss: 0.299885, acc:  33%, G loss: 1.368277\n",
      "Ep: 475, steps: 13, D loss: 0.286207, acc:  39%, G loss: 1.389707\n",
      "Ep: 475, steps: 14, D loss: 0.267918, acc:  44%, G loss: 1.506671\n",
      "Ep: 475, steps: 15, D loss: 0.250291, acc:  53%, G loss: 1.624550\n",
      "Ep: 475, steps: 16, D loss: 0.246183, acc:  57%, G loss: 1.682325\n",
      "Ep: 475, steps: 17, D loss: 0.204084, acc:  74%, G loss: 1.640223\n",
      "Ep: 475, steps: 18, D loss: 0.244381, acc:  57%, G loss: 1.603564\n",
      "Ep: 475, steps: 19, D loss: 0.212551, acc:  68%, G loss: 1.577676\n",
      "Ep: 475, steps: 20, D loss: 0.199902, acc:  70%, G loss: 1.912769\n",
      "Ep: 475, steps: 21, D loss: 0.258837, acc:  48%, G loss: 1.620784\n",
      "Ep: 475, steps: 22, D loss: 0.175160, acc:  75%, G loss: 1.695645\n",
      "Ep: 475, steps: 23, D loss: 0.238932, acc:  60%, G loss: 1.924898\n",
      "Ep: 475, steps: 24, D loss: 0.198741, acc:  70%, G loss: 1.628235\n",
      "Ep: 475, steps: 25, D loss: 0.230440, acc:  61%, G loss: 1.635441\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 476, steps: 1, D loss: 0.256033, acc:  53%, G loss: 1.728648\n",
      "Ep: 476, steps: 2, D loss: 0.252915, acc:  52%, G loss: 1.574942\n",
      "Ep: 476, steps: 3, D loss: 0.161161, acc:  82%, G loss: 2.024286\n",
      "Ep: 476, steps: 4, D loss: 0.188002, acc:  81%, G loss: 1.833305\n",
      "Ep: 476, steps: 5, D loss: 0.285924, acc:  43%, G loss: 1.674018\n",
      "Ep: 476, steps: 6, D loss: 0.243896, acc:  56%, G loss: 1.628994\n",
      "Ep: 476, steps: 7, D loss: 0.329210, acc:  26%, G loss: 1.430794\n",
      "Ep: 476, steps: 8, D loss: 0.231417, acc:  61%, G loss: 1.832826\n",
      "Ep: 476, steps: 9, D loss: 0.217837, acc:  69%, G loss: 1.646331\n",
      "Ep: 476, steps: 10, D loss: 0.197349, acc:  71%, G loss: 1.661164\n",
      "Ep: 476, steps: 11, D loss: 0.251589, acc:  52%, G loss: 1.773819\n",
      "Ep: 476, steps: 12, D loss: 0.293226, acc:  35%, G loss: 1.357776\n",
      "Ep: 476, steps: 13, D loss: 0.282540, acc:  39%, G loss: 1.402593\n",
      "Ep: 476, steps: 14, D loss: 0.275674, acc:  41%, G loss: 1.560780\n",
      "Ep: 476, steps: 15, D loss: 0.249431, acc:  52%, G loss: 1.621258\n",
      "Ep: 476, steps: 16, D loss: 0.241850, acc:  58%, G loss: 1.693263\n",
      "Ep: 476, steps: 17, D loss: 0.203684, acc:  72%, G loss: 1.665222\n",
      "Ep: 476, steps: 18, D loss: 0.239602, acc:  59%, G loss: 1.601116\n",
      "Ep: 476, steps: 19, D loss: 0.210816, acc:  68%, G loss: 1.643406\n",
      "Ep: 476, steps: 20, D loss: 0.192390, acc:  71%, G loss: 1.765177\n",
      "Ep: 476, steps: 21, D loss: 0.259458, acc:  45%, G loss: 1.532537\n",
      "Ep: 476, steps: 22, D loss: 0.170924, acc:  77%, G loss: 1.701243\n",
      "Ep: 476, steps: 23, D loss: 0.235788, acc:  61%, G loss: 1.936950\n",
      "Ep: 476, steps: 24, D loss: 0.199489, acc:  73%, G loss: 1.637299\n",
      "Ep: 476, steps: 25, D loss: 0.249551, acc:  57%, G loss: 1.583469\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 477, steps: 1, D loss: 0.247099, acc:  58%, G loss: 1.787941\n",
      "Ep: 477, steps: 2, D loss: 0.235780, acc:  59%, G loss: 1.511131\n",
      "Ep: 477, steps: 3, D loss: 0.172433, acc:  81%, G loss: 1.963232\n",
      "Ep: 477, steps: 4, D loss: 0.187447, acc:  80%, G loss: 1.859740\n",
      "Ep: 477, steps: 5, D loss: 0.273867, acc:  49%, G loss: 1.727350\n",
      "Saved Model\n",
      "Ep: 477, steps: 6, D loss: 0.247315, acc:  54%, G loss: 1.583199\n",
      "Ep: 477, steps: 7, D loss: 0.254106, acc:  53%, G loss: 1.658806\n",
      "Ep: 477, steps: 8, D loss: 0.176504, acc:  84%, G loss: 1.722770\n",
      "Ep: 477, steps: 9, D loss: 0.142870, acc:  90%, G loss: 1.669024\n",
      "Ep: 477, steps: 10, D loss: 0.237262, acc:  59%, G loss: 1.819602\n",
      "Ep: 477, steps: 11, D loss: 0.340302, acc:  24%, G loss: 1.363515\n",
      "Ep: 477, steps: 12, D loss: 0.316370, acc:  25%, G loss: 1.422047\n",
      "Ep: 477, steps: 13, D loss: 0.292177, acc:  37%, G loss: 1.516373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 477, steps: 14, D loss: 0.246614, acc:  53%, G loss: 1.690562\n",
      "Ep: 477, steps: 15, D loss: 0.244953, acc:  57%, G loss: 1.647977\n",
      "Ep: 477, steps: 16, D loss: 0.206787, acc:  73%, G loss: 1.601722\n",
      "Ep: 477, steps: 17, D loss: 0.237200, acc:  60%, G loss: 1.611824\n",
      "Ep: 477, steps: 18, D loss: 0.197500, acc:  70%, G loss: 1.626361\n",
      "Ep: 477, steps: 19, D loss: 0.170908, acc:  76%, G loss: 1.813630\n",
      "Ep: 477, steps: 20, D loss: 0.285283, acc:  37%, G loss: 1.510973\n",
      "Ep: 477, steps: 21, D loss: 0.164975, acc:  75%, G loss: 1.730266\n",
      "Ep: 477, steps: 22, D loss: 0.232010, acc:  60%, G loss: 1.958923\n",
      "Ep: 477, steps: 23, D loss: 0.212465, acc:  67%, G loss: 1.581460\n",
      "Ep: 477, steps: 24, D loss: 0.255244, acc:  54%, G loss: 1.708139\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 478, steps: 1, D loss: 0.236693, acc:  62%, G loss: 1.719658\n",
      "Ep: 478, steps: 2, D loss: 0.267889, acc:  48%, G loss: 1.501248\n",
      "Ep: 478, steps: 3, D loss: 0.162818, acc:  81%, G loss: 2.045995\n",
      "Ep: 478, steps: 4, D loss: 0.184384, acc:  82%, G loss: 1.911478\n",
      "Ep: 478, steps: 5, D loss: 0.290458, acc:  43%, G loss: 1.704886\n",
      "Ep: 478, steps: 6, D loss: 0.240671, acc:  58%, G loss: 1.629093\n",
      "Ep: 478, steps: 7, D loss: 0.368392, acc:  24%, G loss: 1.532005\n",
      "Ep: 478, steps: 8, D loss: 0.242529, acc:  61%, G loss: 1.777265\n",
      "Ep: 478, steps: 9, D loss: 0.236092, acc:  64%, G loss: 1.625296\n",
      "Ep: 478, steps: 10, D loss: 0.182543, acc:  78%, G loss: 1.619543\n",
      "Ep: 478, steps: 11, D loss: 0.237840, acc:  57%, G loss: 1.844507\n",
      "Ep: 478, steps: 12, D loss: 0.296377, acc:  34%, G loss: 1.376601\n",
      "Ep: 478, steps: 13, D loss: 0.273592, acc:  41%, G loss: 1.435125\n",
      "Ep: 478, steps: 14, D loss: 0.257474, acc:  49%, G loss: 1.533810\n",
      "Ep: 478, steps: 15, D loss: 0.248852, acc:  51%, G loss: 1.634385\n",
      "Ep: 478, steps: 16, D loss: 0.249323, acc:  54%, G loss: 1.627513\n",
      "Ep: 478, steps: 17, D loss: 0.204229, acc:  73%, G loss: 1.606431\n",
      "Ep: 478, steps: 18, D loss: 0.253575, acc:  54%, G loss: 1.589971\n",
      "Ep: 478, steps: 19, D loss: 0.228649, acc:  64%, G loss: 1.592021\n",
      "Ep: 478, steps: 20, D loss: 0.207712, acc:  71%, G loss: 1.704841\n",
      "Ep: 478, steps: 21, D loss: 0.285755, acc:  33%, G loss: 1.473449\n",
      "Ep: 478, steps: 22, D loss: 0.177796, acc:  80%, G loss: 1.646559\n",
      "Ep: 478, steps: 23, D loss: 0.226910, acc:  63%, G loss: 1.926654\n",
      "Ep: 478, steps: 24, D loss: 0.198240, acc:  74%, G loss: 1.590191\n",
      "Ep: 478, steps: 25, D loss: 0.244863, acc:  57%, G loss: 1.716432\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 479, steps: 1, D loss: 0.252398, acc:  55%, G loss: 1.693195\n",
      "Ep: 479, steps: 2, D loss: 0.243230, acc:  56%, G loss: 1.531347\n",
      "Ep: 479, steps: 3, D loss: 0.175516, acc:  78%, G loss: 2.055591\n",
      "Ep: 479, steps: 4, D loss: 0.192329, acc:  81%, G loss: 1.831685\n",
      "Ep: 479, steps: 5, D loss: 0.291570, acc:  42%, G loss: 1.710749\n",
      "Ep: 479, steps: 6, D loss: 0.250555, acc:  54%, G loss: 1.683161\n",
      "Ep: 479, steps: 7, D loss: 0.318630, acc:  29%, G loss: 1.434499\n",
      "Ep: 479, steps: 8, D loss: 0.238171, acc:  59%, G loss: 1.796919\n",
      "Ep: 479, steps: 9, D loss: 0.223365, acc:  69%, G loss: 1.644524\n",
      "Ep: 479, steps: 10, D loss: 0.181457, acc:  79%, G loss: 1.573184\n",
      "Ep: 479, steps: 11, D loss: 0.248422, acc:  53%, G loss: 1.788210\n",
      "Ep: 479, steps: 12, D loss: 0.299757, acc:  29%, G loss: 1.382065\n",
      "Ep: 479, steps: 13, D loss: 0.278646, acc:  37%, G loss: 1.411359\n",
      "Ep: 479, steps: 14, D loss: 0.276142, acc:  38%, G loss: 1.517516\n",
      "Ep: 479, steps: 15, D loss: 0.239205, acc:  58%, G loss: 1.620689\n",
      "Ep: 479, steps: 16, D loss: 0.245527, acc:  56%, G loss: 1.667187\n",
      "Ep: 479, steps: 17, D loss: 0.197975, acc:  76%, G loss: 1.528263\n",
      "Ep: 479, steps: 18, D loss: 0.240237, acc:  60%, G loss: 1.661100\n",
      "Ep: 479, steps: 19, D loss: 0.219444, acc:  65%, G loss: 1.624422\n",
      "Ep: 479, steps: 20, D loss: 0.184953, acc:  73%, G loss: 1.780136\n",
      "Ep: 479, steps: 21, D loss: 0.272249, acc:  40%, G loss: 1.466523\n",
      "Ep: 479, steps: 22, D loss: 0.150137, acc:  84%, G loss: 1.711082\n",
      "Ep: 479, steps: 23, D loss: 0.242450, acc:  56%, G loss: 1.918696\n",
      "Ep: 479, steps: 24, D loss: 0.207959, acc:  69%, G loss: 1.646995\n",
      "Ep: 479, steps: 25, D loss: 0.257748, acc:  51%, G loss: 1.577625\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 480, steps: 1, D loss: 0.245648, acc:  59%, G loss: 1.714470\n",
      "Ep: 480, steps: 2, D loss: 0.246945, acc:  53%, G loss: 1.493336\n",
      "Ep: 480, steps: 3, D loss: 0.164901, acc:  81%, G loss: 1.986946\n",
      "Saved Model\n",
      "Ep: 480, steps: 4, D loss: 0.198503, acc:  78%, G loss: 1.865617\n",
      "Ep: 480, steps: 5, D loss: 0.237861, acc:  54%, G loss: 1.648815\n",
      "Ep: 480, steps: 6, D loss: 0.360132, acc:  22%, G loss: 1.570573\n",
      "Ep: 480, steps: 7, D loss: 0.226317, acc:  63%, G loss: 1.618518\n",
      "Ep: 480, steps: 8, D loss: 0.214186, acc:  71%, G loss: 1.700088\n",
      "Ep: 480, steps: 9, D loss: 0.181258, acc:  77%, G loss: 1.593452\n",
      "Ep: 480, steps: 10, D loss: 0.226397, acc:  62%, G loss: 1.788053\n",
      "Ep: 480, steps: 11, D loss: 0.306666, acc:  33%, G loss: 1.359983\n",
      "Ep: 480, steps: 12, D loss: 0.299591, acc:  33%, G loss: 1.406001\n",
      "Ep: 480, steps: 13, D loss: 0.281719, acc:  40%, G loss: 1.508750\n",
      "Ep: 480, steps: 14, D loss: 0.231713, acc:  63%, G loss: 1.607037\n",
      "Ep: 480, steps: 15, D loss: 0.243762, acc:  56%, G loss: 1.601272\n",
      "Ep: 480, steps: 16, D loss: 0.202383, acc:  73%, G loss: 1.690389\n",
      "Ep: 480, steps: 17, D loss: 0.227548, acc:  64%, G loss: 1.632271\n",
      "Ep: 480, steps: 18, D loss: 0.192464, acc:  75%, G loss: 1.691380\n",
      "Ep: 480, steps: 19, D loss: 0.193702, acc:  73%, G loss: 1.768542\n",
      "Ep: 480, steps: 20, D loss: 0.280539, acc:  37%, G loss: 1.467842\n",
      "Ep: 480, steps: 21, D loss: 0.180678, acc:  78%, G loss: 1.698482\n",
      "Ep: 480, steps: 22, D loss: 0.228264, acc:  63%, G loss: 1.931192\n",
      "Ep: 480, steps: 23, D loss: 0.198589, acc:  73%, G loss: 1.651453\n",
      "Ep: 480, steps: 24, D loss: 0.265585, acc:  54%, G loss: 1.719950\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 481, steps: 1, D loss: 0.228397, acc:  64%, G loss: 1.705894\n",
      "Ep: 481, steps: 2, D loss: 0.230000, acc:  62%, G loss: 1.482848\n",
      "Ep: 481, steps: 3, D loss: 0.165402, acc:  80%, G loss: 1.995370\n",
      "Ep: 481, steps: 4, D loss: 0.190345, acc:  80%, G loss: 1.880753\n",
      "Ep: 481, steps: 5, D loss: 0.294405, acc:  42%, G loss: 1.802299\n",
      "Ep: 481, steps: 6, D loss: 0.236557, acc:  54%, G loss: 1.583234\n",
      "Ep: 481, steps: 7, D loss: 0.317786, acc:  29%, G loss: 1.636493\n",
      "Ep: 481, steps: 8, D loss: 0.231360, acc:  60%, G loss: 1.830587\n",
      "Ep: 481, steps: 9, D loss: 0.212602, acc:  71%, G loss: 1.658204\n",
      "Ep: 481, steps: 10, D loss: 0.184475, acc:  80%, G loss: 1.600609\n",
      "Ep: 481, steps: 11, D loss: 0.243636, acc:  55%, G loss: 1.743298\n",
      "Ep: 481, steps: 12, D loss: 0.287097, acc:  37%, G loss: 1.390971\n",
      "Ep: 481, steps: 13, D loss: 0.269722, acc:  43%, G loss: 1.383359\n",
      "Ep: 481, steps: 14, D loss: 0.274509, acc:  39%, G loss: 1.527086\n",
      "Ep: 481, steps: 15, D loss: 0.264078, acc:  46%, G loss: 1.590909\n",
      "Ep: 481, steps: 16, D loss: 0.250013, acc:  55%, G loss: 1.618444\n",
      "Ep: 481, steps: 17, D loss: 0.193632, acc:  78%, G loss: 1.614627\n",
      "Ep: 481, steps: 18, D loss: 0.246224, acc:  57%, G loss: 1.587794\n",
      "Ep: 481, steps: 19, D loss: 0.215305, acc:  66%, G loss: 1.618872\n",
      "Ep: 481, steps: 20, D loss: 0.179700, acc:  74%, G loss: 1.706367\n",
      "Ep: 481, steps: 21, D loss: 0.250829, acc:  46%, G loss: 1.490019\n",
      "Ep: 481, steps: 22, D loss: 0.152878, acc:  80%, G loss: 1.758519\n",
      "Ep: 481, steps: 23, D loss: 0.234045, acc:  60%, G loss: 1.944482\n",
      "Ep: 481, steps: 24, D loss: 0.200112, acc:  71%, G loss: 1.695177\n",
      "Ep: 481, steps: 25, D loss: 0.256091, acc:  54%, G loss: 1.655560\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 482, steps: 1, D loss: 0.282073, acc:  45%, G loss: 1.640108\n",
      "Ep: 482, steps: 2, D loss: 0.256481, acc:  51%, G loss: 1.508692\n",
      "Ep: 482, steps: 3, D loss: 0.163290, acc:  84%, G loss: 2.037298\n",
      "Ep: 482, steps: 4, D loss: 0.185474, acc:  81%, G loss: 1.927289\n",
      "Ep: 482, steps: 5, D loss: 0.287485, acc:  44%, G loss: 1.767706\n",
      "Ep: 482, steps: 6, D loss: 0.245941, acc:  55%, G loss: 1.567812\n",
      "Ep: 482, steps: 7, D loss: 0.349838, acc:  23%, G loss: 1.423069\n",
      "Ep: 482, steps: 8, D loss: 0.236189, acc:  58%, G loss: 1.827279\n",
      "Ep: 482, steps: 9, D loss: 0.227851, acc:  65%, G loss: 1.655082\n",
      "Ep: 482, steps: 10, D loss: 0.189484, acc:  76%, G loss: 1.558711\n",
      "Ep: 482, steps: 11, D loss: 0.242566, acc:  56%, G loss: 1.805990\n",
      "Ep: 482, steps: 12, D loss: 0.305944, acc:  31%, G loss: 1.346548\n",
      "Ep: 482, steps: 13, D loss: 0.280862, acc:  39%, G loss: 1.382074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 482, steps: 14, D loss: 0.266739, acc:  45%, G loss: 1.569773\n",
      "Ep: 482, steps: 15, D loss: 0.239648, acc:  59%, G loss: 1.701101\n",
      "Ep: 482, steps: 16, D loss: 0.250577, acc:  55%, G loss: 1.624186\n",
      "Ep: 482, steps: 17, D loss: 0.203755, acc:  73%, G loss: 1.563528\n",
      "Ep: 482, steps: 18, D loss: 0.239979, acc:  60%, G loss: 1.654656\n",
      "Ep: 482, steps: 19, D loss: 0.232301, acc:  63%, G loss: 1.609774\n",
      "Ep: 482, steps: 20, D loss: 0.207582, acc:  69%, G loss: 1.732371\n",
      "Ep: 482, steps: 21, D loss: 0.280507, acc:  34%, G loss: 1.561531\n",
      "Ep: 482, steps: 22, D loss: 0.176101, acc:  79%, G loss: 1.590193\n",
      "Ep: 482, steps: 23, D loss: 0.245540, acc:  57%, G loss: 1.868536\n",
      "Ep: 482, steps: 24, D loss: 0.212403, acc:  69%, G loss: 1.617471\n",
      "Ep: 482, steps: 25, D loss: 0.275521, acc:  52%, G loss: 1.782417\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 483, steps: 1, D loss: 0.247775, acc:  59%, G loss: 1.745049\n",
      "Saved Model\n",
      "Ep: 483, steps: 2, D loss: 0.248658, acc:  55%, G loss: 1.468461\n",
      "Ep: 483, steps: 3, D loss: 0.192769, acc:  80%, G loss: 1.612363\n",
      "Ep: 483, steps: 4, D loss: 0.273049, acc:  44%, G loss: 1.653736\n",
      "Ep: 483, steps: 5, D loss: 0.226861, acc:  58%, G loss: 1.583343\n",
      "Ep: 483, steps: 6, D loss: 0.297768, acc:  34%, G loss: 1.467105\n",
      "Ep: 483, steps: 7, D loss: 0.235450, acc:  59%, G loss: 1.661211\n",
      "Ep: 483, steps: 8, D loss: 0.217942, acc:  67%, G loss: 1.649352\n",
      "Ep: 483, steps: 9, D loss: 0.193674, acc:  74%, G loss: 1.641690\n",
      "Ep: 483, steps: 10, D loss: 0.246410, acc:  55%, G loss: 1.775556\n",
      "Ep: 483, steps: 11, D loss: 0.283814, acc:  38%, G loss: 1.375054\n",
      "Ep: 483, steps: 12, D loss: 0.277133, acc:  41%, G loss: 1.390207\n",
      "Ep: 483, steps: 13, D loss: 0.277474, acc:  38%, G loss: 1.538154\n",
      "Ep: 483, steps: 14, D loss: 0.236886, acc:  59%, G loss: 1.652960\n",
      "Ep: 483, steps: 15, D loss: 0.262191, acc:  52%, G loss: 1.619416\n",
      "Ep: 483, steps: 16, D loss: 0.211963, acc:  71%, G loss: 1.601733\n",
      "Ep: 483, steps: 17, D loss: 0.233035, acc:  61%, G loss: 1.620697\n",
      "Ep: 483, steps: 18, D loss: 0.213567, acc:  66%, G loss: 1.608021\n",
      "Ep: 483, steps: 19, D loss: 0.175223, acc:  79%, G loss: 1.831247\n",
      "Ep: 483, steps: 20, D loss: 0.276798, acc:  39%, G loss: 1.453614\n",
      "Ep: 483, steps: 21, D loss: 0.153157, acc:  83%, G loss: 1.648338\n",
      "Ep: 483, steps: 22, D loss: 0.218583, acc:  66%, G loss: 1.910018\n",
      "Ep: 483, steps: 23, D loss: 0.211364, acc:  69%, G loss: 1.655520\n",
      "Ep: 483, steps: 24, D loss: 0.228531, acc:  60%, G loss: 1.611515\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 484, steps: 1, D loss: 0.262133, acc:  54%, G loss: 1.680166\n",
      "Ep: 484, steps: 2, D loss: 0.241431, acc:  57%, G loss: 1.500584\n",
      "Ep: 484, steps: 3, D loss: 0.198218, acc:  72%, G loss: 2.021187\n",
      "Ep: 484, steps: 4, D loss: 0.186824, acc:  81%, G loss: 1.884519\n",
      "Ep: 484, steps: 5, D loss: 0.287574, acc:  46%, G loss: 1.826625\n",
      "Ep: 484, steps: 6, D loss: 0.238267, acc:  57%, G loss: 1.664672\n",
      "Ep: 484, steps: 7, D loss: 0.328768, acc:  27%, G loss: 1.523965\n",
      "Ep: 484, steps: 8, D loss: 0.230682, acc:  61%, G loss: 1.769486\n",
      "Ep: 484, steps: 9, D loss: 0.237132, acc:  62%, G loss: 1.658233\n",
      "Ep: 484, steps: 10, D loss: 0.196389, acc:  73%, G loss: 1.635733\n",
      "Ep: 484, steps: 11, D loss: 0.225784, acc:  63%, G loss: 1.808575\n",
      "Ep: 484, steps: 12, D loss: 0.312640, acc:  29%, G loss: 1.371325\n",
      "Ep: 484, steps: 13, D loss: 0.286477, acc:  38%, G loss: 1.411055\n",
      "Ep: 484, steps: 14, D loss: 0.275170, acc:  41%, G loss: 1.494128\n",
      "Ep: 484, steps: 15, D loss: 0.239128, acc:  57%, G loss: 1.621699\n",
      "Ep: 484, steps: 16, D loss: 0.236488, acc:  60%, G loss: 1.609391\n",
      "Ep: 484, steps: 17, D loss: 0.217464, acc:  68%, G loss: 1.543948\n",
      "Ep: 484, steps: 18, D loss: 0.242683, acc:  58%, G loss: 1.605031\n",
      "Ep: 484, steps: 19, D loss: 0.198677, acc:  72%, G loss: 1.600002\n",
      "Ep: 484, steps: 20, D loss: 0.204560, acc:  69%, G loss: 1.737065\n",
      "Ep: 484, steps: 21, D loss: 0.285947, acc:  34%, G loss: 1.467202\n",
      "Ep: 484, steps: 22, D loss: 0.176266, acc:  78%, G loss: 1.602727\n",
      "Ep: 484, steps: 23, D loss: 0.235045, acc:  60%, G loss: 1.927805\n",
      "Ep: 484, steps: 24, D loss: 0.200101, acc:  73%, G loss: 1.620317\n",
      "Ep: 484, steps: 25, D loss: 0.260330, acc:  53%, G loss: 1.547165\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 485, steps: 1, D loss: 0.253043, acc:  56%, G loss: 1.729361\n",
      "Ep: 485, steps: 2, D loss: 0.242436, acc:  55%, G loss: 1.469875\n",
      "Ep: 485, steps: 3, D loss: 0.174511, acc:  78%, G loss: 1.938778\n",
      "Ep: 485, steps: 4, D loss: 0.192011, acc:  80%, G loss: 1.925800\n",
      "Ep: 485, steps: 5, D loss: 0.271516, acc:  50%, G loss: 1.702214\n",
      "Ep: 485, steps: 6, D loss: 0.234894, acc:  56%, G loss: 1.616517\n",
      "Ep: 485, steps: 7, D loss: 0.330475, acc:  28%, G loss: 1.508905\n",
      "Ep: 485, steps: 8, D loss: 0.239768, acc:  58%, G loss: 1.689633\n",
      "Ep: 485, steps: 9, D loss: 0.228167, acc:  65%, G loss: 1.599034\n",
      "Ep: 485, steps: 10, D loss: 0.185862, acc:  78%, G loss: 1.607752\n",
      "Ep: 485, steps: 11, D loss: 0.245099, acc:  53%, G loss: 1.772028\n",
      "Ep: 485, steps: 12, D loss: 0.291819, acc:  34%, G loss: 1.386721\n",
      "Ep: 485, steps: 13, D loss: 0.293322, acc:  33%, G loss: 1.487089\n",
      "Ep: 485, steps: 14, D loss: 0.263359, acc:  47%, G loss: 1.598803\n",
      "Ep: 485, steps: 15, D loss: 0.250417, acc:  51%, G loss: 1.634035\n",
      "Ep: 485, steps: 16, D loss: 0.237860, acc:  60%, G loss: 1.639501\n",
      "Ep: 485, steps: 17, D loss: 0.211740, acc:  71%, G loss: 1.605862\n",
      "Ep: 485, steps: 18, D loss: 0.237891, acc:  59%, G loss: 1.646403\n",
      "Ep: 485, steps: 19, D loss: 0.210309, acc:  68%, G loss: 1.583257\n",
      "Ep: 485, steps: 20, D loss: 0.192712, acc:  71%, G loss: 1.756997\n",
      "Ep: 485, steps: 21, D loss: 0.269097, acc:  40%, G loss: 1.452234\n",
      "Ep: 485, steps: 22, D loss: 0.151112, acc:  83%, G loss: 1.602901\n",
      "Ep: 485, steps: 23, D loss: 0.232917, acc:  61%, G loss: 1.974183\n",
      "Ep: 485, steps: 24, D loss: 0.219783, acc:  65%, G loss: 1.914447\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 485, steps: 25, D loss: 0.242795, acc:  56%, G loss: 1.655503\n",
      "Ep: 485, steps: 26, D loss: 0.251697, acc:  53%, G loss: 1.544273\n",
      "Ep: 485, steps: 27, D loss: 0.158206, acc:  83%, G loss: 2.046842\n",
      "Ep: 485, steps: 28, D loss: 0.210567, acc:  70%, G loss: 1.831938\n",
      "Ep: 485, steps: 29, D loss: 0.257888, acc:  55%, G loss: 1.732160\n",
      "Ep: 485, steps: 30, D loss: 0.249815, acc:  57%, G loss: 1.693798\n",
      "Ep: 485, steps: 31, D loss: 0.312923, acc:  31%, G loss: 1.543876\n",
      "Ep: 485, steps: 32, D loss: 0.223854, acc:  62%, G loss: 1.892980\n",
      "Ep: 485, steps: 33, D loss: 0.219950, acc:  67%, G loss: 1.800818\n",
      "Ep: 485, steps: 34, D loss: 0.187727, acc:  76%, G loss: 1.610941\n",
      "Ep: 485, steps: 35, D loss: 0.251352, acc:  50%, G loss: 1.850123\n",
      "Ep: 485, steps: 36, D loss: 0.295711, acc:  34%, G loss: 1.440260\n",
      "Ep: 485, steps: 37, D loss: 0.272690, acc:  45%, G loss: 1.469869\n",
      "Ep: 485, steps: 38, D loss: 0.257844, acc:  48%, G loss: 1.562420\n",
      "Ep: 485, steps: 39, D loss: 0.254443, acc:  51%, G loss: 1.638414\n",
      "Ep: 485, steps: 40, D loss: 0.242550, acc:  59%, G loss: 1.616983\n",
      "Ep: 485, steps: 41, D loss: 0.211171, acc:  69%, G loss: 1.672243\n",
      "Ep: 485, steps: 42, D loss: 0.251441, acc:  56%, G loss: 1.619403\n",
      "Ep: 485, steps: 43, D loss: 0.217840, acc:  67%, G loss: 1.609995\n",
      "Ep: 485, steps: 44, D loss: 0.170452, acc:  81%, G loss: 1.765257\n",
      "Ep: 485, steps: 45, D loss: 0.264636, acc:  44%, G loss: 1.511548\n",
      "Ep: 485, steps: 46, D loss: 0.168053, acc:  78%, G loss: 1.635388\n",
      "Ep: 485, steps: 47, D loss: 0.225890, acc:  62%, G loss: 1.894883\n",
      "Ep: 485, steps: 48, D loss: 0.204062, acc:  71%, G loss: 1.699491\n",
      "Ep: 485, steps: 49, D loss: 0.273169, acc:  49%, G loss: 1.734455\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 486, steps: 1, D loss: 0.267874, acc:  52%, G loss: 1.771471\n",
      "Ep: 486, steps: 2, D loss: 0.241558, acc:  57%, G loss: 1.449767\n",
      "Ep: 486, steps: 3, D loss: 0.175934, acc:  79%, G loss: 1.981485\n",
      "Ep: 486, steps: 4, D loss: 0.198306, acc:  78%, G loss: 1.824366\n",
      "Ep: 486, steps: 5, D loss: 0.279945, acc:  46%, G loss: 1.738955\n",
      "Ep: 486, steps: 6, D loss: 0.225640, acc:  59%, G loss: 1.656783\n",
      "Ep: 486, steps: 7, D loss: 0.345122, acc:  22%, G loss: 1.608618\n",
      "Ep: 486, steps: 8, D loss: 0.229113, acc:  61%, G loss: 1.731850\n",
      "Ep: 486, steps: 9, D loss: 0.230311, acc:  65%, G loss: 1.637415\n",
      "Ep: 486, steps: 10, D loss: 0.194133, acc:  73%, G loss: 1.622226\n",
      "Ep: 486, steps: 11, D loss: 0.249303, acc:  51%, G loss: 1.853618\n",
      "Ep: 486, steps: 12, D loss: 0.293056, acc:  34%, G loss: 1.386711\n",
      "Ep: 486, steps: 13, D loss: 0.269533, acc:  44%, G loss: 1.439440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 486, steps: 14, D loss: 0.270246, acc:  43%, G loss: 1.570354\n",
      "Ep: 486, steps: 15, D loss: 0.228390, acc:  63%, G loss: 1.628775\n",
      "Ep: 486, steps: 16, D loss: 0.247304, acc:  57%, G loss: 1.734221\n",
      "Ep: 486, steps: 17, D loss: 0.206909, acc:  73%, G loss: 1.584764\n",
      "Ep: 486, steps: 18, D loss: 0.233746, acc:  62%, G loss: 1.630818\n",
      "Ep: 486, steps: 19, D loss: 0.226512, acc:  64%, G loss: 1.558102\n",
      "Ep: 486, steps: 20, D loss: 0.184415, acc:  76%, G loss: 1.727529\n",
      "Ep: 486, steps: 21, D loss: 0.280059, acc:  37%, G loss: 1.525068\n",
      "Ep: 486, steps: 22, D loss: 0.160196, acc:  81%, G loss: 1.609488\n",
      "Ep: 486, steps: 23, D loss: 0.239026, acc:  59%, G loss: 1.894107\n",
      "Ep: 486, steps: 24, D loss: 0.195395, acc:  74%, G loss: 1.672426\n",
      "Ep: 486, steps: 25, D loss: 0.267212, acc:  50%, G loss: 1.784033\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 487, steps: 1, D loss: 0.262697, acc:  51%, G loss: 1.909907\n",
      "Ep: 487, steps: 2, D loss: 0.236811, acc:  59%, G loss: 1.496334\n",
      "Ep: 487, steps: 3, D loss: 0.168343, acc:  81%, G loss: 1.968287\n",
      "Ep: 487, steps: 4, D loss: 0.182887, acc:  82%, G loss: 1.801437\n",
      "Ep: 487, steps: 5, D loss: 0.280387, acc:  47%, G loss: 1.709906\n",
      "Ep: 487, steps: 6, D loss: 0.240508, acc:  54%, G loss: 1.672774\n",
      "Ep: 487, steps: 7, D loss: 0.312880, acc:  33%, G loss: 1.519263\n",
      "Ep: 487, steps: 8, D loss: 0.228244, acc:  62%, G loss: 1.886977\n",
      "Ep: 487, steps: 9, D loss: 0.222393, acc:  67%, G loss: 1.675447\n",
      "Ep: 487, steps: 10, D loss: 0.189720, acc:  77%, G loss: 1.670712\n",
      "Ep: 487, steps: 11, D loss: 0.250528, acc:  52%, G loss: 1.925733\n",
      "Ep: 487, steps: 12, D loss: 0.290877, acc:  35%, G loss: 1.443312\n",
      "Ep: 487, steps: 13, D loss: 0.274199, acc:  42%, G loss: 1.466032\n",
      "Ep: 487, steps: 14, D loss: 0.271894, acc:  42%, G loss: 1.587621\n",
      "Ep: 487, steps: 15, D loss: 0.236214, acc:  58%, G loss: 1.632303\n",
      "Ep: 487, steps: 16, D loss: 0.249452, acc:  55%, G loss: 1.642424\n",
      "Ep: 487, steps: 17, D loss: 0.208540, acc:  71%, G loss: 1.665045\n",
      "Ep: 487, steps: 18, D loss: 0.249178, acc:  55%, G loss: 1.609341\n",
      "Ep: 487, steps: 19, D loss: 0.219543, acc:  66%, G loss: 1.576418\n",
      "Ep: 487, steps: 20, D loss: 0.190569, acc:  71%, G loss: 1.746085\n",
      "Ep: 487, steps: 21, D loss: 0.274556, acc:  39%, G loss: 1.553946\n",
      "Ep: 487, steps: 22, D loss: 0.144367, acc:  85%, G loss: 1.687749\n",
      "Saved Model\n",
      "Ep: 487, steps: 23, D loss: 0.239789, acc:  60%, G loss: 1.917277\n",
      "Ep: 487, steps: 24, D loss: 0.249041, acc:  53%, G loss: 1.699264\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 488, steps: 1, D loss: 0.250171, acc:  57%, G loss: 1.802742\n",
      "Ep: 488, steps: 2, D loss: 0.246350, acc:  57%, G loss: 1.424306\n",
      "Ep: 488, steps: 3, D loss: 0.186963, acc:  76%, G loss: 1.947854\n",
      "Ep: 488, steps: 4, D loss: 0.195501, acc:  78%, G loss: 1.788791\n",
      "Ep: 488, steps: 5, D loss: 0.295837, acc:  42%, G loss: 1.723602\n",
      "Ep: 488, steps: 6, D loss: 0.232550, acc:  58%, G loss: 1.657884\n",
      "Ep: 488, steps: 7, D loss: 0.344882, acc:  24%, G loss: 1.544441\n",
      "Ep: 488, steps: 8, D loss: 0.243057, acc:  56%, G loss: 1.730492\n",
      "Ep: 488, steps: 9, D loss: 0.220616, acc:  68%, G loss: 1.648288\n",
      "Ep: 488, steps: 10, D loss: 0.191396, acc:  75%, G loss: 1.641020\n",
      "Ep: 488, steps: 11, D loss: 0.232050, acc:  61%, G loss: 1.809400\n",
      "Ep: 488, steps: 12, D loss: 0.286590, acc:  35%, G loss: 1.448525\n",
      "Ep: 488, steps: 13, D loss: 0.280218, acc:  40%, G loss: 1.483780\n",
      "Ep: 488, steps: 14, D loss: 0.273995, acc:  44%, G loss: 1.544530\n",
      "Ep: 488, steps: 15, D loss: 0.257794, acc:  49%, G loss: 1.627398\n",
      "Ep: 488, steps: 16, D loss: 0.238854, acc:  61%, G loss: 1.659863\n",
      "Ep: 488, steps: 17, D loss: 0.206270, acc:  71%, G loss: 1.651904\n",
      "Ep: 488, steps: 18, D loss: 0.232913, acc:  61%, G loss: 1.611126\n",
      "Ep: 488, steps: 19, D loss: 0.224564, acc:  63%, G loss: 1.548488\n",
      "Ep: 488, steps: 20, D loss: 0.204599, acc:  69%, G loss: 1.743897\n",
      "Ep: 488, steps: 21, D loss: 0.283886, acc:  35%, G loss: 1.490246\n",
      "Ep: 488, steps: 22, D loss: 0.168365, acc:  80%, G loss: 1.654106\n",
      "Ep: 488, steps: 23, D loss: 0.230443, acc:  61%, G loss: 1.870447\n",
      "Ep: 488, steps: 24, D loss: 0.196146, acc:  74%, G loss: 1.665827\n",
      "Ep: 488, steps: 25, D loss: 0.255727, acc:  55%, G loss: 1.768197\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 489, steps: 1, D loss: 0.249684, acc:  59%, G loss: 1.745888\n",
      "Ep: 489, steps: 2, D loss: 0.254389, acc:  53%, G loss: 1.442964\n",
      "Ep: 489, steps: 3, D loss: 0.175261, acc:  77%, G loss: 2.005255\n",
      "Ep: 489, steps: 4, D loss: 0.192402, acc:  80%, G loss: 1.795573\n",
      "Ep: 489, steps: 5, D loss: 0.267971, acc:  50%, G loss: 1.693722\n",
      "Ep: 489, steps: 6, D loss: 0.242151, acc:  57%, G loss: 1.654886\n",
      "Ep: 489, steps: 7, D loss: 0.306046, acc:  32%, G loss: 1.450402\n",
      "Ep: 489, steps: 8, D loss: 0.225799, acc:  64%, G loss: 1.733225\n",
      "Ep: 489, steps: 9, D loss: 0.230078, acc:  64%, G loss: 1.639728\n",
      "Ep: 489, steps: 10, D loss: 0.190797, acc:  76%, G loss: 1.626910\n",
      "Ep: 489, steps: 11, D loss: 0.248411, acc:  53%, G loss: 1.905017\n",
      "Ep: 489, steps: 12, D loss: 0.292778, acc:  35%, G loss: 1.489010\n",
      "Ep: 489, steps: 13, D loss: 0.286037, acc:  36%, G loss: 1.512591\n",
      "Ep: 489, steps: 14, D loss: 0.270407, acc:  42%, G loss: 1.559808\n",
      "Ep: 489, steps: 15, D loss: 0.245616, acc:  56%, G loss: 1.509640\n",
      "Ep: 489, steps: 16, D loss: 0.246127, acc:  56%, G loss: 1.719857\n",
      "Ep: 489, steps: 17, D loss: 0.215319, acc:  67%, G loss: 1.607821\n",
      "Ep: 489, steps: 18, D loss: 0.239799, acc:  58%, G loss: 1.627593\n",
      "Ep: 489, steps: 19, D loss: 0.217989, acc:  66%, G loss: 1.600263\n",
      "Ep: 489, steps: 20, D loss: 0.169881, acc:  78%, G loss: 1.795479\n",
      "Ep: 489, steps: 21, D loss: 0.272189, acc:  39%, G loss: 1.492935\n",
      "Ep: 489, steps: 22, D loss: 0.157276, acc:  79%, G loss: 1.676932\n",
      "Ep: 489, steps: 23, D loss: 0.225410, acc:  65%, G loss: 1.936318\n",
      "Ep: 489, steps: 24, D loss: 0.209386, acc:  68%, G loss: 1.639015\n",
      "Ep: 489, steps: 25, D loss: 0.246994, acc:  53%, G loss: 1.671105\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 490, steps: 1, D loss: 0.239671, acc:  59%, G loss: 1.754325\n",
      "Ep: 490, steps: 2, D loss: 0.249593, acc:  54%, G loss: 1.435381\n",
      "Ep: 490, steps: 3, D loss: 0.168381, acc:  84%, G loss: 1.985790\n",
      "Ep: 490, steps: 4, D loss: 0.184818, acc:  81%, G loss: 1.823166\n",
      "Ep: 490, steps: 5, D loss: 0.275112, acc:  50%, G loss: 1.751148\n",
      "Ep: 490, steps: 6, D loss: 0.242449, acc:  57%, G loss: 1.645597\n",
      "Ep: 490, steps: 7, D loss: 0.345431, acc:  25%, G loss: 1.569895\n",
      "Ep: 490, steps: 8, D loss: 0.233463, acc:  60%, G loss: 1.765317\n",
      "Ep: 490, steps: 9, D loss: 0.243719, acc:  59%, G loss: 1.685384\n",
      "Ep: 490, steps: 10, D loss: 0.173077, acc:  83%, G loss: 1.579167\n",
      "Ep: 490, steps: 11, D loss: 0.241600, acc:  56%, G loss: 1.823604\n",
      "Ep: 490, steps: 12, D loss: 0.299759, acc:  33%, G loss: 1.446093\n",
      "Ep: 490, steps: 13, D loss: 0.278993, acc:  40%, G loss: 1.475598\n",
      "Ep: 490, steps: 14, D loss: 0.276335, acc:  40%, G loss: 1.560697\n",
      "Ep: 490, steps: 15, D loss: 0.246995, acc:  53%, G loss: 1.628910\n",
      "Ep: 490, steps: 16, D loss: 0.245464, acc:  57%, G loss: 1.643750\n",
      "Ep: 490, steps: 17, D loss: 0.201595, acc:  76%, G loss: 1.558430\n",
      "Ep: 490, steps: 18, D loss: 0.241433, acc:  57%, G loss: 1.654049\n",
      "Ep: 490, steps: 19, D loss: 0.227114, acc:  65%, G loss: 1.591026\n",
      "Ep: 490, steps: 20, D loss: 0.207141, acc:  68%, G loss: 1.884782\n",
      "Saved Model\n",
      "Ep: 490, steps: 21, D loss: 0.267348, acc:  42%, G loss: 1.509276\n",
      "Ep: 490, steps: 22, D loss: 0.230886, acc:  61%, G loss: 1.903561\n",
      "Ep: 490, steps: 23, D loss: 0.200076, acc:  73%, G loss: 1.684258\n",
      "Ep: 490, steps: 24, D loss: 0.250739, acc:  55%, G loss: 1.664230\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 491, steps: 1, D loss: 0.254123, acc:  55%, G loss: 1.779031\n",
      "Ep: 491, steps: 2, D loss: 0.250954, acc:  52%, G loss: 1.459758\n",
      "Ep: 491, steps: 3, D loss: 0.175454, acc:  78%, G loss: 1.935155\n",
      "Ep: 491, steps: 4, D loss: 0.199778, acc:  78%, G loss: 1.786822\n",
      "Ep: 491, steps: 5, D loss: 0.271940, acc:  47%, G loss: 1.721030\n",
      "Ep: 491, steps: 6, D loss: 0.230881, acc:  56%, G loss: 1.559782\n",
      "Ep: 491, steps: 7, D loss: 0.290142, acc:  38%, G loss: 1.512455\n",
      "Ep: 491, steps: 8, D loss: 0.234955, acc:  59%, G loss: 1.683512\n",
      "Ep: 491, steps: 9, D loss: 0.223560, acc:  66%, G loss: 1.637245\n",
      "Ep: 491, steps: 10, D loss: 0.175605, acc:  84%, G loss: 1.586376\n",
      "Ep: 491, steps: 11, D loss: 0.231334, acc:  61%, G loss: 1.788436\n",
      "Ep: 491, steps: 12, D loss: 0.295967, acc:  34%, G loss: 1.486400\n",
      "Ep: 491, steps: 13, D loss: 0.277945, acc:  41%, G loss: 1.478716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 491, steps: 14, D loss: 0.275391, acc:  37%, G loss: 1.525917\n",
      "Ep: 491, steps: 15, D loss: 0.258500, acc:  48%, G loss: 1.616170\n",
      "Ep: 491, steps: 16, D loss: 0.241369, acc:  57%, G loss: 1.639494\n",
      "Ep: 491, steps: 17, D loss: 0.201913, acc:  72%, G loss: 1.612119\n",
      "Ep: 491, steps: 18, D loss: 0.246017, acc:  58%, G loss: 1.653151\n",
      "Ep: 491, steps: 19, D loss: 0.217222, acc:  65%, G loss: 1.603632\n",
      "Ep: 491, steps: 20, D loss: 0.175531, acc:  77%, G loss: 1.842098\n",
      "Ep: 491, steps: 21, D loss: 0.271905, acc:  40%, G loss: 1.536539\n",
      "Ep: 491, steps: 22, D loss: 0.161112, acc:  78%, G loss: 1.592401\n",
      "Ep: 491, steps: 23, D loss: 0.238088, acc:  58%, G loss: 1.914795\n",
      "Ep: 491, steps: 24, D loss: 0.221533, acc:  65%, G loss: 1.606577\n",
      "Ep: 491, steps: 25, D loss: 0.268488, acc:  50%, G loss: 1.695792\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 492, steps: 1, D loss: 0.241438, acc:  60%, G loss: 1.708256\n",
      "Ep: 492, steps: 2, D loss: 0.244590, acc:  56%, G loss: 1.496723\n",
      "Ep: 492, steps: 3, D loss: 0.173795, acc:  79%, G loss: 1.956624\n",
      "Ep: 492, steps: 4, D loss: 0.193338, acc:  80%, G loss: 1.826177\n",
      "Ep: 492, steps: 5, D loss: 0.274304, acc:  50%, G loss: 1.682942\n",
      "Ep: 492, steps: 6, D loss: 0.237478, acc:  55%, G loss: 1.611354\n",
      "Ep: 492, steps: 7, D loss: 0.340606, acc:  26%, G loss: 1.505217\n",
      "Ep: 492, steps: 8, D loss: 0.224201, acc:  62%, G loss: 1.748258\n",
      "Ep: 492, steps: 9, D loss: 0.220784, acc:  67%, G loss: 1.742610\n",
      "Ep: 492, steps: 10, D loss: 0.182189, acc:  80%, G loss: 1.627068\n",
      "Ep: 492, steps: 11, D loss: 0.246731, acc:  52%, G loss: 1.789573\n",
      "Ep: 492, steps: 12, D loss: 0.291031, acc:  37%, G loss: 1.421240\n",
      "Ep: 492, steps: 13, D loss: 0.277132, acc:  39%, G loss: 1.446754\n",
      "Ep: 492, steps: 14, D loss: 0.263311, acc:  44%, G loss: 1.527686\n",
      "Ep: 492, steps: 15, D loss: 0.246722, acc:  53%, G loss: 1.603529\n",
      "Ep: 492, steps: 16, D loss: 0.244171, acc:  58%, G loss: 1.662889\n",
      "Ep: 492, steps: 17, D loss: 0.210236, acc:  71%, G loss: 1.597969\n",
      "Ep: 492, steps: 18, D loss: 0.234921, acc:  61%, G loss: 1.656967\n",
      "Ep: 492, steps: 19, D loss: 0.223283, acc:  64%, G loss: 1.601214\n",
      "Ep: 492, steps: 20, D loss: 0.185145, acc:  76%, G loss: 1.765315\n",
      "Ep: 492, steps: 21, D loss: 0.272502, acc:  41%, G loss: 1.592500\n",
      "Ep: 492, steps: 22, D loss: 0.162251, acc:  80%, G loss: 1.686358\n",
      "Ep: 492, steps: 23, D loss: 0.231855, acc:  62%, G loss: 1.898533\n",
      "Ep: 492, steps: 24, D loss: 0.213803, acc:  68%, G loss: 1.689818\n",
      "Ep: 492, steps: 25, D loss: 0.243586, acc:  57%, G loss: 1.713742\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 493, steps: 1, D loss: 0.272157, acc:  49%, G loss: 1.776598\n",
      "Ep: 493, steps: 2, D loss: 0.252190, acc:  51%, G loss: 1.477590\n",
      "Ep: 493, steps: 3, D loss: 0.186873, acc:  76%, G loss: 1.936222\n",
      "Ep: 493, steps: 4, D loss: 0.194136, acc:  80%, G loss: 1.793709\n",
      "Ep: 493, steps: 5, D loss: 0.272451, acc:  50%, G loss: 1.761869\n",
      "Ep: 493, steps: 6, D loss: 0.241328, acc:  54%, G loss: 1.572336\n",
      "Ep: 493, steps: 7, D loss: 0.340860, acc:  26%, G loss: 1.570909\n",
      "Ep: 493, steps: 8, D loss: 0.233140, acc:  61%, G loss: 1.747002\n",
      "Ep: 493, steps: 9, D loss: 0.234590, acc:  63%, G loss: 1.664910\n",
      "Ep: 493, steps: 10, D loss: 0.188045, acc:  77%, G loss: 1.570772\n",
      "Ep: 493, steps: 11, D loss: 0.262030, acc:  48%, G loss: 1.766584\n",
      "Ep: 493, steps: 12, D loss: 0.291293, acc:  36%, G loss: 1.395959\n",
      "Ep: 493, steps: 13, D loss: 0.271557, acc:  42%, G loss: 1.462643\n",
      "Ep: 493, steps: 14, D loss: 0.266630, acc:  43%, G loss: 1.540308\n",
      "Ep: 493, steps: 15, D loss: 0.259760, acc:  48%, G loss: 1.625512\n",
      "Ep: 493, steps: 16, D loss: 0.243544, acc:  58%, G loss: 1.636992\n",
      "Ep: 493, steps: 17, D loss: 0.199810, acc:  76%, G loss: 1.608678\n",
      "Ep: 493, steps: 18, D loss: 0.242832, acc:  57%, G loss: 1.663745\n",
      "Saved Model\n",
      "Ep: 493, steps: 19, D loss: 0.213180, acc:  68%, G loss: 1.579893\n",
      "Ep: 493, steps: 20, D loss: 0.263844, acc:  45%, G loss: 1.546947\n",
      "Ep: 493, steps: 21, D loss: 0.199395, acc:  68%, G loss: 1.660541\n",
      "Ep: 493, steps: 22, D loss: 0.226387, acc:  64%, G loss: 1.793816\n",
      "Ep: 493, steps: 23, D loss: 0.200134, acc:  73%, G loss: 1.581408\n",
      "Ep: 493, steps: 24, D loss: 0.278575, acc:  52%, G loss: 2.080136\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 494, steps: 1, D loss: 0.249730, acc:  54%, G loss: 1.861247\n",
      "Ep: 494, steps: 2, D loss: 0.240341, acc:  58%, G loss: 1.482000\n",
      "Ep: 494, steps: 3, D loss: 0.164833, acc:  80%, G loss: 1.975440\n",
      "Ep: 494, steps: 4, D loss: 0.179166, acc:  83%, G loss: 1.747439\n",
      "Ep: 494, steps: 5, D loss: 0.255057, acc:  52%, G loss: 1.673524\n",
      "Ep: 494, steps: 6, D loss: 0.249020, acc:  54%, G loss: 1.601516\n",
      "Ep: 494, steps: 7, D loss: 0.321528, acc:  30%, G loss: 1.494707\n",
      "Ep: 494, steps: 8, D loss: 0.218119, acc:  65%, G loss: 1.737873\n",
      "Ep: 494, steps: 9, D loss: 0.219067, acc:  70%, G loss: 1.623278\n",
      "Ep: 494, steps: 10, D loss: 0.173012, acc:  83%, G loss: 1.600515\n",
      "Ep: 494, steps: 11, D loss: 0.258733, acc:  50%, G loss: 1.788858\n",
      "Ep: 494, steps: 12, D loss: 0.281678, acc:  39%, G loss: 1.464219\n",
      "Ep: 494, steps: 13, D loss: 0.274769, acc:  43%, G loss: 1.497734\n",
      "Ep: 494, steps: 14, D loss: 0.266045, acc:  44%, G loss: 1.577124\n",
      "Ep: 494, steps: 15, D loss: 0.242773, acc:  56%, G loss: 1.568431\n",
      "Ep: 494, steps: 16, D loss: 0.247646, acc:  57%, G loss: 1.613160\n",
      "Ep: 494, steps: 17, D loss: 0.199843, acc:  75%, G loss: 1.569765\n",
      "Ep: 494, steps: 18, D loss: 0.258227, acc:  53%, G loss: 1.610474\n",
      "Ep: 494, steps: 19, D loss: 0.214759, acc:  67%, G loss: 1.597859\n",
      "Ep: 494, steps: 20, D loss: 0.184429, acc:  76%, G loss: 1.827757\n",
      "Ep: 494, steps: 21, D loss: 0.275483, acc:  41%, G loss: 1.625011\n",
      "Ep: 494, steps: 22, D loss: 0.126755, acc:  87%, G loss: 1.679500\n",
      "Ep: 494, steps: 23, D loss: 0.250097, acc:  56%, G loss: 1.855405\n",
      "Ep: 494, steps: 24, D loss: 0.200089, acc:  72%, G loss: 1.642063\n",
      "Ep: 494, steps: 25, D loss: 0.223347, acc:  63%, G loss: 1.829383\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 495, steps: 1, D loss: 0.251026, acc:  58%, G loss: 1.783492\n",
      "Ep: 495, steps: 2, D loss: 0.258914, acc:  51%, G loss: 1.478711\n",
      "Ep: 495, steps: 3, D loss: 0.171420, acc:  80%, G loss: 1.972678\n",
      "Ep: 495, steps: 4, D loss: 0.202118, acc:  76%, G loss: 1.767129\n",
      "Ep: 495, steps: 5, D loss: 0.297246, acc:  45%, G loss: 1.673752\n",
      "Ep: 495, steps: 6, D loss: 0.245973, acc:  55%, G loss: 1.642036\n",
      "Ep: 495, steps: 7, D loss: 0.352391, acc:  24%, G loss: 1.600562\n",
      "Ep: 495, steps: 8, D loss: 0.245134, acc:  56%, G loss: 1.707564\n",
      "Ep: 495, steps: 9, D loss: 0.227271, acc:  65%, G loss: 1.658525\n",
      "Ep: 495, steps: 10, D loss: 0.179489, acc:  80%, G loss: 1.622008\n",
      "Ep: 495, steps: 11, D loss: 0.240133, acc:  56%, G loss: 1.868260\n",
      "Ep: 495, steps: 12, D loss: 0.299287, acc:  33%, G loss: 1.436732\n",
      "Ep: 495, steps: 13, D loss: 0.271702, acc:  44%, G loss: 1.512089\n",
      "Ep: 495, steps: 14, D loss: 0.277152, acc:  40%, G loss: 1.554405\n",
      "Ep: 495, steps: 15, D loss: 0.240626, acc:  58%, G loss: 1.575474\n",
      "Ep: 495, steps: 16, D loss: 0.234129, acc:  63%, G loss: 1.551203\n",
      "Ep: 495, steps: 17, D loss: 0.217411, acc:  68%, G loss: 1.661612\n",
      "Ep: 495, steps: 18, D loss: 0.249020, acc:  56%, G loss: 1.567140\n",
      "Ep: 495, steps: 19, D loss: 0.217217, acc:  67%, G loss: 1.618636\n",
      "Ep: 495, steps: 20, D loss: 0.190027, acc:  76%, G loss: 1.778318\n",
      "Ep: 495, steps: 21, D loss: 0.274494, acc:  39%, G loss: 1.423852\n",
      "Ep: 495, steps: 22, D loss: 0.170119, acc:  81%, G loss: 1.582135\n",
      "Ep: 495, steps: 23, D loss: 0.244881, acc:  57%, G loss: 1.861046\n",
      "Ep: 495, steps: 24, D loss: 0.198195, acc:  73%, G loss: 1.596840\n",
      "Ep: 495, steps: 25, D loss: 0.232037, acc:  62%, G loss: 1.740743\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 496, steps: 1, D loss: 0.259029, acc:  53%, G loss: 1.801397\n",
      "Ep: 496, steps: 2, D loss: 0.242627, acc:  57%, G loss: 1.567060\n",
      "Ep: 496, steps: 3, D loss: 0.167642, acc:  82%, G loss: 1.955874\n",
      "Ep: 496, steps: 4, D loss: 0.198211, acc:  77%, G loss: 1.819802\n",
      "Ep: 496, steps: 5, D loss: 0.282744, acc:  45%, G loss: 1.651434\n",
      "Ep: 496, steps: 6, D loss: 0.233728, acc:  58%, G loss: 1.612959\n",
      "Ep: 496, steps: 7, D loss: 0.313523, acc:  31%, G loss: 1.455049\n",
      "Ep: 496, steps: 8, D loss: 0.233404, acc:  59%, G loss: 1.752448\n",
      "Ep: 496, steps: 9, D loss: 0.238045, acc:  61%, G loss: 1.696163\n",
      "Ep: 496, steps: 10, D loss: 0.178399, acc:  84%, G loss: 1.603531\n",
      "Ep: 496, steps: 11, D loss: 0.241532, acc:  54%, G loss: 1.792756\n",
      "Ep: 496, steps: 12, D loss: 0.302224, acc:  31%, G loss: 1.422498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 496, steps: 13, D loss: 0.272310, acc:  42%, G loss: 1.530818\n",
      "Ep: 496, steps: 14, D loss: 0.263127, acc:  44%, G loss: 1.503881\n",
      "Ep: 496, steps: 15, D loss: 0.256021, acc:  48%, G loss: 1.576824\n",
      "Ep: 496, steps: 16, D loss: 0.248982, acc:  57%, G loss: 1.615304\n",
      "Saved Model\n",
      "Ep: 496, steps: 17, D loss: 0.207148, acc:  72%, G loss: 1.634911\n",
      "Ep: 496, steps: 18, D loss: 0.206193, acc:  69%, G loss: 1.594326\n",
      "Ep: 496, steps: 19, D loss: 0.188128, acc:  74%, G loss: 1.872390\n",
      "Ep: 496, steps: 20, D loss: 0.262790, acc:  43%, G loss: 1.539988\n",
      "Ep: 496, steps: 21, D loss: 0.151003, acc:  82%, G loss: 1.629305\n",
      "Ep: 496, steps: 22, D loss: 0.237938, acc:  61%, G loss: 1.967586\n",
      "Ep: 496, steps: 23, D loss: 0.202439, acc:  71%, G loss: 1.682845\n",
      "Ep: 496, steps: 24, D loss: 0.237551, acc:  57%, G loss: 1.677671\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 497, steps: 1, D loss: 0.258642, acc:  53%, G loss: 1.713607\n",
      "Ep: 497, steps: 2, D loss: 0.247395, acc:  54%, G loss: 1.553986\n",
      "Ep: 497, steps: 3, D loss: 0.165526, acc:  83%, G loss: 1.993526\n",
      "Ep: 497, steps: 4, D loss: 0.199434, acc:  77%, G loss: 1.825376\n",
      "Ep: 497, steps: 5, D loss: 0.279488, acc:  48%, G loss: 1.684086\n",
      "Ep: 497, steps: 6, D loss: 0.237953, acc:  59%, G loss: 1.583120\n",
      "Ep: 497, steps: 7, D loss: 0.342524, acc:  23%, G loss: 1.549757\n",
      "Ep: 497, steps: 8, D loss: 0.209655, acc:  68%, G loss: 1.778457\n",
      "Ep: 497, steps: 9, D loss: 0.243542, acc:  56%, G loss: 1.698965\n",
      "Ep: 497, steps: 10, D loss: 0.192691, acc:  75%, G loss: 1.650199\n",
      "Ep: 497, steps: 11, D loss: 0.248123, acc:  53%, G loss: 1.801676\n",
      "Ep: 497, steps: 12, D loss: 0.284009, acc:  38%, G loss: 1.436192\n",
      "Ep: 497, steps: 13, D loss: 0.283267, acc:  41%, G loss: 1.431017\n",
      "Ep: 497, steps: 14, D loss: 0.279825, acc:  41%, G loss: 1.621768\n",
      "Ep: 497, steps: 15, D loss: 0.256999, acc:  48%, G loss: 1.663420\n",
      "Ep: 497, steps: 16, D loss: 0.254779, acc:  53%, G loss: 1.613224\n",
      "Ep: 497, steps: 17, D loss: 0.203893, acc:  72%, G loss: 1.655667\n",
      "Ep: 497, steps: 18, D loss: 0.249513, acc:  56%, G loss: 1.568423\n",
      "Ep: 497, steps: 19, D loss: 0.224568, acc:  65%, G loss: 1.622554\n",
      "Ep: 497, steps: 20, D loss: 0.177581, acc:  76%, G loss: 1.815337\n",
      "Ep: 497, steps: 21, D loss: 0.260088, acc:  46%, G loss: 1.483961\n",
      "Ep: 497, steps: 22, D loss: 0.155458, acc:  81%, G loss: 1.619517\n",
      "Ep: 497, steps: 23, D loss: 0.240286, acc:  57%, G loss: 1.923196\n",
      "Ep: 497, steps: 24, D loss: 0.199369, acc:  74%, G loss: 1.568328\n",
      "Ep: 497, steps: 25, D loss: 0.261444, acc:  51%, G loss: 1.557640\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 498, steps: 1, D loss: 0.243581, acc:  58%, G loss: 1.821609\n",
      "Ep: 498, steps: 2, D loss: 0.244724, acc:  58%, G loss: 1.487702\n",
      "Ep: 498, steps: 3, D loss: 0.184375, acc:  74%, G loss: 1.970059\n",
      "Ep: 498, steps: 4, D loss: 0.190371, acc:  82%, G loss: 1.878551\n",
      "Ep: 498, steps: 5, D loss: 0.281981, acc:  45%, G loss: 1.814166\n",
      "Ep: 498, steps: 6, D loss: 0.223767, acc:  59%, G loss: 1.672268\n",
      "Ep: 498, steps: 7, D loss: 0.328787, acc:  27%, G loss: 1.502102\n",
      "Ep: 498, steps: 8, D loss: 0.235265, acc:  57%, G loss: 1.666906\n",
      "Ep: 498, steps: 9, D loss: 0.223949, acc:  65%, G loss: 1.659154\n",
      "Ep: 498, steps: 10, D loss: 0.189169, acc:  77%, G loss: 1.582207\n",
      "Ep: 498, steps: 11, D loss: 0.245211, acc:  53%, G loss: 1.832131\n",
      "Ep: 498, steps: 12, D loss: 0.295005, acc:  33%, G loss: 1.416651\n",
      "Ep: 498, steps: 13, D loss: 0.279914, acc:  39%, G loss: 1.483616\n",
      "Ep: 498, steps: 14, D loss: 0.279751, acc:  39%, G loss: 1.575089\n",
      "Ep: 498, steps: 15, D loss: 0.259531, acc:  48%, G loss: 1.611869\n",
      "Ep: 498, steps: 16, D loss: 0.239113, acc:  59%, G loss: 1.604335\n",
      "Ep: 498, steps: 17, D loss: 0.201619, acc:  75%, G loss: 1.661366\n",
      "Ep: 498, steps: 18, D loss: 0.246526, acc:  56%, G loss: 1.561380\n",
      "Ep: 498, steps: 19, D loss: 0.203617, acc:  72%, G loss: 1.604474\n",
      "Ep: 498, steps: 20, D loss: 0.180191, acc:  78%, G loss: 1.806928\n",
      "Ep: 498, steps: 21, D loss: 0.275732, acc:  38%, G loss: 1.458654\n",
      "Ep: 498, steps: 22, D loss: 0.165204, acc:  78%, G loss: 1.696627\n",
      "Ep: 498, steps: 23, D loss: 0.226606, acc:  63%, G loss: 1.917035\n",
      "Ep: 498, steps: 24, D loss: 0.211623, acc:  68%, G loss: 1.636382\n",
      "Ep: 498, steps: 25, D loss: 0.260149, acc:  51%, G loss: 1.635136\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 499, steps: 1, D loss: 0.232462, acc:  62%, G loss: 1.816279\n",
      "Ep: 499, steps: 2, D loss: 0.236094, acc:  60%, G loss: 1.593165\n",
      "Ep: 499, steps: 3, D loss: 0.177005, acc:  80%, G loss: 2.025417\n",
      "Ep: 499, steps: 4, D loss: 0.180163, acc:  84%, G loss: 1.877315\n",
      "Ep: 499, steps: 5, D loss: 0.284197, acc:  46%, G loss: 1.698764\n",
      "Ep: 499, steps: 6, D loss: 0.244419, acc:  56%, G loss: 1.696414\n",
      "Ep: 499, steps: 7, D loss: 0.323817, acc:  29%, G loss: 1.549037\n",
      "Ep: 499, steps: 8, D loss: 0.233726, acc:  59%, G loss: 1.736161\n",
      "Ep: 499, steps: 9, D loss: 0.223907, acc:  65%, G loss: 1.650190\n",
      "Ep: 499, steps: 10, D loss: 0.189560, acc:  78%, G loss: 1.581940\n",
      "Ep: 499, steps: 11, D loss: 0.260364, acc:  49%, G loss: 1.837573\n",
      "Ep: 499, steps: 12, D loss: 0.296255, acc:  34%, G loss: 1.430056\n",
      "Ep: 499, steps: 13, D loss: 0.270420, acc:  46%, G loss: 1.500531\n",
      "Ep: 499, steps: 14, D loss: 0.264516, acc:  46%, G loss: 1.545819\n",
      "Saved Model\n",
      "Ep: 499, steps: 15, D loss: 0.246670, acc:  55%, G loss: 1.570743\n",
      "Ep: 499, steps: 16, D loss: 0.215401, acc:  67%, G loss: 1.652283\n",
      "Ep: 499, steps: 17, D loss: 0.249355, acc:  55%, G loss: 1.640612\n",
      "Ep: 499, steps: 18, D loss: 0.218140, acc:  65%, G loss: 1.613727\n",
      "Ep: 499, steps: 19, D loss: 0.183623, acc:  77%, G loss: 1.846678\n",
      "Ep: 499, steps: 20, D loss: 0.279767, acc:  37%, G loss: 1.534074\n",
      "Ep: 499, steps: 21, D loss: 0.160091, acc:  78%, G loss: 1.677557\n",
      "Ep: 499, steps: 22, D loss: 0.246875, acc:  61%, G loss: 2.055341\n",
      "Ep: 499, steps: 23, D loss: 0.216194, acc:  67%, G loss: 1.604658\n",
      "Ep: 499, steps: 24, D loss: 0.254378, acc:  52%, G loss: 1.607858\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 500, steps: 1, D loss: 0.243350, acc:  60%, G loss: 1.777275\n",
      "Ep: 500, steps: 2, D loss: 0.237187, acc:  61%, G loss: 1.577572\n",
      "Ep: 500, steps: 3, D loss: 0.173790, acc:  81%, G loss: 2.067273\n",
      "Ep: 500, steps: 4, D loss: 0.188639, acc:  82%, G loss: 1.878378\n",
      "Ep: 500, steps: 5, D loss: 0.261569, acc:  53%, G loss: 1.725809\n",
      "Ep: 500, steps: 6, D loss: 0.235250, acc:  60%, G loss: 1.632222\n",
      "Ep: 500, steps: 7, D loss: 0.326728, acc:  29%, G loss: 1.511171\n",
      "Ep: 500, steps: 8, D loss: 0.231683, acc:  62%, G loss: 1.726265\n",
      "Ep: 500, steps: 9, D loss: 0.213114, acc:  70%, G loss: 1.666636\n",
      "Ep: 500, steps: 10, D loss: 0.174729, acc:  82%, G loss: 1.617249\n",
      "Ep: 500, steps: 11, D loss: 0.240579, acc:  54%, G loss: 1.773992\n",
      "Ep: 500, steps: 12, D loss: 0.292393, acc:  36%, G loss: 1.367435\n",
      "Ep: 500, steps: 13, D loss: 0.289673, acc:  38%, G loss: 1.454447\n",
      "Ep: 500, steps: 14, D loss: 0.270664, acc:  43%, G loss: 1.578639\n",
      "Ep: 500, steps: 15, D loss: 0.261093, acc:  45%, G loss: 1.613553\n",
      "Ep: 500, steps: 16, D loss: 0.243680, acc:  58%, G loss: 1.631631\n",
      "Ep: 500, steps: 17, D loss: 0.201572, acc:  73%, G loss: 1.667868\n",
      "Ep: 500, steps: 18, D loss: 0.235956, acc:  61%, G loss: 1.566025\n",
      "Ep: 500, steps: 19, D loss: 0.193894, acc:  73%, G loss: 1.643941\n",
      "Ep: 500, steps: 20, D loss: 0.181023, acc:  75%, G loss: 1.744136\n",
      "Ep: 500, steps: 21, D loss: 0.281042, acc:  37%, G loss: 1.563071\n",
      "Ep: 500, steps: 22, D loss: 0.144771, acc:  83%, G loss: 1.763091\n",
      "Ep: 500, steps: 23, D loss: 0.217816, acc:  68%, G loss: 1.914224\n",
      "Ep: 500, steps: 24, D loss: 0.221051, acc:  63%, G loss: 1.580042\n",
      "Ep: 500, steps: 25, D loss: 0.268703, acc:  49%, G loss: 1.730976\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 501, steps: 1, D loss: 0.239166, acc:  59%, G loss: 1.893152\n",
      "Ep: 501, steps: 2, D loss: 0.247074, acc:  55%, G loss: 1.583463\n",
      "Ep: 501, steps: 3, D loss: 0.168768, acc:  81%, G loss: 2.012491\n",
      "Ep: 501, steps: 4, D loss: 0.190932, acc:  81%, G loss: 1.826762\n",
      "Ep: 501, steps: 5, D loss: 0.295666, acc:  44%, G loss: 1.713526\n",
      "Ep: 501, steps: 6, D loss: 0.228531, acc:  57%, G loss: 1.651353\n",
      "Ep: 501, steps: 7, D loss: 0.303666, acc:  34%, G loss: 1.553907\n",
      "Ep: 501, steps: 8, D loss: 0.217677, acc:  64%, G loss: 1.714772\n",
      "Ep: 501, steps: 9, D loss: 0.241050, acc:  61%, G loss: 1.613346\n",
      "Ep: 501, steps: 10, D loss: 0.181144, acc:  77%, G loss: 1.654944\n",
      "Ep: 501, steps: 11, D loss: 0.260476, acc:  50%, G loss: 1.867821\n",
      "Ep: 501, steps: 12, D loss: 0.282943, acc:  40%, G loss: 1.445628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 501, steps: 13, D loss: 0.282548, acc:  40%, G loss: 1.471283\n",
      "Ep: 501, steps: 14, D loss: 0.275079, acc:  39%, G loss: 1.589581\n",
      "Ep: 501, steps: 15, D loss: 0.235964, acc:  59%, G loss: 1.588207\n",
      "Ep: 501, steps: 16, D loss: 0.257923, acc:  53%, G loss: 1.621493\n",
      "Ep: 501, steps: 17, D loss: 0.213573, acc:  71%, G loss: 1.559152\n",
      "Ep: 501, steps: 18, D loss: 0.233553, acc:  60%, G loss: 1.579964\n",
      "Ep: 501, steps: 19, D loss: 0.224098, acc:  64%, G loss: 1.629399\n",
      "Ep: 501, steps: 20, D loss: 0.181260, acc:  77%, G loss: 1.822129\n",
      "Ep: 501, steps: 21, D loss: 0.270126, acc:  41%, G loss: 1.599783\n",
      "Ep: 501, steps: 22, D loss: 0.150507, acc:  84%, G loss: 1.624354\n",
      "Ep: 501, steps: 23, D loss: 0.236042, acc:  59%, G loss: 1.962954\n",
      "Ep: 501, steps: 24, D loss: 0.208575, acc:  69%, G loss: 1.649070\n",
      "Ep: 501, steps: 25, D loss: 0.225813, acc:  62%, G loss: 1.722607\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 502, steps: 1, D loss: 0.264540, acc:  52%, G loss: 1.849100\n",
      "Ep: 502, steps: 2, D loss: 0.253873, acc:  53%, G loss: 1.703129\n",
      "Ep: 502, steps: 3, D loss: 0.179199, acc:  76%, G loss: 2.080978\n",
      "Ep: 502, steps: 4, D loss: 0.197800, acc:  78%, G loss: 1.873186\n",
      "Ep: 502, steps: 5, D loss: 0.272914, acc:  51%, G loss: 1.717129\n",
      "Ep: 502, steps: 6, D loss: 0.240953, acc:  57%, G loss: 1.752358\n",
      "Ep: 502, steps: 7, D loss: 0.302518, acc:  33%, G loss: 1.480078\n",
      "Ep: 502, steps: 8, D loss: 0.225134, acc:  63%, G loss: 1.807402\n",
      "Ep: 502, steps: 9, D loss: 0.225477, acc:  65%, G loss: 1.647475\n",
      "Ep: 502, steps: 10, D loss: 0.179980, acc:  78%, G loss: 1.666982\n",
      "Ep: 502, steps: 11, D loss: 0.254103, acc:  53%, G loss: 1.865921\n",
      "Ep: 502, steps: 12, D loss: 0.302838, acc:  33%, G loss: 1.401294\n",
      "Saved Model\n",
      "Ep: 502, steps: 13, D loss: 0.276228, acc:  43%, G loss: 1.476986\n",
      "Ep: 502, steps: 14, D loss: 0.246384, acc:  55%, G loss: 1.645569\n",
      "Ep: 502, steps: 15, D loss: 0.249334, acc:  55%, G loss: 1.604011\n",
      "Ep: 502, steps: 16, D loss: 0.205447, acc:  73%, G loss: 1.694527\n",
      "Ep: 502, steps: 17, D loss: 0.228609, acc:  61%, G loss: 1.560330\n",
      "Ep: 502, steps: 18, D loss: 0.207424, acc:  66%, G loss: 1.652810\n",
      "Ep: 502, steps: 19, D loss: 0.186429, acc:  73%, G loss: 1.821751\n",
      "Ep: 502, steps: 20, D loss: 0.280338, acc:  40%, G loss: 1.515798\n",
      "Ep: 502, steps: 21, D loss: 0.146515, acc:  83%, G loss: 1.708729\n",
      "Ep: 502, steps: 22, D loss: 0.235727, acc:  60%, G loss: 1.967425\n",
      "Ep: 502, steps: 23, D loss: 0.221477, acc:  63%, G loss: 1.590981\n",
      "Ep: 502, steps: 24, D loss: 0.244187, acc:  56%, G loss: 1.668437\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 503, steps: 1, D loss: 0.242591, acc:  60%, G loss: 1.795986\n",
      "Ep: 503, steps: 2, D loss: 0.241508, acc:  56%, G loss: 1.604227\n",
      "Ep: 503, steps: 3, D loss: 0.176370, acc:  81%, G loss: 2.053296\n",
      "Ep: 503, steps: 4, D loss: 0.188953, acc:  82%, G loss: 1.848808\n",
      "Ep: 503, steps: 5, D loss: 0.287756, acc:  46%, G loss: 1.695929\n",
      "Ep: 503, steps: 6, D loss: 0.232174, acc:  58%, G loss: 1.760756\n",
      "Ep: 503, steps: 7, D loss: 0.337161, acc:  25%, G loss: 1.595408\n",
      "Ep: 503, steps: 8, D loss: 0.233251, acc:  58%, G loss: 1.780676\n",
      "Ep: 503, steps: 9, D loss: 0.241009, acc:  61%, G loss: 1.664566\n",
      "Ep: 503, steps: 10, D loss: 0.177220, acc:  81%, G loss: 1.684376\n",
      "Ep: 503, steps: 11, D loss: 0.236963, acc:  56%, G loss: 1.810464\n",
      "Ep: 503, steps: 12, D loss: 0.289581, acc:  37%, G loss: 1.408602\n",
      "Ep: 503, steps: 13, D loss: 0.282230, acc:  42%, G loss: 1.453205\n",
      "Ep: 503, steps: 14, D loss: 0.282781, acc:  38%, G loss: 1.572847\n",
      "Ep: 503, steps: 15, D loss: 0.241659, acc:  58%, G loss: 1.610708\n",
      "Ep: 503, steps: 16, D loss: 0.245154, acc:  57%, G loss: 1.654560\n",
      "Ep: 503, steps: 17, D loss: 0.210515, acc:  70%, G loss: 1.648353\n",
      "Ep: 503, steps: 18, D loss: 0.242456, acc:  58%, G loss: 1.740974\n",
      "Ep: 503, steps: 19, D loss: 0.213709, acc:  67%, G loss: 1.703464\n",
      "Ep: 503, steps: 20, D loss: 0.175780, acc:  77%, G loss: 1.816396\n",
      "Ep: 503, steps: 21, D loss: 0.287722, acc:  33%, G loss: 1.555628\n",
      "Ep: 503, steps: 22, D loss: 0.147789, acc:  83%, G loss: 1.726317\n",
      "Ep: 503, steps: 23, D loss: 0.233950, acc:  62%, G loss: 1.978888\n",
      "Ep: 503, steps: 24, D loss: 0.197929, acc:  73%, G loss: 1.603603\n",
      "Ep: 503, steps: 25, D loss: 0.264984, acc:  52%, G loss: 1.565937\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 504, steps: 1, D loss: 0.256978, acc:  53%, G loss: 1.707775\n",
      "Ep: 504, steps: 2, D loss: 0.244920, acc:  56%, G loss: 1.631661\n",
      "Ep: 504, steps: 3, D loss: 0.181560, acc:  76%, G loss: 2.026731\n",
      "Ep: 504, steps: 4, D loss: 0.194106, acc:  79%, G loss: 1.878861\n",
      "Ep: 504, steps: 5, D loss: 0.259421, acc:  52%, G loss: 1.845954\n",
      "Ep: 504, steps: 6, D loss: 0.225353, acc:  60%, G loss: 1.704591\n",
      "Ep: 504, steps: 7, D loss: 0.304474, acc:  31%, G loss: 1.437598\n",
      "Ep: 504, steps: 8, D loss: 0.229588, acc:  62%, G loss: 1.705897\n",
      "Ep: 504, steps: 9, D loss: 0.218650, acc:  69%, G loss: 1.642299\n",
      "Ep: 504, steps: 10, D loss: 0.183859, acc:  78%, G loss: 1.687153\n",
      "Ep: 504, steps: 11, D loss: 0.253177, acc:  51%, G loss: 1.827209\n",
      "Ep: 504, steps: 12, D loss: 0.295612, acc:  35%, G loss: 1.441023\n",
      "Ep: 504, steps: 13, D loss: 0.287736, acc:  37%, G loss: 1.489107\n",
      "Ep: 504, steps: 14, D loss: 0.272734, acc:  43%, G loss: 1.560351\n",
      "Ep: 504, steps: 15, D loss: 0.263858, acc:  45%, G loss: 1.634550\n",
      "Ep: 504, steps: 16, D loss: 0.253431, acc:  54%, G loss: 1.617622\n",
      "Ep: 504, steps: 17, D loss: 0.212811, acc:  70%, G loss: 1.614995\n",
      "Ep: 504, steps: 18, D loss: 0.230307, acc:  63%, G loss: 1.637014\n",
      "Ep: 504, steps: 19, D loss: 0.216651, acc:  65%, G loss: 1.658618\n",
      "Ep: 504, steps: 20, D loss: 0.174379, acc:  79%, G loss: 1.727543\n",
      "Ep: 504, steps: 21, D loss: 0.279298, acc:  38%, G loss: 1.500393\n",
      "Ep: 504, steps: 22, D loss: 0.140312, acc:  85%, G loss: 1.644486\n",
      "Ep: 504, steps: 23, D loss: 0.231350, acc:  61%, G loss: 1.999354\n",
      "Ep: 504, steps: 24, D loss: 0.213525, acc:  67%, G loss: 1.585250\n",
      "Ep: 504, steps: 25, D loss: 0.250159, acc:  53%, G loss: 1.685243\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 505, steps: 1, D loss: 0.251515, acc:  57%, G loss: 1.675612\n",
      "Ep: 505, steps: 2, D loss: 0.249911, acc:  54%, G loss: 1.566598\n",
      "Ep: 505, steps: 3, D loss: 0.176347, acc:  76%, G loss: 2.126155\n",
      "Ep: 505, steps: 4, D loss: 0.187376, acc:  83%, G loss: 1.868229\n",
      "Ep: 505, steps: 5, D loss: 0.261693, acc:  52%, G loss: 1.774278\n",
      "Ep: 505, steps: 6, D loss: 0.236487, acc:  59%, G loss: 1.762397\n",
      "Ep: 505, steps: 7, D loss: 0.304986, acc:  34%, G loss: 1.624264\n",
      "Ep: 505, steps: 8, D loss: 0.239869, acc:  58%, G loss: 1.692763\n",
      "Ep: 505, steps: 9, D loss: 0.231372, acc:  63%, G loss: 1.609021\n",
      "Ep: 505, steps: 10, D loss: 0.174122, acc:  82%, G loss: 1.673225\n",
      "Saved Model\n",
      "Ep: 505, steps: 11, D loss: 0.239533, acc:  56%, G loss: 1.835847\n",
      "Ep: 505, steps: 12, D loss: 0.295265, acc:  33%, G loss: 1.408960\n",
      "Ep: 505, steps: 13, D loss: 0.290446, acc:  36%, G loss: 1.489605\n",
      "Ep: 505, steps: 14, D loss: 0.247790, acc:  54%, G loss: 1.615384\n",
      "Ep: 505, steps: 15, D loss: 0.245691, acc:  57%, G loss: 1.609113\n",
      "Ep: 505, steps: 16, D loss: 0.212606, acc:  70%, G loss: 1.733086\n",
      "Ep: 505, steps: 17, D loss: 0.238806, acc:  59%, G loss: 1.632572\n",
      "Ep: 505, steps: 18, D loss: 0.212439, acc:  67%, G loss: 1.628153\n",
      "Ep: 505, steps: 19, D loss: 0.189567, acc:  70%, G loss: 1.762317\n",
      "Ep: 505, steps: 20, D loss: 0.280053, acc:  40%, G loss: 1.548379\n",
      "Ep: 505, steps: 21, D loss: 0.152247, acc:  79%, G loss: 1.590466\n",
      "Ep: 505, steps: 22, D loss: 0.218160, acc:  66%, G loss: 1.949716\n",
      "Ep: 505, steps: 23, D loss: 0.214060, acc:  66%, G loss: 1.566751\n",
      "Ep: 505, steps: 24, D loss: 0.254240, acc:  54%, G loss: 1.633896\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 506, steps: 1, D loss: 0.277651, acc:  47%, G loss: 1.739395\n",
      "Ep: 506, steps: 2, D loss: 0.233932, acc:  60%, G loss: 1.641072\n",
      "Ep: 506, steps: 3, D loss: 0.166426, acc:  82%, G loss: 2.079355\n",
      "Ep: 506, steps: 4, D loss: 0.182447, acc:  83%, G loss: 1.864787\n",
      "Ep: 506, steps: 5, D loss: 0.282987, acc:  44%, G loss: 1.693650\n",
      "Ep: 506, steps: 6, D loss: 0.236552, acc:  58%, G loss: 1.751096\n",
      "Ep: 506, steps: 7, D loss: 0.338843, acc:  24%, G loss: 1.476094\n",
      "Ep: 506, steps: 8, D loss: 0.242926, acc:  58%, G loss: 1.722478\n",
      "Ep: 506, steps: 9, D loss: 0.219004, acc:  68%, G loss: 1.637833\n",
      "Ep: 506, steps: 10, D loss: 0.191181, acc:  74%, G loss: 1.624468\n",
      "Ep: 506, steps: 11, D loss: 0.236300, acc:  57%, G loss: 1.783329\n",
      "Ep: 506, steps: 12, D loss: 0.296394, acc:  35%, G loss: 1.331139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 506, steps: 13, D loss: 0.283029, acc:  41%, G loss: 1.435872\n",
      "Ep: 506, steps: 14, D loss: 0.273938, acc:  40%, G loss: 1.514746\n",
      "Ep: 506, steps: 15, D loss: 0.260107, acc:  46%, G loss: 1.559546\n",
      "Ep: 506, steps: 16, D loss: 0.245593, acc:  58%, G loss: 1.632198\n",
      "Ep: 506, steps: 17, D loss: 0.200387, acc:  74%, G loss: 1.585671\n",
      "Ep: 506, steps: 18, D loss: 0.249709, acc:  54%, G loss: 1.687083\n",
      "Ep: 506, steps: 19, D loss: 0.210394, acc:  69%, G loss: 1.634481\n",
      "Ep: 506, steps: 20, D loss: 0.188410, acc:  75%, G loss: 1.831684\n",
      "Ep: 506, steps: 21, D loss: 0.278952, acc:  36%, G loss: 1.520838\n",
      "Ep: 506, steps: 22, D loss: 0.163009, acc:  80%, G loss: 1.639355\n",
      "Ep: 506, steps: 23, D loss: 0.219006, acc:  67%, G loss: 1.879488\n",
      "Ep: 506, steps: 24, D loss: 0.190466, acc:  77%, G loss: 1.592101\n",
      "Ep: 506, steps: 25, D loss: 0.265387, acc:  51%, G loss: 1.588722\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 507, steps: 1, D loss: 0.244071, acc:  56%, G loss: 1.733727\n",
      "Ep: 507, steps: 2, D loss: 0.241406, acc:  57%, G loss: 1.513365\n",
      "Ep: 507, steps: 3, D loss: 0.184710, acc:  75%, G loss: 2.046863\n",
      "Ep: 507, steps: 4, D loss: 0.198205, acc:  79%, G loss: 1.818217\n",
      "Ep: 507, steps: 5, D loss: 0.254610, acc:  54%, G loss: 1.686989\n",
      "Ep: 507, steps: 6, D loss: 0.227926, acc:  60%, G loss: 1.726486\n",
      "Ep: 507, steps: 7, D loss: 0.293592, acc:  35%, G loss: 1.500759\n",
      "Ep: 507, steps: 8, D loss: 0.219469, acc:  66%, G loss: 1.647804\n",
      "Ep: 507, steps: 9, D loss: 0.230935, acc:  61%, G loss: 1.641735\n",
      "Ep: 507, steps: 10, D loss: 0.170700, acc:  85%, G loss: 1.644518\n",
      "Ep: 507, steps: 11, D loss: 0.242663, acc:  54%, G loss: 1.785950\n",
      "Ep: 507, steps: 12, D loss: 0.315623, acc:  30%, G loss: 1.367773\n",
      "Ep: 507, steps: 13, D loss: 0.294497, acc:  34%, G loss: 1.444034\n",
      "Ep: 507, steps: 14, D loss: 0.289810, acc:  34%, G loss: 1.536487\n",
      "Ep: 507, steps: 15, D loss: 0.255529, acc:  50%, G loss: 1.580190\n",
      "Ep: 507, steps: 16, D loss: 0.238502, acc:  58%, G loss: 1.605622\n",
      "Ep: 507, steps: 17, D loss: 0.204005, acc:  73%, G loss: 1.569364\n",
      "Ep: 507, steps: 18, D loss: 0.246582, acc:  58%, G loss: 1.619491\n",
      "Ep: 507, steps: 19, D loss: 0.222246, acc:  65%, G loss: 1.613866\n",
      "Ep: 507, steps: 20, D loss: 0.170540, acc:  79%, G loss: 1.800650\n",
      "Ep: 507, steps: 21, D loss: 0.274261, acc:  38%, G loss: 1.722218\n",
      "Ep: 507, steps: 22, D loss: 0.148454, acc:  83%, G loss: 1.646578\n",
      "Ep: 507, steps: 23, D loss: 0.241849, acc:  59%, G loss: 1.978938\n",
      "Ep: 507, steps: 24, D loss: 0.220706, acc:  63%, G loss: 1.624896\n",
      "Ep: 507, steps: 25, D loss: 0.242200, acc:  56%, G loss: 1.623987\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 508, steps: 1, D loss: 0.244145, acc:  58%, G loss: 1.743833\n",
      "Ep: 508, steps: 2, D loss: 0.246505, acc:  55%, G loss: 1.481243\n",
      "Ep: 508, steps: 3, D loss: 0.185031, acc:  75%, G loss: 1.983289\n",
      "Ep: 508, steps: 4, D loss: 0.198486, acc:  77%, G loss: 1.781870\n",
      "Ep: 508, steps: 5, D loss: 0.282213, acc:  45%, G loss: 1.711399\n",
      "Ep: 508, steps: 6, D loss: 0.225422, acc:  60%, G loss: 1.767027\n",
      "Ep: 508, steps: 7, D loss: 0.319962, acc:  30%, G loss: 1.596063\n",
      "Ep: 508, steps: 8, D loss: 0.230859, acc:  61%, G loss: 1.705919\n",
      "Saved Model\n",
      "Ep: 508, steps: 9, D loss: 0.223436, acc:  67%, G loss: 1.647310\n",
      "Ep: 508, steps: 10, D loss: 0.271804, acc:  45%, G loss: 1.772724\n",
      "Ep: 508, steps: 11, D loss: 0.286936, acc:  35%, G loss: 1.448035\n",
      "Ep: 508, steps: 12, D loss: 0.274586, acc:  40%, G loss: 1.455737\n",
      "Ep: 508, steps: 13, D loss: 0.269627, acc:  44%, G loss: 1.563347\n",
      "Ep: 508, steps: 14, D loss: 0.270644, acc:  43%, G loss: 1.565302\n",
      "Ep: 508, steps: 15, D loss: 0.243721, acc:  55%, G loss: 1.608377\n",
      "Ep: 508, steps: 16, D loss: 0.221548, acc:  67%, G loss: 1.553663\n",
      "Ep: 508, steps: 17, D loss: 0.238154, acc:  60%, G loss: 1.589014\n",
      "Ep: 508, steps: 18, D loss: 0.229629, acc:  63%, G loss: 1.627914\n",
      "Ep: 508, steps: 19, D loss: 0.192271, acc:  74%, G loss: 1.839430\n",
      "Ep: 508, steps: 20, D loss: 0.268210, acc:  41%, G loss: 1.499702\n",
      "Ep: 508, steps: 21, D loss: 0.172864, acc:  75%, G loss: 1.579301\n",
      "Ep: 508, steps: 22, D loss: 0.246801, acc:  57%, G loss: 1.812247\n",
      "Ep: 508, steps: 23, D loss: 0.202294, acc:  71%, G loss: 1.580431\n",
      "Ep: 508, steps: 24, D loss: 0.251132, acc:  53%, G loss: 1.499966\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 509, steps: 1, D loss: 0.266244, acc:  50%, G loss: 1.722944\n",
      "Ep: 509, steps: 2, D loss: 0.238619, acc:  60%, G loss: 1.557866\n",
      "Ep: 509, steps: 3, D loss: 0.178462, acc:  74%, G loss: 1.956475\n",
      "Ep: 509, steps: 4, D loss: 0.201491, acc:  75%, G loss: 1.796267\n",
      "Ep: 509, steps: 5, D loss: 0.262295, acc:  51%, G loss: 1.672737\n",
      "Ep: 509, steps: 6, D loss: 0.226801, acc:  63%, G loss: 1.705863\n",
      "Ep: 509, steps: 7, D loss: 0.315179, acc:  29%, G loss: 1.503471\n",
      "Ep: 509, steps: 8, D loss: 0.236951, acc:  60%, G loss: 1.683402\n",
      "Ep: 509, steps: 9, D loss: 0.224070, acc:  68%, G loss: 1.630917\n",
      "Ep: 509, steps: 10, D loss: 0.198870, acc:  71%, G loss: 1.666544\n",
      "Ep: 509, steps: 11, D loss: 0.239403, acc:  54%, G loss: 1.838197\n",
      "Ep: 509, steps: 12, D loss: 0.286315, acc:  39%, G loss: 1.373129\n",
      "Ep: 509, steps: 13, D loss: 0.282152, acc:  40%, G loss: 1.463970\n",
      "Ep: 509, steps: 14, D loss: 0.266931, acc:  43%, G loss: 1.512330\n",
      "Ep: 509, steps: 15, D loss: 0.240664, acc:  56%, G loss: 1.624351\n",
      "Ep: 509, steps: 16, D loss: 0.247498, acc:  55%, G loss: 1.649150\n",
      "Ep: 509, steps: 17, D loss: 0.204806, acc:  72%, G loss: 1.551824\n",
      "Ep: 509, steps: 18, D loss: 0.241448, acc:  58%, G loss: 1.618766\n",
      "Ep: 509, steps: 19, D loss: 0.227216, acc:  64%, G loss: 1.659504\n",
      "Ep: 509, steps: 20, D loss: 0.190015, acc:  74%, G loss: 1.772350\n",
      "Ep: 509, steps: 21, D loss: 0.274599, acc:  40%, G loss: 1.446160\n",
      "Ep: 509, steps: 22, D loss: 0.163359, acc:  81%, G loss: 1.625321\n",
      "Ep: 509, steps: 23, D loss: 0.224910, acc:  63%, G loss: 1.838415\n",
      "Ep: 509, steps: 24, D loss: 0.220748, acc:  64%, G loss: 1.552964\n",
      "Ep: 509, steps: 25, D loss: 0.257445, acc:  52%, G loss: 1.973781\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 510, steps: 1, D loss: 0.227105, acc:  64%, G loss: 1.763732\n",
      "Ep: 510, steps: 2, D loss: 0.234371, acc:  59%, G loss: 1.485814\n",
      "Ep: 510, steps: 3, D loss: 0.167462, acc:  81%, G loss: 1.995320\n",
      "Ep: 510, steps: 4, D loss: 0.188393, acc:  81%, G loss: 1.866756\n",
      "Ep: 510, steps: 5, D loss: 0.258012, acc:  53%, G loss: 1.646768\n",
      "Ep: 510, steps: 6, D loss: 0.227358, acc:  61%, G loss: 1.714969\n",
      "Ep: 510, steps: 7, D loss: 0.301340, acc:  37%, G loss: 1.757364\n",
      "Ep: 510, steps: 8, D loss: 0.229013, acc:  61%, G loss: 1.679254\n",
      "Ep: 510, steps: 9, D loss: 0.248260, acc:  56%, G loss: 1.622615\n",
      "Ep: 510, steps: 10, D loss: 0.170389, acc:  84%, G loss: 1.615237\n",
      "Ep: 510, steps: 11, D loss: 0.257616, acc:  50%, G loss: 1.765230\n",
      "Ep: 510, steps: 12, D loss: 0.294668, acc:  36%, G loss: 1.416429\n",
      "Ep: 510, steps: 13, D loss: 0.278179, acc:  43%, G loss: 1.470145\n",
      "Ep: 510, steps: 14, D loss: 0.283078, acc:  38%, G loss: 1.538077\n",
      "Ep: 510, steps: 15, D loss: 0.251667, acc:  53%, G loss: 1.585766\n",
      "Ep: 510, steps: 16, D loss: 0.236518, acc:  59%, G loss: 1.655451\n",
      "Ep: 510, steps: 17, D loss: 0.211777, acc:  69%, G loss: 1.564382\n",
      "Ep: 510, steps: 18, D loss: 0.242781, acc:  57%, G loss: 1.626220\n",
      "Ep: 510, steps: 19, D loss: 0.206147, acc:  69%, G loss: 1.678359\n",
      "Ep: 510, steps: 20, D loss: 0.178563, acc:  75%, G loss: 1.827217\n",
      "Ep: 510, steps: 21, D loss: 0.263126, acc:  44%, G loss: 1.587008\n",
      "Ep: 510, steps: 22, D loss: 0.140098, acc:  83%, G loss: 1.654586\n",
      "Ep: 510, steps: 23, D loss: 0.227854, acc:  63%, G loss: 1.900390\n",
      "Ep: 510, steps: 24, D loss: 0.212891, acc:  68%, G loss: 1.662404\n",
      "Ep: 510, steps: 25, D loss: 0.212002, acc:  68%, G loss: 1.643714\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 511, steps: 1, D loss: 0.269568, acc:  52%, G loss: 1.732857\n",
      "Ep: 511, steps: 2, D loss: 0.252867, acc:  51%, G loss: 1.496350\n",
      "Ep: 511, steps: 3, D loss: 0.180331, acc:  77%, G loss: 1.981078\n",
      "Ep: 511, steps: 4, D loss: 0.193307, acc:  79%, G loss: 1.876644\n",
      "Ep: 511, steps: 5, D loss: 0.287998, acc:  44%, G loss: 1.688431\n",
      "Ep: 511, steps: 6, D loss: 0.235525, acc:  60%, G loss: 1.725721\n",
      "Saved Model\n",
      "Ep: 511, steps: 7, D loss: 0.326431, acc:  26%, G loss: 1.446160\n",
      "Ep: 511, steps: 8, D loss: 0.188944, acc:  80%, G loss: 1.715716\n",
      "Ep: 511, steps: 9, D loss: 0.173743, acc:  82%, G loss: 1.740797\n",
      "Ep: 511, steps: 10, D loss: 0.222952, acc:  61%, G loss: 1.895087\n",
      "Ep: 511, steps: 11, D loss: 0.309968, acc:  31%, G loss: 1.487708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 511, steps: 12, D loss: 0.283581, acc:  40%, G loss: 1.505619\n",
      "Ep: 511, steps: 13, D loss: 0.282205, acc:  39%, G loss: 1.544469\n",
      "Ep: 511, steps: 14, D loss: 0.252790, acc:  52%, G loss: 1.615373\n",
      "Ep: 511, steps: 15, D loss: 0.260645, acc:  52%, G loss: 1.598652\n",
      "Ep: 511, steps: 16, D loss: 0.211399, acc:  70%, G loss: 1.661591\n",
      "Ep: 511, steps: 17, D loss: 0.215889, acc:  69%, G loss: 1.626068\n",
      "Ep: 511, steps: 18, D loss: 0.218883, acc:  66%, G loss: 1.669218\n",
      "Ep: 511, steps: 19, D loss: 0.189306, acc:  72%, G loss: 1.844967\n",
      "Ep: 511, steps: 20, D loss: 0.290678, acc:  33%, G loss: 1.503578\n",
      "Ep: 511, steps: 21, D loss: 0.156434, acc:  81%, G loss: 1.663012\n",
      "Ep: 511, steps: 22, D loss: 0.227186, acc:  63%, G loss: 1.928122\n",
      "Ep: 511, steps: 23, D loss: 0.212164, acc:  66%, G loss: 1.610499\n",
      "Ep: 511, steps: 24, D loss: 0.233801, acc:  59%, G loss: 1.656621\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 512, steps: 1, D loss: 0.240031, acc:  61%, G loss: 1.758289\n",
      "Ep: 512, steps: 2, D loss: 0.255498, acc:  53%, G loss: 1.439570\n",
      "Ep: 512, steps: 3, D loss: 0.183055, acc:  77%, G loss: 1.984980\n",
      "Ep: 512, steps: 4, D loss: 0.185221, acc:  83%, G loss: 1.861495\n",
      "Ep: 512, steps: 5, D loss: 0.268583, acc:  51%, G loss: 1.874135\n",
      "Ep: 512, steps: 6, D loss: 0.229968, acc:  60%, G loss: 1.722685\n",
      "Ep: 512, steps: 7, D loss: 0.339968, acc:  25%, G loss: 1.562552\n",
      "Ep: 512, steps: 8, D loss: 0.254539, acc:  53%, G loss: 1.826082\n",
      "Ep: 512, steps: 9, D loss: 0.245195, acc:  57%, G loss: 1.603577\n",
      "Ep: 512, steps: 10, D loss: 0.184615, acc:  78%, G loss: 1.669667\n",
      "Ep: 512, steps: 11, D loss: 0.266400, acc:  48%, G loss: 1.824030\n",
      "Ep: 512, steps: 12, D loss: 0.284005, acc:  40%, G loss: 1.441123\n",
      "Ep: 512, steps: 13, D loss: 0.272677, acc:  44%, G loss: 1.482640\n",
      "Ep: 512, steps: 14, D loss: 0.272030, acc:  42%, G loss: 1.518729\n",
      "Ep: 512, steps: 15, D loss: 0.243281, acc:  55%, G loss: 1.647691\n",
      "Ep: 512, steps: 16, D loss: 0.249509, acc:  56%, G loss: 1.624287\n",
      "Ep: 512, steps: 17, D loss: 0.210363, acc:  71%, G loss: 1.579652\n",
      "Ep: 512, steps: 18, D loss: 0.250594, acc:  55%, G loss: 1.579732\n",
      "Ep: 512, steps: 19, D loss: 0.213944, acc:  67%, G loss: 1.669738\n",
      "Ep: 512, steps: 20, D loss: 0.177941, acc:  76%, G loss: 1.742029\n",
      "Ep: 512, steps: 21, D loss: 0.285194, acc:  35%, G loss: 1.612034\n",
      "Ep: 512, steps: 22, D loss: 0.146711, acc:  84%, G loss: 1.671229\n",
      "Ep: 512, steps: 23, D loss: 0.235308, acc:  60%, G loss: 1.906869\n",
      "Ep: 512, steps: 24, D loss: 0.212663, acc:  68%, G loss: 1.592767\n",
      "Ep: 512, steps: 25, D loss: 0.240648, acc:  58%, G loss: 1.553935\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 513, steps: 1, D loss: 0.257904, acc:  53%, G loss: 1.734843\n",
      "Ep: 513, steps: 2, D loss: 0.240932, acc:  59%, G loss: 1.462581\n",
      "Ep: 513, steps: 3, D loss: 0.179268, acc:  77%, G loss: 1.915009\n",
      "Ep: 513, steps: 4, D loss: 0.187079, acc:  82%, G loss: 1.800525\n",
      "Ep: 513, steps: 5, D loss: 0.267957, acc:  50%, G loss: 1.632186\n",
      "Ep: 513, steps: 6, D loss: 0.232114, acc:  59%, G loss: 1.641090\n",
      "Ep: 513, steps: 7, D loss: 0.313529, acc:  29%, G loss: 1.690476\n",
      "Ep: 513, steps: 8, D loss: 0.232392, acc:  60%, G loss: 1.850563\n",
      "Ep: 513, steps: 9, D loss: 0.225732, acc:  65%, G loss: 1.627982\n",
      "Ep: 513, steps: 10, D loss: 0.172840, acc:  83%, G loss: 1.610718\n",
      "Ep: 513, steps: 11, D loss: 0.260077, acc:  48%, G loss: 1.805228\n",
      "Ep: 513, steps: 12, D loss: 0.308814, acc:  31%, G loss: 1.393234\n",
      "Ep: 513, steps: 13, D loss: 0.270672, acc:  45%, G loss: 1.528112\n",
      "Ep: 513, steps: 14, D loss: 0.272319, acc:  43%, G loss: 1.529119\n",
      "Ep: 513, steps: 15, D loss: 0.255314, acc:  49%, G loss: 1.574782\n",
      "Ep: 513, steps: 16, D loss: 0.249437, acc:  55%, G loss: 1.658381\n",
      "Ep: 513, steps: 17, D loss: 0.198313, acc:  76%, G loss: 1.588998\n",
      "Ep: 513, steps: 18, D loss: 0.230104, acc:  64%, G loss: 1.654185\n",
      "Ep: 513, steps: 19, D loss: 0.221023, acc:  64%, G loss: 1.617142\n",
      "Ep: 513, steps: 20, D loss: 0.173046, acc:  78%, G loss: 1.803510\n",
      "Ep: 513, steps: 21, D loss: 0.259049, acc:  45%, G loss: 1.578956\n",
      "Ep: 513, steps: 22, D loss: 0.147549, acc:  82%, G loss: 1.634023\n",
      "Ep: 513, steps: 23, D loss: 0.227766, acc:  62%, G loss: 1.955296\n",
      "Ep: 513, steps: 24, D loss: 0.217838, acc:  66%, G loss: 1.597135\n",
      "Ep: 513, steps: 25, D loss: 0.262170, acc:  52%, G loss: 1.563061\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 514, steps: 1, D loss: 0.247031, acc:  56%, G loss: 1.625551\n",
      "Ep: 514, steps: 2, D loss: 0.250981, acc:  53%, G loss: 1.491073\n",
      "Ep: 514, steps: 3, D loss: 0.173652, acc:  79%, G loss: 1.967069\n",
      "Ep: 514, steps: 4, D loss: 0.185947, acc:  83%, G loss: 1.795894\n",
      "Saved Model\n",
      "Ep: 514, steps: 5, D loss: 0.262670, acc:  51%, G loss: 1.647937\n",
      "Ep: 514, steps: 6, D loss: 0.288635, acc:  36%, G loss: 1.479813\n",
      "Ep: 514, steps: 7, D loss: 0.225254, acc:  65%, G loss: 1.839775\n",
      "Ep: 514, steps: 8, D loss: 0.271424, acc:  47%, G loss: 1.630177\n",
      "Ep: 514, steps: 9, D loss: 0.179806, acc:  80%, G loss: 1.626606\n",
      "Ep: 514, steps: 10, D loss: 0.249386, acc:  52%, G loss: 1.761012\n",
      "Ep: 514, steps: 11, D loss: 0.298670, acc:  35%, G loss: 1.440903\n",
      "Ep: 514, steps: 12, D loss: 0.289711, acc:  40%, G loss: 1.493668\n",
      "Ep: 514, steps: 13, D loss: 0.280160, acc:  38%, G loss: 1.526163\n",
      "Ep: 514, steps: 14, D loss: 0.259808, acc:  46%, G loss: 1.605116\n",
      "Ep: 514, steps: 15, D loss: 0.262440, acc:  52%, G loss: 1.676722\n",
      "Ep: 514, steps: 16, D loss: 0.222745, acc:  64%, G loss: 1.653368\n",
      "Ep: 514, steps: 17, D loss: 0.239626, acc:  58%, G loss: 1.552896\n",
      "Ep: 514, steps: 18, D loss: 0.197789, acc:  71%, G loss: 1.684246\n",
      "Ep: 514, steps: 19, D loss: 0.190178, acc:  73%, G loss: 1.752114\n",
      "Ep: 514, steps: 20, D loss: 0.272609, acc:  39%, G loss: 1.577047\n",
      "Ep: 514, steps: 21, D loss: 0.157102, acc:  79%, G loss: 1.647125\n",
      "Ep: 514, steps: 22, D loss: 0.221086, acc:  64%, G loss: 1.925929\n",
      "Ep: 514, steps: 23, D loss: 0.203155, acc:  72%, G loss: 1.538460\n",
      "Ep: 514, steps: 24, D loss: 0.263924, acc:  51%, G loss: 1.770973\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 515, steps: 1, D loss: 0.242256, acc:  60%, G loss: 1.699052\n",
      "Ep: 515, steps: 2, D loss: 0.243520, acc:  56%, G loss: 1.442664\n",
      "Ep: 515, steps: 3, D loss: 0.185690, acc:  76%, G loss: 1.953074\n",
      "Ep: 515, steps: 4, D loss: 0.192521, acc:  82%, G loss: 1.793355\n",
      "Ep: 515, steps: 5, D loss: 0.273199, acc:  48%, G loss: 1.761436\n",
      "Ep: 515, steps: 6, D loss: 0.237938, acc:  58%, G loss: 1.662727\n",
      "Ep: 515, steps: 7, D loss: 0.304026, acc:  35%, G loss: 1.629070\n",
      "Ep: 515, steps: 8, D loss: 0.225280, acc:  62%, G loss: 1.942075\n",
      "Ep: 515, steps: 9, D loss: 0.239318, acc:  61%, G loss: 1.648594\n",
      "Ep: 515, steps: 10, D loss: 0.185416, acc:  78%, G loss: 1.605403\n",
      "Ep: 515, steps: 11, D loss: 0.252290, acc:  51%, G loss: 1.821720\n",
      "Ep: 515, steps: 12, D loss: 0.307565, acc:  29%, G loss: 1.428420\n",
      "Ep: 515, steps: 13, D loss: 0.275812, acc:  45%, G loss: 1.483952\n",
      "Ep: 515, steps: 14, D loss: 0.275373, acc:  41%, G loss: 1.529621\n",
      "Ep: 515, steps: 15, D loss: 0.252968, acc:  50%, G loss: 1.594096\n",
      "Ep: 515, steps: 16, D loss: 0.246377, acc:  57%, G loss: 1.611654\n",
      "Ep: 515, steps: 17, D loss: 0.212551, acc:  71%, G loss: 1.673896\n",
      "Ep: 515, steps: 18, D loss: 0.239621, acc:  58%, G loss: 1.612681\n",
      "Ep: 515, steps: 19, D loss: 0.203144, acc:  69%, G loss: 1.632510\n",
      "Ep: 515, steps: 20, D loss: 0.177621, acc:  74%, G loss: 1.743116\n",
      "Ep: 515, steps: 21, D loss: 0.281018, acc:  37%, G loss: 1.463092\n",
      "Ep: 515, steps: 22, D loss: 0.153915, acc:  79%, G loss: 1.611872\n",
      "Ep: 515, steps: 23, D loss: 0.223573, acc:  65%, G loss: 1.840647\n",
      "Ep: 515, steps: 24, D loss: 0.216042, acc:  65%, G loss: 1.581956\n",
      "Ep: 515, steps: 25, D loss: 0.252133, acc:  52%, G loss: 1.625692\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 516, steps: 1, D loss: 0.249798, acc:  55%, G loss: 1.730853\n",
      "Ep: 516, steps: 2, D loss: 0.244000, acc:  56%, G loss: 1.510070\n",
      "Ep: 516, steps: 3, D loss: 0.168447, acc:  80%, G loss: 1.970942\n",
      "Ep: 516, steps: 4, D loss: 0.188516, acc:  81%, G loss: 1.846275\n",
      "Ep: 516, steps: 5, D loss: 0.300724, acc:  39%, G loss: 1.707626\n",
      "Ep: 516, steps: 6, D loss: 0.233169, acc:  60%, G loss: 1.693883\n",
      "Ep: 516, steps: 7, D loss: 0.310514, acc:  31%, G loss: 1.448818\n",
      "Ep: 516, steps: 8, D loss: 0.232743, acc:  63%, G loss: 1.775583\n",
      "Ep: 516, steps: 9, D loss: 0.223668, acc:  67%, G loss: 1.700296\n",
      "Ep: 516, steps: 10, D loss: 0.189711, acc:  77%, G loss: 1.618999\n",
      "Ep: 516, steps: 11, D loss: 0.234650, acc:  59%, G loss: 1.849705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 516, steps: 12, D loss: 0.297766, acc:  32%, G loss: 1.435336\n",
      "Ep: 516, steps: 13, D loss: 0.281712, acc:  39%, G loss: 1.451983\n",
      "Ep: 516, steps: 14, D loss: 0.268978, acc:  42%, G loss: 1.489940\n",
      "Ep: 516, steps: 15, D loss: 0.254153, acc:  50%, G loss: 1.574382\n",
      "Ep: 516, steps: 16, D loss: 0.233697, acc:  61%, G loss: 1.610381\n",
      "Ep: 516, steps: 17, D loss: 0.201305, acc:  73%, G loss: 1.605493\n",
      "Ep: 516, steps: 18, D loss: 0.254856, acc:  54%, G loss: 1.597445\n",
      "Ep: 516, steps: 19, D loss: 0.200448, acc:  70%, G loss: 1.733372\n",
      "Ep: 516, steps: 20, D loss: 0.181812, acc:  74%, G loss: 1.827027\n",
      "Ep: 516, steps: 21, D loss: 0.268084, acc:  44%, G loss: 1.546217\n",
      "Ep: 516, steps: 22, D loss: 0.145141, acc:  81%, G loss: 1.626508\n",
      "Ep: 516, steps: 23, D loss: 0.234866, acc:  59%, G loss: 1.904984\n",
      "Ep: 516, steps: 24, D loss: 0.217954, acc:  66%, G loss: 1.667441\n",
      "Ep: 516, steps: 25, D loss: 0.262493, acc:  51%, G loss: 1.673676\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 517, steps: 1, D loss: 0.245051, acc:  57%, G loss: 1.738992\n",
      "Ep: 517, steps: 2, D loss: 0.249362, acc:  56%, G loss: 1.521659\n",
      "Saved Model\n",
      "Ep: 517, steps: 3, D loss: 0.177693, acc:  78%, G loss: 1.964785\n",
      "Ep: 517, steps: 4, D loss: 0.264096, acc:  52%, G loss: 1.769317\n",
      "Ep: 517, steps: 5, D loss: 0.236234, acc:  61%, G loss: 1.635753\n",
      "Ep: 517, steps: 6, D loss: 0.282052, acc:  39%, G loss: 1.680328\n",
      "Ep: 517, steps: 7, D loss: 0.223655, acc:  64%, G loss: 1.894518\n",
      "Ep: 517, steps: 8, D loss: 0.219364, acc:  67%, G loss: 1.647460\n",
      "Ep: 517, steps: 9, D loss: 0.174596, acc:  83%, G loss: 1.653042\n",
      "Ep: 517, steps: 10, D loss: 0.278135, acc:  44%, G loss: 1.774051\n",
      "Ep: 517, steps: 11, D loss: 0.289472, acc:  38%, G loss: 1.473535\n",
      "Ep: 517, steps: 12, D loss: 0.262060, acc:  48%, G loss: 1.622454\n",
      "Ep: 517, steps: 13, D loss: 0.261735, acc:  48%, G loss: 1.587507\n",
      "Ep: 517, steps: 14, D loss: 0.262391, acc:  46%, G loss: 1.630586\n",
      "Ep: 517, steps: 15, D loss: 0.236264, acc:  60%, G loss: 1.625480\n",
      "Ep: 517, steps: 16, D loss: 0.219663, acc:  67%, G loss: 1.628796\n",
      "Ep: 517, steps: 17, D loss: 0.241198, acc:  59%, G loss: 1.622501\n",
      "Ep: 517, steps: 18, D loss: 0.207385, acc:  69%, G loss: 1.559439\n",
      "Ep: 517, steps: 19, D loss: 0.172791, acc:  78%, G loss: 1.824019\n",
      "Ep: 517, steps: 20, D loss: 0.267339, acc:  43%, G loss: 1.576512\n",
      "Ep: 517, steps: 21, D loss: 0.143115, acc:  81%, G loss: 1.629607\n",
      "Ep: 517, steps: 22, D loss: 0.246489, acc:  57%, G loss: 1.946484\n",
      "Ep: 517, steps: 23, D loss: 0.219611, acc:  67%, G loss: 1.613275\n",
      "Ep: 517, steps: 24, D loss: 0.259616, acc:  52%, G loss: 1.618823\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 518, steps: 1, D loss: 0.266351, acc:  50%, G loss: 1.787581\n",
      "Ep: 518, steps: 2, D loss: 0.251556, acc:  55%, G loss: 1.595815\n",
      "Ep: 518, steps: 3, D loss: 0.187538, acc:  77%, G loss: 1.992102\n",
      "Ep: 518, steps: 4, D loss: 0.194216, acc:  78%, G loss: 1.755526\n",
      "Ep: 518, steps: 5, D loss: 0.259869, acc:  52%, G loss: 1.661452\n",
      "Ep: 518, steps: 6, D loss: 0.241011, acc:  58%, G loss: 1.615553\n",
      "Ep: 518, steps: 7, D loss: 0.313124, acc:  31%, G loss: 1.463639\n",
      "Ep: 518, steps: 8, D loss: 0.224439, acc:  63%, G loss: 1.796084\n",
      "Ep: 518, steps: 9, D loss: 0.231621, acc:  63%, G loss: 1.641095\n",
      "Ep: 518, steps: 10, D loss: 0.183477, acc:  80%, G loss: 1.642751\n",
      "Ep: 518, steps: 11, D loss: 0.252695, acc:  50%, G loss: 1.840586\n",
      "Ep: 518, steps: 12, D loss: 0.305445, acc:  31%, G loss: 1.492252\n",
      "Ep: 518, steps: 13, D loss: 0.277469, acc:  42%, G loss: 1.490246\n",
      "Ep: 518, steps: 14, D loss: 0.274705, acc:  41%, G loss: 1.514252\n",
      "Ep: 518, steps: 15, D loss: 0.245257, acc:  52%, G loss: 1.568410\n",
      "Ep: 518, steps: 16, D loss: 0.238962, acc:  59%, G loss: 1.673992\n",
      "Ep: 518, steps: 17, D loss: 0.212913, acc:  68%, G loss: 1.579330\n",
      "Ep: 518, steps: 18, D loss: 0.235866, acc:  59%, G loss: 1.561352\n",
      "Ep: 518, steps: 19, D loss: 0.207548, acc:  67%, G loss: 1.612294\n",
      "Ep: 518, steps: 20, D loss: 0.182782, acc:  74%, G loss: 1.761709\n",
      "Ep: 518, steps: 21, D loss: 0.286129, acc:  36%, G loss: 1.533421\n",
      "Ep: 518, steps: 22, D loss: 0.156737, acc:  78%, G loss: 1.623165\n",
      "Ep: 518, steps: 23, D loss: 0.234373, acc:  61%, G loss: 1.907372\n",
      "Ep: 518, steps: 24, D loss: 0.224358, acc:  63%, G loss: 1.623950\n",
      "Ep: 518, steps: 25, D loss: 0.261121, acc:  52%, G loss: 1.630522\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 519, steps: 1, D loss: 0.241628, acc:  59%, G loss: 1.796522\n",
      "Ep: 519, steps: 2, D loss: 0.246751, acc:  55%, G loss: 1.535822\n",
      "Ep: 519, steps: 3, D loss: 0.174248, acc:  78%, G loss: 2.017213\n",
      "Ep: 519, steps: 4, D loss: 0.193568, acc:  79%, G loss: 1.774100\n",
      "Ep: 519, steps: 5, D loss: 0.300470, acc:  42%, G loss: 1.701110\n",
      "Ep: 519, steps: 6, D loss: 0.229386, acc:  58%, G loss: 1.640445\n",
      "Ep: 519, steps: 7, D loss: 0.328610, acc:  26%, G loss: 1.511404\n",
      "Ep: 519, steps: 8, D loss: 0.237666, acc:  60%, G loss: 1.751008\n",
      "Ep: 519, steps: 9, D loss: 0.223615, acc:  66%, G loss: 1.605892\n",
      "Ep: 519, steps: 10, D loss: 0.194645, acc:  76%, G loss: 1.567227\n",
      "Ep: 519, steps: 11, D loss: 0.261024, acc:  50%, G loss: 1.814451\n",
      "Ep: 519, steps: 12, D loss: 0.289663, acc:  38%, G loss: 1.427633\n",
      "Ep: 519, steps: 13, D loss: 0.267681, acc:  44%, G loss: 1.492451\n",
      "Ep: 519, steps: 14, D loss: 0.266626, acc:  43%, G loss: 1.490385\n",
      "Ep: 519, steps: 15, D loss: 0.256283, acc:  50%, G loss: 1.685249\n",
      "Ep: 519, steps: 16, D loss: 0.243775, acc:  58%, G loss: 1.618037\n",
      "Ep: 519, steps: 17, D loss: 0.223216, acc:  66%, G loss: 1.560165\n",
      "Ep: 519, steps: 18, D loss: 0.243829, acc:  57%, G loss: 1.584646\n",
      "Ep: 519, steps: 19, D loss: 0.201594, acc:  72%, G loss: 1.608783\n",
      "Ep: 519, steps: 20, D loss: 0.192751, acc:  73%, G loss: 1.719462\n",
      "Ep: 519, steps: 21, D loss: 0.284778, acc:  36%, G loss: 1.553702\n",
      "Ep: 519, steps: 22, D loss: 0.154682, acc:  81%, G loss: 1.596712\n",
      "Ep: 519, steps: 23, D loss: 0.231830, acc:  61%, G loss: 1.911331\n",
      "Ep: 519, steps: 24, D loss: 0.224326, acc:  64%, G loss: 1.672745\n",
      "Ep: 519, steps: 25, D loss: 0.258339, acc:  51%, G loss: 1.542447\n",
      "Data exhausted, Re Initialize\n",
      "Saved Model\n",
      "Ep: 520, steps: 1, D loss: 0.248530, acc:  57%, G loss: 1.717443\n",
      "Ep: 520, steps: 2, D loss: 0.167600, acc:  80%, G loss: 1.886893\n",
      "Ep: 520, steps: 3, D loss: 0.217285, acc:  68%, G loss: 1.693054\n",
      "Ep: 520, steps: 4, D loss: 0.269305, acc:  47%, G loss: 1.695757\n",
      "Ep: 520, steps: 5, D loss: 0.230819, acc:  63%, G loss: 1.639223\n",
      "Ep: 520, steps: 6, D loss: 0.294944, acc:  33%, G loss: 1.557502\n",
      "Ep: 520, steps: 7, D loss: 0.235618, acc:  60%, G loss: 1.784864\n",
      "Ep: 520, steps: 8, D loss: 0.231971, acc:  63%, G loss: 1.686563\n",
      "Ep: 520, steps: 9, D loss: 0.190913, acc:  75%, G loss: 1.597654\n",
      "Ep: 520, steps: 10, D loss: 0.254269, acc:  51%, G loss: 1.845431\n",
      "Ep: 520, steps: 11, D loss: 0.278429, acc:  40%, G loss: 1.466645\n",
      "Ep: 520, steps: 12, D loss: 0.271582, acc:  44%, G loss: 1.513533\n",
      "Ep: 520, steps: 13, D loss: 0.268417, acc:  44%, G loss: 1.498491\n",
      "Ep: 520, steps: 14, D loss: 0.264009, acc:  46%, G loss: 1.640859\n",
      "Ep: 520, steps: 15, D loss: 0.244626, acc:  57%, G loss: 1.643634\n",
      "Ep: 520, steps: 16, D loss: 0.215107, acc:  69%, G loss: 1.553971\n",
      "Ep: 520, steps: 17, D loss: 0.248716, acc:  55%, G loss: 1.590866\n",
      "Ep: 520, steps: 18, D loss: 0.211573, acc:  68%, G loss: 1.658120\n",
      "Ep: 520, steps: 19, D loss: 0.175222, acc:  78%, G loss: 1.799944\n",
      "Ep: 520, steps: 20, D loss: 0.275611, acc:  38%, G loss: 1.503921\n",
      "Ep: 520, steps: 21, D loss: 0.146552, acc:  82%, G loss: 1.709534\n",
      "Ep: 520, steps: 22, D loss: 0.231114, acc:  61%, G loss: 1.925714\n",
      "Ep: 520, steps: 23, D loss: 0.204421, acc:  71%, G loss: 1.627936\n",
      "Ep: 520, steps: 24, D loss: 0.259707, acc:  51%, G loss: 1.554490\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 521, steps: 1, D loss: 0.244627, acc:  57%, G loss: 1.691405\n",
      "Ep: 521, steps: 2, D loss: 0.244668, acc:  56%, G loss: 1.563064\n",
      "Ep: 521, steps: 3, D loss: 0.186188, acc:  76%, G loss: 1.940005\n",
      "Ep: 521, steps: 4, D loss: 0.191678, acc:  80%, G loss: 1.732012\n",
      "Ep: 521, steps: 5, D loss: 0.272077, acc:  45%, G loss: 1.657478\n",
      "Ep: 521, steps: 6, D loss: 0.227452, acc:  60%, G loss: 1.753641\n",
      "Ep: 521, steps: 7, D loss: 0.318451, acc:  28%, G loss: 1.544090\n",
      "Ep: 521, steps: 8, D loss: 0.237042, acc:  56%, G loss: 1.783379\n",
      "Ep: 521, steps: 9, D loss: 0.229912, acc:  65%, G loss: 1.612864\n",
      "Ep: 521, steps: 10, D loss: 0.190475, acc:  76%, G loss: 1.624812\n",
      "Ep: 521, steps: 11, D loss: 0.245359, acc:  54%, G loss: 1.965881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 521, steps: 12, D loss: 0.298262, acc:  35%, G loss: 1.477348\n",
      "Ep: 521, steps: 13, D loss: 0.270210, acc:  47%, G loss: 1.521867\n",
      "Ep: 521, steps: 14, D loss: 0.281873, acc:  40%, G loss: 1.479131\n",
      "Ep: 521, steps: 15, D loss: 0.240326, acc:  58%, G loss: 1.600206\n",
      "Ep: 521, steps: 16, D loss: 0.254015, acc:  53%, G loss: 1.548862\n",
      "Ep: 521, steps: 17, D loss: 0.204337, acc:  69%, G loss: 1.602537\n",
      "Ep: 521, steps: 18, D loss: 0.244984, acc:  56%, G loss: 1.595526\n",
      "Ep: 521, steps: 19, D loss: 0.214230, acc:  67%, G loss: 1.601817\n",
      "Ep: 521, steps: 20, D loss: 0.175819, acc:  76%, G loss: 1.766844\n",
      "Ep: 521, steps: 21, D loss: 0.266991, acc:  42%, G loss: 1.472208\n",
      "Ep: 521, steps: 22, D loss: 0.158182, acc:  76%, G loss: 1.733078\n",
      "Ep: 521, steps: 23, D loss: 0.218906, acc:  66%, G loss: 1.894133\n",
      "Ep: 521, steps: 24, D loss: 0.210093, acc:  67%, G loss: 1.571622\n",
      "Ep: 521, steps: 25, D loss: 0.265195, acc:  53%, G loss: 1.816227\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 522, steps: 1, D loss: 0.239766, acc:  59%, G loss: 1.709495\n",
      "Ep: 522, steps: 2, D loss: 0.241176, acc:  56%, G loss: 1.520874\n",
      "Ep: 522, steps: 3, D loss: 0.162696, acc:  79%, G loss: 1.966621\n",
      "Ep: 522, steps: 4, D loss: 0.186948, acc:  81%, G loss: 1.748555\n",
      "Ep: 522, steps: 5, D loss: 0.280029, acc:  44%, G loss: 1.753196\n",
      "Ep: 522, steps: 6, D loss: 0.223739, acc:  64%, G loss: 1.621959\n",
      "Ep: 522, steps: 7, D loss: 0.323425, acc:  29%, G loss: 1.596416\n",
      "Ep: 522, steps: 8, D loss: 0.229993, acc:  63%, G loss: 1.773277\n",
      "Ep: 522, steps: 9, D loss: 0.234499, acc:  62%, G loss: 1.651647\n",
      "Ep: 522, steps: 10, D loss: 0.178956, acc:  81%, G loss: 1.605114\n",
      "Ep: 522, steps: 11, D loss: 0.244788, acc:  54%, G loss: 1.787302\n",
      "Ep: 522, steps: 12, D loss: 0.292376, acc:  34%, G loss: 1.442685\n",
      "Ep: 522, steps: 13, D loss: 0.279690, acc:  39%, G loss: 1.476501\n",
      "Ep: 522, steps: 14, D loss: 0.272085, acc:  42%, G loss: 1.464685\n",
      "Ep: 522, steps: 15, D loss: 0.254237, acc:  51%, G loss: 1.581260\n",
      "Ep: 522, steps: 16, D loss: 0.239365, acc:  60%, G loss: 1.672978\n",
      "Ep: 522, steps: 17, D loss: 0.201301, acc:  74%, G loss: 1.582209\n",
      "Ep: 522, steps: 18, D loss: 0.240391, acc:  59%, G loss: 1.543357\n",
      "Ep: 522, steps: 19, D loss: 0.208354, acc:  69%, G loss: 1.608773\n",
      "Ep: 522, steps: 20, D loss: 0.180847, acc:  76%, G loss: 1.717426\n",
      "Ep: 522, steps: 21, D loss: 0.279462, acc:  37%, G loss: 1.544459\n",
      "Ep: 522, steps: 22, D loss: 0.144296, acc:  81%, G loss: 1.693684\n",
      "Ep: 522, steps: 23, D loss: 0.234119, acc:  60%, G loss: 1.899421\n",
      "Saved Model\n",
      "Ep: 522, steps: 24, D loss: 0.194931, acc:  74%, G loss: 1.616061\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 523, steps: 1, D loss: 0.256045, acc:  54%, G loss: 1.692663\n",
      "Ep: 523, steps: 2, D loss: 0.239468, acc:  59%, G loss: 1.471611\n",
      "Ep: 523, steps: 3, D loss: 0.192411, acc:  74%, G loss: 2.018420\n",
      "Ep: 523, steps: 4, D loss: 0.173444, acc:  85%, G loss: 1.859605\n",
      "Ep: 523, steps: 5, D loss: 0.302424, acc:  45%, G loss: 1.603873\n",
      "Ep: 523, steps: 6, D loss: 0.237768, acc:  57%, G loss: 1.600794\n",
      "Ep: 523, steps: 7, D loss: 0.331771, acc:  27%, G loss: 1.488884\n",
      "Ep: 523, steps: 8, D loss: 0.226913, acc:  61%, G loss: 1.832007\n",
      "Ep: 523, steps: 9, D loss: 0.217991, acc:  68%, G loss: 1.649474\n",
      "Ep: 523, steps: 10, D loss: 0.179176, acc:  81%, G loss: 1.647480\n",
      "Ep: 523, steps: 11, D loss: 0.253087, acc:  52%, G loss: 1.842936\n",
      "Ep: 523, steps: 12, D loss: 0.290110, acc:  37%, G loss: 1.457994\n",
      "Ep: 523, steps: 13, D loss: 0.284009, acc:  43%, G loss: 1.502444\n",
      "Ep: 523, steps: 14, D loss: 0.261628, acc:  47%, G loss: 1.478136\n",
      "Ep: 523, steps: 15, D loss: 0.263524, acc:  47%, G loss: 1.669290\n",
      "Ep: 523, steps: 16, D loss: 0.237558, acc:  59%, G loss: 1.611734\n",
      "Ep: 523, steps: 17, D loss: 0.207081, acc:  72%, G loss: 1.663257\n",
      "Ep: 523, steps: 18, D loss: 0.233336, acc:  61%, G loss: 1.632196\n",
      "Ep: 523, steps: 19, D loss: 0.203471, acc:  70%, G loss: 1.630615\n",
      "Ep: 523, steps: 20, D loss: 0.180816, acc:  75%, G loss: 1.795102\n",
      "Ep: 523, steps: 21, D loss: 0.280087, acc:  36%, G loss: 1.483024\n",
      "Ep: 523, steps: 22, D loss: 0.147903, acc:  83%, G loss: 1.725910\n",
      "Ep: 523, steps: 23, D loss: 0.247822, acc:  56%, G loss: 1.971917\n",
      "Ep: 523, steps: 24, D loss: 0.213274, acc:  68%, G loss: 1.647316\n",
      "Ep: 523, steps: 25, D loss: 0.256235, acc:  53%, G loss: 1.582263\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 524, steps: 1, D loss: 0.255647, acc:  53%, G loss: 1.667685\n",
      "Ep: 524, steps: 2, D loss: 0.249867, acc:  55%, G loss: 1.493648\n",
      "Ep: 524, steps: 3, D loss: 0.179721, acc:  79%, G loss: 1.957138\n",
      "Ep: 524, steps: 4, D loss: 0.193595, acc:  79%, G loss: 1.692671\n",
      "Ep: 524, steps: 5, D loss: 0.280966, acc:  45%, G loss: 1.703839\n",
      "Ep: 524, steps: 6, D loss: 0.228718, acc:  60%, G loss: 1.539147\n",
      "Ep: 524, steps: 7, D loss: 0.318363, acc:  29%, G loss: 1.445935\n",
      "Ep: 524, steps: 8, D loss: 0.230197, acc:  63%, G loss: 1.706944\n",
      "Ep: 524, steps: 9, D loss: 0.224790, acc:  66%, G loss: 1.639426\n",
      "Ep: 524, steps: 10, D loss: 0.182917, acc:  80%, G loss: 1.590403\n",
      "Ep: 524, steps: 11, D loss: 0.259716, acc:  50%, G loss: 1.857560\n",
      "Ep: 524, steps: 12, D loss: 0.295831, acc:  35%, G loss: 1.515840\n",
      "Ep: 524, steps: 13, D loss: 0.275933, acc:  43%, G loss: 1.444395\n",
      "Ep: 524, steps: 14, D loss: 0.288607, acc:  35%, G loss: 1.488096\n",
      "Ep: 524, steps: 15, D loss: 0.267058, acc:  44%, G loss: 1.538398\n",
      "Ep: 524, steps: 16, D loss: 0.245017, acc:  57%, G loss: 1.609008\n",
      "Ep: 524, steps: 17, D loss: 0.201550, acc:  74%, G loss: 1.569203\n",
      "Ep: 524, steps: 18, D loss: 0.246057, acc:  56%, G loss: 1.530266\n",
      "Ep: 524, steps: 19, D loss: 0.220775, acc:  64%, G loss: 1.595220\n",
      "Ep: 524, steps: 20, D loss: 0.189118, acc:  71%, G loss: 1.817121\n",
      "Ep: 524, steps: 21, D loss: 0.255476, acc:  46%, G loss: 1.560204\n",
      "Ep: 524, steps: 22, D loss: 0.144493, acc:  85%, G loss: 1.650317\n",
      "Ep: 524, steps: 23, D loss: 0.230717, acc:  62%, G loss: 1.889752\n",
      "Ep: 524, steps: 24, D loss: 0.205238, acc:  72%, G loss: 1.647002\n",
      "Ep: 524, steps: 25, D loss: 0.259675, acc:  51%, G loss: 1.516328\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 525, steps: 1, D loss: 0.249283, acc:  57%, G loss: 1.871330\n",
      "Ep: 525, steps: 2, D loss: 0.240024, acc:  58%, G loss: 1.484652\n",
      "Ep: 525, steps: 3, D loss: 0.180234, acc:  76%, G loss: 1.975429\n",
      "Ep: 525, steps: 4, D loss: 0.188826, acc:  81%, G loss: 1.729763\n",
      "Ep: 525, steps: 5, D loss: 0.293430, acc:  42%, G loss: 1.684835\n",
      "Ep: 525, steps: 6, D loss: 0.225016, acc:  59%, G loss: 1.572386\n",
      "Ep: 525, steps: 7, D loss: 0.327012, acc:  28%, G loss: 1.476239\n",
      "Ep: 525, steps: 8, D loss: 0.234074, acc:  62%, G loss: 1.693465\n",
      "Ep: 525, steps: 9, D loss: 0.249007, acc:  54%, G loss: 1.626297\n",
      "Ep: 525, steps: 10, D loss: 0.192642, acc:  76%, G loss: 1.620383\n",
      "Ep: 525, steps: 11, D loss: 0.233972, acc:  59%, G loss: 1.819805\n",
      "Ep: 525, steps: 12, D loss: 0.296902, acc:  34%, G loss: 1.425905\n",
      "Ep: 525, steps: 13, D loss: 0.278279, acc:  42%, G loss: 1.413810\n",
      "Ep: 525, steps: 14, D loss: 0.271688, acc:  43%, G loss: 1.474324\n",
      "Ep: 525, steps: 15, D loss: 0.244050, acc:  56%, G loss: 1.582150\n",
      "Ep: 525, steps: 16, D loss: 0.239375, acc:  60%, G loss: 1.600666\n",
      "Ep: 525, steps: 17, D loss: 0.227089, acc:  66%, G loss: 1.754075\n",
      "Ep: 525, steps: 18, D loss: 0.235656, acc:  60%, G loss: 1.561333\n",
      "Ep: 525, steps: 19, D loss: 0.212102, acc:  69%, G loss: 1.591471\n",
      "Ep: 525, steps: 20, D loss: 0.192444, acc:  74%, G loss: 1.738599\n",
      "Ep: 525, steps: 21, D loss: 0.281320, acc:  35%, G loss: 1.388232\n",
      "Saved Model\n",
      "Ep: 525, steps: 22, D loss: 0.157107, acc:  83%, G loss: 1.623832\n",
      "Ep: 525, steps: 23, D loss: 0.223733, acc:  64%, G loss: 1.521587\n",
      "Ep: 525, steps: 24, D loss: 0.257104, acc:  52%, G loss: 1.597309\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 526, steps: 1, D loss: 0.261947, acc:  53%, G loss: 1.662896\n",
      "Ep: 526, steps: 2, D loss: 0.229347, acc:  63%, G loss: 1.503084\n",
      "Ep: 526, steps: 3, D loss: 0.186082, acc:  75%, G loss: 1.955243\n",
      "Ep: 526, steps: 4, D loss: 0.209417, acc:  74%, G loss: 1.706494\n",
      "Ep: 526, steps: 5, D loss: 0.268600, acc:  51%, G loss: 1.639546\n",
      "Ep: 526, steps: 6, D loss: 0.209915, acc:  63%, G loss: 1.617329\n",
      "Ep: 526, steps: 7, D loss: 0.313927, acc:  29%, G loss: 1.570891\n",
      "Ep: 526, steps: 8, D loss: 0.225771, acc:  63%, G loss: 1.790511\n",
      "Ep: 526, steps: 9, D loss: 0.240616, acc:  58%, G loss: 1.679407\n",
      "Ep: 526, steps: 10, D loss: 0.179303, acc:  80%, G loss: 1.566821\n",
      "Ep: 526, steps: 11, D loss: 0.262685, acc:  48%, G loss: 1.867201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 526, steps: 12, D loss: 0.298737, acc:  33%, G loss: 1.456910\n",
      "Ep: 526, steps: 13, D loss: 0.278411, acc:  41%, G loss: 1.458212\n",
      "Ep: 526, steps: 14, D loss: 0.273567, acc:  41%, G loss: 1.516731\n",
      "Ep: 526, steps: 15, D loss: 0.238098, acc:  57%, G loss: 1.578887\n",
      "Ep: 526, steps: 16, D loss: 0.250275, acc:  55%, G loss: 1.671827\n",
      "Ep: 526, steps: 17, D loss: 0.201558, acc:  75%, G loss: 1.611411\n",
      "Ep: 526, steps: 18, D loss: 0.243825, acc:  57%, G loss: 1.652893\n",
      "Ep: 526, steps: 19, D loss: 0.215433, acc:  66%, G loss: 1.642644\n",
      "Ep: 526, steps: 20, D loss: 0.183540, acc:  77%, G loss: 1.750908\n",
      "Ep: 526, steps: 21, D loss: 0.272671, acc:  39%, G loss: 1.513769\n",
      "Ep: 526, steps: 22, D loss: 0.167321, acc:  78%, G loss: 1.601632\n",
      "Ep: 526, steps: 23, D loss: 0.226401, acc:  62%, G loss: 1.873982\n",
      "Ep: 526, steps: 24, D loss: 0.222473, acc:  65%, G loss: 1.664590\n",
      "Ep: 526, steps: 25, D loss: 0.239695, acc:  59%, G loss: 1.713118\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 527, steps: 1, D loss: 0.257253, acc:  55%, G loss: 1.615000\n",
      "Ep: 527, steps: 2, D loss: 0.258645, acc:  50%, G loss: 1.495261\n",
      "Ep: 527, steps: 3, D loss: 0.172103, acc:  80%, G loss: 2.022410\n",
      "Ep: 527, steps: 4, D loss: 0.203589, acc:  75%, G loss: 1.760674\n",
      "Ep: 527, steps: 5, D loss: 0.279893, acc:  46%, G loss: 1.722090\n",
      "Ep: 527, steps: 6, D loss: 0.223342, acc:  60%, G loss: 1.617419\n",
      "Ep: 527, steps: 7, D loss: 0.305349, acc:  30%, G loss: 1.524368\n",
      "Ep: 527, steps: 8, D loss: 0.228227, acc:  62%, G loss: 1.696471\n",
      "Ep: 527, steps: 9, D loss: 0.237557, acc:  62%, G loss: 1.677225\n",
      "Ep: 527, steps: 10, D loss: 0.190220, acc:  78%, G loss: 1.601578\n",
      "Ep: 527, steps: 11, D loss: 0.239053, acc:  58%, G loss: 1.771207\n",
      "Ep: 527, steps: 12, D loss: 0.287347, acc:  36%, G loss: 1.441979\n",
      "Ep: 527, steps: 13, D loss: 0.273764, acc:  45%, G loss: 1.519830\n",
      "Ep: 527, steps: 14, D loss: 0.270474, acc:  44%, G loss: 1.511468\n",
      "Ep: 527, steps: 15, D loss: 0.255434, acc:  50%, G loss: 1.672841\n",
      "Ep: 527, steps: 16, D loss: 0.243576, acc:  57%, G loss: 1.596961\n",
      "Ep: 527, steps: 17, D loss: 0.201124, acc:  72%, G loss: 1.592357\n",
      "Ep: 527, steps: 18, D loss: 0.233632, acc:  61%, G loss: 1.561118\n",
      "Ep: 527, steps: 19, D loss: 0.197701, acc:  70%, G loss: 1.593651\n",
      "Ep: 527, steps: 20, D loss: 0.157966, acc:  82%, G loss: 1.810601\n",
      "Ep: 527, steps: 21, D loss: 0.279320, acc:  40%, G loss: 1.618712\n",
      "Ep: 527, steps: 22, D loss: 0.157149, acc:  78%, G loss: 1.658008\n",
      "Ep: 527, steps: 23, D loss: 0.246178, acc:  57%, G loss: 1.900768\n",
      "Ep: 527, steps: 24, D loss: 0.210590, acc:  68%, G loss: 1.660330\n",
      "Ep: 527, steps: 25, D loss: 0.258952, acc:  56%, G loss: 1.757135\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 528, steps: 1, D loss: 0.255436, acc:  54%, G loss: 1.677252\n",
      "Ep: 528, steps: 2, D loss: 0.263109, acc:  48%, G loss: 1.488134\n",
      "Ep: 528, steps: 3, D loss: 0.182106, acc:  78%, G loss: 1.962750\n",
      "Ep: 528, steps: 4, D loss: 0.192747, acc:  80%, G loss: 1.752837\n",
      "Ep: 528, steps: 5, D loss: 0.270107, acc:  49%, G loss: 1.687087\n",
      "Ep: 528, steps: 6, D loss: 0.237684, acc:  60%, G loss: 1.622864\n",
      "Ep: 528, steps: 7, D loss: 0.337562, acc:  27%, G loss: 1.496816\n",
      "Ep: 528, steps: 8, D loss: 0.229105, acc:  63%, G loss: 1.718900\n",
      "Ep: 528, steps: 9, D loss: 0.219010, acc:  67%, G loss: 1.637032\n",
      "Ep: 528, steps: 10, D loss: 0.173042, acc:  83%, G loss: 1.593552\n",
      "Ep: 528, steps: 11, D loss: 0.265188, acc:  48%, G loss: 1.813726\n",
      "Ep: 528, steps: 12, D loss: 0.281664, acc:  40%, G loss: 1.396189\n",
      "Ep: 528, steps: 13, D loss: 0.286302, acc:  37%, G loss: 1.450389\n",
      "Ep: 528, steps: 14, D loss: 0.268034, acc:  44%, G loss: 1.497517\n",
      "Ep: 528, steps: 15, D loss: 0.245341, acc:  54%, G loss: 1.580764\n",
      "Ep: 528, steps: 16, D loss: 0.250932, acc:  54%, G loss: 1.630695\n",
      "Ep: 528, steps: 17, D loss: 0.220182, acc:  67%, G loss: 1.654203\n",
      "Ep: 528, steps: 18, D loss: 0.234087, acc:  61%, G loss: 1.608125\n",
      "Ep: 528, steps: 19, D loss: 0.224341, acc:  64%, G loss: 1.613944\n",
      "Saved Model\n",
      "Ep: 528, steps: 20, D loss: 0.187089, acc:  74%, G loss: 1.728474\n",
      "Ep: 528, steps: 21, D loss: 0.158919, acc:  78%, G loss: 1.630128\n",
      "Ep: 528, steps: 22, D loss: 0.210186, acc:  69%, G loss: 1.932947\n",
      "Ep: 528, steps: 23, D loss: 0.196087, acc:  74%, G loss: 1.526981\n",
      "Ep: 528, steps: 24, D loss: 0.264267, acc:  50%, G loss: 1.444289\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 529, steps: 1, D loss: 0.231087, acc:  61%, G loss: 1.720127\n",
      "Ep: 529, steps: 2, D loss: 0.239924, acc:  58%, G loss: 1.448921\n",
      "Ep: 529, steps: 3, D loss: 0.179093, acc:  80%, G loss: 1.962312\n",
      "Ep: 529, steps: 4, D loss: 0.174718, acc:  85%, G loss: 1.787174\n",
      "Ep: 529, steps: 5, D loss: 0.291858, acc:  43%, G loss: 1.662715\n",
      "Ep: 529, steps: 6, D loss: 0.222438, acc:  62%, G loss: 1.663348\n",
      "Ep: 529, steps: 7, D loss: 0.327113, acc:  27%, G loss: 1.602620\n",
      "Ep: 529, steps: 8, D loss: 0.228777, acc:  59%, G loss: 1.699314\n",
      "Ep: 529, steps: 9, D loss: 0.238412, acc:  64%, G loss: 1.607690\n",
      "Ep: 529, steps: 10, D loss: 0.193956, acc:  79%, G loss: 1.566698\n",
      "Ep: 529, steps: 11, D loss: 0.262663, acc:  49%, G loss: 1.816494\n",
      "Ep: 529, steps: 12, D loss: 0.299157, acc:  31%, G loss: 1.455027\n",
      "Ep: 529, steps: 13, D loss: 0.281613, acc:  40%, G loss: 1.427900\n",
      "Ep: 529, steps: 14, D loss: 0.285987, acc:  36%, G loss: 1.461159\n",
      "Ep: 529, steps: 15, D loss: 0.245765, acc:  52%, G loss: 1.611440\n",
      "Ep: 529, steps: 16, D loss: 0.251660, acc:  54%, G loss: 1.642234\n",
      "Ep: 529, steps: 17, D loss: 0.206546, acc:  71%, G loss: 1.622112\n",
      "Ep: 529, steps: 18, D loss: 0.232201, acc:  62%, G loss: 1.617359\n",
      "Ep: 529, steps: 19, D loss: 0.202588, acc:  71%, G loss: 1.618360\n",
      "Ep: 529, steps: 20, D loss: 0.185944, acc:  74%, G loss: 1.733612\n",
      "Ep: 529, steps: 21, D loss: 0.303943, acc:  29%, G loss: 1.482724\n",
      "Ep: 529, steps: 22, D loss: 0.163424, acc:  76%, G loss: 1.716976\n",
      "Ep: 529, steps: 23, D loss: 0.242595, acc:  57%, G loss: 1.978223\n",
      "Ep: 529, steps: 24, D loss: 0.212682, acc:  67%, G loss: 1.651671\n",
      "Ep: 529, steps: 25, D loss: 0.255205, acc:  52%, G loss: 1.536897\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 530, steps: 1, D loss: 0.242143, acc:  59%, G loss: 1.690016\n",
      "Ep: 530, steps: 2, D loss: 0.238101, acc:  57%, G loss: 1.454182\n",
      "Ep: 530, steps: 3, D loss: 0.177596, acc:  80%, G loss: 1.896123\n",
      "Ep: 530, steps: 4, D loss: 0.192445, acc:  80%, G loss: 1.710181\n",
      "Ep: 530, steps: 5, D loss: 0.280696, acc:  45%, G loss: 1.685871\n",
      "Ep: 530, steps: 6, D loss: 0.237916, acc:  57%, G loss: 1.626745\n",
      "Ep: 530, steps: 7, D loss: 0.307074, acc:  33%, G loss: 1.458426\n",
      "Ep: 530, steps: 8, D loss: 0.234018, acc:  61%, G loss: 1.729052\n",
      "Ep: 530, steps: 9, D loss: 0.236104, acc:  60%, G loss: 1.613108\n",
      "Ep: 530, steps: 10, D loss: 0.185829, acc:  77%, G loss: 1.575489\n",
      "Ep: 530, steps: 11, D loss: 0.260287, acc:  48%, G loss: 1.738696\n",
      "Ep: 530, steps: 12, D loss: 0.294516, acc:  33%, G loss: 1.446892\n",
      "Ep: 530, steps: 13, D loss: 0.277308, acc:  41%, G loss: 1.446454\n",
      "Ep: 530, steps: 14, D loss: 0.279975, acc:  38%, G loss: 1.489330\n",
      "Ep: 530, steps: 15, D loss: 0.235606, acc:  60%, G loss: 1.536661\n",
      "Ep: 530, steps: 16, D loss: 0.251880, acc:  53%, G loss: 1.597871\n",
      "Ep: 530, steps: 17, D loss: 0.206375, acc:  71%, G loss: 1.656867\n",
      "Ep: 530, steps: 18, D loss: 0.234990, acc:  59%, G loss: 1.607742\n",
      "Ep: 530, steps: 19, D loss: 0.208404, acc:  67%, G loss: 1.613532\n",
      "Ep: 530, steps: 20, D loss: 0.195056, acc:  68%, G loss: 1.839893\n",
      "Ep: 530, steps: 21, D loss: 0.278124, acc:  39%, G loss: 1.453911\n",
      "Ep: 530, steps: 22, D loss: 0.157303, acc:  79%, G loss: 1.629892\n",
      "Ep: 530, steps: 23, D loss: 0.218325, acc:  67%, G loss: 1.953128\n",
      "Ep: 530, steps: 24, D loss: 0.222469, acc:  63%, G loss: 1.617736\n",
      "Ep: 530, steps: 25, D loss: 0.238212, acc:  60%, G loss: 1.534178\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 531, steps: 1, D loss: 0.245530, acc:  59%, G loss: 1.744071\n",
      "Ep: 531, steps: 2, D loss: 0.246433, acc:  54%, G loss: 1.439068\n",
      "Ep: 531, steps: 3, D loss: 0.176532, acc:  79%, G loss: 1.969882\n",
      "Ep: 531, steps: 4, D loss: 0.204160, acc:  74%, G loss: 1.732949\n",
      "Ep: 531, steps: 5, D loss: 0.270479, acc:  47%, G loss: 1.653512\n",
      "Ep: 531, steps: 6, D loss: 0.227547, acc:  61%, G loss: 1.681366\n",
      "Ep: 531, steps: 7, D loss: 0.337432, acc:  24%, G loss: 1.616787\n",
      "Ep: 531, steps: 8, D loss: 0.230017, acc:  63%, G loss: 1.698213\n",
      "Ep: 531, steps: 9, D loss: 0.232606, acc:  63%, G loss: 1.615428\n",
      "Ep: 531, steps: 10, D loss: 0.180615, acc:  81%, G loss: 1.687389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 531, steps: 11, D loss: 0.256631, acc:  53%, G loss: 1.830977\n",
      "Ep: 531, steps: 12, D loss: 0.290466, acc:  36%, G loss: 1.372143\n",
      "Ep: 531, steps: 13, D loss: 0.285287, acc:  37%, G loss: 1.475285\n",
      "Ep: 531, steps: 14, D loss: 0.265282, acc:  47%, G loss: 1.489651\n",
      "Ep: 531, steps: 15, D loss: 0.254545, acc:  50%, G loss: 1.673646\n",
      "Ep: 531, steps: 16, D loss: 0.244374, acc:  57%, G loss: 1.626067\n",
      "Ep: 531, steps: 17, D loss: 0.217423, acc:  69%, G loss: 1.606398\n",
      "Saved Model\n",
      "Ep: 531, steps: 18, D loss: 0.229997, acc:  63%, G loss: 1.583542\n",
      "Ep: 531, steps: 19, D loss: 0.193696, acc:  72%, G loss: 1.728318\n",
      "Ep: 531, steps: 20, D loss: 0.261266, acc:  43%, G loss: 1.532088\n",
      "Ep: 531, steps: 21, D loss: 0.187459, acc:  70%, G loss: 1.597788\n",
      "Ep: 531, steps: 22, D loss: 0.251338, acc:  52%, G loss: 1.796407\n",
      "Ep: 531, steps: 23, D loss: 0.209791, acc:  72%, G loss: 1.648448\n",
      "Ep: 531, steps: 24, D loss: 0.256643, acc:  54%, G loss: 1.553895\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 532, steps: 1, D loss: 0.244532, acc:  57%, G loss: 1.693748\n",
      "Ep: 532, steps: 2, D loss: 0.250186, acc:  56%, G loss: 1.401776\n",
      "Ep: 532, steps: 3, D loss: 0.175379, acc:  79%, G loss: 1.929855\n",
      "Ep: 532, steps: 4, D loss: 0.204554, acc:  77%, G loss: 1.700753\n",
      "Ep: 532, steps: 5, D loss: 0.283236, acc:  43%, G loss: 1.649511\n",
      "Ep: 532, steps: 6, D loss: 0.230353, acc:  58%, G loss: 1.648560\n",
      "Ep: 532, steps: 7, D loss: 0.309210, acc:  31%, G loss: 1.498981\n",
      "Ep: 532, steps: 8, D loss: 0.231682, acc:  62%, G loss: 1.671053\n",
      "Ep: 532, steps: 9, D loss: 0.243105, acc:  57%, G loss: 1.589015\n",
      "Ep: 532, steps: 10, D loss: 0.205581, acc:  70%, G loss: 1.538085\n",
      "Ep: 532, steps: 11, D loss: 0.251661, acc:  53%, G loss: 1.728211\n",
      "Ep: 532, steps: 12, D loss: 0.293648, acc:  34%, G loss: 1.367197\n",
      "Ep: 532, steps: 13, D loss: 0.282335, acc:  39%, G loss: 1.448717\n",
      "Ep: 532, steps: 14, D loss: 0.267738, acc:  44%, G loss: 1.501970\n",
      "Ep: 532, steps: 15, D loss: 0.252093, acc:  52%, G loss: 1.596871\n",
      "Ep: 532, steps: 16, D loss: 0.238089, acc:  60%, G loss: 1.648517\n",
      "Ep: 532, steps: 17, D loss: 0.213125, acc:  71%, G loss: 1.615186\n",
      "Ep: 532, steps: 18, D loss: 0.233546, acc:  62%, G loss: 1.618286\n",
      "Ep: 532, steps: 19, D loss: 0.220675, acc:  67%, G loss: 1.614987\n",
      "Ep: 532, steps: 20, D loss: 0.184727, acc:  77%, G loss: 1.712049\n",
      "Ep: 532, steps: 21, D loss: 0.283742, acc:  35%, G loss: 1.506805\n",
      "Ep: 532, steps: 22, D loss: 0.189810, acc:  71%, G loss: 1.617228\n",
      "Ep: 532, steps: 23, D loss: 0.235873, acc:  61%, G loss: 1.871213\n",
      "Ep: 532, steps: 24, D loss: 0.211440, acc:  70%, G loss: 1.654764\n",
      "Ep: 532, steps: 25, D loss: 0.263924, acc:  50%, G loss: 1.600207\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 533, steps: 1, D loss: 0.264129, acc:  52%, G loss: 1.691141\n",
      "Ep: 533, steps: 2, D loss: 0.253755, acc:  51%, G loss: 1.460645\n",
      "Ep: 533, steps: 3, D loss: 0.178327, acc:  78%, G loss: 1.997613\n",
      "Ep: 533, steps: 4, D loss: 0.194186, acc:  80%, G loss: 1.741686\n",
      "Ep: 533, steps: 5, D loss: 0.278294, acc:  47%, G loss: 1.674384\n",
      "Ep: 533, steps: 6, D loss: 0.230198, acc:  57%, G loss: 1.645434\n",
      "Ep: 533, steps: 7, D loss: 0.297914, acc:  32%, G loss: 1.499011\n",
      "Ep: 533, steps: 8, D loss: 0.233932, acc:  59%, G loss: 1.683529\n",
      "Ep: 533, steps: 9, D loss: 0.252250, acc:  56%, G loss: 1.640545\n",
      "Ep: 533, steps: 10, D loss: 0.189371, acc:  78%, G loss: 1.615741\n",
      "Ep: 533, steps: 11, D loss: 0.259878, acc:  49%, G loss: 1.866576\n",
      "Ep: 533, steps: 12, D loss: 0.291036, acc:  36%, G loss: 1.443189\n",
      "Ep: 533, steps: 13, D loss: 0.275601, acc:  41%, G loss: 1.430079\n",
      "Ep: 533, steps: 14, D loss: 0.266767, acc:  46%, G loss: 1.499475\n",
      "Ep: 533, steps: 15, D loss: 0.249461, acc:  52%, G loss: 1.575088\n",
      "Ep: 533, steps: 16, D loss: 0.243672, acc:  58%, G loss: 1.673602\n",
      "Ep: 533, steps: 17, D loss: 0.216381, acc:  69%, G loss: 1.527662\n",
      "Ep: 533, steps: 18, D loss: 0.225311, acc:  65%, G loss: 1.644989\n",
      "Ep: 533, steps: 19, D loss: 0.215470, acc:  66%, G loss: 1.582978\n",
      "Ep: 533, steps: 20, D loss: 0.178986, acc:  74%, G loss: 1.748212\n",
      "Ep: 533, steps: 21, D loss: 0.275648, acc:  38%, G loss: 1.572383\n",
      "Ep: 533, steps: 22, D loss: 0.171156, acc:  74%, G loss: 1.596458\n",
      "Ep: 533, steps: 23, D loss: 0.222630, acc:  65%, G loss: 1.960790\n",
      "Ep: 533, steps: 24, D loss: 0.204271, acc:  70%, G loss: 1.659526\n",
      "Ep: 533, steps: 25, D loss: 0.257232, acc:  52%, G loss: 1.806583\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 534, steps: 1, D loss: 0.245806, acc:  56%, G loss: 1.693975\n",
      "Ep: 534, steps: 2, D loss: 0.244444, acc:  55%, G loss: 1.511196\n",
      "Ep: 534, steps: 3, D loss: 0.165316, acc:  82%, G loss: 1.938786\n",
      "Ep: 534, steps: 4, D loss: 0.190275, acc:  82%, G loss: 1.711739\n",
      "Ep: 534, steps: 5, D loss: 0.281649, acc:  46%, G loss: 1.632309\n",
      "Ep: 534, steps: 6, D loss: 0.224806, acc:  59%, G loss: 1.674244\n",
      "Ep: 534, steps: 7, D loss: 0.311918, acc:  33%, G loss: 1.594352\n",
      "Ep: 534, steps: 8, D loss: 0.226408, acc:  63%, G loss: 1.796855\n",
      "Ep: 534, steps: 9, D loss: 0.250542, acc:  56%, G loss: 1.643885\n",
      "Ep: 534, steps: 10, D loss: 0.181354, acc:  79%, G loss: 1.605156\n",
      "Ep: 534, steps: 11, D loss: 0.260785, acc:  49%, G loss: 1.804294\n",
      "Ep: 534, steps: 12, D loss: 0.309057, acc:  28%, G loss: 1.444375\n",
      "Ep: 534, steps: 13, D loss: 0.288091, acc:  38%, G loss: 1.449454\n",
      "Ep: 534, steps: 14, D loss: 0.273653, acc:  41%, G loss: 1.466782\n",
      "Saved Model\n",
      "Ep: 534, steps: 15, D loss: 0.244246, acc:  55%, G loss: 1.646869\n",
      "Ep: 534, steps: 16, D loss: 0.200705, acc:  75%, G loss: 1.685844\n",
      "Ep: 534, steps: 17, D loss: 0.253076, acc:  53%, G loss: 1.560755\n",
      "Ep: 534, steps: 18, D loss: 0.225459, acc:  63%, G loss: 1.588388\n",
      "Ep: 534, steps: 19, D loss: 0.192358, acc:  74%, G loss: 1.699239\n",
      "Ep: 534, steps: 20, D loss: 0.278796, acc:  35%, G loss: 1.525135\n",
      "Ep: 534, steps: 21, D loss: 0.174961, acc:  74%, G loss: 1.670113\n",
      "Ep: 534, steps: 22, D loss: 0.204463, acc:  72%, G loss: 1.958143\n",
      "Ep: 534, steps: 23, D loss: 0.209446, acc:  69%, G loss: 1.619705\n",
      "Ep: 534, steps: 24, D loss: 0.228622, acc:  62%, G loss: 1.571479\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 535, steps: 1, D loss: 0.262318, acc:  52%, G loss: 1.789638\n",
      "Ep: 535, steps: 2, D loss: 0.247921, acc:  53%, G loss: 1.458330\n",
      "Ep: 535, steps: 3, D loss: 0.183087, acc:  77%, G loss: 1.920471\n",
      "Ep: 535, steps: 4, D loss: 0.185237, acc:  84%, G loss: 1.772787\n",
      "Ep: 535, steps: 5, D loss: 0.285743, acc:  47%, G loss: 1.687082\n",
      "Ep: 535, steps: 6, D loss: 0.224887, acc:  57%, G loss: 1.644106\n",
      "Ep: 535, steps: 7, D loss: 0.339604, acc:  25%, G loss: 1.558653\n",
      "Ep: 535, steps: 8, D loss: 0.213867, acc:  66%, G loss: 1.663495\n",
      "Ep: 535, steps: 9, D loss: 0.235124, acc:  62%, G loss: 1.610318\n",
      "Ep: 535, steps: 10, D loss: 0.192542, acc:  76%, G loss: 1.591456\n",
      "Ep: 535, steps: 11, D loss: 0.240970, acc:  56%, G loss: 1.732506\n",
      "Ep: 535, steps: 12, D loss: 0.286527, acc:  38%, G loss: 1.376985\n",
      "Ep: 535, steps: 13, D loss: 0.278056, acc:  40%, G loss: 1.402412\n",
      "Ep: 535, steps: 14, D loss: 0.263783, acc:  48%, G loss: 1.513400\n",
      "Ep: 535, steps: 15, D loss: 0.242878, acc:  55%, G loss: 1.595883\n",
      "Ep: 535, steps: 16, D loss: 0.244538, acc:  58%, G loss: 1.625401\n",
      "Ep: 535, steps: 17, D loss: 0.213536, acc:  69%, G loss: 1.525532\n",
      "Ep: 535, steps: 18, D loss: 0.237456, acc:  59%, G loss: 1.605226\n",
      "Ep: 535, steps: 19, D loss: 0.218910, acc:  67%, G loss: 1.596839\n",
      "Ep: 535, steps: 20, D loss: 0.189104, acc:  73%, G loss: 1.723297\n",
      "Ep: 535, steps: 21, D loss: 0.280573, acc:  36%, G loss: 1.495296\n",
      "Ep: 535, steps: 22, D loss: 0.157750, acc:  81%, G loss: 1.641121\n",
      "Ep: 535, steps: 23, D loss: 0.248910, acc:  55%, G loss: 1.861210\n",
      "Ep: 535, steps: 24, D loss: 0.207777, acc:  70%, G loss: 1.621820\n",
      "Ep: 535, steps: 25, D loss: 0.253759, acc:  53%, G loss: 1.516812\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 536, steps: 1, D loss: 0.246298, acc:  57%, G loss: 1.700014\n",
      "Ep: 536, steps: 2, D loss: 0.254114, acc:  51%, G loss: 1.504951\n",
      "Ep: 536, steps: 3, D loss: 0.166241, acc:  81%, G loss: 1.953892\n",
      "Ep: 536, steps: 4, D loss: 0.196255, acc:  78%, G loss: 1.755344\n",
      "Ep: 536, steps: 5, D loss: 0.280262, acc:  46%, G loss: 1.690083\n",
      "Ep: 536, steps: 6, D loss: 0.229988, acc:  60%, G loss: 1.666963\n",
      "Ep: 536, steps: 7, D loss: 0.324351, acc:  26%, G loss: 1.697564\n",
      "Ep: 536, steps: 8, D loss: 0.229971, acc:  61%, G loss: 1.804441\n",
      "Ep: 536, steps: 9, D loss: 0.237904, acc:  59%, G loss: 1.643160\n",
      "Ep: 536, steps: 10, D loss: 0.184008, acc:  80%, G loss: 1.571404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 536, steps: 11, D loss: 0.276253, acc:  45%, G loss: 1.812479\n",
      "Ep: 536, steps: 12, D loss: 0.292269, acc:  34%, G loss: 1.405472\n",
      "Ep: 536, steps: 13, D loss: 0.277368, acc:  41%, G loss: 1.428838\n",
      "Ep: 536, steps: 14, D loss: 0.270046, acc:  43%, G loss: 1.490191\n",
      "Ep: 536, steps: 15, D loss: 0.261327, acc:  48%, G loss: 1.584694\n",
      "Ep: 536, steps: 16, D loss: 0.237810, acc:  60%, G loss: 1.670011\n",
      "Ep: 536, steps: 17, D loss: 0.209316, acc:  72%, G loss: 1.566789\n",
      "Ep: 536, steps: 18, D loss: 0.244011, acc:  56%, G loss: 1.649066\n",
      "Ep: 536, steps: 19, D loss: 0.218829, acc:  65%, G loss: 1.604650\n",
      "Ep: 536, steps: 20, D loss: 0.169403, acc:  79%, G loss: 1.762294\n",
      "Ep: 536, steps: 21, D loss: 0.279040, acc:  39%, G loss: 1.456556\n",
      "Ep: 536, steps: 22, D loss: 0.171134, acc:  75%, G loss: 1.674753\n",
      "Ep: 536, steps: 23, D loss: 0.229881, acc:  61%, G loss: 1.934464\n",
      "Ep: 536, steps: 24, D loss: 0.214879, acc:  68%, G loss: 1.598933\n",
      "Ep: 536, steps: 25, D loss: 0.256045, acc:  53%, G loss: 1.664857\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 537, steps: 1, D loss: 0.251226, acc:  56%, G loss: 1.646132\n",
      "Ep: 537, steps: 2, D loss: 0.234864, acc:  59%, G loss: 1.490901\n",
      "Ep: 537, steps: 3, D loss: 0.177076, acc:  79%, G loss: 1.961935\n",
      "Ep: 537, steps: 4, D loss: 0.198977, acc:  76%, G loss: 1.797531\n",
      "Ep: 537, steps: 5, D loss: 0.275133, acc:  46%, G loss: 1.708400\n",
      "Ep: 537, steps: 6, D loss: 0.234464, acc:  57%, G loss: 1.643974\n",
      "Ep: 537, steps: 7, D loss: 0.314127, acc:  30%, G loss: 1.627222\n",
      "Ep: 537, steps: 8, D loss: 0.221111, acc:  64%, G loss: 1.739956\n",
      "Ep: 537, steps: 9, D loss: 0.245934, acc:  58%, G loss: 1.642946\n",
      "Ep: 537, steps: 10, D loss: 0.190928, acc:  76%, G loss: 1.550716\n",
      "Ep: 537, steps: 11, D loss: 0.240865, acc:  55%, G loss: 1.844941\n",
      "Ep: 537, steps: 12, D loss: 0.292297, acc:  34%, G loss: 1.388443\n",
      "Saved Model\n",
      "Ep: 537, steps: 13, D loss: 0.291153, acc:  33%, G loss: 1.423143\n",
      "Ep: 537, steps: 14, D loss: 0.223888, acc:  64%, G loss: 1.643749\n",
      "Ep: 537, steps: 15, D loss: 0.242443, acc:  59%, G loss: 1.600752\n",
      "Ep: 537, steps: 16, D loss: 0.204068, acc:  73%, G loss: 1.653108\n",
      "Ep: 537, steps: 17, D loss: 0.214786, acc:  68%, G loss: 1.613352\n",
      "Ep: 537, steps: 18, D loss: 0.217515, acc:  65%, G loss: 1.585279\n",
      "Ep: 537, steps: 19, D loss: 0.207258, acc:  65%, G loss: 1.752548\n",
      "Ep: 537, steps: 20, D loss: 0.279974, acc:  36%, G loss: 1.454069\n",
      "Ep: 537, steps: 21, D loss: 0.182283, acc:  73%, G loss: 1.655541\n",
      "Ep: 537, steps: 22, D loss: 0.217734, acc:  66%, G loss: 1.905309\n",
      "Ep: 537, steps: 23, D loss: 0.206491, acc:  71%, G loss: 1.597254\n",
      "Ep: 537, steps: 24, D loss: 0.243173, acc:  58%, G loss: 1.629415\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 538, steps: 1, D loss: 0.248172, acc:  57%, G loss: 1.740878\n",
      "Ep: 538, steps: 2, D loss: 0.242405, acc:  57%, G loss: 1.523655\n",
      "Ep: 538, steps: 3, D loss: 0.181965, acc:  78%, G loss: 1.910816\n",
      "Ep: 538, steps: 4, D loss: 0.186790, acc:  82%, G loss: 1.841826\n",
      "Ep: 538, steps: 5, D loss: 0.296754, acc:  43%, G loss: 1.648208\n",
      "Ep: 538, steps: 6, D loss: 0.230031, acc:  56%, G loss: 1.682874\n",
      "Ep: 538, steps: 7, D loss: 0.332177, acc:  28%, G loss: 1.455044\n",
      "Ep: 538, steps: 8, D loss: 0.228588, acc:  62%, G loss: 1.710483\n",
      "Ep: 538, steps: 9, D loss: 0.225563, acc:  66%, G loss: 1.642184\n",
      "Ep: 538, steps: 10, D loss: 0.201763, acc:  73%, G loss: 1.623728\n",
      "Ep: 538, steps: 11, D loss: 0.279639, acc:  45%, G loss: 1.764994\n",
      "Ep: 538, steps: 12, D loss: 0.291228, acc:  33%, G loss: 1.384366\n",
      "Ep: 538, steps: 13, D loss: 0.281623, acc:  38%, G loss: 1.388251\n",
      "Ep: 538, steps: 14, D loss: 0.272821, acc:  42%, G loss: 1.536557\n",
      "Ep: 538, steps: 15, D loss: 0.240462, acc:  57%, G loss: 1.612165\n",
      "Ep: 538, steps: 16, D loss: 0.238602, acc:  60%, G loss: 1.633587\n",
      "Ep: 538, steps: 17, D loss: 0.206778, acc:  73%, G loss: 1.629621\n",
      "Ep: 538, steps: 18, D loss: 0.245159, acc:  57%, G loss: 1.631967\n",
      "Ep: 538, steps: 19, D loss: 0.212914, acc:  66%, G loss: 1.608156\n",
      "Ep: 538, steps: 20, D loss: 0.181851, acc:  74%, G loss: 1.725551\n",
      "Ep: 538, steps: 21, D loss: 0.273903, acc:  39%, G loss: 1.513615\n",
      "Ep: 538, steps: 22, D loss: 0.169083, acc:  72%, G loss: 1.701510\n",
      "Ep: 538, steps: 23, D loss: 0.229773, acc:  61%, G loss: 1.934290\n",
      "Ep: 538, steps: 24, D loss: 0.212820, acc:  69%, G loss: 1.709989\n",
      "Ep: 538, steps: 25, D loss: 0.255535, acc:  54%, G loss: 1.693621\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 539, steps: 1, D loss: 0.248004, acc:  56%, G loss: 1.727850\n",
      "Ep: 539, steps: 2, D loss: 0.241994, acc:  56%, G loss: 1.510540\n",
      "Ep: 539, steps: 3, D loss: 0.170864, acc:  81%, G loss: 1.970893\n",
      "Ep: 539, steps: 4, D loss: 0.180770, acc:  83%, G loss: 1.857539\n",
      "Ep: 539, steps: 5, D loss: 0.290639, acc:  42%, G loss: 1.661788\n",
      "Ep: 539, steps: 6, D loss: 0.237582, acc:  57%, G loss: 1.657011\n",
      "Ep: 539, steps: 7, D loss: 0.342390, acc:  26%, G loss: 1.621415\n",
      "Ep: 539, steps: 8, D loss: 0.229463, acc:  61%, G loss: 1.730555\n",
      "Ep: 539, steps: 9, D loss: 0.235716, acc:  62%, G loss: 1.640458\n",
      "Ep: 539, steps: 10, D loss: 0.194953, acc:  74%, G loss: 1.588926\n",
      "Ep: 539, steps: 11, D loss: 0.257738, acc:  51%, G loss: 1.824025\n",
      "Ep: 539, steps: 12, D loss: 0.291696, acc:  36%, G loss: 1.336885\n",
      "Ep: 539, steps: 13, D loss: 0.282605, acc:  37%, G loss: 1.376917\n",
      "Ep: 539, steps: 14, D loss: 0.263735, acc:  45%, G loss: 1.502535\n",
      "Ep: 539, steps: 15, D loss: 0.264684, acc:  46%, G loss: 1.555826\n",
      "Ep: 539, steps: 16, D loss: 0.242739, acc:  59%, G loss: 1.606932\n",
      "Ep: 539, steps: 17, D loss: 0.220867, acc:  69%, G loss: 1.622733\n",
      "Ep: 539, steps: 18, D loss: 0.235381, acc:  60%, G loss: 1.637049\n",
      "Ep: 539, steps: 19, D loss: 0.216928, acc:  66%, G loss: 1.574110\n",
      "Ep: 539, steps: 20, D loss: 0.199174, acc:  72%, G loss: 1.681518\n",
      "Ep: 539, steps: 21, D loss: 0.275078, acc:  38%, G loss: 1.487653\n",
      "Ep: 539, steps: 22, D loss: 0.176252, acc:  78%, G loss: 1.615003\n",
      "Ep: 539, steps: 23, D loss: 0.236076, acc:  59%, G loss: 1.910932\n",
      "Ep: 539, steps: 24, D loss: 0.216379, acc:  67%, G loss: 1.635888\n",
      "Ep: 539, steps: 25, D loss: 0.252410, acc:  55%, G loss: 1.578869\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 540, steps: 1, D loss: 0.239936, acc:  59%, G loss: 1.621209\n",
      "Ep: 540, steps: 2, D loss: 0.241562, acc:  56%, G loss: 1.513948\n",
      "Ep: 540, steps: 3, D loss: 0.172032, acc:  80%, G loss: 1.906530\n",
      "Ep: 540, steps: 4, D loss: 0.191950, acc:  79%, G loss: 1.843197\n",
      "Ep: 540, steps: 5, D loss: 0.288501, acc:  43%, G loss: 1.687902\n",
      "Ep: 540, steps: 6, D loss: 0.232922, acc:  59%, G loss: 1.616850\n",
      "Ep: 540, steps: 7, D loss: 0.304301, acc:  33%, G loss: 1.500020\n",
      "Ep: 540, steps: 8, D loss: 0.222691, acc:  65%, G loss: 1.748583\n",
      "Ep: 540, steps: 9, D loss: 0.225357, acc:  65%, G loss: 1.581414\n",
      "Ep: 540, steps: 10, D loss: 0.195696, acc:  77%, G loss: 1.589755\n",
      "Saved Model\n",
      "Ep: 540, steps: 11, D loss: 0.257822, acc:  52%, G loss: 1.959556\n",
      "Ep: 540, steps: 12, D loss: 0.308037, acc:  29%, G loss: 1.351949\n",
      "Ep: 540, steps: 13, D loss: 0.272749, acc:  44%, G loss: 1.502073\n",
      "Ep: 540, steps: 14, D loss: 0.251035, acc:  52%, G loss: 1.615193\n",
      "Ep: 540, steps: 15, D loss: 0.242149, acc:  57%, G loss: 1.605696\n",
      "Ep: 540, steps: 16, D loss: 0.190992, acc:  77%, G loss: 1.712629\n",
      "Ep: 540, steps: 17, D loss: 0.232841, acc:  62%, G loss: 1.594255\n",
      "Ep: 540, steps: 18, D loss: 0.193305, acc:  71%, G loss: 1.602058\n",
      "Ep: 540, steps: 19, D loss: 0.180462, acc:  75%, G loss: 1.830557\n",
      "Ep: 540, steps: 20, D loss: 0.269971, acc:  41%, G loss: 1.521332\n",
      "Ep: 540, steps: 21, D loss: 0.165591, acc:  75%, G loss: 1.776678\n",
      "Ep: 540, steps: 22, D loss: 0.233082, acc:  61%, G loss: 1.941354\n",
      "Ep: 540, steps: 23, D loss: 0.206461, acc:  69%, G loss: 1.598408\n",
      "Ep: 540, steps: 24, D loss: 0.287306, acc:  48%, G loss: 2.046984\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 541, steps: 1, D loss: 0.256074, acc:  54%, G loss: 1.755887\n",
      "Ep: 541, steps: 2, D loss: 0.237027, acc:  58%, G loss: 1.593485\n",
      "Ep: 541, steps: 3, D loss: 0.160662, acc:  83%, G loss: 2.100346\n",
      "Ep: 541, steps: 4, D loss: 0.189489, acc:  82%, G loss: 1.828525\n",
      "Ep: 541, steps: 5, D loss: 0.282552, acc:  43%, G loss: 1.693453\n",
      "Ep: 541, steps: 6, D loss: 0.240734, acc:  56%, G loss: 1.593615\n",
      "Ep: 541, steps: 7, D loss: 0.346234, acc:  24%, G loss: 1.491585\n",
      "Ep: 541, steps: 8, D loss: 0.226833, acc:  62%, G loss: 1.726915\n",
      "Ep: 541, steps: 9, D loss: 0.233476, acc:  62%, G loss: 1.608322\n",
      "Ep: 541, steps: 10, D loss: 0.177987, acc:  81%, G loss: 1.604890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 541, steps: 11, D loss: 0.253587, acc:  54%, G loss: 1.792156\n",
      "Ep: 541, steps: 12, D loss: 0.303771, acc:  31%, G loss: 1.357872\n",
      "Ep: 541, steps: 13, D loss: 0.291550, acc:  35%, G loss: 1.400104\n",
      "Ep: 541, steps: 14, D loss: 0.261345, acc:  48%, G loss: 1.474813\n",
      "Ep: 541, steps: 15, D loss: 0.259258, acc:  47%, G loss: 1.570734\n",
      "Ep: 541, steps: 16, D loss: 0.233122, acc:  63%, G loss: 1.608594\n",
      "Ep: 541, steps: 17, D loss: 0.211390, acc:  71%, G loss: 1.603776\n",
      "Ep: 541, steps: 18, D loss: 0.252636, acc:  54%, G loss: 1.593184\n",
      "Ep: 541, steps: 19, D loss: 0.211455, acc:  69%, G loss: 1.563697\n",
      "Ep: 541, steps: 20, D loss: 0.204828, acc:  71%, G loss: 1.722177\n",
      "Ep: 541, steps: 21, D loss: 0.264092, acc:  44%, G loss: 1.426499\n",
      "Ep: 541, steps: 22, D loss: 0.195589, acc:  71%, G loss: 1.656960\n",
      "Ep: 541, steps: 23, D loss: 0.212204, acc:  68%, G loss: 1.830989\n",
      "Ep: 541, steps: 24, D loss: 0.203833, acc:  72%, G loss: 1.605774\n",
      "Ep: 541, steps: 25, D loss: 0.213621, acc:  66%, G loss: 1.667141\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 542, steps: 1, D loss: 0.257956, acc:  53%, G loss: 1.762443\n",
      "Ep: 542, steps: 2, D loss: 0.233420, acc:  59%, G loss: 1.573114\n",
      "Ep: 542, steps: 3, D loss: 0.166702, acc:  81%, G loss: 2.023146\n",
      "Ep: 542, steps: 4, D loss: 0.190092, acc:  79%, G loss: 1.845656\n",
      "Ep: 542, steps: 5, D loss: 0.320501, acc:  37%, G loss: 1.650055\n",
      "Ep: 542, steps: 6, D loss: 0.241921, acc:  55%, G loss: 1.622661\n",
      "Ep: 542, steps: 7, D loss: 0.313379, acc:  30%, G loss: 1.529280\n",
      "Ep: 542, steps: 8, D loss: 0.234928, acc:  61%, G loss: 1.708168\n",
      "Ep: 542, steps: 9, D loss: 0.246306, acc:  59%, G loss: 1.635085\n",
      "Ep: 542, steps: 10, D loss: 0.194414, acc:  72%, G loss: 1.555715\n",
      "Ep: 542, steps: 11, D loss: 0.239265, acc:  59%, G loss: 1.816591\n",
      "Ep: 542, steps: 12, D loss: 0.304860, acc:  32%, G loss: 1.408847\n",
      "Ep: 542, steps: 13, D loss: 0.282114, acc:  39%, G loss: 1.348369\n",
      "Ep: 542, steps: 14, D loss: 0.274787, acc:  42%, G loss: 1.451035\n",
      "Ep: 542, steps: 15, D loss: 0.239369, acc:  57%, G loss: 1.580419\n",
      "Ep: 542, steps: 16, D loss: 0.245642, acc:  57%, G loss: 1.589629\n",
      "Ep: 542, steps: 17, D loss: 0.209810, acc:  71%, G loss: 1.600934\n",
      "Ep: 542, steps: 18, D loss: 0.226490, acc:  62%, G loss: 1.683667\n",
      "Ep: 542, steps: 19, D loss: 0.215352, acc:  65%, G loss: 1.564565\n",
      "Ep: 542, steps: 20, D loss: 0.170796, acc:  81%, G loss: 1.693086\n",
      "Ep: 542, steps: 21, D loss: 0.268667, acc:  40%, G loss: 1.446870\n",
      "Ep: 542, steps: 22, D loss: 0.174489, acc:  74%, G loss: 1.724943\n",
      "Ep: 542, steps: 23, D loss: 0.234575, acc:  60%, G loss: 1.861823\n",
      "Ep: 542, steps: 24, D loss: 0.206918, acc:  69%, G loss: 1.666559\n",
      "Ep: 542, steps: 25, D loss: 0.237500, acc:  59%, G loss: 1.557561\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 543, steps: 1, D loss: 0.243094, acc:  58%, G loss: 1.743829\n",
      "Ep: 543, steps: 2, D loss: 0.261732, acc:  50%, G loss: 1.558588\n",
      "Ep: 543, steps: 3, D loss: 0.172653, acc:  82%, G loss: 2.123766\n",
      "Ep: 543, steps: 4, D loss: 0.189028, acc:  80%, G loss: 1.906924\n",
      "Ep: 543, steps: 5, D loss: 0.285814, acc:  42%, G loss: 1.663855\n",
      "Ep: 543, steps: 6, D loss: 0.244096, acc:  56%, G loss: 1.650358\n",
      "Ep: 543, steps: 7, D loss: 0.315918, acc:  30%, G loss: 1.559963\n",
      "Ep: 543, steps: 8, D loss: 0.227550, acc:  62%, G loss: 1.711637\n",
      "Saved Model\n",
      "Ep: 543, steps: 9, D loss: 0.239467, acc:  61%, G loss: 1.650354\n",
      "Ep: 543, steps: 10, D loss: 0.272999, acc:  48%, G loss: 1.765459\n",
      "Ep: 543, steps: 11, D loss: 0.284819, acc:  37%, G loss: 1.400713\n",
      "Ep: 543, steps: 12, D loss: 0.275667, acc:  39%, G loss: 1.424759\n",
      "Ep: 543, steps: 13, D loss: 0.256107, acc:  48%, G loss: 1.519308\n",
      "Ep: 543, steps: 14, D loss: 0.261780, acc:  46%, G loss: 1.505993\n",
      "Ep: 543, steps: 15, D loss: 0.241835, acc:  58%, G loss: 1.618154\n",
      "Ep: 543, steps: 16, D loss: 0.217887, acc:  71%, G loss: 1.569075\n",
      "Ep: 543, steps: 17, D loss: 0.257683, acc:  53%, G loss: 1.661647\n",
      "Ep: 543, steps: 18, D loss: 0.224207, acc:  62%, G loss: 1.554075\n",
      "Ep: 543, steps: 19, D loss: 0.183521, acc:  76%, G loss: 1.730865\n",
      "Ep: 543, steps: 20, D loss: 0.269298, acc:  41%, G loss: 1.506891\n",
      "Ep: 543, steps: 21, D loss: 0.176480, acc:  71%, G loss: 1.684811\n",
      "Ep: 543, steps: 22, D loss: 0.219694, acc:  66%, G loss: 1.878269\n",
      "Ep: 543, steps: 23, D loss: 0.211436, acc:  70%, G loss: 1.608090\n",
      "Ep: 543, steps: 24, D loss: 0.245890, acc:  57%, G loss: 1.654572\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 544, steps: 1, D loss: 0.255950, acc:  52%, G loss: 1.646276\n",
      "Ep: 544, steps: 2, D loss: 0.225739, acc:  62%, G loss: 1.496667\n",
      "Ep: 544, steps: 3, D loss: 0.173152, acc:  79%, G loss: 1.987810\n",
      "Ep: 544, steps: 4, D loss: 0.190484, acc:  80%, G loss: 1.774196\n",
      "Ep: 544, steps: 5, D loss: 0.297048, acc:  44%, G loss: 1.639677\n",
      "Ep: 544, steps: 6, D loss: 0.240297, acc:  54%, G loss: 1.602408\n",
      "Ep: 544, steps: 7, D loss: 0.307153, acc:  32%, G loss: 1.583763\n",
      "Ep: 544, steps: 8, D loss: 0.219447, acc:  63%, G loss: 1.744899\n",
      "Ep: 544, steps: 9, D loss: 0.236452, acc:  62%, G loss: 1.618182\n",
      "Ep: 544, steps: 10, D loss: 0.193423, acc:  74%, G loss: 1.529324\n",
      "Ep: 544, steps: 11, D loss: 0.253654, acc:  52%, G loss: 1.801977\n",
      "Ep: 544, steps: 12, D loss: 0.293348, acc:  34%, G loss: 1.366799\n",
      "Ep: 544, steps: 13, D loss: 0.292190, acc:  32%, G loss: 1.386613\n",
      "Ep: 544, steps: 14, D loss: 0.269405, acc:  45%, G loss: 1.464036\n",
      "Ep: 544, steps: 15, D loss: 0.257038, acc:  50%, G loss: 1.594353\n",
      "Ep: 544, steps: 16, D loss: 0.242195, acc:  58%, G loss: 1.564509\n",
      "Ep: 544, steps: 17, D loss: 0.205235, acc:  73%, G loss: 1.502495\n",
      "Ep: 544, steps: 18, D loss: 0.239535, acc:  58%, G loss: 1.582232\n",
      "Ep: 544, steps: 19, D loss: 0.236170, acc:  60%, G loss: 1.580305\n",
      "Ep: 544, steps: 20, D loss: 0.187899, acc:  77%, G loss: 1.753055\n",
      "Ep: 544, steps: 21, D loss: 0.270396, acc:  40%, G loss: 1.499831\n",
      "Ep: 544, steps: 22, D loss: 0.192621, acc:  70%, G loss: 1.668545\n",
      "Ep: 544, steps: 23, D loss: 0.227547, acc:  64%, G loss: 1.894880\n",
      "Ep: 544, steps: 24, D loss: 0.223511, acc:  65%, G loss: 1.600204\n",
      "Ep: 544, steps: 25, D loss: 0.235223, acc:  59%, G loss: 1.495353\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 545, steps: 1, D loss: 0.241565, acc:  58%, G loss: 1.646194\n",
      "Ep: 545, steps: 2, D loss: 0.243108, acc:  57%, G loss: 1.489958\n",
      "Ep: 545, steps: 3, D loss: 0.172712, acc:  80%, G loss: 1.928365\n",
      "Ep: 545, steps: 4, D loss: 0.192404, acc:  78%, G loss: 1.813652\n",
      "Ep: 545, steps: 5, D loss: 0.277614, acc:  49%, G loss: 1.612281\n",
      "Ep: 545, steps: 6, D loss: 0.219972, acc:  61%, G loss: 1.617746\n",
      "Ep: 545, steps: 7, D loss: 0.314352, acc:  30%, G loss: 1.524506\n",
      "Ep: 545, steps: 8, D loss: 0.223499, acc:  64%, G loss: 1.798769\n",
      "Ep: 545, steps: 9, D loss: 0.221661, acc:  68%, G loss: 1.626008\n",
      "Ep: 545, steps: 10, D loss: 0.201345, acc:  72%, G loss: 1.709087\n",
      "Ep: 545, steps: 11, D loss: 0.264419, acc:  46%, G loss: 1.691235\n",
      "Ep: 545, steps: 12, D loss: 0.279002, acc:  39%, G loss: 1.367146\n",
      "Ep: 545, steps: 13, D loss: 0.281146, acc:  38%, G loss: 1.427557\n",
      "Ep: 545, steps: 14, D loss: 0.277698, acc:  40%, G loss: 1.491333\n",
      "Ep: 545, steps: 15, D loss: 0.264993, acc:  46%, G loss: 1.677813\n",
      "Ep: 545, steps: 16, D loss: 0.248339, acc:  55%, G loss: 1.659962\n",
      "Ep: 545, steps: 17, D loss: 0.213228, acc:  68%, G loss: 1.583212\n",
      "Ep: 545, steps: 18, D loss: 0.236604, acc:  61%, G loss: 1.590272\n",
      "Ep: 545, steps: 19, D loss: 0.201064, acc:  68%, G loss: 1.596814\n",
      "Ep: 545, steps: 20, D loss: 0.173801, acc:  78%, G loss: 1.726961\n",
      "Ep: 545, steps: 21, D loss: 0.260225, acc:  44%, G loss: 1.559444\n",
      "Ep: 545, steps: 22, D loss: 0.169052, acc:  74%, G loss: 1.796879\n",
      "Ep: 545, steps: 23, D loss: 0.224803, acc:  64%, G loss: 1.878143\n",
      "Ep: 545, steps: 24, D loss: 0.202649, acc:  71%, G loss: 1.576573\n",
      "Ep: 545, steps: 25, D loss: 0.261179, acc:  53%, G loss: 1.492743\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 546, steps: 1, D loss: 0.243999, acc:  56%, G loss: 1.680090\n",
      "Ep: 546, steps: 2, D loss: 0.257291, acc:  52%, G loss: 1.510294\n",
      "Ep: 546, steps: 3, D loss: 0.175513, acc:  81%, G loss: 1.955010\n",
      "Ep: 546, steps: 4, D loss: 0.196755, acc:  78%, G loss: 1.757799\n",
      "Ep: 546, steps: 5, D loss: 0.294429, acc:  41%, G loss: 1.663601\n",
      "Ep: 546, steps: 6, D loss: 0.232896, acc:  57%, G loss: 1.571653\n",
      "Saved Model\n",
      "Ep: 546, steps: 7, D loss: 0.337861, acc:  24%, G loss: 1.596913\n",
      "Ep: 546, steps: 8, D loss: 0.202636, acc:  75%, G loss: 1.717892\n",
      "Ep: 546, steps: 9, D loss: 0.179665, acc:  81%, G loss: 1.699096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 546, steps: 10, D loss: 0.255347, acc:  50%, G loss: 1.831929\n",
      "Ep: 546, steps: 11, D loss: 0.287113, acc:  37%, G loss: 1.346428\n",
      "Ep: 546, steps: 12, D loss: 0.292636, acc:  32%, G loss: 1.362107\n",
      "Ep: 546, steps: 13, D loss: 0.266410, acc:  46%, G loss: 1.441038\n",
      "Ep: 546, steps: 14, D loss: 0.250798, acc:  51%, G loss: 1.619716\n",
      "Ep: 546, steps: 15, D loss: 0.249200, acc:  54%, G loss: 1.598386\n",
      "Ep: 546, steps: 16, D loss: 0.198642, acc:  75%, G loss: 1.605176\n",
      "Ep: 546, steps: 17, D loss: 0.230440, acc:  62%, G loss: 1.619875\n",
      "Ep: 546, steps: 18, D loss: 0.206842, acc:  70%, G loss: 1.601750\n",
      "Ep: 546, steps: 19, D loss: 0.178574, acc:  79%, G loss: 1.700340\n",
      "Ep: 546, steps: 20, D loss: 0.279205, acc:  36%, G loss: 1.478248\n",
      "Ep: 546, steps: 21, D loss: 0.180084, acc:  70%, G loss: 1.730218\n",
      "Ep: 546, steps: 22, D loss: 0.224618, acc:  65%, G loss: 1.859731\n",
      "Ep: 546, steps: 23, D loss: 0.205168, acc:  70%, G loss: 1.599056\n",
      "Ep: 546, steps: 24, D loss: 0.274644, acc:  49%, G loss: 1.590220\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 547, steps: 1, D loss: 0.233639, acc:  61%, G loss: 1.630484\n",
      "Ep: 547, steps: 2, D loss: 0.230892, acc:  61%, G loss: 1.521551\n",
      "Ep: 547, steps: 3, D loss: 0.175576, acc:  80%, G loss: 1.926005\n",
      "Ep: 547, steps: 4, D loss: 0.186540, acc:  82%, G loss: 1.761541\n",
      "Ep: 547, steps: 5, D loss: 0.289765, acc:  43%, G loss: 1.668074\n",
      "Ep: 547, steps: 6, D loss: 0.234499, acc:  58%, G loss: 1.578822\n",
      "Ep: 547, steps: 7, D loss: 0.310666, acc:  31%, G loss: 1.536293\n",
      "Ep: 547, steps: 8, D loss: 0.240195, acc:  57%, G loss: 1.828776\n",
      "Ep: 547, steps: 9, D loss: 0.241670, acc:  59%, G loss: 1.619097\n",
      "Ep: 547, steps: 10, D loss: 0.192368, acc:  72%, G loss: 1.518547\n",
      "Ep: 547, steps: 11, D loss: 0.259350, acc:  49%, G loss: 1.764845\n",
      "Ep: 547, steps: 12, D loss: 0.290906, acc:  35%, G loss: 1.373201\n",
      "Ep: 547, steps: 13, D loss: 0.285420, acc:  38%, G loss: 1.438562\n",
      "Ep: 547, steps: 14, D loss: 0.269658, acc:  44%, G loss: 1.468866\n",
      "Ep: 547, steps: 15, D loss: 0.238269, acc:  57%, G loss: 1.585944\n",
      "Ep: 547, steps: 16, D loss: 0.245082, acc:  56%, G loss: 1.570634\n",
      "Ep: 547, steps: 17, D loss: 0.213894, acc:  69%, G loss: 1.528207\n",
      "Ep: 547, steps: 18, D loss: 0.249377, acc:  55%, G loss: 1.627805\n",
      "Ep: 547, steps: 19, D loss: 0.216565, acc:  66%, G loss: 1.613376\n",
      "Ep: 547, steps: 20, D loss: 0.192044, acc:  72%, G loss: 1.696094\n",
      "Ep: 547, steps: 21, D loss: 0.263918, acc:  43%, G loss: 1.531507\n",
      "Ep: 547, steps: 22, D loss: 0.178638, acc:  73%, G loss: 1.773203\n",
      "Ep: 547, steps: 23, D loss: 0.230378, acc:  61%, G loss: 1.942971\n",
      "Ep: 547, steps: 24, D loss: 0.200048, acc:  72%, G loss: 1.611174\n",
      "Ep: 547, steps: 25, D loss: 0.261052, acc:  51%, G loss: 1.579422\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 548, steps: 1, D loss: 0.260024, acc:  51%, G loss: 1.622624\n",
      "Ep: 548, steps: 2, D loss: 0.224097, acc:  62%, G loss: 1.478926\n",
      "Ep: 548, steps: 3, D loss: 0.173070, acc:  79%, G loss: 2.014398\n",
      "Ep: 548, steps: 4, D loss: 0.193526, acc:  76%, G loss: 1.810668\n",
      "Ep: 548, steps: 5, D loss: 0.286589, acc:  46%, G loss: 1.789350\n",
      "Ep: 548, steps: 6, D loss: 0.226794, acc:  59%, G loss: 1.647951\n",
      "Ep: 548, steps: 7, D loss: 0.308172, acc:  32%, G loss: 1.613785\n",
      "Ep: 548, steps: 8, D loss: 0.235145, acc:  61%, G loss: 1.841913\n",
      "Ep: 548, steps: 9, D loss: 0.227771, acc:  66%, G loss: 1.624409\n",
      "Ep: 548, steps: 10, D loss: 0.176987, acc:  79%, G loss: 1.585706\n",
      "Ep: 548, steps: 11, D loss: 0.271753, acc:  47%, G loss: 1.889122\n",
      "Ep: 548, steps: 12, D loss: 0.285056, acc:  40%, G loss: 1.410853\n",
      "Ep: 548, steps: 13, D loss: 0.291489, acc:  34%, G loss: 1.406080\n",
      "Ep: 548, steps: 14, D loss: 0.274956, acc:  42%, G loss: 1.506054\n",
      "Ep: 548, steps: 15, D loss: 0.243004, acc:  56%, G loss: 1.567930\n",
      "Ep: 548, steps: 16, D loss: 0.236109, acc:  59%, G loss: 1.657830\n",
      "Ep: 548, steps: 17, D loss: 0.210800, acc:  68%, G loss: 1.530627\n",
      "Ep: 548, steps: 18, D loss: 0.241919, acc:  59%, G loss: 1.621030\n",
      "Ep: 548, steps: 19, D loss: 0.205190, acc:  70%, G loss: 1.562839\n",
      "Ep: 548, steps: 20, D loss: 0.182704, acc:  76%, G loss: 1.743667\n",
      "Ep: 548, steps: 21, D loss: 0.273425, acc:  40%, G loss: 1.486986\n",
      "Ep: 548, steps: 22, D loss: 0.153273, acc:  80%, G loss: 1.635185\n",
      "Ep: 548, steps: 23, D loss: 0.240557, acc:  57%, G loss: 1.873106\n",
      "Ep: 548, steps: 24, D loss: 0.212029, acc:  67%, G loss: 1.534334\n",
      "Ep: 548, steps: 25, D loss: 0.274850, acc:  49%, G loss: 1.609654\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 549, steps: 1, D loss: 0.246638, acc:  56%, G loss: 1.729662\n",
      "Ep: 549, steps: 2, D loss: 0.257932, acc:  51%, G loss: 1.526828\n",
      "Ep: 549, steps: 3, D loss: 0.170451, acc:  80%, G loss: 1.970150\n",
      "Ep: 549, steps: 4, D loss: 0.196278, acc:  78%, G loss: 1.782701\n",
      "Saved Model\n",
      "Ep: 549, steps: 5, D loss: 0.281438, acc:  46%, G loss: 1.717476\n",
      "Ep: 549, steps: 6, D loss: 0.319425, acc:  29%, G loss: 1.491463\n",
      "Ep: 549, steps: 7, D loss: 0.212966, acc:  67%, G loss: 1.825732\n",
      "Ep: 549, steps: 8, D loss: 0.257398, acc:  51%, G loss: 1.515787\n",
      "Ep: 549, steps: 9, D loss: 0.180319, acc:  80%, G loss: 1.565453\n",
      "Ep: 549, steps: 10, D loss: 0.256878, acc:  50%, G loss: 1.726119\n",
      "Ep: 549, steps: 11, D loss: 0.291425, acc:  35%, G loss: 1.335526\n",
      "Ep: 549, steps: 12, D loss: 0.279879, acc:  42%, G loss: 1.375327\n",
      "Ep: 549, steps: 13, D loss: 0.275927, acc:  41%, G loss: 1.510113\n",
      "Ep: 549, steps: 14, D loss: 0.247935, acc:  52%, G loss: 1.658491\n",
      "Ep: 549, steps: 15, D loss: 0.248037, acc:  58%, G loss: 1.562684\n",
      "Ep: 549, steps: 16, D loss: 0.209218, acc:  71%, G loss: 1.610453\n",
      "Ep: 549, steps: 17, D loss: 0.246525, acc:  57%, G loss: 1.666712\n",
      "Ep: 549, steps: 18, D loss: 0.224852, acc:  66%, G loss: 1.588371\n",
      "Ep: 549, steps: 19, D loss: 0.190078, acc:  72%, G loss: 1.752652\n",
      "Ep: 549, steps: 20, D loss: 0.255732, acc:  45%, G loss: 1.611686\n",
      "Ep: 549, steps: 21, D loss: 0.184219, acc:  72%, G loss: 1.605034\n",
      "Ep: 549, steps: 22, D loss: 0.218763, acc:  67%, G loss: 1.841428\n",
      "Ep: 549, steps: 23, D loss: 0.212121, acc:  68%, G loss: 1.607851\n",
      "Ep: 549, steps: 24, D loss: 0.240202, acc:  59%, G loss: 1.596064\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 550, steps: 1, D loss: 0.264395, acc:  50%, G loss: 1.646703\n",
      "Ep: 550, steps: 2, D loss: 0.243703, acc:  57%, G loss: 1.508250\n",
      "Ep: 550, steps: 3, D loss: 0.182199, acc:  78%, G loss: 2.029647\n",
      "Ep: 550, steps: 4, D loss: 0.197240, acc:  78%, G loss: 1.806559\n",
      "Ep: 550, steps: 5, D loss: 0.295172, acc:  46%, G loss: 1.692322\n",
      "Ep: 550, steps: 6, D loss: 0.230310, acc:  57%, G loss: 1.616577\n",
      "Ep: 550, steps: 7, D loss: 0.301927, acc:  33%, G loss: 1.486089\n",
      "Ep: 550, steps: 8, D loss: 0.228442, acc:  61%, G loss: 1.720009\n",
      "Ep: 550, steps: 9, D loss: 0.227219, acc:  65%, G loss: 1.593871\n",
      "Ep: 550, steps: 10, D loss: 0.180790, acc:  80%, G loss: 1.630612\n",
      "Ep: 550, steps: 11, D loss: 0.254074, acc:  52%, G loss: 1.864301\n",
      "Ep: 550, steps: 12, D loss: 0.305007, acc:  30%, G loss: 1.361125\n",
      "Ep: 550, steps: 13, D loss: 0.284611, acc:  40%, G loss: 1.399891\n",
      "Ep: 550, steps: 14, D loss: 0.280276, acc:  39%, G loss: 1.515929\n",
      "Ep: 550, steps: 15, D loss: 0.239014, acc:  57%, G loss: 1.606820\n",
      "Ep: 550, steps: 16, D loss: 0.239078, acc:  60%, G loss: 1.589609\n",
      "Ep: 550, steps: 17, D loss: 0.215994, acc:  69%, G loss: 1.584618\n",
      "Ep: 550, steps: 18, D loss: 0.231037, acc:  61%, G loss: 1.616084\n",
      "Ep: 550, steps: 19, D loss: 0.206488, acc:  68%, G loss: 1.580879\n",
      "Ep: 550, steps: 20, D loss: 0.186356, acc:  74%, G loss: 1.846479\n",
      "Ep: 550, steps: 21, D loss: 0.259986, acc:  43%, G loss: 1.533181\n",
      "Ep: 550, steps: 22, D loss: 0.176268, acc:  71%, G loss: 1.702413\n",
      "Ep: 550, steps: 23, D loss: 0.233677, acc:  60%, G loss: 1.893085\n",
      "Ep: 550, steps: 24, D loss: 0.200892, acc:  73%, G loss: 1.603017\n",
      "Ep: 550, steps: 25, D loss: 0.264313, acc:  52%, G loss: 1.553420\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 551, steps: 1, D loss: 0.256437, acc:  54%, G loss: 1.672868\n",
      "Ep: 551, steps: 2, D loss: 0.250180, acc:  54%, G loss: 1.529399\n",
      "Ep: 551, steps: 3, D loss: 0.173552, acc:  79%, G loss: 2.025052\n",
      "Ep: 551, steps: 4, D loss: 0.183593, acc:  83%, G loss: 1.831323\n",
      "Ep: 551, steps: 5, D loss: 0.289096, acc:  41%, G loss: 1.709657\n",
      "Ep: 551, steps: 6, D loss: 0.251479, acc:  53%, G loss: 1.600463\n",
      "Ep: 551, steps: 7, D loss: 0.318524, acc:  30%, G loss: 1.475693\n",
      "Ep: 551, steps: 8, D loss: 0.233136, acc:  59%, G loss: 1.685867\n",
      "Ep: 551, steps: 9, D loss: 0.228007, acc:  67%, G loss: 1.632957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 551, steps: 10, D loss: 0.193708, acc:  76%, G loss: 1.646043\n",
      "Ep: 551, steps: 11, D loss: 0.248875, acc:  54%, G loss: 1.736882\n",
      "Ep: 551, steps: 12, D loss: 0.292095, acc:  34%, G loss: 1.402596\n",
      "Ep: 551, steps: 13, D loss: 0.269178, acc:  46%, G loss: 1.431270\n",
      "Ep: 551, steps: 14, D loss: 0.264433, acc:  46%, G loss: 1.496313\n",
      "Ep: 551, steps: 15, D loss: 0.248208, acc:  54%, G loss: 1.566794\n",
      "Ep: 551, steps: 16, D loss: 0.234566, acc:  61%, G loss: 1.643332\n",
      "Ep: 551, steps: 17, D loss: 0.217704, acc:  68%, G loss: 1.578915\n",
      "Ep: 551, steps: 18, D loss: 0.230437, acc:  62%, G loss: 1.596801\n",
      "Ep: 551, steps: 19, D loss: 0.211690, acc:  67%, G loss: 1.626543\n",
      "Ep: 551, steps: 20, D loss: 0.185344, acc:  74%, G loss: 1.780675\n",
      "Ep: 551, steps: 21, D loss: 0.267038, acc:  40%, G loss: 1.480682\n",
      "Ep: 551, steps: 22, D loss: 0.161554, acc:  78%, G loss: 1.579336\n",
      "Ep: 551, steps: 23, D loss: 0.224954, acc:  63%, G loss: 1.907305\n",
      "Ep: 551, steps: 24, D loss: 0.220619, acc:  65%, G loss: 1.774902\n",
      "Ep: 551, steps: 25, D loss: 0.254689, acc:  52%, G loss: 1.720830\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 552, steps: 1, D loss: 0.235337, acc:  62%, G loss: 1.696807\n",
      "Ep: 552, steps: 2, D loss: 0.247376, acc:  54%, G loss: 1.470591\n",
      "Saved Model\n",
      "Ep: 552, steps: 3, D loss: 0.173307, acc:  80%, G loss: 1.967313\n",
      "Ep: 552, steps: 4, D loss: 0.276147, acc:  47%, G loss: 1.643653\n",
      "Ep: 552, steps: 5, D loss: 0.226471, acc:  59%, G loss: 1.583312\n",
      "Ep: 552, steps: 6, D loss: 0.291793, acc:  34%, G loss: 1.651991\n",
      "Ep: 552, steps: 7, D loss: 0.219043, acc:  64%, G loss: 1.793057\n",
      "Ep: 552, steps: 8, D loss: 0.244520, acc:  59%, G loss: 1.627081\n",
      "Ep: 552, steps: 9, D loss: 0.189974, acc:  75%, G loss: 1.652548\n",
      "Ep: 552, steps: 10, D loss: 0.257230, acc:  50%, G loss: 1.882546\n",
      "Ep: 552, steps: 11, D loss: 0.294061, acc:  36%, G loss: 1.424682\n",
      "Ep: 552, steps: 12, D loss: 0.283301, acc:  39%, G loss: 1.505009\n",
      "Ep: 552, steps: 13, D loss: 0.266585, acc:  44%, G loss: 1.578302\n",
      "Ep: 552, steps: 14, D loss: 0.256580, acc:  48%, G loss: 1.669265\n",
      "Ep: 552, steps: 15, D loss: 0.244498, acc:  57%, G loss: 1.588177\n",
      "Ep: 552, steps: 16, D loss: 0.212257, acc:  72%, G loss: 1.613998\n",
      "Ep: 552, steps: 17, D loss: 0.236667, acc:  59%, G loss: 1.595723\n",
      "Ep: 552, steps: 18, D loss: 0.216365, acc:  66%, G loss: 1.613370\n",
      "Ep: 552, steps: 19, D loss: 0.182740, acc:  76%, G loss: 1.807539\n",
      "Ep: 552, steps: 20, D loss: 0.270716, acc:  39%, G loss: 1.577152\n",
      "Ep: 552, steps: 21, D loss: 0.163089, acc:  78%, G loss: 1.696232\n",
      "Ep: 552, steps: 22, D loss: 0.233861, acc:  59%, G loss: 1.840255\n",
      "Ep: 552, steps: 23, D loss: 0.210900, acc:  69%, G loss: 1.567192\n",
      "Ep: 552, steps: 24, D loss: 0.267078, acc:  52%, G loss: 1.562934\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 553, steps: 1, D loss: 0.251501, acc:  56%, G loss: 1.712646\n",
      "Ep: 553, steps: 2, D loss: 0.237318, acc:  59%, G loss: 1.539582\n",
      "Ep: 553, steps: 3, D loss: 0.170101, acc:  82%, G loss: 1.940154\n",
      "Ep: 553, steps: 4, D loss: 0.196733, acc:  79%, G loss: 1.700859\n",
      "Ep: 553, steps: 5, D loss: 0.266740, acc:  50%, G loss: 1.647410\n",
      "Ep: 553, steps: 6, D loss: 0.250155, acc:  53%, G loss: 1.603468\n",
      "Ep: 553, steps: 7, D loss: 0.327117, acc:  29%, G loss: 1.542486\n",
      "Ep: 553, steps: 8, D loss: 0.232084, acc:  61%, G loss: 1.659097\n",
      "Ep: 553, steps: 9, D loss: 0.239836, acc:  59%, G loss: 1.609967\n",
      "Ep: 553, steps: 10, D loss: 0.187109, acc:  78%, G loss: 1.557233\n",
      "Ep: 553, steps: 11, D loss: 0.254569, acc:  52%, G loss: 1.803438\n",
      "Ep: 553, steps: 12, D loss: 0.297055, acc:  35%, G loss: 1.403593\n",
      "Ep: 553, steps: 13, D loss: 0.286931, acc:  37%, G loss: 1.387073\n",
      "Ep: 553, steps: 14, D loss: 0.258659, acc:  47%, G loss: 1.485244\n",
      "Ep: 553, steps: 15, D loss: 0.265539, acc:  46%, G loss: 1.608406\n",
      "Ep: 553, steps: 16, D loss: 0.240336, acc:  58%, G loss: 1.593990\n",
      "Ep: 553, steps: 17, D loss: 0.214071, acc:  69%, G loss: 1.607246\n",
      "Ep: 553, steps: 18, D loss: 0.244961, acc:  55%, G loss: 1.626004\n",
      "Ep: 553, steps: 19, D loss: 0.220569, acc:  64%, G loss: 1.624383\n",
      "Ep: 553, steps: 20, D loss: 0.196880, acc:  74%, G loss: 1.737133\n",
      "Ep: 553, steps: 21, D loss: 0.259552, acc:  45%, G loss: 1.607432\n",
      "Ep: 553, steps: 22, D loss: 0.189279, acc:  72%, G loss: 1.629922\n",
      "Ep: 553, steps: 23, D loss: 0.222661, acc:  65%, G loss: 1.885402\n",
      "Ep: 553, steps: 24, D loss: 0.201667, acc:  72%, G loss: 1.579418\n",
      "Ep: 553, steps: 25, D loss: 0.281447, acc:  47%, G loss: 1.625827\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 554, steps: 1, D loss: 0.240120, acc:  60%, G loss: 1.779849\n",
      "Ep: 554, steps: 2, D loss: 0.249209, acc:  53%, G loss: 1.541986\n",
      "Ep: 554, steps: 3, D loss: 0.173452, acc:  80%, G loss: 1.944357\n",
      "Ep: 554, steps: 4, D loss: 0.196635, acc:  79%, G loss: 1.736311\n",
      "Ep: 554, steps: 5, D loss: 0.283668, acc:  46%, G loss: 1.674965\n",
      "Ep: 554, steps: 6, D loss: 0.232288, acc:  57%, G loss: 1.597580\n",
      "Ep: 554, steps: 7, D loss: 0.306482, acc:  32%, G loss: 1.463625\n",
      "Ep: 554, steps: 8, D loss: 0.225089, acc:  62%, G loss: 1.697880\n",
      "Ep: 554, steps: 9, D loss: 0.218610, acc:  69%, G loss: 1.655270\n",
      "Ep: 554, steps: 10, D loss: 0.188420, acc:  76%, G loss: 1.586223\n",
      "Ep: 554, steps: 11, D loss: 0.275735, acc:  46%, G loss: 1.882672\n",
      "Ep: 554, steps: 12, D loss: 0.298685, acc:  33%, G loss: 1.333300\n",
      "Ep: 554, steps: 13, D loss: 0.284947, acc:  37%, G loss: 1.358425\n",
      "Ep: 554, steps: 14, D loss: 0.282077, acc:  39%, G loss: 1.508430\n",
      "Ep: 554, steps: 15, D loss: 0.246826, acc:  52%, G loss: 1.598199\n",
      "Ep: 554, steps: 16, D loss: 0.230111, acc:  62%, G loss: 1.656386\n",
      "Ep: 554, steps: 17, D loss: 0.218814, acc:  68%, G loss: 1.607478\n",
      "Ep: 554, steps: 18, D loss: 0.234053, acc:  60%, G loss: 1.610267\n",
      "Ep: 554, steps: 19, D loss: 0.207418, acc:  68%, G loss: 1.616128\n",
      "Ep: 554, steps: 20, D loss: 0.181197, acc:  77%, G loss: 1.786201\n",
      "Ep: 554, steps: 21, D loss: 0.267303, acc:  43%, G loss: 1.515803\n",
      "Ep: 554, steps: 22, D loss: 0.153378, acc:  79%, G loss: 1.642205\n",
      "Ep: 554, steps: 23, D loss: 0.226656, acc:  63%, G loss: 1.946499\n",
      "Ep: 554, steps: 24, D loss: 0.210922, acc:  68%, G loss: 1.725825\n",
      "Ep: 554, steps: 25, D loss: 0.254762, acc:  52%, G loss: 1.547388\n",
      "Data exhausted, Re Initialize\n",
      "Saved Model\n",
      "Ep: 555, steps: 1, D loss: 0.244616, acc:  58%, G loss: 1.812199\n",
      "Ep: 555, steps: 2, D loss: 0.161351, acc:  79%, G loss: 1.978431\n",
      "Ep: 555, steps: 3, D loss: 0.198456, acc:  74%, G loss: 1.703231\n",
      "Ep: 555, steps: 4, D loss: 0.285389, acc:  47%, G loss: 1.700926\n",
      "Ep: 555, steps: 5, D loss: 0.244693, acc:  56%, G loss: 1.590662\n",
      "Ep: 555, steps: 6, D loss: 0.318229, acc:  31%, G loss: 1.558403\n",
      "Ep: 555, steps: 7, D loss: 0.226805, acc:  65%, G loss: 1.817372\n",
      "Ep: 555, steps: 8, D loss: 0.240166, acc:  61%, G loss: 1.661538\n",
      "Ep: 555, steps: 9, D loss: 0.179479, acc:  78%, G loss: 1.646384\n",
      "Ep: 555, steps: 10, D loss: 0.262708, acc:  49%, G loss: 1.881610\n",
      "Ep: 555, steps: 11, D loss: 0.286251, acc:  39%, G loss: 1.413361\n",
      "Ep: 555, steps: 12, D loss: 0.274749, acc:  42%, G loss: 1.420323\n",
      "Ep: 555, steps: 13, D loss: 0.268549, acc:  43%, G loss: 1.506165\n",
      "Ep: 555, steps: 14, D loss: 0.254797, acc:  49%, G loss: 1.619977\n",
      "Ep: 555, steps: 15, D loss: 0.260776, acc:  51%, G loss: 1.617770\n",
      "Ep: 555, steps: 16, D loss: 0.221501, acc:  65%, G loss: 1.626910\n",
      "Ep: 555, steps: 17, D loss: 0.238741, acc:  60%, G loss: 1.630928\n",
      "Ep: 555, steps: 18, D loss: 0.224759, acc:  63%, G loss: 1.651668\n",
      "Ep: 555, steps: 19, D loss: 0.183939, acc:  75%, G loss: 1.825931\n",
      "Ep: 555, steps: 20, D loss: 0.277570, acc:  34%, G loss: 1.508610\n",
      "Ep: 555, steps: 21, D loss: 0.176978, acc:  74%, G loss: 1.617159\n",
      "Ep: 555, steps: 22, D loss: 0.234055, acc:  61%, G loss: 1.950978\n",
      "Ep: 555, steps: 23, D loss: 0.206772, acc:  70%, G loss: 1.621461\n",
      "Ep: 555, steps: 24, D loss: 0.272467, acc:  50%, G loss: 1.655051\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 556, steps: 1, D loss: 0.250411, acc:  55%, G loss: 1.773543\n",
      "Ep: 556, steps: 2, D loss: 0.255260, acc:  52%, G loss: 1.521844\n",
      "Ep: 556, steps: 3, D loss: 0.167439, acc:  82%, G loss: 1.935064\n",
      "Ep: 556, steps: 4, D loss: 0.183682, acc:  83%, G loss: 1.730946\n",
      "Ep: 556, steps: 5, D loss: 0.268956, acc:  49%, G loss: 1.661327\n",
      "Ep: 556, steps: 6, D loss: 0.231476, acc:  57%, G loss: 1.580882\n",
      "Ep: 556, steps: 7, D loss: 0.343354, acc:  25%, G loss: 1.495816\n",
      "Ep: 556, steps: 8, D loss: 0.226310, acc:  63%, G loss: 1.685119\n",
      "Ep: 556, steps: 9, D loss: 0.223033, acc:  66%, G loss: 1.643037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 556, steps: 10, D loss: 0.180462, acc:  82%, G loss: 1.566315\n",
      "Ep: 556, steps: 11, D loss: 0.272136, acc:  47%, G loss: 1.848739\n",
      "Ep: 556, steps: 12, D loss: 0.290887, acc:  37%, G loss: 1.349406\n",
      "Ep: 556, steps: 13, D loss: 0.286086, acc:  35%, G loss: 1.420936\n",
      "Ep: 556, steps: 14, D loss: 0.279870, acc:  38%, G loss: 1.483537\n",
      "Ep: 556, steps: 15, D loss: 0.235738, acc:  58%, G loss: 1.611205\n",
      "Ep: 556, steps: 16, D loss: 0.244259, acc:  57%, G loss: 1.592181\n",
      "Ep: 556, steps: 17, D loss: 0.218135, acc:  69%, G loss: 1.525862\n",
      "Ep: 556, steps: 18, D loss: 0.238860, acc:  59%, G loss: 1.673076\n",
      "Ep: 556, steps: 19, D loss: 0.226483, acc:  64%, G loss: 1.632748\n",
      "Ep: 556, steps: 20, D loss: 0.194573, acc:  74%, G loss: 1.733645\n",
      "Ep: 556, steps: 21, D loss: 0.265562, acc:  41%, G loss: 1.453926\n",
      "Ep: 556, steps: 22, D loss: 0.200995, acc:  69%, G loss: 1.576925\n",
      "Ep: 556, steps: 23, D loss: 0.230439, acc:  61%, G loss: 1.817222\n",
      "Ep: 556, steps: 24, D loss: 0.204872, acc:  72%, G loss: 1.626844\n",
      "Ep: 556, steps: 25, D loss: 0.222990, acc:  65%, G loss: 1.563796\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 557, steps: 1, D loss: 0.266179, acc:  51%, G loss: 1.744108\n",
      "Ep: 557, steps: 2, D loss: 0.257396, acc:  52%, G loss: 1.499489\n",
      "Ep: 557, steps: 3, D loss: 0.176768, acc:  79%, G loss: 1.919679\n",
      "Ep: 557, steps: 4, D loss: 0.193900, acc:  78%, G loss: 1.749628\n",
      "Ep: 557, steps: 5, D loss: 0.280568, acc:  49%, G loss: 1.657788\n",
      "Ep: 557, steps: 6, D loss: 0.226568, acc:  59%, G loss: 1.567181\n",
      "Ep: 557, steps: 7, D loss: 0.315475, acc:  29%, G loss: 1.408986\n",
      "Ep: 557, steps: 8, D loss: 0.221751, acc:  62%, G loss: 1.655785\n",
      "Ep: 557, steps: 9, D loss: 0.232379, acc:  62%, G loss: 1.637244\n",
      "Ep: 557, steps: 10, D loss: 0.174246, acc:  83%, G loss: 1.589204\n",
      "Ep: 557, steps: 11, D loss: 0.248162, acc:  52%, G loss: 1.787331\n",
      "Ep: 557, steps: 12, D loss: 0.275346, acc:  41%, G loss: 1.365957\n",
      "Ep: 557, steps: 13, D loss: 0.289110, acc:  34%, G loss: 1.443897\n",
      "Ep: 557, steps: 14, D loss: 0.272246, acc:  43%, G loss: 1.521513\n",
      "Ep: 557, steps: 15, D loss: 0.241749, acc:  56%, G loss: 1.544893\n",
      "Ep: 557, steps: 16, D loss: 0.247051, acc:  55%, G loss: 1.567554\n",
      "Ep: 557, steps: 17, D loss: 0.201077, acc:  73%, G loss: 1.594619\n",
      "Ep: 557, steps: 18, D loss: 0.236117, acc:  60%, G loss: 1.658960\n",
      "Ep: 557, steps: 19, D loss: 0.200293, acc:  70%, G loss: 1.674453\n",
      "Ep: 557, steps: 20, D loss: 0.181311, acc:  78%, G loss: 1.775546\n",
      "Ep: 557, steps: 21, D loss: 0.270036, acc:  39%, G loss: 1.447949\n",
      "Ep: 557, steps: 22, D loss: 0.158236, acc:  76%, G loss: 1.651626\n",
      "Ep: 557, steps: 23, D loss: 0.227432, acc:  63%, G loss: 1.912648\n",
      "Saved Model\n",
      "Ep: 557, steps: 24, D loss: 0.199938, acc:  72%, G loss: 1.630879\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 558, steps: 1, D loss: 0.225333, acc:  65%, G loss: 1.793415\n",
      "Ep: 558, steps: 2, D loss: 0.238954, acc:  59%, G loss: 1.548393\n",
      "Ep: 558, steps: 3, D loss: 0.178219, acc:  76%, G loss: 1.985702\n",
      "Ep: 558, steps: 4, D loss: 0.172680, acc:  85%, G loss: 1.877954\n",
      "Ep: 558, steps: 5, D loss: 0.358895, acc:  29%, G loss: 1.624846\n",
      "Ep: 558, steps: 6, D loss: 0.236807, acc:  56%, G loss: 1.572442\n",
      "Ep: 558, steps: 7, D loss: 0.329011, acc:  29%, G loss: 1.447617\n",
      "Ep: 558, steps: 8, D loss: 0.232447, acc:  62%, G loss: 1.826820\n",
      "Ep: 558, steps: 9, D loss: 0.239928, acc:  59%, G loss: 1.626404\n",
      "Ep: 558, steps: 10, D loss: 0.191981, acc:  73%, G loss: 1.596134\n",
      "Ep: 558, steps: 11, D loss: 0.253313, acc:  53%, G loss: 1.757019\n",
      "Ep: 558, steps: 12, D loss: 0.281209, acc:  39%, G loss: 1.429699\n",
      "Ep: 558, steps: 13, D loss: 0.284433, acc:  37%, G loss: 1.430728\n",
      "Ep: 558, steps: 14, D loss: 0.270388, acc:  43%, G loss: 1.535405\n",
      "Ep: 558, steps: 15, D loss: 0.251398, acc:  51%, G loss: 1.511964\n",
      "Ep: 558, steps: 16, D loss: 0.256097, acc:  52%, G loss: 1.592406\n",
      "Ep: 558, steps: 17, D loss: 0.208063, acc:  73%, G loss: 1.576906\n",
      "Ep: 558, steps: 18, D loss: 0.241135, acc:  58%, G loss: 1.680977\n",
      "Ep: 558, steps: 19, D loss: 0.204600, acc:  72%, G loss: 1.611798\n",
      "Ep: 558, steps: 20, D loss: 0.176455, acc:  78%, G loss: 1.801300\n",
      "Ep: 558, steps: 21, D loss: 0.276802, acc:  35%, G loss: 1.489965\n",
      "Ep: 558, steps: 22, D loss: 0.166005, acc:  78%, G loss: 1.562087\n",
      "Ep: 558, steps: 23, D loss: 0.227075, acc:  63%, G loss: 1.906285\n",
      "Ep: 558, steps: 24, D loss: 0.206013, acc:  72%, G loss: 1.622179\n",
      "Ep: 558, steps: 25, D loss: 0.255974, acc:  53%, G loss: 1.531696\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 559, steps: 1, D loss: 0.246895, acc:  57%, G loss: 1.673395\n",
      "Ep: 559, steps: 2, D loss: 0.253293, acc:  51%, G loss: 1.486427\n",
      "Ep: 559, steps: 3, D loss: 0.177963, acc:  78%, G loss: 2.057710\n",
      "Ep: 559, steps: 4, D loss: 0.181418, acc:  84%, G loss: 1.747676\n",
      "Ep: 559, steps: 5, D loss: 0.274991, acc:  48%, G loss: 1.590460\n",
      "Ep: 559, steps: 6, D loss: 0.246716, acc:  54%, G loss: 1.642143\n",
      "Ep: 559, steps: 7, D loss: 0.335777, acc:  25%, G loss: 1.623893\n",
      "Ep: 559, steps: 8, D loss: 0.235155, acc:  62%, G loss: 1.692693\n",
      "Ep: 559, steps: 9, D loss: 0.245223, acc:  60%, G loss: 1.614423\n",
      "Ep: 559, steps: 10, D loss: 0.181966, acc:  80%, G loss: 1.593220\n",
      "Ep: 559, steps: 11, D loss: 0.251581, acc:  52%, G loss: 1.781364\n",
      "Ep: 559, steps: 12, D loss: 0.287783, acc:  37%, G loss: 1.336762\n",
      "Ep: 559, steps: 13, D loss: 0.274712, acc:  40%, G loss: 1.456548\n",
      "Ep: 559, steps: 14, D loss: 0.270023, acc:  44%, G loss: 1.467308\n",
      "Ep: 559, steps: 15, D loss: 0.254314, acc:  52%, G loss: 1.518013\n",
      "Ep: 559, steps: 16, D loss: 0.237401, acc:  59%, G loss: 1.594962\n",
      "Ep: 559, steps: 17, D loss: 0.218227, acc:  68%, G loss: 1.528593\n",
      "Ep: 559, steps: 18, D loss: 0.234821, acc:  61%, G loss: 1.658027\n",
      "Ep: 559, steps: 19, D loss: 0.217435, acc:  67%, G loss: 1.592251\n",
      "Ep: 559, steps: 20, D loss: 0.186164, acc:  74%, G loss: 1.728588\n",
      "Ep: 559, steps: 21, D loss: 0.263477, acc:  43%, G loss: 1.521399\n",
      "Ep: 559, steps: 22, D loss: 0.179546, acc:  73%, G loss: 1.655537\n",
      "Ep: 559, steps: 23, D loss: 0.218210, acc:  65%, G loss: 1.830858\n",
      "Ep: 559, steps: 24, D loss: 0.207777, acc:  70%, G loss: 1.627612\n",
      "Ep: 559, steps: 25, D loss: 0.262869, acc:  53%, G loss: 1.625669\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 560, steps: 1, D loss: 0.252276, acc:  56%, G loss: 1.871505\n",
      "Ep: 560, steps: 2, D loss: 0.237839, acc:  58%, G loss: 1.552781\n",
      "Ep: 560, steps: 3, D loss: 0.177966, acc:  78%, G loss: 1.910593\n",
      "Ep: 560, steps: 4, D loss: 0.187891, acc:  81%, G loss: 1.762107\n",
      "Ep: 560, steps: 5, D loss: 0.289234, acc:  48%, G loss: 1.767588\n",
      "Ep: 560, steps: 6, D loss: 0.230817, acc:  60%, G loss: 1.613127\n",
      "Ep: 560, steps: 7, D loss: 0.323116, acc:  28%, G loss: 1.467713\n",
      "Ep: 560, steps: 8, D loss: 0.239003, acc:  58%, G loss: 1.725246\n",
      "Ep: 560, steps: 9, D loss: 0.225991, acc:  65%, G loss: 1.670616\n",
      "Ep: 560, steps: 10, D loss: 0.185306, acc:  78%, G loss: 1.615779\n",
      "Ep: 560, steps: 11, D loss: 0.270284, acc:  47%, G loss: 1.841848\n",
      "Ep: 560, steps: 12, D loss: 0.294636, acc:  34%, G loss: 1.322808\n",
      "Ep: 560, steps: 13, D loss: 0.279897, acc:  39%, G loss: 1.445633\n",
      "Ep: 560, steps: 14, D loss: 0.284690, acc:  39%, G loss: 1.517251\n",
      "Ep: 560, steps: 15, D loss: 0.239743, acc:  57%, G loss: 1.607304\n",
      "Ep: 560, steps: 16, D loss: 0.243321, acc:  56%, G loss: 1.646820\n",
      "Ep: 560, steps: 17, D loss: 0.217235, acc:  67%, G loss: 1.542320\n",
      "Ep: 560, steps: 18, D loss: 0.235859, acc:  59%, G loss: 1.590922\n",
      "Ep: 560, steps: 19, D loss: 0.217076, acc:  67%, G loss: 1.615816\n",
      "Ep: 560, steps: 20, D loss: 0.188261, acc:  77%, G loss: 1.832170\n",
      "Ep: 560, steps: 21, D loss: 0.274543, acc:  38%, G loss: 1.581573\n",
      "Saved Model\n",
      "Ep: 560, steps: 22, D loss: 0.168178, acc:  77%, G loss: 1.625380\n",
      "Ep: 560, steps: 23, D loss: 0.226393, acc:  60%, G loss: 1.597764\n",
      "Ep: 560, steps: 24, D loss: 0.265276, acc:  49%, G loss: 1.620638\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 561, steps: 1, D loss: 0.257791, acc:  54%, G loss: 1.657407\n",
      "Ep: 561, steps: 2, D loss: 0.228080, acc:  63%, G loss: 1.469035\n",
      "Ep: 561, steps: 3, D loss: 0.178092, acc:  78%, G loss: 1.904142\n",
      "Ep: 561, steps: 4, D loss: 0.185990, acc:  83%, G loss: 1.684162\n",
      "Ep: 561, steps: 5, D loss: 0.267527, acc:  52%, G loss: 1.747482\n",
      "Ep: 561, steps: 6, D loss: 0.226587, acc:  60%, G loss: 1.573281\n",
      "Ep: 561, steps: 7, D loss: 0.306874, acc:  34%, G loss: 1.544905\n",
      "Ep: 561, steps: 8, D loss: 0.225432, acc:  63%, G loss: 1.754223\n",
      "Ep: 561, steps: 9, D loss: 0.243641, acc:  58%, G loss: 1.580242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 561, steps: 10, D loss: 0.182018, acc:  79%, G loss: 1.560740\n",
      "Ep: 561, steps: 11, D loss: 0.247059, acc:  53%, G loss: 1.815559\n",
      "Ep: 561, steps: 12, D loss: 0.303657, acc:  30%, G loss: 1.332950\n",
      "Ep: 561, steps: 13, D loss: 0.281102, acc:  40%, G loss: 1.503788\n",
      "Ep: 561, steps: 14, D loss: 0.272793, acc:  42%, G loss: 1.518354\n",
      "Ep: 561, steps: 15, D loss: 0.249796, acc:  53%, G loss: 1.565660\n",
      "Ep: 561, steps: 16, D loss: 0.250321, acc:  54%, G loss: 1.589689\n",
      "Ep: 561, steps: 17, D loss: 0.214075, acc:  69%, G loss: 1.535438\n",
      "Ep: 561, steps: 18, D loss: 0.244194, acc:  58%, G loss: 1.635547\n",
      "Ep: 561, steps: 19, D loss: 0.216031, acc:  65%, G loss: 1.658729\n",
      "Ep: 561, steps: 20, D loss: 0.187154, acc:  74%, G loss: 1.791953\n",
      "Ep: 561, steps: 21, D loss: 0.268297, acc:  40%, G loss: 1.623454\n",
      "Ep: 561, steps: 22, D loss: 0.174111, acc:  73%, G loss: 1.640733\n",
      "Ep: 561, steps: 23, D loss: 0.235746, acc:  60%, G loss: 1.936599\n",
      "Ep: 561, steps: 24, D loss: 0.210569, acc:  68%, G loss: 1.533421\n",
      "Ep: 561, steps: 25, D loss: 0.242691, acc:  57%, G loss: 1.611666\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 562, steps: 1, D loss: 0.246341, acc:  55%, G loss: 1.732345\n",
      "Ep: 562, steps: 2, D loss: 0.235017, acc:  59%, G loss: 1.567063\n",
      "Ep: 562, steps: 3, D loss: 0.169216, acc:  83%, G loss: 2.004275\n",
      "Ep: 562, steps: 4, D loss: 0.191089, acc:  79%, G loss: 1.769286\n",
      "Ep: 562, steps: 5, D loss: 0.257279, acc:  54%, G loss: 1.701958\n",
      "Ep: 562, steps: 6, D loss: 0.248368, acc:  55%, G loss: 1.628442\n",
      "Ep: 562, steps: 7, D loss: 0.331999, acc:  28%, G loss: 1.488086\n",
      "Ep: 562, steps: 8, D loss: 0.226207, acc:  64%, G loss: 1.732395\n",
      "Ep: 562, steps: 9, D loss: 0.231914, acc:  63%, G loss: 1.662923\n",
      "Ep: 562, steps: 10, D loss: 0.191570, acc:  74%, G loss: 1.544489\n",
      "Ep: 562, steps: 11, D loss: 0.252922, acc:  50%, G loss: 1.746035\n",
      "Ep: 562, steps: 12, D loss: 0.287175, acc:  37%, G loss: 1.349791\n",
      "Ep: 562, steps: 13, D loss: 0.270719, acc:  42%, G loss: 1.523000\n",
      "Ep: 562, steps: 14, D loss: 0.269432, acc:  43%, G loss: 1.506050\n",
      "Ep: 562, steps: 15, D loss: 0.248186, acc:  52%, G loss: 1.542159\n",
      "Ep: 562, steps: 16, D loss: 0.236195, acc:  60%, G loss: 1.612284\n",
      "Ep: 562, steps: 17, D loss: 0.228325, acc:  65%, G loss: 1.707089\n",
      "Ep: 562, steps: 18, D loss: 0.251481, acc:  54%, G loss: 1.594880\n",
      "Ep: 562, steps: 19, D loss: 0.223674, acc:  65%, G loss: 1.618539\n",
      "Ep: 562, steps: 20, D loss: 0.203280, acc:  73%, G loss: 1.755187\n",
      "Ep: 562, steps: 21, D loss: 0.257933, acc:  45%, G loss: 1.544210\n",
      "Ep: 562, steps: 22, D loss: 0.194881, acc:  69%, G loss: 1.566999\n",
      "Ep: 562, steps: 23, D loss: 0.238563, acc:  58%, G loss: 1.864173\n",
      "Ep: 562, steps: 24, D loss: 0.216874, acc:  66%, G loss: 1.580022\n",
      "Ep: 562, steps: 25, D loss: 0.254128, acc:  54%, G loss: 1.576703\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 563, steps: 1, D loss: 0.236252, acc:  61%, G loss: 1.698425\n",
      "Ep: 563, steps: 2, D loss: 0.234286, acc:  61%, G loss: 1.513512\n",
      "Ep: 563, steps: 3, D loss: 0.180020, acc:  78%, G loss: 1.958719\n",
      "Ep: 563, steps: 4, D loss: 0.192444, acc:  80%, G loss: 1.765999\n",
      "Ep: 563, steps: 5, D loss: 0.289226, acc:  47%, G loss: 1.678257\n",
      "Ep: 563, steps: 6, D loss: 0.231259, acc:  57%, G loss: 1.617803\n",
      "Ep: 563, steps: 7, D loss: 0.331902, acc:  25%, G loss: 1.470348\n",
      "Ep: 563, steps: 8, D loss: 0.223841, acc:  62%, G loss: 1.698276\n",
      "Ep: 563, steps: 9, D loss: 0.235879, acc:  61%, G loss: 1.693672\n",
      "Ep: 563, steps: 10, D loss: 0.177176, acc:  84%, G loss: 1.571421\n",
      "Ep: 563, steps: 11, D loss: 0.276723, acc:  46%, G loss: 1.725409\n",
      "Ep: 563, steps: 12, D loss: 0.296429, acc:  32%, G loss: 1.325297\n",
      "Ep: 563, steps: 13, D loss: 0.285338, acc:  39%, G loss: 1.484198\n",
      "Ep: 563, steps: 14, D loss: 0.274015, acc:  41%, G loss: 1.515839\n",
      "Ep: 563, steps: 15, D loss: 0.262929, acc:  46%, G loss: 1.534960\n",
      "Ep: 563, steps: 16, D loss: 0.247176, acc:  56%, G loss: 1.637031\n",
      "Ep: 563, steps: 17, D loss: 0.211621, acc:  69%, G loss: 1.574592\n",
      "Ep: 563, steps: 18, D loss: 0.232226, acc:  63%, G loss: 1.626261\n",
      "Ep: 563, steps: 19, D loss: 0.217761, acc:  65%, G loss: 1.623693\n",
      "Saved Model\n",
      "Ep: 563, steps: 20, D loss: 0.178819, acc:  78%, G loss: 1.745921\n",
      "Ep: 563, steps: 21, D loss: 0.177218, acc:  74%, G loss: 1.784715\n",
      "Ep: 563, steps: 22, D loss: 0.219669, acc:  66%, G loss: 1.862821\n",
      "Ep: 563, steps: 23, D loss: 0.202898, acc:  71%, G loss: 1.637219\n",
      "Ep: 563, steps: 24, D loss: 0.262742, acc:  50%, G loss: 1.497541\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 564, steps: 1, D loss: 0.231483, acc:  63%, G loss: 1.674866\n",
      "Ep: 564, steps: 2, D loss: 0.247724, acc:  54%, G loss: 1.517121\n",
      "Ep: 564, steps: 3, D loss: 0.162632, acc:  82%, G loss: 1.985589\n",
      "Ep: 564, steps: 4, D loss: 0.172861, acc:  88%, G loss: 1.788508\n",
      "Ep: 564, steps: 5, D loss: 0.275292, acc:  49%, G loss: 1.700509\n",
      "Ep: 564, steps: 6, D loss: 0.233460, acc:  57%, G loss: 1.589111\n",
      "Ep: 564, steps: 7, D loss: 0.328137, acc:  28%, G loss: 1.533810\n",
      "Ep: 564, steps: 8, D loss: 0.227432, acc:  63%, G loss: 1.736557\n",
      "Ep: 564, steps: 9, D loss: 0.232109, acc:  63%, G loss: 1.683434\n",
      "Ep: 564, steps: 10, D loss: 0.168046, acc:  84%, G loss: 1.628556\n",
      "Ep: 564, steps: 11, D loss: 0.241526, acc:  57%, G loss: 1.811067\n",
      "Ep: 564, steps: 12, D loss: 0.299441, acc:  31%, G loss: 1.325050\n",
      "Ep: 564, steps: 13, D loss: 0.285598, acc:  37%, G loss: 1.455925\n",
      "Ep: 564, steps: 14, D loss: 0.268180, acc:  43%, G loss: 1.499352\n",
      "Ep: 564, steps: 15, D loss: 0.248351, acc:  53%, G loss: 1.580855\n",
      "Ep: 564, steps: 16, D loss: 0.244576, acc:  56%, G loss: 1.613644\n",
      "Ep: 564, steps: 17, D loss: 0.215221, acc:  70%, G loss: 1.641585\n",
      "Ep: 564, steps: 18, D loss: 0.232064, acc:  60%, G loss: 1.618146\n",
      "Ep: 564, steps: 19, D loss: 0.219387, acc:  65%, G loss: 1.580864\n",
      "Ep: 564, steps: 20, D loss: 0.178347, acc:  79%, G loss: 1.753248\n",
      "Ep: 564, steps: 21, D loss: 0.296959, acc:  31%, G loss: 1.496497\n",
      "Ep: 564, steps: 22, D loss: 0.170467, acc:  78%, G loss: 1.578629\n",
      "Ep: 564, steps: 23, D loss: 0.224959, acc:  62%, G loss: 1.851504\n",
      "Ep: 564, steps: 24, D loss: 0.217051, acc:  65%, G loss: 1.596030\n",
      "Ep: 564, steps: 25, D loss: 0.266331, acc:  50%, G loss: 1.570086\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 565, steps: 1, D loss: 0.247012, acc:  61%, G loss: 1.758264\n",
      "Ep: 565, steps: 2, D loss: 0.249886, acc:  54%, G loss: 1.587474\n",
      "Ep: 565, steps: 3, D loss: 0.183615, acc:  75%, G loss: 1.946802\n",
      "Ep: 565, steps: 4, D loss: 0.188974, acc:  80%, G loss: 1.778463\n",
      "Ep: 565, steps: 5, D loss: 0.293241, acc:  44%, G loss: 1.731559\n",
      "Ep: 565, steps: 6, D loss: 0.234443, acc:  56%, G loss: 1.641615\n",
      "Ep: 565, steps: 7, D loss: 0.319066, acc:  30%, G loss: 1.540870\n",
      "Ep: 565, steps: 8, D loss: 0.224013, acc:  62%, G loss: 1.755135\n",
      "Ep: 565, steps: 9, D loss: 0.243631, acc:  60%, G loss: 1.688132\n",
      "Ep: 565, steps: 10, D loss: 0.185915, acc:  80%, G loss: 1.584204\n",
      "Ep: 565, steps: 11, D loss: 0.247939, acc:  54%, G loss: 1.772739\n",
      "Ep: 565, steps: 12, D loss: 0.284917, acc:  39%, G loss: 1.345081\n",
      "Ep: 565, steps: 13, D loss: 0.277491, acc:  41%, G loss: 1.469251\n",
      "Ep: 565, steps: 14, D loss: 0.284571, acc:  38%, G loss: 1.507560\n",
      "Ep: 565, steps: 15, D loss: 0.253761, acc:  50%, G loss: 1.543203\n",
      "Ep: 565, steps: 16, D loss: 0.237754, acc:  60%, G loss: 1.632921\n",
      "Ep: 565, steps: 17, D loss: 0.214601, acc:  72%, G loss: 1.602908\n",
      "Ep: 565, steps: 18, D loss: 0.238327, acc:  60%, G loss: 1.637112\n",
      "Ep: 565, steps: 19, D loss: 0.218810, acc:  65%, G loss: 1.653352\n",
      "Ep: 565, steps: 20, D loss: 0.181493, acc:  79%, G loss: 1.711774\n",
      "Ep: 565, steps: 21, D loss: 0.265168, acc:  42%, G loss: 1.504221\n",
      "Ep: 565, steps: 22, D loss: 0.193323, acc:  69%, G loss: 1.602935\n",
      "Ep: 565, steps: 23, D loss: 0.219391, acc:  65%, G loss: 1.844413\n",
      "Ep: 565, steps: 24, D loss: 0.210262, acc:  70%, G loss: 1.607097\n",
      "Ep: 565, steps: 25, D loss: 0.260668, acc:  51%, G loss: 1.653399\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 566, steps: 1, D loss: 0.236397, acc:  60%, G loss: 1.681249\n",
      "Ep: 566, steps: 2, D loss: 0.256707, acc:  50%, G loss: 1.507901\n",
      "Ep: 566, steps: 3, D loss: 0.161246, acc:  84%, G loss: 1.927419\n",
      "Ep: 566, steps: 4, D loss: 0.195888, acc:  81%, G loss: 1.728768\n",
      "Ep: 566, steps: 5, D loss: 0.275717, acc:  45%, G loss: 1.633916\n",
      "Ep: 566, steps: 6, D loss: 0.242782, acc:  56%, G loss: 1.600466\n",
      "Ep: 566, steps: 7, D loss: 0.355114, acc:  24%, G loss: 1.525395\n",
      "Ep: 566, steps: 8, D loss: 0.228826, acc:  63%, G loss: 1.698686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 566, steps: 9, D loss: 0.225903, acc:  65%, G loss: 1.632684\n",
      "Ep: 566, steps: 10, D loss: 0.174911, acc:  81%, G loss: 1.608733\n",
      "Ep: 566, steps: 11, D loss: 0.245372, acc:  54%, G loss: 1.756848\n",
      "Ep: 566, steps: 12, D loss: 0.315333, acc:  30%, G loss: 1.325026\n",
      "Ep: 566, steps: 13, D loss: 0.288784, acc:  37%, G loss: 1.463533\n",
      "Ep: 566, steps: 14, D loss: 0.267651, acc:  43%, G loss: 1.513669\n",
      "Ep: 566, steps: 15, D loss: 0.250030, acc:  53%, G loss: 1.617747\n",
      "Ep: 566, steps: 16, D loss: 0.257867, acc:  50%, G loss: 1.612816\n",
      "Ep: 566, steps: 17, D loss: 0.212406, acc:  69%, G loss: 1.670699\n",
      "Saved Model\n",
      "Ep: 566, steps: 18, D loss: 0.246674, acc:  55%, G loss: 1.661436\n",
      "Ep: 566, steps: 19, D loss: 0.190714, acc:  76%, G loss: 1.700761\n",
      "Ep: 566, steps: 20, D loss: 0.251461, acc:  49%, G loss: 1.498006\n",
      "Ep: 566, steps: 21, D loss: 0.202754, acc:  67%, G loss: 1.564514\n",
      "Ep: 566, steps: 22, D loss: 0.245102, acc:  57%, G loss: 1.822174\n",
      "Ep: 566, steps: 23, D loss: 0.200556, acc:  75%, G loss: 1.622934\n",
      "Ep: 566, steps: 24, D loss: 0.259883, acc:  54%, G loss: 1.745375\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 567, steps: 1, D loss: 0.259817, acc:  50%, G loss: 1.763277\n",
      "Ep: 567, steps: 2, D loss: 0.256618, acc:  51%, G loss: 1.468359\n",
      "Ep: 567, steps: 3, D loss: 0.194339, acc:  77%, G loss: 1.922161\n",
      "Ep: 567, steps: 4, D loss: 0.190104, acc:  80%, G loss: 1.661512\n",
      "Ep: 567, steps: 5, D loss: 0.280767, acc:  48%, G loss: 1.630042\n",
      "Ep: 567, steps: 6, D loss: 0.245184, acc:  55%, G loss: 1.553101\n",
      "Ep: 567, steps: 7, D loss: 0.287227, acc:  37%, G loss: 1.540563\n",
      "Ep: 567, steps: 8, D loss: 0.214528, acc:  67%, G loss: 1.755167\n",
      "Ep: 567, steps: 9, D loss: 0.246010, acc:  56%, G loss: 1.646676\n",
      "Ep: 567, steps: 10, D loss: 0.178465, acc:  80%, G loss: 1.567142\n",
      "Ep: 567, steps: 11, D loss: 0.254065, acc:  49%, G loss: 1.717600\n",
      "Ep: 567, steps: 12, D loss: 0.287871, acc:  35%, G loss: 1.372670\n",
      "Ep: 567, steps: 13, D loss: 0.275581, acc:  41%, G loss: 1.503281\n",
      "Ep: 567, steps: 14, D loss: 0.271955, acc:  42%, G loss: 1.541825\n",
      "Ep: 567, steps: 15, D loss: 0.261835, acc:  49%, G loss: 1.509923\n",
      "Ep: 567, steps: 16, D loss: 0.241480, acc:  57%, G loss: 1.605418\n",
      "Ep: 567, steps: 17, D loss: 0.218810, acc:  67%, G loss: 1.592448\n",
      "Ep: 567, steps: 18, D loss: 0.245291, acc:  56%, G loss: 1.586406\n",
      "Ep: 567, steps: 19, D loss: 0.220031, acc:  62%, G loss: 1.558623\n",
      "Ep: 567, steps: 20, D loss: 0.172136, acc:  80%, G loss: 1.769992\n",
      "Ep: 567, steps: 21, D loss: 0.264878, acc:  42%, G loss: 1.483261\n",
      "Ep: 567, steps: 22, D loss: 0.174450, acc:  71%, G loss: 1.608762\n",
      "Ep: 567, steps: 23, D loss: 0.238700, acc:  59%, G loss: 1.882560\n",
      "Ep: 567, steps: 24, D loss: 0.208648, acc:  68%, G loss: 1.635231\n",
      "Ep: 567, steps: 25, D loss: 0.236961, acc:  58%, G loss: 1.617446\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 568, steps: 1, D loss: 0.243389, acc:  57%, G loss: 1.645820\n",
      "Ep: 568, steps: 2, D loss: 0.252630, acc:  52%, G loss: 1.473710\n",
      "Ep: 568, steps: 3, D loss: 0.173542, acc:  79%, G loss: 1.941721\n",
      "Ep: 568, steps: 4, D loss: 0.181810, acc:  83%, G loss: 1.776839\n",
      "Ep: 568, steps: 5, D loss: 0.302524, acc:  38%, G loss: 1.619960\n",
      "Ep: 568, steps: 6, D loss: 0.237866, acc:  57%, G loss: 1.557246\n",
      "Ep: 568, steps: 7, D loss: 0.339027, acc:  24%, G loss: 1.500627\n",
      "Ep: 568, steps: 8, D loss: 0.233955, acc:  61%, G loss: 1.723378\n",
      "Ep: 568, steps: 9, D loss: 0.235476, acc:  60%, G loss: 1.630658\n",
      "Ep: 568, steps: 10, D loss: 0.177259, acc:  82%, G loss: 1.614745\n",
      "Ep: 568, steps: 11, D loss: 0.251127, acc:  51%, G loss: 1.721641\n",
      "Ep: 568, steps: 12, D loss: 0.276169, acc:  42%, G loss: 1.365846\n",
      "Ep: 568, steps: 13, D loss: 0.271311, acc:  43%, G loss: 1.426504\n",
      "Ep: 568, steps: 14, D loss: 0.288502, acc:  36%, G loss: 1.498571\n",
      "Ep: 568, steps: 15, D loss: 0.237766, acc:  58%, G loss: 1.550848\n",
      "Ep: 568, steps: 16, D loss: 0.233234, acc:  62%, G loss: 1.605523\n",
      "Ep: 568, steps: 17, D loss: 0.208217, acc:  70%, G loss: 1.582187\n",
      "Ep: 568, steps: 18, D loss: 0.236367, acc:  60%, G loss: 1.591439\n",
      "Ep: 568, steps: 19, D loss: 0.216517, acc:  67%, G loss: 1.590990\n",
      "Ep: 568, steps: 20, D loss: 0.195044, acc:  74%, G loss: 1.741626\n",
      "Ep: 568, steps: 21, D loss: 0.275212, acc:  37%, G loss: 1.441638\n",
      "Ep: 568, steps: 22, D loss: 0.182739, acc:  74%, G loss: 1.594868\n",
      "Ep: 568, steps: 23, D loss: 0.220260, acc:  65%, G loss: 1.878249\n",
      "Ep: 568, steps: 24, D loss: 0.202813, acc:  72%, G loss: 1.592412\n",
      "Ep: 568, steps: 25, D loss: 0.258125, acc:  53%, G loss: 1.549653\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 569, steps: 1, D loss: 0.245988, acc:  58%, G loss: 1.739082\n",
      "Ep: 569, steps: 2, D loss: 0.250170, acc:  54%, G loss: 1.487720\n",
      "Ep: 569, steps: 3, D loss: 0.177792, acc:  82%, G loss: 1.879084\n",
      "Ep: 569, steps: 4, D loss: 0.190835, acc:  82%, G loss: 1.714322\n",
      "Ep: 569, steps: 5, D loss: 0.264554, acc:  52%, G loss: 1.664402\n",
      "Ep: 569, steps: 6, D loss: 0.239244, acc:  57%, G loss: 1.565482\n",
      "Ep: 569, steps: 7, D loss: 0.333752, acc:  24%, G loss: 1.463436\n",
      "Ep: 569, steps: 8, D loss: 0.240344, acc:  58%, G loss: 1.683756\n",
      "Ep: 569, steps: 9, D loss: 0.233848, acc:  62%, G loss: 1.687215\n",
      "Ep: 569, steps: 10, D loss: 0.193887, acc:  76%, G loss: 1.585448\n",
      "Ep: 569, steps: 11, D loss: 0.256435, acc:  51%, G loss: 1.708284\n",
      "Ep: 569, steps: 12, D loss: 0.289365, acc:  36%, G loss: 1.317555\n",
      "Ep: 569, steps: 13, D loss: 0.282040, acc:  38%, G loss: 1.401991\n",
      "Ep: 569, steps: 14, D loss: 0.265452, acc:  43%, G loss: 1.489294\n",
      "Ep: 569, steps: 15, D loss: 0.246118, acc:  54%, G loss: 1.523537\n",
      "Saved Model\n",
      "Ep: 569, steps: 16, D loss: 0.238077, acc:  60%, G loss: 1.602315\n",
      "Ep: 569, steps: 17, D loss: 0.243448, acc:  55%, G loss: 1.671959\n",
      "Ep: 569, steps: 18, D loss: 0.244393, acc:  59%, G loss: 1.511058\n",
      "Ep: 569, steps: 19, D loss: 0.191889, acc:  76%, G loss: 1.717917\n",
      "Ep: 569, steps: 20, D loss: 0.274014, acc:  36%, G loss: 1.431378\n",
      "Ep: 569, steps: 21, D loss: 0.194963, acc:  71%, G loss: 1.643442\n",
      "Ep: 569, steps: 22, D loss: 0.239433, acc:  57%, G loss: 1.785409\n",
      "Ep: 569, steps: 23, D loss: 0.197314, acc:  76%, G loss: 1.570083\n",
      "Ep: 569, steps: 24, D loss: 0.250262, acc:  53%, G loss: 1.676878\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 570, steps: 1, D loss: 0.236132, acc:  61%, G loss: 1.647883\n",
      "Ep: 570, steps: 2, D loss: 0.251274, acc:  53%, G loss: 1.511806\n",
      "Ep: 570, steps: 3, D loss: 0.177589, acc:  79%, G loss: 1.935537\n",
      "Ep: 570, steps: 4, D loss: 0.189761, acc:  81%, G loss: 1.719380\n",
      "Ep: 570, steps: 5, D loss: 0.262110, acc:  51%, G loss: 1.685072\n",
      "Ep: 570, steps: 6, D loss: 0.236830, acc:  57%, G loss: 1.582781\n",
      "Ep: 570, steps: 7, D loss: 0.309329, acc:  33%, G loss: 1.467988\n",
      "Ep: 570, steps: 8, D loss: 0.215429, acc:  65%, G loss: 1.739726\n",
      "Ep: 570, steps: 9, D loss: 0.241084, acc:  59%, G loss: 1.714201\n",
      "Ep: 570, steps: 10, D loss: 0.177953, acc:  82%, G loss: 1.555411\n",
      "Ep: 570, steps: 11, D loss: 0.259372, acc:  49%, G loss: 1.787870\n",
      "Ep: 570, steps: 12, D loss: 0.279639, acc:  39%, G loss: 1.332243\n",
      "Ep: 570, steps: 13, D loss: 0.279127, acc:  38%, G loss: 1.387357\n",
      "Ep: 570, steps: 14, D loss: 0.271881, acc:  43%, G loss: 1.494575\n",
      "Ep: 570, steps: 15, D loss: 0.263785, acc:  45%, G loss: 1.565873\n",
      "Ep: 570, steps: 16, D loss: 0.240313, acc:  59%, G loss: 1.606393\n",
      "Ep: 570, steps: 17, D loss: 0.223889, acc:  65%, G loss: 1.643370\n",
      "Ep: 570, steps: 18, D loss: 0.248284, acc:  56%, G loss: 1.636156\n",
      "Ep: 570, steps: 19, D loss: 0.220708, acc:  65%, G loss: 1.592648\n",
      "Ep: 570, steps: 20, D loss: 0.185383, acc:  75%, G loss: 1.761699\n",
      "Ep: 570, steps: 21, D loss: 0.273536, acc:  39%, G loss: 1.470441\n",
      "Ep: 570, steps: 22, D loss: 0.162576, acc:  78%, G loss: 1.612525\n",
      "Ep: 570, steps: 23, D loss: 0.215774, acc:  67%, G loss: 1.841505\n",
      "Ep: 570, steps: 24, D loss: 0.215904, acc:  67%, G loss: 1.640886\n",
      "Ep: 570, steps: 25, D loss: 0.252332, acc:  53%, G loss: 1.581237\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 571, steps: 1, D loss: 0.253562, acc:  55%, G loss: 1.664294\n",
      "Ep: 571, steps: 2, D loss: 0.246388, acc:  54%, G loss: 1.442621\n",
      "Ep: 571, steps: 3, D loss: 0.182079, acc:  76%, G loss: 1.934760\n",
      "Ep: 571, steps: 4, D loss: 0.190160, acc:  80%, G loss: 1.739909\n",
      "Ep: 571, steps: 5, D loss: 0.282568, acc:  46%, G loss: 1.647279\n",
      "Ep: 571, steps: 6, D loss: 0.241496, acc:  56%, G loss: 1.582603\n",
      "Ep: 571, steps: 7, D loss: 0.327438, acc:  26%, G loss: 1.628821\n",
      "Ep: 571, steps: 8, D loss: 0.225212, acc:  66%, G loss: 1.775106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 571, steps: 9, D loss: 0.246025, acc:  59%, G loss: 1.625595\n",
      "Ep: 571, steps: 10, D loss: 0.186584, acc:  76%, G loss: 1.578893\n",
      "Ep: 571, steps: 11, D loss: 0.238055, acc:  57%, G loss: 1.735814\n",
      "Ep: 571, steps: 12, D loss: 0.286380, acc:  36%, G loss: 1.348743\n",
      "Ep: 571, steps: 13, D loss: 0.271437, acc:  42%, G loss: 1.414651\n",
      "Ep: 571, steps: 14, D loss: 0.266005, acc:  45%, G loss: 1.445554\n",
      "Ep: 571, steps: 15, D loss: 0.246848, acc:  55%, G loss: 1.567418\n",
      "Ep: 571, steps: 16, D loss: 0.249548, acc:  53%, G loss: 1.565835\n",
      "Ep: 571, steps: 17, D loss: 0.229962, acc:  66%, G loss: 1.635751\n",
      "Ep: 571, steps: 18, D loss: 0.237695, acc:  59%, G loss: 1.593469\n",
      "Ep: 571, steps: 19, D loss: 0.225000, acc:  64%, G loss: 1.592707\n",
      "Ep: 571, steps: 20, D loss: 0.201669, acc:  73%, G loss: 1.690106\n",
      "Ep: 571, steps: 21, D loss: 0.265526, acc:  43%, G loss: 1.454858\n",
      "Ep: 571, steps: 22, D loss: 0.188064, acc:  72%, G loss: 1.527294\n",
      "Ep: 571, steps: 23, D loss: 0.233177, acc:  61%, G loss: 1.804057\n",
      "Ep: 571, steps: 24, D loss: 0.197871, acc:  72%, G loss: 1.531806\n",
      "Ep: 571, steps: 25, D loss: 0.257190, acc:  54%, G loss: 1.623891\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 572, steps: 1, D loss: 0.247339, acc:  57%, G loss: 1.651515\n",
      "Ep: 572, steps: 2, D loss: 0.230989, acc:  61%, G loss: 1.483205\n",
      "Ep: 572, steps: 3, D loss: 0.169416, acc:  79%, G loss: 1.910512\n",
      "Ep: 572, steps: 4, D loss: 0.195242, acc:  79%, G loss: 1.791079\n",
      "Ep: 572, steps: 5, D loss: 0.260701, acc:  53%, G loss: 1.686604\n",
      "Ep: 572, steps: 6, D loss: 0.232168, acc:  56%, G loss: 1.533144\n",
      "Ep: 572, steps: 7, D loss: 0.302779, acc:  34%, G loss: 1.471262\n",
      "Ep: 572, steps: 8, D loss: 0.231866, acc:  61%, G loss: 1.810482\n",
      "Ep: 572, steps: 9, D loss: 0.237514, acc:  64%, G loss: 1.641050\n",
      "Ep: 572, steps: 10, D loss: 0.175674, acc:  82%, G loss: 1.587217\n",
      "Ep: 572, steps: 11, D loss: 0.251558, acc:  51%, G loss: 1.758315\n",
      "Ep: 572, steps: 12, D loss: 0.294302, acc:  33%, G loss: 1.334237\n",
      "Ep: 572, steps: 13, D loss: 0.275387, acc:  41%, G loss: 1.420971\n",
      "Saved Model\n",
      "Ep: 572, steps: 14, D loss: 0.284719, acc:  38%, G loss: 1.495353\n",
      "Ep: 572, steps: 15, D loss: 0.236801, acc:  60%, G loss: 1.650886\n",
      "Ep: 572, steps: 16, D loss: 0.232602, acc:  61%, G loss: 1.584152\n",
      "Ep: 572, steps: 17, D loss: 0.253700, acc:  53%, G loss: 1.526365\n",
      "Ep: 572, steps: 18, D loss: 0.214710, acc:  66%, G loss: 1.572110\n",
      "Ep: 572, steps: 19, D loss: 0.173266, acc:  77%, G loss: 1.751929\n",
      "Ep: 572, steps: 20, D loss: 0.263367, acc:  43%, G loss: 1.525874\n",
      "Ep: 572, steps: 21, D loss: 0.203216, acc:  65%, G loss: 1.721471\n",
      "Ep: 572, steps: 22, D loss: 0.228387, acc:  65%, G loss: 1.859531\n",
      "Ep: 572, steps: 23, D loss: 0.204358, acc:  71%, G loss: 1.616547\n",
      "Ep: 572, steps: 24, D loss: 0.262960, acc:  51%, G loss: 1.593201\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 573, steps: 1, D loss: 0.280249, acc:  43%, G loss: 1.769849\n",
      "Ep: 573, steps: 2, D loss: 0.236825, acc:  57%, G loss: 1.450656\n",
      "Ep: 573, steps: 3, D loss: 0.172619, acc:  80%, G loss: 1.872886\n",
      "Ep: 573, steps: 4, D loss: 0.184792, acc:  84%, G loss: 1.729129\n",
      "Ep: 573, steps: 5, D loss: 0.268502, acc:  49%, G loss: 1.703386\n",
      "Ep: 573, steps: 6, D loss: 0.253045, acc:  53%, G loss: 1.576327\n",
      "Ep: 573, steps: 7, D loss: 0.326616, acc:  29%, G loss: 1.465965\n",
      "Ep: 573, steps: 8, D loss: 0.228098, acc:  64%, G loss: 1.689385\n",
      "Ep: 573, steps: 9, D loss: 0.256795, acc:  54%, G loss: 1.629975\n",
      "Ep: 573, steps: 10, D loss: 0.195556, acc:  72%, G loss: 1.590171\n",
      "Ep: 573, steps: 11, D loss: 0.249162, acc:  54%, G loss: 1.722423\n",
      "Ep: 573, steps: 12, D loss: 0.286357, acc:  35%, G loss: 1.309446\n",
      "Ep: 573, steps: 13, D loss: 0.280529, acc:  37%, G loss: 1.361539\n",
      "Ep: 573, steps: 14, D loss: 0.277722, acc:  38%, G loss: 1.464795\n",
      "Ep: 573, steps: 15, D loss: 0.245467, acc:  53%, G loss: 1.548333\n",
      "Ep: 573, steps: 16, D loss: 0.256909, acc:  51%, G loss: 1.650874\n",
      "Ep: 573, steps: 17, D loss: 0.213870, acc:  69%, G loss: 1.553069\n",
      "Ep: 573, steps: 18, D loss: 0.235134, acc:  60%, G loss: 1.538817\n",
      "Ep: 573, steps: 19, D loss: 0.216929, acc:  65%, G loss: 1.557435\n",
      "Ep: 573, steps: 20, D loss: 0.178929, acc:  77%, G loss: 1.710971\n",
      "Ep: 573, steps: 21, D loss: 0.248248, acc:  49%, G loss: 1.474654\n",
      "Ep: 573, steps: 22, D loss: 0.190278, acc:  69%, G loss: 1.639278\n",
      "Ep: 573, steps: 23, D loss: 0.210722, acc:  69%, G loss: 1.811622\n",
      "Ep: 573, steps: 24, D loss: 0.211913, acc:  69%, G loss: 1.574749\n",
      "Ep: 573, steps: 25, D loss: 0.252715, acc:  54%, G loss: 1.569832\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 574, steps: 1, D loss: 0.233535, acc:  62%, G loss: 1.683140\n",
      "Ep: 574, steps: 2, D loss: 0.248599, acc:  54%, G loss: 1.420445\n",
      "Ep: 574, steps: 3, D loss: 0.174110, acc:  80%, G loss: 1.894398\n",
      "Ep: 574, steps: 4, D loss: 0.182113, acc:  86%, G loss: 1.762356\n",
      "Ep: 574, steps: 5, D loss: 0.266816, acc:  51%, G loss: 1.653627\n",
      "Ep: 574, steps: 6, D loss: 0.225967, acc:  58%, G loss: 1.580398\n",
      "Ep: 574, steps: 7, D loss: 0.314464, acc:  31%, G loss: 1.482706\n",
      "Ep: 574, steps: 8, D loss: 0.212861, acc:  65%, G loss: 1.690746\n",
      "Ep: 574, steps: 9, D loss: 0.246476, acc:  59%, G loss: 1.628896\n",
      "Ep: 574, steps: 10, D loss: 0.184472, acc:  77%, G loss: 1.542948\n",
      "Ep: 574, steps: 11, D loss: 0.257270, acc:  50%, G loss: 1.825291\n",
      "Ep: 574, steps: 12, D loss: 0.299096, acc:  31%, G loss: 1.336340\n",
      "Ep: 574, steps: 13, D loss: 0.274395, acc:  40%, G loss: 1.363344\n",
      "Ep: 574, steps: 14, D loss: 0.278865, acc:  39%, G loss: 1.440689\n",
      "Ep: 574, steps: 15, D loss: 0.253200, acc:  49%, G loss: 1.584815\n",
      "Ep: 574, steps: 16, D loss: 0.252686, acc:  54%, G loss: 1.584211\n",
      "Ep: 574, steps: 17, D loss: 0.204601, acc:  74%, G loss: 1.548979\n",
      "Ep: 574, steps: 18, D loss: 0.241011, acc:  57%, G loss: 1.609881\n",
      "Ep: 574, steps: 19, D loss: 0.201437, acc:  70%, G loss: 1.592655\n",
      "Ep: 574, steps: 20, D loss: 0.176474, acc:  76%, G loss: 1.761689\n",
      "Ep: 574, steps: 21, D loss: 0.277017, acc:  38%, G loss: 1.546960\n",
      "Ep: 574, steps: 22, D loss: 0.148826, acc:  81%, G loss: 1.647647\n",
      "Ep: 574, steps: 23, D loss: 0.243198, acc:  58%, G loss: 1.845414\n",
      "Ep: 574, steps: 24, D loss: 0.221761, acc:  63%, G loss: 1.561039\n",
      "Ep: 574, steps: 25, D loss: 0.258112, acc:  52%, G loss: 1.804872\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 575, steps: 1, D loss: 0.241401, acc:  60%, G loss: 1.740280\n",
      "Ep: 575, steps: 2, D loss: 0.254441, acc:  52%, G loss: 1.411084\n",
      "Ep: 575, steps: 3, D loss: 0.170460, acc:  79%, G loss: 1.851995\n",
      "Ep: 575, steps: 4, D loss: 0.193142, acc:  79%, G loss: 1.702003\n",
      "Ep: 575, steps: 5, D loss: 0.291178, acc:  43%, G loss: 1.648807\n",
      "Ep: 575, steps: 6, D loss: 0.220943, acc:  58%, G loss: 1.571302\n",
      "Ep: 575, steps: 7, D loss: 0.321760, acc:  29%, G loss: 1.542946\n",
      "Ep: 575, steps: 8, D loss: 0.231309, acc:  61%, G loss: 1.756355\n",
      "Ep: 575, steps: 9, D loss: 0.230118, acc:  64%, G loss: 1.605982\n",
      "Ep: 575, steps: 10, D loss: 0.194422, acc:  73%, G loss: 1.600758\n",
      "Ep: 575, steps: 11, D loss: 0.253280, acc:  50%, G loss: 1.792544\n",
      "Saved Model\n",
      "Ep: 575, steps: 12, D loss: 0.279317, acc:  41%, G loss: 1.330565\n",
      "Ep: 575, steps: 13, D loss: 0.273300, acc:  42%, G loss: 1.416952\n",
      "Ep: 575, steps: 14, D loss: 0.235480, acc:  59%, G loss: 1.601346\n",
      "Ep: 575, steps: 15, D loss: 0.246331, acc:  57%, G loss: 1.604481\n",
      "Ep: 575, steps: 16, D loss: 0.204639, acc:  73%, G loss: 1.563455\n",
      "Ep: 575, steps: 17, D loss: 0.238808, acc:  58%, G loss: 1.603937\n",
      "Ep: 575, steps: 18, D loss: 0.221253, acc:  65%, G loss: 1.614785\n",
      "Ep: 575, steps: 19, D loss: 0.201800, acc:  72%, G loss: 1.729344\n",
      "Ep: 575, steps: 20, D loss: 0.275236, acc:  38%, G loss: 1.429447\n",
      "Ep: 575, steps: 21, D loss: 0.164658, acc:  80%, G loss: 1.610674\n",
      "Ep: 575, steps: 22, D loss: 0.237794, acc:  58%, G loss: 1.834118\n",
      "Ep: 575, steps: 23, D loss: 0.207348, acc:  70%, G loss: 1.567858\n",
      "Ep: 575, steps: 24, D loss: 0.260992, acc:  51%, G loss: 1.659897\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 576, steps: 1, D loss: 0.233496, acc:  62%, G loss: 1.696338\n",
      "Ep: 576, steps: 2, D loss: 0.248886, acc:  54%, G loss: 1.421763\n",
      "Ep: 576, steps: 3, D loss: 0.174405, acc:  79%, G loss: 1.909461\n",
      "Ep: 576, steps: 4, D loss: 0.186744, acc:  81%, G loss: 1.723354\n",
      "Ep: 576, steps: 5, D loss: 0.283043, acc:  48%, G loss: 1.665118\n",
      "Ep: 576, steps: 6, D loss: 0.246409, acc:  54%, G loss: 1.582912\n",
      "Ep: 576, steps: 7, D loss: 0.308191, acc:  30%, G loss: 1.534348\n",
      "Ep: 576, steps: 8, D loss: 0.226027, acc:  62%, G loss: 1.838184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 576, steps: 9, D loss: 0.246467, acc:  59%, G loss: 1.658080\n",
      "Ep: 576, steps: 10, D loss: 0.169857, acc:  83%, G loss: 1.566741\n",
      "Ep: 576, steps: 11, D loss: 0.248695, acc:  53%, G loss: 1.753912\n",
      "Ep: 576, steps: 12, D loss: 0.293182, acc:  34%, G loss: 1.348626\n",
      "Ep: 576, steps: 13, D loss: 0.295691, acc:  33%, G loss: 1.398195\n",
      "Ep: 576, steps: 14, D loss: 0.270423, acc:  42%, G loss: 1.523856\n",
      "Ep: 576, steps: 15, D loss: 0.242093, acc:  56%, G loss: 1.588441\n",
      "Ep: 576, steps: 16, D loss: 0.240496, acc:  59%, G loss: 1.571155\n",
      "Ep: 576, steps: 17, D loss: 0.223119, acc:  65%, G loss: 1.598477\n",
      "Ep: 576, steps: 18, D loss: 0.239003, acc:  59%, G loss: 1.585708\n",
      "Ep: 576, steps: 19, D loss: 0.223070, acc:  64%, G loss: 1.620839\n",
      "Ep: 576, steps: 20, D loss: 0.177964, acc:  78%, G loss: 1.770319\n",
      "Ep: 576, steps: 21, D loss: 0.269923, acc:  40%, G loss: 1.550138\n",
      "Ep: 576, steps: 22, D loss: 0.171376, acc:  73%, G loss: 1.694590\n",
      "Ep: 576, steps: 23, D loss: 0.216370, acc:  67%, G loss: 1.845162\n",
      "Ep: 576, steps: 24, D loss: 0.214761, acc:  65%, G loss: 1.673898\n",
      "Ep: 576, steps: 25, D loss: 0.250067, acc:  55%, G loss: 1.578322\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 577, steps: 1, D loss: 0.255223, acc:  54%, G loss: 1.654594\n",
      "Ep: 577, steps: 2, D loss: 0.245831, acc:  56%, G loss: 1.429598\n",
      "Ep: 577, steps: 3, D loss: 0.177718, acc:  78%, G loss: 1.897532\n",
      "Ep: 577, steps: 4, D loss: 0.185804, acc:  81%, G loss: 1.693690\n",
      "Ep: 577, steps: 5, D loss: 0.299860, acc:  43%, G loss: 1.612022\n",
      "Ep: 577, steps: 6, D loss: 0.236757, acc:  56%, G loss: 1.566460\n",
      "Ep: 577, steps: 7, D loss: 0.320738, acc:  29%, G loss: 1.488303\n",
      "Ep: 577, steps: 8, D loss: 0.229558, acc:  64%, G loss: 1.798392\n",
      "Ep: 577, steps: 9, D loss: 0.244265, acc:  60%, G loss: 1.682942\n",
      "Ep: 577, steps: 10, D loss: 0.184458, acc:  78%, G loss: 1.577041\n",
      "Ep: 577, steps: 11, D loss: 0.255622, acc:  49%, G loss: 1.732496\n",
      "Ep: 577, steps: 12, D loss: 0.284515, acc:  40%, G loss: 1.421485\n",
      "Ep: 577, steps: 13, D loss: 0.284382, acc:  36%, G loss: 1.431132\n",
      "Ep: 577, steps: 14, D loss: 0.269820, acc:  43%, G loss: 1.518466\n",
      "Ep: 577, steps: 15, D loss: 0.254425, acc:  51%, G loss: 1.564000\n",
      "Ep: 577, steps: 16, D loss: 0.253883, acc:  53%, G loss: 1.586568\n",
      "Ep: 577, steps: 17, D loss: 0.209523, acc:  71%, G loss: 1.627755\n",
      "Ep: 577, steps: 18, D loss: 0.237705, acc:  58%, G loss: 1.579421\n",
      "Ep: 577, steps: 19, D loss: 0.229699, acc:  61%, G loss: 1.656640\n",
      "Ep: 577, steps: 20, D loss: 0.177993, acc:  77%, G loss: 1.753282\n",
      "Ep: 577, steps: 21, D loss: 0.268553, acc:  38%, G loss: 1.576443\n",
      "Ep: 577, steps: 22, D loss: 0.173860, acc:  72%, G loss: 1.618702\n",
      "Ep: 577, steps: 23, D loss: 0.213683, acc:  67%, G loss: 1.868961\n",
      "Ep: 577, steps: 24, D loss: 0.206559, acc:  69%, G loss: 1.578403\n",
      "Ep: 577, steps: 25, D loss: 0.263263, acc:  53%, G loss: 1.849033\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 578, steps: 1, D loss: 0.230741, acc:  62%, G loss: 1.684166\n",
      "Ep: 578, steps: 2, D loss: 0.253005, acc:  52%, G loss: 1.438370\n",
      "Ep: 578, steps: 3, D loss: 0.177177, acc:  79%, G loss: 1.975629\n",
      "Ep: 578, steps: 4, D loss: 0.186010, acc:  81%, G loss: 1.808852\n",
      "Ep: 578, steps: 5, D loss: 0.287763, acc:  46%, G loss: 1.689048\n",
      "Ep: 578, steps: 6, D loss: 0.239240, acc:  57%, G loss: 1.663543\n",
      "Ep: 578, steps: 7, D loss: 0.314291, acc:  29%, G loss: 1.499522\n",
      "Ep: 578, steps: 8, D loss: 0.224690, acc:  64%, G loss: 1.865313\n",
      "Ep: 578, steps: 9, D loss: 0.213876, acc:  71%, G loss: 1.650193\n",
      "Saved Model\n",
      "Ep: 578, steps: 10, D loss: 0.188624, acc:  78%, G loss: 1.550632\n",
      "Ep: 578, steps: 11, D loss: 0.272545, acc:  42%, G loss: 1.323712\n",
      "Ep: 578, steps: 12, D loss: 0.274636, acc:  40%, G loss: 1.423880\n",
      "Ep: 578, steps: 13, D loss: 0.267226, acc:  46%, G loss: 1.495058\n",
      "Ep: 578, steps: 14, D loss: 0.317332, acc:  29%, G loss: 1.498669\n",
      "Ep: 578, steps: 15, D loss: 0.246433, acc:  57%, G loss: 1.564905\n",
      "Ep: 578, steps: 16, D loss: 0.212618, acc:  69%, G loss: 1.548252\n",
      "Ep: 578, steps: 17, D loss: 0.260220, acc:  50%, G loss: 1.603429\n",
      "Ep: 578, steps: 18, D loss: 0.208278, acc:  67%, G loss: 1.540453\n",
      "Ep: 578, steps: 19, D loss: 0.173900, acc:  79%, G loss: 1.756476\n",
      "Ep: 578, steps: 20, D loss: 0.246215, acc:  50%, G loss: 1.556087\n",
      "Ep: 578, steps: 21, D loss: 0.160699, acc:  76%, G loss: 1.635728\n",
      "Ep: 578, steps: 22, D loss: 0.222352, acc:  67%, G loss: 1.918181\n",
      "Ep: 578, steps: 23, D loss: 0.190637, acc:  76%, G loss: 1.667884\n",
      "Ep: 578, steps: 24, D loss: 0.255249, acc:  54%, G loss: 1.656879\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 579, steps: 1, D loss: 0.263412, acc:  51%, G loss: 1.778772\n",
      "Ep: 579, steps: 2, D loss: 0.246792, acc:  54%, G loss: 1.478383\n",
      "Ep: 579, steps: 3, D loss: 0.167010, acc:  82%, G loss: 1.873938\n",
      "Ep: 579, steps: 4, D loss: 0.196434, acc:  79%, G loss: 1.689509\n",
      "Ep: 579, steps: 5, D loss: 0.259601, acc:  51%, G loss: 1.596187\n",
      "Ep: 579, steps: 6, D loss: 0.232957, acc:  58%, G loss: 1.573329\n",
      "Ep: 579, steps: 7, D loss: 0.294910, acc:  36%, G loss: 1.614406\n",
      "Ep: 579, steps: 8, D loss: 0.214983, acc:  67%, G loss: 1.762566\n",
      "Ep: 579, steps: 9, D loss: 0.231297, acc:  65%, G loss: 1.685342\n",
      "Ep: 579, steps: 10, D loss: 0.183660, acc:  78%, G loss: 1.577452\n",
      "Ep: 579, steps: 11, D loss: 0.270709, acc:  47%, G loss: 1.734357\n",
      "Ep: 579, steps: 12, D loss: 0.292420, acc:  36%, G loss: 1.369667\n",
      "Ep: 579, steps: 13, D loss: 0.292089, acc:  34%, G loss: 1.401106\n",
      "Ep: 579, steps: 14, D loss: 0.275231, acc:  42%, G loss: 1.471506\n",
      "Ep: 579, steps: 15, D loss: 0.241535, acc:  57%, G loss: 1.537740\n",
      "Ep: 579, steps: 16, D loss: 0.257197, acc:  52%, G loss: 1.552053\n",
      "Ep: 579, steps: 17, D loss: 0.224860, acc:  65%, G loss: 1.649147\n",
      "Ep: 579, steps: 18, D loss: 0.231341, acc:  62%, G loss: 1.616478\n",
      "Ep: 579, steps: 19, D loss: 0.210689, acc:  67%, G loss: 1.641972\n",
      "Ep: 579, steps: 20, D loss: 0.169736, acc:  82%, G loss: 1.730446\n",
      "Ep: 579, steps: 21, D loss: 0.266207, acc:  41%, G loss: 1.523555\n",
      "Ep: 579, steps: 22, D loss: 0.166534, acc:  76%, G loss: 1.704133\n",
      "Ep: 579, steps: 23, D loss: 0.225207, acc:  65%, G loss: 1.873400\n",
      "Ep: 579, steps: 24, D loss: 0.201326, acc:  71%, G loss: 1.601942\n",
      "Ep: 579, steps: 25, D loss: 0.233589, acc:  61%, G loss: 1.529495\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 580, steps: 1, D loss: 0.253423, acc:  52%, G loss: 1.658317\n",
      "Ep: 580, steps: 2, D loss: 0.254259, acc:  51%, G loss: 1.460002\n",
      "Ep: 580, steps: 3, D loss: 0.178854, acc:  79%, G loss: 1.920072\n",
      "Ep: 580, steps: 4, D loss: 0.185564, acc:  83%, G loss: 1.742892\n",
      "Ep: 580, steps: 5, D loss: 0.290753, acc:  44%, G loss: 1.643610\n",
      "Ep: 580, steps: 6, D loss: 0.246299, acc:  56%, G loss: 1.593220\n",
      "Ep: 580, steps: 7, D loss: 0.309014, acc:  32%, G loss: 1.603474\n",
      "Ep: 580, steps: 8, D loss: 0.218329, acc:  67%, G loss: 1.771781\n",
      "Ep: 580, steps: 9, D loss: 0.253746, acc:  57%, G loss: 1.667066\n",
      "Ep: 580, steps: 10, D loss: 0.185531, acc:  75%, G loss: 1.591347\n",
      "Ep: 580, steps: 11, D loss: 0.258219, acc:  50%, G loss: 1.719321\n",
      "Ep: 580, steps: 12, D loss: 0.304528, acc:  29%, G loss: 1.338120\n",
      "Ep: 580, steps: 13, D loss: 0.293507, acc:  35%, G loss: 1.392118\n",
      "Ep: 580, steps: 14, D loss: 0.266634, acc:  43%, G loss: 1.516847\n",
      "Ep: 580, steps: 15, D loss: 0.238016, acc:  57%, G loss: 1.600959\n",
      "Ep: 580, steps: 16, D loss: 0.242434, acc:  58%, G loss: 1.600193\n",
      "Ep: 580, steps: 17, D loss: 0.205451, acc:  73%, G loss: 1.597594\n",
      "Ep: 580, steps: 18, D loss: 0.245194, acc:  56%, G loss: 1.621951\n",
      "Ep: 580, steps: 19, D loss: 0.210497, acc:  67%, G loss: 1.664690\n",
      "Ep: 580, steps: 20, D loss: 0.180754, acc:  77%, G loss: 1.793806\n",
      "Ep: 580, steps: 21, D loss: 0.256029, acc:  45%, G loss: 1.501137\n",
      "Ep: 580, steps: 22, D loss: 0.171797, acc:  75%, G loss: 1.559877\n",
      "Ep: 580, steps: 23, D loss: 0.231234, acc:  59%, G loss: 1.837710\n",
      "Ep: 580, steps: 24, D loss: 0.204194, acc:  70%, G loss: 1.655645\n",
      "Ep: 580, steps: 25, D loss: 0.259239, acc:  52%, G loss: 1.492102\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 581, steps: 1, D loss: 0.231930, acc:  64%, G loss: 1.739187\n",
      "Ep: 581, steps: 2, D loss: 0.235368, acc:  58%, G loss: 1.387649\n",
      "Ep: 581, steps: 3, D loss: 0.180190, acc:  76%, G loss: 1.849029\n",
      "Ep: 581, steps: 4, D loss: 0.193215, acc:  80%, G loss: 1.737852\n",
      "Ep: 581, steps: 5, D loss: 0.278401, acc:  45%, G loss: 1.638831\n",
      "Ep: 581, steps: 6, D loss: 0.225308, acc:  59%, G loss: 1.558095\n",
      "Ep: 581, steps: 7, D loss: 0.299901, acc:  34%, G loss: 1.495648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Ep: 581, steps: 8, D loss: 0.226512, acc:  62%, G loss: 1.796966\n",
      "Ep: 581, steps: 9, D loss: 0.190414, acc:  76%, G loss: 1.659314\n",
      "Ep: 581, steps: 10, D loss: 0.299062, acc:  35%, G loss: 1.700687\n",
      "Ep: 581, steps: 11, D loss: 0.282456, acc:  38%, G loss: 1.306973\n",
      "Ep: 581, steps: 12, D loss: 0.267806, acc:  43%, G loss: 1.366052\n",
      "Ep: 581, steps: 13, D loss: 0.260199, acc:  46%, G loss: 1.452209\n",
      "Ep: 581, steps: 14, D loss: 0.270437, acc:  46%, G loss: 1.552804\n",
      "Ep: 581, steps: 15, D loss: 0.259732, acc:  48%, G loss: 1.597952\n",
      "Ep: 581, steps: 16, D loss: 0.207377, acc:  73%, G loss: 1.568678\n",
      "Ep: 581, steps: 17, D loss: 0.228647, acc:  63%, G loss: 1.606827\n",
      "Ep: 581, steps: 18, D loss: 0.214391, acc:  67%, G loss: 1.651242\n",
      "Ep: 581, steps: 19, D loss: 0.170596, acc:  80%, G loss: 1.726057\n",
      "Ep: 581, steps: 20, D loss: 0.264156, acc:  42%, G loss: 1.469735\n",
      "Ep: 581, steps: 21, D loss: 0.186552, acc:  71%, G loss: 1.725633\n",
      "Ep: 581, steps: 22, D loss: 0.225001, acc:  63%, G loss: 1.823434\n",
      "Ep: 581, steps: 23, D loss: 0.195792, acc:  75%, G loss: 1.549400\n",
      "Ep: 581, steps: 24, D loss: 0.296638, acc:  44%, G loss: 1.533781\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 582, steps: 1, D loss: 0.266395, acc:  48%, G loss: 1.743927\n",
      "Ep: 582, steps: 2, D loss: 0.242413, acc:  57%, G loss: 1.409164\n",
      "Ep: 582, steps: 3, D loss: 0.169528, acc:  83%, G loss: 1.952907\n",
      "Ep: 582, steps: 4, D loss: 0.187541, acc:  82%, G loss: 1.724041\n",
      "Ep: 582, steps: 5, D loss: 0.278647, acc:  46%, G loss: 1.747855\n",
      "Ep: 582, steps: 6, D loss: 0.237927, acc:  57%, G loss: 1.637546\n",
      "Ep: 582, steps: 7, D loss: 0.318134, acc:  29%, G loss: 1.601627\n",
      "Ep: 582, steps: 8, D loss: 0.219729, acc:  63%, G loss: 1.832348\n",
      "Ep: 582, steps: 9, D loss: 0.262546, acc:  51%, G loss: 1.605462\n",
      "Ep: 582, steps: 10, D loss: 0.180823, acc:  79%, G loss: 1.536944\n",
      "Ep: 582, steps: 11, D loss: 0.249061, acc:  55%, G loss: 1.707845\n",
      "Ep: 582, steps: 12, D loss: 0.287903, acc:  38%, G loss: 1.317378\n",
      "Ep: 582, steps: 13, D loss: 0.297259, acc:  31%, G loss: 1.399323\n",
      "Ep: 582, steps: 14, D loss: 0.272687, acc:  40%, G loss: 1.510999\n",
      "Ep: 582, steps: 15, D loss: 0.238323, acc:  56%, G loss: 1.573424\n",
      "Ep: 582, steps: 16, D loss: 0.247531, acc:  55%, G loss: 1.636034\n",
      "Ep: 582, steps: 17, D loss: 0.212935, acc:  71%, G loss: 1.580136\n",
      "Ep: 582, steps: 18, D loss: 0.235938, acc:  61%, G loss: 1.647201\n",
      "Ep: 582, steps: 19, D loss: 0.217684, acc:  65%, G loss: 1.603509\n",
      "Ep: 582, steps: 20, D loss: 0.187044, acc:  77%, G loss: 1.719443\n",
      "Ep: 582, steps: 21, D loss: 0.250829, acc:  48%, G loss: 1.495627\n",
      "Ep: 582, steps: 22, D loss: 0.169709, acc:  74%, G loss: 1.635682\n",
      "Ep: 582, steps: 23, D loss: 0.225000, acc:  65%, G loss: 1.856123\n",
      "Ep: 582, steps: 24, D loss: 0.195023, acc:  74%, G loss: 1.607819\n",
      "Ep: 582, steps: 25, D loss: 0.261993, acc:  50%, G loss: 1.639633\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 583, steps: 1, D loss: 0.248884, acc:  56%, G loss: 1.696364\n",
      "Ep: 583, steps: 2, D loss: 0.240172, acc:  57%, G loss: 1.465506\n",
      "Ep: 583, steps: 3, D loss: 0.195011, acc:  72%, G loss: 1.918946\n",
      "Ep: 583, steps: 4, D loss: 0.191058, acc:  82%, G loss: 1.693203\n",
      "Ep: 583, steps: 5, D loss: 0.287504, acc:  45%, G loss: 1.584502\n",
      "Ep: 583, steps: 6, D loss: 0.228174, acc:  56%, G loss: 1.626648\n",
      "Ep: 583, steps: 7, D loss: 0.299201, acc:  35%, G loss: 1.461771\n",
      "Ep: 583, steps: 8, D loss: 0.217468, acc:  64%, G loss: 1.811541\n",
      "Ep: 583, steps: 9, D loss: 0.239505, acc:  61%, G loss: 1.639260\n",
      "Ep: 583, steps: 10, D loss: 0.185385, acc:  82%, G loss: 1.565068\n",
      "Ep: 583, steps: 11, D loss: 0.274272, acc:  46%, G loss: 1.678173\n",
      "Ep: 583, steps: 12, D loss: 0.285074, acc:  37%, G loss: 1.413385\n",
      "Ep: 583, steps: 13, D loss: 0.301513, acc:  30%, G loss: 1.380617\n",
      "Ep: 583, steps: 14, D loss: 0.279550, acc:  38%, G loss: 1.506920\n",
      "Ep: 583, steps: 15, D loss: 0.254397, acc:  50%, G loss: 1.628747\n",
      "Ep: 583, steps: 16, D loss: 0.242374, acc:  58%, G loss: 1.595919\n",
      "Ep: 583, steps: 17, D loss: 0.206539, acc:  72%, G loss: 1.596690\n",
      "Ep: 583, steps: 18, D loss: 0.240677, acc:  57%, G loss: 1.646594\n",
      "Ep: 583, steps: 19, D loss: 0.205975, acc:  69%, G loss: 1.626011\n",
      "Ep: 583, steps: 20, D loss: 0.183599, acc:  75%, G loss: 1.767175\n",
      "Ep: 583, steps: 21, D loss: 0.267226, acc:  40%, G loss: 1.504398\n",
      "Ep: 583, steps: 22, D loss: 0.201620, acc:  69%, G loss: 1.638485\n",
      "Ep: 583, steps: 23, D loss: 0.235920, acc:  61%, G loss: 1.864464\n",
      "Ep: 583, steps: 24, D loss: 0.216447, acc:  65%, G loss: 1.585230\n",
      "Ep: 583, steps: 25, D loss: 0.260791, acc:  53%, G loss: 1.656040\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 584, steps: 1, D loss: 0.229253, acc:  63%, G loss: 1.642276\n",
      "Ep: 584, steps: 2, D loss: 0.257105, acc:  51%, G loss: 1.394991\n",
      "Ep: 584, steps: 3, D loss: 0.167234, acc:  83%, G loss: 1.891369\n",
      "Ep: 584, steps: 4, D loss: 0.190131, acc:  81%, G loss: 1.727156\n",
      "Ep: 584, steps: 5, D loss: 0.290348, acc:  41%, G loss: 1.670974\n",
      "Saved Model\n",
      "Ep: 584, steps: 6, D loss: 0.234981, acc:  56%, G loss: 1.568789\n",
      "Ep: 584, steps: 7, D loss: 0.248606, acc:  56%, G loss: 1.779743\n",
      "Ep: 584, steps: 8, D loss: 0.210441, acc:  73%, G loss: 1.715672\n",
      "Ep: 584, steps: 9, D loss: 0.160521, acc:  85%, G loss: 1.651541\n",
      "Ep: 584, steps: 10, D loss: 0.239820, acc:  60%, G loss: 1.879871\n",
      "Ep: 584, steps: 11, D loss: 0.381139, acc:  20%, G loss: 1.396594\n",
      "Ep: 584, steps: 12, D loss: 0.321646, acc:  20%, G loss: 1.373274\n",
      "Ep: 584, steps: 13, D loss: 0.294499, acc:  34%, G loss: 1.473135\n",
      "Ep: 584, steps: 14, D loss: 0.240860, acc:  55%, G loss: 1.559865\n",
      "Ep: 584, steps: 15, D loss: 0.262751, acc:  47%, G loss: 1.567095\n",
      "Ep: 584, steps: 16, D loss: 0.204270, acc:  70%, G loss: 1.613414\n",
      "Ep: 584, steps: 17, D loss: 0.250801, acc:  52%, G loss: 1.526008\n",
      "Ep: 584, steps: 18, D loss: 0.196022, acc:  71%, G loss: 1.670153\n",
      "Ep: 584, steps: 19, D loss: 0.174903, acc:  78%, G loss: 1.779996\n",
      "Ep: 584, steps: 20, D loss: 0.276479, acc:  37%, G loss: 1.450303\n",
      "Ep: 584, steps: 21, D loss: 0.177948, acc:  74%, G loss: 1.654587\n",
      "Ep: 584, steps: 22, D loss: 0.212526, acc:  67%, G loss: 1.843504\n",
      "Ep: 584, steps: 23, D loss: 0.208523, acc:  70%, G loss: 1.551402\n",
      "Ep: 584, steps: 24, D loss: 0.284386, acc:  47%, G loss: 1.604725\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 585, steps: 1, D loss: 0.221708, acc:  65%, G loss: 1.653517\n",
      "Ep: 585, steps: 2, D loss: 0.227196, acc:  62%, G loss: 1.439843\n",
      "Ep: 585, steps: 3, D loss: 0.170704, acc:  79%, G loss: 1.886632\n",
      "Ep: 585, steps: 4, D loss: 0.177336, acc:  84%, G loss: 1.745443\n",
      "Ep: 585, steps: 5, D loss: 0.287889, acc:  42%, G loss: 1.629954\n",
      "Ep: 585, steps: 6, D loss: 0.237962, acc:  55%, G loss: 1.592021\n",
      "Ep: 585, steps: 7, D loss: 0.319981, acc:  30%, G loss: 1.425992\n",
      "Ep: 585, steps: 8, D loss: 0.228834, acc:  62%, G loss: 1.748123\n",
      "Ep: 585, steps: 9, D loss: 0.241779, acc:  60%, G loss: 1.565002\n",
      "Ep: 585, steps: 10, D loss: 0.173458, acc:  82%, G loss: 1.569008\n",
      "Ep: 585, steps: 11, D loss: 0.234015, acc:  57%, G loss: 1.707906\n",
      "Ep: 585, steps: 12, D loss: 0.297541, acc:  32%, G loss: 1.432026\n",
      "Ep: 585, steps: 13, D loss: 0.267212, acc:  44%, G loss: 1.401397\n",
      "Ep: 585, steps: 14, D loss: 0.265364, acc:  43%, G loss: 1.477970\n",
      "Ep: 585, steps: 15, D loss: 0.271680, acc:  43%, G loss: 1.572091\n",
      "Ep: 585, steps: 16, D loss: 0.246051, acc:  56%, G loss: 1.582864\n",
      "Ep: 585, steps: 17, D loss: 0.223179, acc:  66%, G loss: 1.493057\n",
      "Ep: 585, steps: 18, D loss: 0.249920, acc:  53%, G loss: 1.624207\n",
      "Ep: 585, steps: 19, D loss: 0.211574, acc:  68%, G loss: 1.628494\n",
      "Ep: 585, steps: 20, D loss: 0.169788, acc:  79%, G loss: 1.766867\n",
      "Ep: 585, steps: 21, D loss: 0.261483, acc:  43%, G loss: 1.495796\n",
      "Ep: 585, steps: 22, D loss: 0.193504, acc:  69%, G loss: 1.628156\n",
      "Ep: 585, steps: 23, D loss: 0.227259, acc:  62%, G loss: 1.803604\n",
      "Ep: 585, steps: 24, D loss: 0.210285, acc:  67%, G loss: 1.549935\n",
      "Ep: 585, steps: 25, D loss: 0.249159, acc:  57%, G loss: 1.587460\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 586, steps: 1, D loss: 0.237410, acc:  62%, G loss: 1.681483\n",
      "Ep: 586, steps: 2, D loss: 0.239272, acc:  56%, G loss: 1.491672\n",
      "Ep: 586, steps: 3, D loss: 0.182632, acc:  75%, G loss: 1.891033\n",
      "Ep: 586, steps: 4, D loss: 0.181266, acc:  84%, G loss: 1.806176\n",
      "Ep: 586, steps: 5, D loss: 0.311471, acc:  42%, G loss: 1.666686\n",
      "Ep: 586, steps: 6, D loss: 0.240511, acc:  56%, G loss: 1.558492\n",
      "Ep: 586, steps: 7, D loss: 0.312424, acc:  31%, G loss: 1.491865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 586, steps: 8, D loss: 0.220323, acc:  65%, G loss: 1.797636\n",
      "Ep: 586, steps: 9, D loss: 0.241762, acc:  58%, G loss: 1.635380\n",
      "Ep: 586, steps: 10, D loss: 0.188854, acc:  77%, G loss: 1.516606\n",
      "Ep: 586, steps: 11, D loss: 0.245265, acc:  54%, G loss: 1.756142\n",
      "Ep: 586, steps: 12, D loss: 0.287045, acc:  37%, G loss: 1.346972\n",
      "Ep: 586, steps: 13, D loss: 0.295287, acc:  32%, G loss: 1.376161\n",
      "Ep: 586, steps: 14, D loss: 0.272509, acc:  41%, G loss: 1.502556\n",
      "Ep: 586, steps: 15, D loss: 0.234367, acc:  59%, G loss: 1.562963\n",
      "Ep: 586, steps: 16, D loss: 0.251928, acc:  53%, G loss: 1.583161\n",
      "Ep: 586, steps: 17, D loss: 0.230321, acc:  63%, G loss: 1.521364\n",
      "Ep: 586, steps: 18, D loss: 0.238869, acc:  58%, G loss: 1.655815\n",
      "Ep: 586, steps: 19, D loss: 0.213132, acc:  67%, G loss: 1.604613\n",
      "Ep: 586, steps: 20, D loss: 0.184752, acc:  78%, G loss: 1.746829\n",
      "Ep: 586, steps: 21, D loss: 0.274280, acc:  37%, G loss: 1.455220\n",
      "Ep: 586, steps: 22, D loss: 0.193432, acc:  69%, G loss: 1.651861\n",
      "Ep: 586, steps: 23, D loss: 0.232229, acc:  62%, G loss: 1.800079\n",
      "Ep: 586, steps: 24, D loss: 0.221398, acc:  63%, G loss: 1.602537\n",
      "Ep: 586, steps: 25, D loss: 0.262102, acc:  52%, G loss: 1.552334\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 587, steps: 1, D loss: 0.250413, acc:  57%, G loss: 1.727739\n",
      "Ep: 587, steps: 2, D loss: 0.229862, acc:  62%, G loss: 1.487557\n",
      "Ep: 587, steps: 3, D loss: 0.175659, acc:  80%, G loss: 1.952955\n",
      "Saved Model\n",
      "Ep: 587, steps: 4, D loss: 0.184851, acc:  81%, G loss: 1.747298\n",
      "Ep: 587, steps: 5, D loss: 0.239364, acc:  56%, G loss: 1.688333\n",
      "Ep: 587, steps: 6, D loss: 0.340726, acc:  27%, G loss: 1.467898\n",
      "Ep: 587, steps: 7, D loss: 0.218544, acc:  66%, G loss: 1.649756\n",
      "Ep: 587, steps: 8, D loss: 0.233523, acc:  63%, G loss: 1.666766\n",
      "Ep: 587, steps: 9, D loss: 0.180530, acc:  81%, G loss: 1.569178\n",
      "Ep: 587, steps: 10, D loss: 0.243285, acc:  54%, G loss: 1.753268\n",
      "Ep: 587, steps: 11, D loss: 0.291832, acc:  34%, G loss: 1.369735\n",
      "Ep: 587, steps: 12, D loss: 0.288057, acc:  35%, G loss: 1.379415\n",
      "Ep: 587, steps: 13, D loss: 0.263761, acc:  49%, G loss: 1.460090\n",
      "Ep: 587, steps: 14, D loss: 0.253901, acc:  53%, G loss: 1.533759\n",
      "Ep: 587, steps: 15, D loss: 0.239876, acc:  59%, G loss: 1.596209\n",
      "Ep: 587, steps: 16, D loss: 0.212897, acc:  68%, G loss: 1.542965\n",
      "Ep: 587, steps: 17, D loss: 0.231191, acc:  60%, G loss: 1.595976\n",
      "Ep: 587, steps: 18, D loss: 0.197295, acc:  72%, G loss: 1.580741\n",
      "Ep: 587, steps: 19, D loss: 0.178988, acc:  75%, G loss: 1.735308\n",
      "Ep: 587, steps: 20, D loss: 0.268860, acc:  42%, G loss: 1.464229\n",
      "Ep: 587, steps: 21, D loss: 0.206449, acc:  65%, G loss: 1.649641\n",
      "Ep: 587, steps: 22, D loss: 0.219362, acc:  67%, G loss: 1.841675\n",
      "Ep: 587, steps: 23, D loss: 0.204062, acc:  72%, G loss: 1.584994\n",
      "Ep: 587, steps: 24, D loss: 0.262937, acc:  52%, G loss: 1.522469\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 588, steps: 1, D loss: 0.232090, acc:  63%, G loss: 1.700633\n",
      "Ep: 588, steps: 2, D loss: 0.253109, acc:  52%, G loss: 1.466449\n",
      "Ep: 588, steps: 3, D loss: 0.166264, acc:  80%, G loss: 1.948480\n",
      "Ep: 588, steps: 4, D loss: 0.197116, acc:  78%, G loss: 1.818193\n",
      "Ep: 588, steps: 5, D loss: 0.268808, acc:  48%, G loss: 1.703420\n",
      "Ep: 588, steps: 6, D loss: 0.244518, acc:  57%, G loss: 1.639621\n",
      "Ep: 588, steps: 7, D loss: 0.324814, acc:  30%, G loss: 1.449864\n",
      "Ep: 588, steps: 8, D loss: 0.230306, acc:  64%, G loss: 1.674457\n",
      "Ep: 588, steps: 9, D loss: 0.241539, acc:  56%, G loss: 1.676132\n",
      "Ep: 588, steps: 10, D loss: 0.180677, acc:  79%, G loss: 1.526888\n",
      "Ep: 588, steps: 11, D loss: 0.257564, acc:  51%, G loss: 1.731804\n",
      "Ep: 588, steps: 12, D loss: 0.294698, acc:  34%, G loss: 1.370566\n",
      "Ep: 588, steps: 13, D loss: 0.286343, acc:  37%, G loss: 1.388774\n",
      "Ep: 588, steps: 14, D loss: 0.267910, acc:  42%, G loss: 1.470518\n",
      "Ep: 588, steps: 15, D loss: 0.246596, acc:  54%, G loss: 1.553589\n",
      "Ep: 588, steps: 16, D loss: 0.252380, acc:  53%, G loss: 1.572424\n",
      "Ep: 588, steps: 17, D loss: 0.217438, acc:  67%, G loss: 1.601903\n",
      "Ep: 588, steps: 18, D loss: 0.242510, acc:  57%, G loss: 1.606014\n",
      "Ep: 588, steps: 19, D loss: 0.212798, acc:  67%, G loss: 1.602104\n",
      "Ep: 588, steps: 20, D loss: 0.191177, acc:  73%, G loss: 1.708105\n",
      "Ep: 588, steps: 21, D loss: 0.261817, acc:  43%, G loss: 1.411356\n",
      "Ep: 588, steps: 22, D loss: 0.160951, acc:  79%, G loss: 1.590584\n",
      "Ep: 588, steps: 23, D loss: 0.236694, acc:  57%, G loss: 1.841602\n",
      "Ep: 588, steps: 24, D loss: 0.206492, acc:  73%, G loss: 1.515856\n",
      "Ep: 588, steps: 25, D loss: 0.249923, acc:  56%, G loss: 1.546299\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 589, steps: 1, D loss: 0.234277, acc:  61%, G loss: 1.739881\n",
      "Ep: 589, steps: 2, D loss: 0.229215, acc:  63%, G loss: 1.457950\n",
      "Ep: 589, steps: 3, D loss: 0.172255, acc:  79%, G loss: 1.892667\n",
      "Ep: 589, steps: 4, D loss: 0.189695, acc:  82%, G loss: 1.750776\n",
      "Ep: 589, steps: 5, D loss: 0.311650, acc:  38%, G loss: 1.696098\n",
      "Ep: 589, steps: 6, D loss: 0.225581, acc:  57%, G loss: 1.576848\n",
      "Ep: 589, steps: 7, D loss: 0.309928, acc:  32%, G loss: 1.573483\n",
      "Ep: 589, steps: 8, D loss: 0.230177, acc:  60%, G loss: 1.725074\n",
      "Ep: 589, steps: 9, D loss: 0.231514, acc:  63%, G loss: 1.651893\n",
      "Ep: 589, steps: 10, D loss: 0.182941, acc:  77%, G loss: 1.649531\n",
      "Ep: 589, steps: 11, D loss: 0.247529, acc:  53%, G loss: 1.759022\n",
      "Ep: 589, steps: 12, D loss: 0.290747, acc:  36%, G loss: 1.388933\n",
      "Ep: 589, steps: 13, D loss: 0.275762, acc:  40%, G loss: 1.415765\n",
      "Ep: 589, steps: 14, D loss: 0.273923, acc:  41%, G loss: 1.508787\n",
      "Ep: 589, steps: 15, D loss: 0.248609, acc:  52%, G loss: 1.509682\n",
      "Ep: 589, steps: 16, D loss: 0.250291, acc:  54%, G loss: 1.592415\n",
      "Ep: 589, steps: 17, D loss: 0.220246, acc:  68%, G loss: 1.493954\n",
      "Ep: 589, steps: 18, D loss: 0.243533, acc:  56%, G loss: 1.572009\n",
      "Ep: 589, steps: 19, D loss: 0.217356, acc:  64%, G loss: 1.575337\n",
      "Ep: 589, steps: 20, D loss: 0.180599, acc:  76%, G loss: 1.728565\n",
      "Ep: 589, steps: 21, D loss: 0.265248, acc:  42%, G loss: 1.445054\n",
      "Ep: 589, steps: 22, D loss: 0.213915, acc:  63%, G loss: 1.674989\n",
      "Ep: 589, steps: 23, D loss: 0.225972, acc:  64%, G loss: 1.859346\n",
      "Ep: 589, steps: 24, D loss: 0.202630, acc:  72%, G loss: 1.579798\n",
      "Ep: 589, steps: 25, D loss: 0.299989, acc:  41%, G loss: 1.639785\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 590, steps: 1, D loss: 0.245123, acc:  57%, G loss: 1.634932\n",
      "Saved Model\n",
      "Ep: 590, steps: 2, D loss: 0.250807, acc:  53%, G loss: 1.478835\n",
      "Ep: 590, steps: 3, D loss: 0.201817, acc:  77%, G loss: 1.587654\n",
      "Ep: 590, steps: 4, D loss: 0.275580, acc:  46%, G loss: 1.665424\n",
      "Ep: 590, steps: 5, D loss: 0.208749, acc:  66%, G loss: 1.554143\n",
      "Ep: 590, steps: 6, D loss: 0.284083, acc:  38%, G loss: 1.509007\n",
      "Ep: 590, steps: 7, D loss: 0.242279, acc:  58%, G loss: 1.724096\n",
      "Ep: 590, steps: 8, D loss: 0.244871, acc:  58%, G loss: 1.588864\n",
      "Ep: 590, steps: 9, D loss: 0.181397, acc:  83%, G loss: 1.608930\n",
      "Ep: 590, steps: 10, D loss: 0.251618, acc:  51%, G loss: 1.803923\n",
      "Ep: 590, steps: 11, D loss: 0.300839, acc:  30%, G loss: 1.397764\n",
      "Ep: 590, steps: 12, D loss: 0.293836, acc:  31%, G loss: 1.399250\n",
      "Ep: 590, steps: 13, D loss: 0.263141, acc:  46%, G loss: 1.508422\n",
      "Ep: 590, steps: 14, D loss: 0.241918, acc:  56%, G loss: 1.594623\n",
      "Ep: 590, steps: 15, D loss: 0.249071, acc:  53%, G loss: 1.555948\n",
      "Ep: 590, steps: 16, D loss: 0.211737, acc:  71%, G loss: 1.557724\n",
      "Ep: 590, steps: 17, D loss: 0.236455, acc:  59%, G loss: 1.617885\n",
      "Ep: 590, steps: 18, D loss: 0.214874, acc:  66%, G loss: 1.634966\n",
      "Ep: 590, steps: 19, D loss: 0.173025, acc:  80%, G loss: 1.718840\n",
      "Ep: 590, steps: 20, D loss: 0.262236, acc:  44%, G loss: 1.575964\n",
      "Ep: 590, steps: 21, D loss: 0.156935, acc:  78%, G loss: 1.560179\n",
      "Ep: 590, steps: 22, D loss: 0.230805, acc:  61%, G loss: 1.829238\n",
      "Ep: 590, steps: 23, D loss: 0.218944, acc:  66%, G loss: 1.679992\n",
      "Ep: 590, steps: 24, D loss: 0.253468, acc:  53%, G loss: 1.659470\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 591, steps: 1, D loss: 0.234569, acc:  61%, G loss: 1.746927\n",
      "Ep: 591, steps: 2, D loss: 0.233561, acc:  60%, G loss: 1.485288\n",
      "Ep: 591, steps: 3, D loss: 0.179977, acc:  78%, G loss: 1.920873\n",
      "Ep: 591, steps: 4, D loss: 0.186683, acc:  81%, G loss: 1.730294\n",
      "Ep: 591, steps: 5, D loss: 0.278444, acc:  49%, G loss: 1.630944\n",
      "Ep: 591, steps: 6, D loss: 0.221251, acc:  60%, G loss: 1.615448\n",
      "Ep: 591, steps: 7, D loss: 0.323472, acc:  28%, G loss: 1.610282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 591, steps: 8, D loss: 0.225088, acc:  64%, G loss: 1.797893\n",
      "Ep: 591, steps: 9, D loss: 0.241847, acc:  60%, G loss: 1.653170\n",
      "Ep: 591, steps: 10, D loss: 0.188535, acc:  76%, G loss: 1.560120\n",
      "Ep: 591, steps: 11, D loss: 0.272921, acc:  44%, G loss: 1.698897\n",
      "Ep: 591, steps: 12, D loss: 0.298818, acc:  35%, G loss: 1.441105\n",
      "Ep: 591, steps: 13, D loss: 0.278143, acc:  40%, G loss: 1.497340\n",
      "Ep: 591, steps: 14, D loss: 0.267299, acc:  44%, G loss: 1.464695\n",
      "Ep: 591, steps: 15, D loss: 0.244407, acc:  54%, G loss: 1.643888\n",
      "Ep: 591, steps: 16, D loss: 0.239051, acc:  59%, G loss: 1.561728\n",
      "Ep: 591, steps: 17, D loss: 0.213939, acc:  69%, G loss: 1.585796\n",
      "Ep: 591, steps: 18, D loss: 0.240351, acc:  57%, G loss: 1.642231\n",
      "Ep: 591, steps: 19, D loss: 0.207134, acc:  68%, G loss: 1.594642\n",
      "Ep: 591, steps: 20, D loss: 0.188302, acc:  73%, G loss: 1.695244\n",
      "Ep: 591, steps: 21, D loss: 0.266103, acc:  41%, G loss: 1.424758\n",
      "Ep: 591, steps: 22, D loss: 0.200749, acc:  69%, G loss: 1.747290\n",
      "Ep: 591, steps: 23, D loss: 0.228583, acc:  64%, G loss: 1.892043\n",
      "Ep: 591, steps: 24, D loss: 0.218664, acc:  65%, G loss: 1.568411\n",
      "Ep: 591, steps: 25, D loss: 0.255613, acc:  54%, G loss: 1.631941\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 592, steps: 1, D loss: 0.251468, acc:  55%, G loss: 1.714290\n",
      "Ep: 592, steps: 2, D loss: 0.241118, acc:  56%, G loss: 1.480171\n",
      "Ep: 592, steps: 3, D loss: 0.183601, acc:  76%, G loss: 1.909955\n",
      "Ep: 592, steps: 4, D loss: 0.192100, acc:  81%, G loss: 1.701192\n",
      "Ep: 592, steps: 5, D loss: 0.281122, acc:  45%, G loss: 1.664318\n",
      "Ep: 592, steps: 6, D loss: 0.223792, acc:  59%, G loss: 1.564496\n",
      "Ep: 592, steps: 7, D loss: 0.308054, acc:  32%, G loss: 1.464363\n",
      "Ep: 592, steps: 8, D loss: 0.217624, acc:  65%, G loss: 1.784249\n",
      "Ep: 592, steps: 9, D loss: 0.238059, acc:  63%, G loss: 1.624155\n",
      "Ep: 592, steps: 10, D loss: 0.189175, acc:  76%, G loss: 1.643855\n",
      "Ep: 592, steps: 11, D loss: 0.274441, acc:  45%, G loss: 1.725637\n",
      "Ep: 592, steps: 12, D loss: 0.291384, acc:  35%, G loss: 1.434985\n",
      "Ep: 592, steps: 13, D loss: 0.286638, acc:  35%, G loss: 1.448169\n",
      "Ep: 592, steps: 14, D loss: 0.276135, acc:  38%, G loss: 1.509282\n",
      "Ep: 592, steps: 15, D loss: 0.247115, acc:  54%, G loss: 1.640623\n",
      "Ep: 592, steps: 16, D loss: 0.250936, acc:  54%, G loss: 1.539103\n",
      "Ep: 592, steps: 17, D loss: 0.199499, acc:  77%, G loss: 1.512718\n",
      "Ep: 592, steps: 18, D loss: 0.240379, acc:  57%, G loss: 1.608671\n",
      "Ep: 592, steps: 19, D loss: 0.210489, acc:  68%, G loss: 1.614317\n",
      "Ep: 592, steps: 20, D loss: 0.178434, acc:  75%, G loss: 1.725852\n",
      "Ep: 592, steps: 21, D loss: 0.267505, acc:  41%, G loss: 1.440656\n",
      "Ep: 592, steps: 22, D loss: 0.195392, acc:  71%, G loss: 1.605431\n",
      "Ep: 592, steps: 23, D loss: 0.213776, acc:  67%, G loss: 1.894453\n",
      "Ep: 592, steps: 24, D loss: 0.213794, acc:  66%, G loss: 1.601645\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 592, steps: 25, D loss: 0.268258, acc:  51%, G loss: 1.634965\n",
      "Ep: 592, steps: 26, D loss: 0.243837, acc:  57%, G loss: 1.417771\n",
      "Ep: 592, steps: 27, D loss: 0.164449, acc:  82%, G loss: 1.951175\n",
      "Ep: 592, steps: 28, D loss: 0.206279, acc:  74%, G loss: 1.672933\n",
      "Ep: 592, steps: 29, D loss: 0.263767, acc:  52%, G loss: 1.679976\n",
      "Ep: 592, steps: 30, D loss: 0.235716, acc:  56%, G loss: 1.597431\n",
      "Ep: 592, steps: 31, D loss: 0.303082, acc:  34%, G loss: 1.669853\n",
      "Ep: 592, steps: 32, D loss: 0.228030, acc:  64%, G loss: 1.675683\n",
      "Ep: 592, steps: 33, D loss: 0.236382, acc:  60%, G loss: 1.614306\n",
      "Ep: 592, steps: 34, D loss: 0.191751, acc:  74%, G loss: 1.541204\n",
      "Ep: 592, steps: 35, D loss: 0.279789, acc:  42%, G loss: 1.768005\n",
      "Ep: 592, steps: 36, D loss: 0.302847, acc:  30%, G loss: 1.405219\n",
      "Ep: 592, steps: 37, D loss: 0.269202, acc:  44%, G loss: 1.431129\n",
      "Ep: 592, steps: 38, D loss: 0.271748, acc:  41%, G loss: 1.587259\n",
      "Ep: 592, steps: 39, D loss: 0.232493, acc:  60%, G loss: 1.555012\n",
      "Ep: 592, steps: 40, D loss: 0.243556, acc:  57%, G loss: 1.595087\n",
      "Ep: 592, steps: 41, D loss: 0.214657, acc:  68%, G loss: 1.543521\n",
      "Ep: 592, steps: 42, D loss: 0.239741, acc:  58%, G loss: 1.615215\n",
      "Ep: 592, steps: 43, D loss: 0.218081, acc:  67%, G loss: 1.600567\n",
      "Ep: 592, steps: 44, D loss: 0.195421, acc:  69%, G loss: 1.841848\n",
      "Ep: 592, steps: 45, D loss: 0.251407, acc:  48%, G loss: 1.429766\n",
      "Ep: 592, steps: 46, D loss: 0.182553, acc:  74%, G loss: 1.678275\n",
      "Ep: 592, steps: 47, D loss: 0.230630, acc:  62%, G loss: 1.852322\n",
      "Ep: 592, steps: 48, D loss: 0.209605, acc:  70%, G loss: 1.559937\n",
      "Ep: 592, steps: 49, D loss: 0.245662, acc:  56%, G loss: 1.660175\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 593, steps: 1, D loss: 0.269801, acc:  50%, G loss: 1.746820\n",
      "Ep: 593, steps: 2, D loss: 0.262204, acc:  50%, G loss: 1.501796\n",
      "Ep: 593, steps: 3, D loss: 0.168581, acc:  81%, G loss: 1.899794\n",
      "Ep: 593, steps: 4, D loss: 0.195831, acc:  79%, G loss: 1.728973\n",
      "Ep: 593, steps: 5, D loss: 0.272884, acc:  50%, G loss: 1.606092\n",
      "Ep: 593, steps: 6, D loss: 0.236753, acc:  57%, G loss: 1.688729\n",
      "Ep: 593, steps: 7, D loss: 0.304394, acc:  34%, G loss: 1.464275\n",
      "Ep: 593, steps: 8, D loss: 0.239316, acc:  61%, G loss: 1.756065\n",
      "Ep: 593, steps: 9, D loss: 0.227402, acc:  65%, G loss: 1.650691\n",
      "Ep: 593, steps: 10, D loss: 0.177216, acc:  82%, G loss: 1.613528\n",
      "Ep: 593, steps: 11, D loss: 0.254354, acc:  52%, G loss: 1.725880\n",
      "Ep: 593, steps: 12, D loss: 0.296970, acc:  35%, G loss: 1.383388\n",
      "Ep: 593, steps: 13, D loss: 0.285788, acc:  36%, G loss: 1.391127\n",
      "Ep: 593, steps: 14, D loss: 0.279105, acc:  39%, G loss: 1.530477\n",
      "Ep: 593, steps: 15, D loss: 0.237698, acc:  58%, G loss: 1.630038\n",
      "Ep: 593, steps: 16, D loss: 0.241726, acc:  58%, G loss: 1.605148\n",
      "Ep: 593, steps: 17, D loss: 0.211353, acc:  72%, G loss: 1.464072\n",
      "Ep: 593, steps: 18, D loss: 0.238055, acc:  59%, G loss: 1.602062\n",
      "Ep: 593, steps: 19, D loss: 0.217462, acc:  66%, G loss: 1.591164\n",
      "Ep: 593, steps: 20, D loss: 0.185938, acc:  78%, G loss: 1.733902\n",
      "Ep: 593, steps: 21, D loss: 0.266724, acc:  41%, G loss: 1.467722\n",
      "Ep: 593, steps: 22, D loss: 0.183107, acc:  72%, G loss: 1.630820\n",
      "Ep: 593, steps: 23, D loss: 0.220838, acc:  66%, G loss: 1.806822\n",
      "Ep: 593, steps: 24, D loss: 0.210050, acc:  70%, G loss: 1.638135\n",
      "Ep: 593, steps: 25, D loss: 0.270038, acc:  50%, G loss: 1.582864\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 594, steps: 1, D loss: 0.242514, acc:  57%, G loss: 1.673262\n",
      "Ep: 594, steps: 2, D loss: 0.258533, acc:  49%, G loss: 1.444764\n",
      "Ep: 594, steps: 3, D loss: 0.160031, acc:  83%, G loss: 1.876181\n",
      "Ep: 594, steps: 4, D loss: 0.202692, acc:  77%, G loss: 1.672989\n",
      "Ep: 594, steps: 5, D loss: 0.269164, acc:  48%, G loss: 1.729360\n",
      "Ep: 594, steps: 6, D loss: 0.231179, acc:  57%, G loss: 1.653159\n",
      "Ep: 594, steps: 7, D loss: 0.296441, acc:  35%, G loss: 1.553185\n",
      "Ep: 594, steps: 8, D loss: 0.227046, acc:  62%, G loss: 1.772024\n",
      "Ep: 594, steps: 9, D loss: 0.252507, acc:  56%, G loss: 1.605689\n",
      "Ep: 594, steps: 10, D loss: 0.198205, acc:  74%, G loss: 1.600502\n",
      "Ep: 594, steps: 11, D loss: 0.276320, acc:  46%, G loss: 1.782276\n",
      "Ep: 594, steps: 12, D loss: 0.293524, acc:  35%, G loss: 1.321790\n",
      "Ep: 594, steps: 13, D loss: 0.285110, acc:  33%, G loss: 1.405399\n",
      "Ep: 594, steps: 14, D loss: 0.270667, acc:  41%, G loss: 1.554664\n",
      "Ep: 594, steps: 15, D loss: 0.236552, acc:  59%, G loss: 1.617312\n",
      "Ep: 594, steps: 16, D loss: 0.235641, acc:  61%, G loss: 1.610421\n",
      "Ep: 594, steps: 17, D loss: 0.208632, acc:  72%, G loss: 1.520901\n",
      "Ep: 594, steps: 18, D loss: 0.220439, acc:  68%, G loss: 1.650527\n",
      "Ep: 594, steps: 19, D loss: 0.221670, acc:  65%, G loss: 1.625963\n",
      "Ep: 594, steps: 20, D loss: 0.181031, acc:  77%, G loss: 1.753769\n",
      "Ep: 594, steps: 21, D loss: 0.265960, acc:  44%, G loss: 1.476076\n",
      "Ep: 594, steps: 22, D loss: 0.196164, acc:  70%, G loss: 1.751952\n",
      "Saved Model\n",
      "Ep: 594, steps: 23, D loss: 0.225058, acc:  63%, G loss: 1.794631\n",
      "Ep: 594, steps: 24, D loss: 0.278337, acc:  47%, G loss: 1.837211\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 595, steps: 1, D loss: 0.220657, acc:  67%, G loss: 1.641421\n",
      "Ep: 595, steps: 2, D loss: 0.226439, acc:  64%, G loss: 1.450429\n",
      "Ep: 595, steps: 3, D loss: 0.169298, acc:  79%, G loss: 1.852056\n",
      "Ep: 595, steps: 4, D loss: 0.187055, acc:  82%, G loss: 1.725115\n",
      "Ep: 595, steps: 5, D loss: 0.286719, acc:  44%, G loss: 1.621461\n",
      "Ep: 595, steps: 6, D loss: 0.240379, acc:  57%, G loss: 1.574420\n",
      "Ep: 595, steps: 7, D loss: 0.298861, acc:  33%, G loss: 1.557849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 595, steps: 8, D loss: 0.243448, acc:  59%, G loss: 1.728348\n",
      "Ep: 595, steps: 9, D loss: 0.223847, acc:  65%, G loss: 1.570544\n",
      "Ep: 595, steps: 10, D loss: 0.172249, acc:  81%, G loss: 1.573283\n",
      "Ep: 595, steps: 11, D loss: 0.229530, acc:  61%, G loss: 1.799557\n",
      "Ep: 595, steps: 12, D loss: 0.317814, acc:  32%, G loss: 1.381299\n",
      "Ep: 595, steps: 13, D loss: 0.290924, acc:  36%, G loss: 1.447582\n",
      "Ep: 595, steps: 14, D loss: 0.272885, acc:  42%, G loss: 1.565185\n",
      "Ep: 595, steps: 15, D loss: 0.259267, acc:  49%, G loss: 1.578904\n",
      "Ep: 595, steps: 16, D loss: 0.255299, acc:  49%, G loss: 1.553622\n",
      "Ep: 595, steps: 17, D loss: 0.203968, acc:  71%, G loss: 1.607704\n",
      "Ep: 595, steps: 18, D loss: 0.242582, acc:  57%, G loss: 1.689318\n",
      "Ep: 595, steps: 19, D loss: 0.208764, acc:  68%, G loss: 1.644193\n",
      "Ep: 595, steps: 20, D loss: 0.181758, acc:  76%, G loss: 1.739862\n",
      "Ep: 595, steps: 21, D loss: 0.259072, acc:  46%, G loss: 1.458368\n",
      "Ep: 595, steps: 22, D loss: 0.193759, acc:  68%, G loss: 1.631994\n",
      "Ep: 595, steps: 23, D loss: 0.248556, acc:  54%, G loss: 1.857294\n",
      "Ep: 595, steps: 24, D loss: 0.216024, acc:  65%, G loss: 1.591997\n",
      "Ep: 595, steps: 25, D loss: 0.237744, acc:  59%, G loss: 1.518739\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 596, steps: 1, D loss: 0.246793, acc:  56%, G loss: 1.666700\n",
      "Ep: 596, steps: 2, D loss: 0.257774, acc:  51%, G loss: 1.463554\n",
      "Ep: 596, steps: 3, D loss: 0.184917, acc:  75%, G loss: 1.876479\n",
      "Ep: 596, steps: 4, D loss: 0.186413, acc:  82%, G loss: 1.719716\n",
      "Ep: 596, steps: 5, D loss: 0.290039, acc:  44%, G loss: 1.667313\n",
      "Ep: 596, steps: 6, D loss: 0.239633, acc:  58%, G loss: 1.634334\n",
      "Ep: 596, steps: 7, D loss: 0.301545, acc:  34%, G loss: 1.447496\n",
      "Ep: 596, steps: 8, D loss: 0.223090, acc:  65%, G loss: 1.726299\n",
      "Ep: 596, steps: 9, D loss: 0.252521, acc:  54%, G loss: 1.599040\n",
      "Ep: 596, steps: 10, D loss: 0.184653, acc:  76%, G loss: 1.628165\n",
      "Ep: 596, steps: 11, D loss: 0.272410, acc:  45%, G loss: 1.695701\n",
      "Ep: 596, steps: 12, D loss: 0.297686, acc:  34%, G loss: 1.373661\n",
      "Ep: 596, steps: 13, D loss: 0.276183, acc:  41%, G loss: 1.409092\n",
      "Ep: 596, steps: 14, D loss: 0.265846, acc:  44%, G loss: 1.512846\n",
      "Ep: 596, steps: 15, D loss: 0.249864, acc:  50%, G loss: 1.620498\n",
      "Ep: 596, steps: 16, D loss: 0.250573, acc:  54%, G loss: 1.617691\n",
      "Ep: 596, steps: 17, D loss: 0.211461, acc:  74%, G loss: 1.526532\n",
      "Ep: 596, steps: 18, D loss: 0.226951, acc:  63%, G loss: 1.618059\n",
      "Ep: 596, steps: 19, D loss: 0.208756, acc:  69%, G loss: 1.600454\n",
      "Ep: 596, steps: 20, D loss: 0.180195, acc:  77%, G loss: 1.765062\n",
      "Ep: 596, steps: 21, D loss: 0.272333, acc:  38%, G loss: 1.423157\n",
      "Ep: 596, steps: 22, D loss: 0.204335, acc:  69%, G loss: 1.586527\n",
      "Ep: 596, steps: 23, D loss: 0.226189, acc:  63%, G loss: 1.855176\n",
      "Ep: 596, steps: 24, D loss: 0.208231, acc:  69%, G loss: 1.557171\n",
      "Ep: 596, steps: 25, D loss: 0.254733, acc:  54%, G loss: 1.535721\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 597, steps: 1, D loss: 0.241812, acc:  59%, G loss: 1.706686\n",
      "Ep: 597, steps: 2, D loss: 0.247843, acc:  54%, G loss: 1.443654\n",
      "Ep: 597, steps: 3, D loss: 0.178256, acc:  79%, G loss: 1.891143\n",
      "Ep: 597, steps: 4, D loss: 0.185800, acc:  82%, G loss: 1.708529\n",
      "Ep: 597, steps: 5, D loss: 0.287312, acc:  44%, G loss: 1.613093\n",
      "Ep: 597, steps: 6, D loss: 0.229459, acc:  57%, G loss: 1.535347\n",
      "Ep: 597, steps: 7, D loss: 0.305313, acc:  32%, G loss: 1.511800\n",
      "Ep: 597, steps: 8, D loss: 0.233035, acc:  61%, G loss: 1.948920\n",
      "Ep: 597, steps: 9, D loss: 0.249943, acc:  57%, G loss: 1.659672\n",
      "Ep: 597, steps: 10, D loss: 0.191897, acc:  77%, G loss: 1.533877\n",
      "Ep: 597, steps: 11, D loss: 0.266775, acc:  45%, G loss: 1.724175\n",
      "Ep: 597, steps: 12, D loss: 0.281623, acc:  38%, G loss: 1.387604\n",
      "Ep: 597, steps: 13, D loss: 0.286848, acc:  35%, G loss: 1.407130\n",
      "Ep: 597, steps: 14, D loss: 0.270053, acc:  42%, G loss: 1.551882\n",
      "Ep: 597, steps: 15, D loss: 0.236153, acc:  59%, G loss: 1.581424\n",
      "Ep: 597, steps: 16, D loss: 0.235075, acc:  61%, G loss: 1.696403\n",
      "Ep: 597, steps: 17, D loss: 0.191285, acc:  77%, G loss: 1.539555\n",
      "Ep: 597, steps: 18, D loss: 0.246801, acc:  55%, G loss: 1.579961\n",
      "Ep: 597, steps: 19, D loss: 0.233479, acc:  61%, G loss: 1.638849\n",
      "Ep: 597, steps: 20, D loss: 0.183004, acc:  77%, G loss: 1.663901\n",
      "Saved Model\n",
      "Ep: 597, steps: 21, D loss: 0.267767, acc:  38%, G loss: 1.418554\n",
      "Ep: 597, steps: 22, D loss: 0.222306, acc:  63%, G loss: 1.752984\n",
      "Ep: 597, steps: 23, D loss: 0.218786, acc:  66%, G loss: 1.479111\n",
      "Ep: 597, steps: 24, D loss: 0.252380, acc:  55%, G loss: 1.669694\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 598, steps: 1, D loss: 0.230459, acc:  61%, G loss: 1.824321\n",
      "Ep: 598, steps: 2, D loss: 0.253511, acc:  51%, G loss: 1.564648\n",
      "Ep: 598, steps: 3, D loss: 0.176110, acc:  77%, G loss: 1.876192\n",
      "Ep: 598, steps: 4, D loss: 0.186461, acc:  83%, G loss: 1.724836\n",
      "Ep: 598, steps: 5, D loss: 0.307352, acc:  37%, G loss: 1.665057\n",
      "Ep: 598, steps: 6, D loss: 0.220470, acc:  59%, G loss: 1.699523\n",
      "Ep: 598, steps: 7, D loss: 0.311926, acc:  29%, G loss: 1.633943\n",
      "Ep: 598, steps: 8, D loss: 0.224100, acc:  64%, G loss: 1.753967\n",
      "Ep: 598, steps: 9, D loss: 0.241245, acc:  61%, G loss: 1.606320\n",
      "Ep: 598, steps: 10, D loss: 0.198180, acc:  72%, G loss: 1.589297\n",
      "Ep: 598, steps: 11, D loss: 0.247197, acc:  54%, G loss: 1.740791\n",
      "Ep: 598, steps: 12, D loss: 0.273302, acc:  42%, G loss: 1.397572\n",
      "Ep: 598, steps: 13, D loss: 0.269730, acc:  43%, G loss: 1.438148\n",
      "Ep: 598, steps: 14, D loss: 0.270934, acc:  42%, G loss: 1.532248\n",
      "Ep: 598, steps: 15, D loss: 0.250534, acc:  53%, G loss: 1.655614\n",
      "Ep: 598, steps: 16, D loss: 0.237638, acc:  59%, G loss: 1.622861\n",
      "Ep: 598, steps: 17, D loss: 0.215256, acc:  69%, G loss: 1.523911\n",
      "Ep: 598, steps: 18, D loss: 0.232809, acc:  60%, G loss: 1.633373\n",
      "Ep: 598, steps: 19, D loss: 0.207370, acc:  70%, G loss: 1.628302\n",
      "Ep: 598, steps: 20, D loss: 0.176528, acc:  78%, G loss: 1.809792\n",
      "Ep: 598, steps: 21, D loss: 0.267136, acc:  42%, G loss: 1.508025\n",
      "Ep: 598, steps: 22, D loss: 0.207138, acc:  67%, G loss: 1.725073\n",
      "Ep: 598, steps: 23, D loss: 0.224216, acc:  65%, G loss: 1.871910\n",
      "Ep: 598, steps: 24, D loss: 0.213841, acc:  66%, G loss: 1.558143\n",
      "Ep: 598, steps: 25, D loss: 0.251190, acc:  56%, G loss: 1.625122\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 599, steps: 1, D loss: 0.258643, acc:  53%, G loss: 1.795863\n",
      "Ep: 599, steps: 2, D loss: 0.242230, acc:  57%, G loss: 1.442807\n",
      "Ep: 599, steps: 3, D loss: 0.167345, acc:  81%, G loss: 1.880633\n",
      "Ep: 599, steps: 4, D loss: 0.192793, acc:  79%, G loss: 1.679108\n",
      "Ep: 599, steps: 5, D loss: 0.267553, acc:  50%, G loss: 1.616817\n",
      "Ep: 599, steps: 6, D loss: 0.219251, acc:  59%, G loss: 1.593248\n",
      "Ep: 599, steps: 7, D loss: 0.304368, acc:  34%, G loss: 1.563252\n",
      "Ep: 599, steps: 8, D loss: 0.215389, acc:  65%, G loss: 1.773136\n",
      "Ep: 599, steps: 9, D loss: 0.260656, acc:  50%, G loss: 1.670573\n",
      "Ep: 599, steps: 10, D loss: 0.180764, acc:  76%, G loss: 1.584332\n",
      "Ep: 599, steps: 11, D loss: 0.271556, acc:  45%, G loss: 1.719789\n",
      "Ep: 599, steps: 12, D loss: 0.295140, acc:  32%, G loss: 1.365706\n",
      "Ep: 599, steps: 13, D loss: 0.281079, acc:  37%, G loss: 1.466958\n",
      "Ep: 599, steps: 14, D loss: 0.264141, acc:  44%, G loss: 1.511755\n",
      "Ep: 599, steps: 15, D loss: 0.235084, acc:  59%, G loss: 1.536326\n",
      "Ep: 599, steps: 16, D loss: 0.251905, acc:  54%, G loss: 1.575381\n",
      "Ep: 599, steps: 17, D loss: 0.216393, acc:  68%, G loss: 1.503165\n",
      "Ep: 599, steps: 18, D loss: 0.230133, acc:  63%, G loss: 1.636935\n",
      "Ep: 599, steps: 19, D loss: 0.207916, acc:  68%, G loss: 1.594473\n",
      "Ep: 599, steps: 20, D loss: 0.183821, acc:  75%, G loss: 1.769198\n",
      "Ep: 599, steps: 21, D loss: 0.258135, acc:  46%, G loss: 1.471061\n",
      "Ep: 599, steps: 22, D loss: 0.148490, acc:  83%, G loss: 1.543898\n",
      "Ep: 599, steps: 23, D loss: 0.231464, acc:  59%, G loss: 1.838602\n",
      "Ep: 599, steps: 24, D loss: 0.224724, acc:  63%, G loss: 1.513618\n",
      "Ep: 599, steps: 25, D loss: 0.236356, acc:  59%, G loss: 1.607517\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 600, steps: 1, D loss: 0.231495, acc:  63%, G loss: 1.769678\n",
      "Ep: 600, steps: 2, D loss: 0.250047, acc:  52%, G loss: 1.472354\n",
      "Ep: 600, steps: 3, D loss: 0.187435, acc:  76%, G loss: 1.947058\n",
      "Ep: 600, steps: 4, D loss: 0.185555, acc:  82%, G loss: 1.724801\n",
      "Ep: 600, steps: 5, D loss: 0.304846, acc:  40%, G loss: 1.601969\n",
      "Ep: 600, steps: 6, D loss: 0.221410, acc:  59%, G loss: 1.638836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 600, steps: 7, D loss: 0.321514, acc:  28%, G loss: 1.549440\n",
      "Ep: 600, steps: 8, D loss: 0.226231, acc:  64%, G loss: 1.720985\n",
      "Ep: 600, steps: 9, D loss: 0.235524, acc:  60%, G loss: 1.563256\n",
      "Ep: 600, steps: 10, D loss: 0.191876, acc:  77%, G loss: 1.548254\n",
      "Ep: 600, steps: 11, D loss: 0.271552, acc:  44%, G loss: 1.707410\n",
      "Ep: 600, steps: 12, D loss: 0.290195, acc:  33%, G loss: 1.381626\n",
      "Ep: 600, steps: 13, D loss: 0.280884, acc:  38%, G loss: 1.444331\n",
      "Ep: 600, steps: 14, D loss: 0.274711, acc:  39%, G loss: 1.488113\n",
      "Ep: 600, steps: 15, D loss: 0.258543, acc:  50%, G loss: 1.617708\n",
      "Ep: 600, steps: 16, D loss: 0.246787, acc:  55%, G loss: 1.565822\n",
      "Ep: 600, steps: 17, D loss: 0.220334, acc:  68%, G loss: 1.497535\n",
      "Ep: 600, steps: 18, D loss: 0.235768, acc:  60%, G loss: 1.647580\n",
      "Saved Model\n",
      "Ep: 600, steps: 19, D loss: 0.216280, acc:  65%, G loss: 1.606672\n",
      "Ep: 600, steps: 20, D loss: 0.271256, acc:  38%, G loss: 1.502893\n",
      "Ep: 600, steps: 21, D loss: 0.217232, acc:  66%, G loss: 1.535752\n",
      "Ep: 600, steps: 22, D loss: 0.220635, acc:  66%, G loss: 1.857892\n",
      "Ep: 600, steps: 23, D loss: 0.219574, acc:  64%, G loss: 1.512066\n",
      "Ep: 600, steps: 24, D loss: 0.263792, acc:  51%, G loss: 1.755784\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 601, steps: 1, D loss: 0.234350, acc:  59%, G loss: 1.667821\n",
      "Ep: 601, steps: 2, D loss: 0.233524, acc:  61%, G loss: 1.450501\n",
      "Ep: 601, steps: 3, D loss: 0.179203, acc:  77%, G loss: 1.879271\n",
      "Ep: 601, steps: 4, D loss: 0.196041, acc:  79%, G loss: 1.658255\n",
      "Ep: 601, steps: 5, D loss: 0.291260, acc:  45%, G loss: 1.718183\n",
      "Ep: 601, steps: 6, D loss: 0.235462, acc:  57%, G loss: 1.602197\n",
      "Ep: 601, steps: 7, D loss: 0.300823, acc:  35%, G loss: 1.474157\n",
      "Ep: 601, steps: 8, D loss: 0.215618, acc:  66%, G loss: 1.765255\n",
      "Ep: 601, steps: 9, D loss: 0.230124, acc:  63%, G loss: 1.663784\n",
      "Ep: 601, steps: 10, D loss: 0.181388, acc:  79%, G loss: 1.541098\n",
      "Ep: 601, steps: 11, D loss: 0.259276, acc:  50%, G loss: 1.806011\n",
      "Ep: 601, steps: 12, D loss: 0.280547, acc:  39%, G loss: 1.448424\n",
      "Ep: 601, steps: 13, D loss: 0.279661, acc:  38%, G loss: 1.432799\n",
      "Ep: 601, steps: 14, D loss: 0.273659, acc:  41%, G loss: 1.490366\n",
      "Ep: 601, steps: 15, D loss: 0.254438, acc:  51%, G loss: 1.572970\n",
      "Ep: 601, steps: 16, D loss: 0.236229, acc:  62%, G loss: 1.546279\n",
      "Ep: 601, steps: 17, D loss: 0.227921, acc:  64%, G loss: 1.566997\n",
      "Ep: 601, steps: 18, D loss: 0.236303, acc:  59%, G loss: 1.667919\n",
      "Ep: 601, steps: 19, D loss: 0.215841, acc:  65%, G loss: 1.565847\n",
      "Ep: 601, steps: 20, D loss: 0.189573, acc:  74%, G loss: 1.740117\n",
      "Ep: 601, steps: 21, D loss: 0.266201, acc:  41%, G loss: 1.537194\n",
      "Ep: 601, steps: 22, D loss: 0.188500, acc:  70%, G loss: 1.704570\n",
      "Ep: 601, steps: 23, D loss: 0.223500, acc:  65%, G loss: 1.858023\n",
      "Ep: 601, steps: 24, D loss: 0.215025, acc:  67%, G loss: 1.547418\n",
      "Ep: 601, steps: 25, D loss: 0.249815, acc:  57%, G loss: 1.720270\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 602, steps: 1, D loss: 0.244362, acc:  56%, G loss: 1.734969\n",
      "Ep: 602, steps: 2, D loss: 0.242881, acc:  58%, G loss: 1.519841\n",
      "Ep: 602, steps: 3, D loss: 0.177951, acc:  78%, G loss: 1.870283\n",
      "Ep: 602, steps: 4, D loss: 0.191112, acc:  80%, G loss: 1.675668\n",
      "Ep: 602, steps: 5, D loss: 0.285990, acc:  45%, G loss: 1.678777\n",
      "Ep: 602, steps: 6, D loss: 0.242538, acc:  56%, G loss: 1.623124\n",
      "Ep: 602, steps: 7, D loss: 0.304584, acc:  33%, G loss: 1.585076\n",
      "Ep: 602, steps: 8, D loss: 0.229837, acc:  62%, G loss: 1.693437\n",
      "Ep: 602, steps: 9, D loss: 0.244701, acc:  58%, G loss: 1.673005\n",
      "Ep: 602, steps: 10, D loss: 0.198250, acc:  74%, G loss: 1.537188\n",
      "Ep: 602, steps: 11, D loss: 0.256115, acc:  48%, G loss: 1.814897\n",
      "Ep: 602, steps: 12, D loss: 0.293468, acc:  35%, G loss: 1.414927\n",
      "Ep: 602, steps: 13, D loss: 0.280695, acc:  37%, G loss: 1.459460\n",
      "Ep: 602, steps: 14, D loss: 0.267441, acc:  44%, G loss: 1.545403\n",
      "Ep: 602, steps: 15, D loss: 0.247834, acc:  54%, G loss: 1.589550\n",
      "Ep: 602, steps: 16, D loss: 0.261723, acc:  48%, G loss: 1.566747\n",
      "Ep: 602, steps: 17, D loss: 0.209614, acc:  71%, G loss: 1.523720\n",
      "Ep: 602, steps: 18, D loss: 0.245350, acc:  55%, G loss: 1.602129\n",
      "Ep: 602, steps: 19, D loss: 0.214061, acc:  68%, G loss: 1.565407\n",
      "Ep: 602, steps: 20, D loss: 0.185846, acc:  76%, G loss: 1.735786\n",
      "Ep: 602, steps: 21, D loss: 0.261937, acc:  43%, G loss: 1.537897\n",
      "Ep: 602, steps: 22, D loss: 0.205776, acc:  68%, G loss: 1.677401\n",
      "Ep: 602, steps: 23, D loss: 0.223797, acc:  64%, G loss: 1.848412\n",
      "Ep: 602, steps: 24, D loss: 0.209686, acc:  69%, G loss: 1.576711\n",
      "Ep: 602, steps: 25, D loss: 0.257485, acc:  54%, G loss: 1.578682\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 603, steps: 1, D loss: 0.243734, acc:  57%, G loss: 1.702486\n",
      "Ep: 603, steps: 2, D loss: 0.247236, acc:  54%, G loss: 1.499234\n",
      "Ep: 603, steps: 3, D loss: 0.168962, acc:  80%, G loss: 1.843222\n",
      "Ep: 603, steps: 4, D loss: 0.191888, acc:  81%, G loss: 1.683302\n",
      "Ep: 603, steps: 5, D loss: 0.282771, acc:  45%, G loss: 1.581775\n",
      "Ep: 603, steps: 6, D loss: 0.245110, acc:  55%, G loss: 1.569719\n",
      "Ep: 603, steps: 7, D loss: 0.313383, acc:  29%, G loss: 1.562508\n",
      "Ep: 603, steps: 8, D loss: 0.227092, acc:  61%, G loss: 1.675868\n",
      "Ep: 603, steps: 9, D loss: 0.229041, acc:  64%, G loss: 1.629555\n",
      "Ep: 603, steps: 10, D loss: 0.190533, acc:  78%, G loss: 1.513140\n",
      "Ep: 603, steps: 11, D loss: 0.267097, acc:  49%, G loss: 1.788439\n",
      "Ep: 603, steps: 12, D loss: 0.275492, acc:  41%, G loss: 1.420829\n",
      "Ep: 603, steps: 13, D loss: 0.281683, acc:  39%, G loss: 1.491389\n",
      "Ep: 603, steps: 14, D loss: 0.270635, acc:  42%, G loss: 1.517056\n",
      "Ep: 603, steps: 15, D loss: 0.258521, acc:  48%, G loss: 1.536837\n",
      "Ep: 603, steps: 16, D loss: 0.247449, acc:  54%, G loss: 1.590343\n",
      "Saved Model\n",
      "Ep: 603, steps: 17, D loss: 0.224091, acc:  65%, G loss: 1.497526\n",
      "Ep: 603, steps: 18, D loss: 0.205722, acc:  71%, G loss: 1.561230\n",
      "Ep: 603, steps: 19, D loss: 0.193722, acc:  71%, G loss: 1.687921\n",
      "Ep: 603, steps: 20, D loss: 0.266276, acc:  41%, G loss: 1.448781\n",
      "Ep: 603, steps: 21, D loss: 0.192236, acc:  68%, G loss: 1.616785\n",
      "Ep: 603, steps: 22, D loss: 0.214218, acc:  68%, G loss: 1.848044\n",
      "Ep: 603, steps: 23, D loss: 0.206277, acc:  71%, G loss: 1.620082\n",
      "Ep: 603, steps: 24, D loss: 0.240882, acc:  57%, G loss: 1.630210\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 604, steps: 1, D loss: 0.262766, acc:  50%, G loss: 1.708222\n",
      "Ep: 604, steps: 2, D loss: 0.259167, acc:  50%, G loss: 1.543522\n",
      "Ep: 604, steps: 3, D loss: 0.167175, acc:  83%, G loss: 1.905526\n",
      "Ep: 604, steps: 4, D loss: 0.193813, acc:  79%, G loss: 1.668128\n",
      "Ep: 604, steps: 5, D loss: 0.293874, acc:  42%, G loss: 1.661951\n",
      "Ep: 604, steps: 6, D loss: 0.237699, acc:  56%, G loss: 1.643981\n",
      "Ep: 604, steps: 7, D loss: 0.308155, acc:  32%, G loss: 1.514241\n",
      "Ep: 604, steps: 8, D loss: 0.216393, acc:  66%, G loss: 1.761230\n",
      "Ep: 604, steps: 9, D loss: 0.247185, acc:  56%, G loss: 1.605906\n",
      "Ep: 604, steps: 10, D loss: 0.188447, acc:  75%, G loss: 1.627865\n",
      "Ep: 604, steps: 11, D loss: 0.257730, acc:  49%, G loss: 1.817354\n",
      "Ep: 604, steps: 12, D loss: 0.290923, acc:  34%, G loss: 1.425219\n",
      "Ep: 604, steps: 13, D loss: 0.272989, acc:  43%, G loss: 1.434634\n",
      "Ep: 604, steps: 14, D loss: 0.266488, acc:  44%, G loss: 1.486208\n",
      "Ep: 604, steps: 15, D loss: 0.255528, acc:  49%, G loss: 1.596157\n",
      "Ep: 604, steps: 16, D loss: 0.248139, acc:  54%, G loss: 1.594574\n",
      "Ep: 604, steps: 17, D loss: 0.214706, acc:  69%, G loss: 1.516615\n",
      "Ep: 604, steps: 18, D loss: 0.245451, acc:  57%, G loss: 1.617623\n",
      "Ep: 604, steps: 19, D loss: 0.214962, acc:  67%, G loss: 1.601908\n",
      "Ep: 604, steps: 20, D loss: 0.186587, acc:  76%, G loss: 1.724295\n",
      "Ep: 604, steps: 21, D loss: 0.253740, acc:  47%, G loss: 1.446527\n",
      "Ep: 604, steps: 22, D loss: 0.218338, acc:  66%, G loss: 1.616828\n",
      "Ep: 604, steps: 23, D loss: 0.243332, acc:  57%, G loss: 1.813504\n",
      "Ep: 604, steps: 24, D loss: 0.210378, acc:  68%, G loss: 1.541180\n",
      "Ep: 604, steps: 25, D loss: 0.264593, acc:  50%, G loss: 1.523621\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 605, steps: 1, D loss: 0.228367, acc:  64%, G loss: 1.671964\n",
      "Ep: 605, steps: 2, D loss: 0.236547, acc:  59%, G loss: 1.526937\n",
      "Ep: 605, steps: 3, D loss: 0.181805, acc:  78%, G loss: 1.845404\n",
      "Ep: 605, steps: 4, D loss: 0.197401, acc:  80%, G loss: 1.699746\n",
      "Ep: 605, steps: 5, D loss: 0.272228, acc:  51%, G loss: 1.634178\n",
      "Ep: 605, steps: 6, D loss: 0.235108, acc:  55%, G loss: 1.633019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 605, steps: 7, D loss: 0.306875, acc:  30%, G loss: 1.504065\n",
      "Ep: 605, steps: 8, D loss: 0.222191, acc:  64%, G loss: 1.710296\n",
      "Ep: 605, steps: 9, D loss: 0.239840, acc:  59%, G loss: 1.634111\n",
      "Ep: 605, steps: 10, D loss: 0.205698, acc:  69%, G loss: 1.545738\n",
      "Ep: 605, steps: 11, D loss: 0.253166, acc:  52%, G loss: 1.765807\n",
      "Ep: 605, steps: 12, D loss: 0.285672, acc:  37%, G loss: 1.437864\n",
      "Ep: 605, steps: 13, D loss: 0.283472, acc:  36%, G loss: 1.432257\n",
      "Ep: 605, steps: 14, D loss: 0.272649, acc:  43%, G loss: 1.496400\n",
      "Ep: 605, steps: 15, D loss: 0.250634, acc:  53%, G loss: 1.561303\n",
      "Ep: 605, steps: 16, D loss: 0.237514, acc:  59%, G loss: 1.641738\n",
      "Ep: 605, steps: 17, D loss: 0.217806, acc:  68%, G loss: 1.509993\n",
      "Ep: 605, steps: 18, D loss: 0.239518, acc:  60%, G loss: 1.673361\n",
      "Ep: 605, steps: 19, D loss: 0.216252, acc:  67%, G loss: 1.599430\n",
      "Ep: 605, steps: 20, D loss: 0.174199, acc:  78%, G loss: 1.803244\n",
      "Ep: 605, steps: 21, D loss: 0.253091, acc:  47%, G loss: 1.489755\n",
      "Ep: 605, steps: 22, D loss: 0.208185, acc:  67%, G loss: 1.680583\n",
      "Ep: 605, steps: 23, D loss: 0.228872, acc:  61%, G loss: 1.809110\n",
      "Ep: 605, steps: 24, D loss: 0.206006, acc:  70%, G loss: 1.554495\n",
      "Ep: 605, steps: 25, D loss: 0.270567, acc:  49%, G loss: 1.661986\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 606, steps: 1, D loss: 0.232198, acc:  62%, G loss: 1.671997\n",
      "Ep: 606, steps: 2, D loss: 0.254852, acc:  52%, G loss: 1.528948\n",
      "Ep: 606, steps: 3, D loss: 0.173577, acc:  80%, G loss: 1.868044\n",
      "Ep: 606, steps: 4, D loss: 0.187328, acc:  83%, G loss: 1.688896\n",
      "Ep: 606, steps: 5, D loss: 0.286410, acc:  45%, G loss: 1.631729\n",
      "Ep: 606, steps: 6, D loss: 0.230617, acc:  57%, G loss: 1.623890\n",
      "Ep: 606, steps: 7, D loss: 0.317603, acc:  30%, G loss: 1.501712\n",
      "Ep: 606, steps: 8, D loss: 0.233523, acc:  62%, G loss: 1.692418\n",
      "Ep: 606, steps: 9, D loss: 0.244215, acc:  58%, G loss: 1.559861\n",
      "Ep: 606, steps: 10, D loss: 0.194228, acc:  73%, G loss: 1.558447\n",
      "Ep: 606, steps: 11, D loss: 0.264277, acc:  46%, G loss: 1.706148\n",
      "Ep: 606, steps: 12, D loss: 0.287333, acc:  37%, G loss: 1.374475\n",
      "Ep: 606, steps: 13, D loss: 0.275572, acc:  38%, G loss: 1.394839\n",
      "Ep: 606, steps: 14, D loss: 0.273998, acc:  38%, G loss: 1.474670\n",
      "Saved Model\n",
      "Ep: 606, steps: 15, D loss: 0.258654, acc:  51%, G loss: 1.546112\n",
      "Ep: 606, steps: 16, D loss: 0.215813, acc:  69%, G loss: 1.511982\n",
      "Ep: 606, steps: 17, D loss: 0.234126, acc:  59%, G loss: 1.633269\n",
      "Ep: 606, steps: 18, D loss: 0.204716, acc:  70%, G loss: 1.538521\n",
      "Ep: 606, steps: 19, D loss: 0.193444, acc:  71%, G loss: 1.710098\n",
      "Ep: 606, steps: 20, D loss: 0.255978, acc:  45%, G loss: 1.464035\n",
      "Ep: 606, steps: 21, D loss: 0.192443, acc:  71%, G loss: 1.612610\n",
      "Ep: 606, steps: 22, D loss: 0.221233, acc:  66%, G loss: 1.774892\n",
      "Ep: 606, steps: 23, D loss: 0.211672, acc:  68%, G loss: 1.482481\n",
      "Ep: 606, steps: 24, D loss: 0.261313, acc:  50%, G loss: 1.669279\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 607, steps: 1, D loss: 0.240117, acc:  59%, G loss: 1.729964\n",
      "Ep: 607, steps: 2, D loss: 0.245237, acc:  55%, G loss: 1.558894\n",
      "Ep: 607, steps: 3, D loss: 0.183058, acc:  78%, G loss: 1.901084\n",
      "Ep: 607, steps: 4, D loss: 0.209104, acc:  73%, G loss: 1.782208\n",
      "Ep: 607, steps: 5, D loss: 0.277121, acc:  46%, G loss: 1.606798\n",
      "Ep: 607, steps: 6, D loss: 0.227663, acc:  59%, G loss: 1.600885\n",
      "Ep: 607, steps: 7, D loss: 0.295911, acc:  36%, G loss: 1.487029\n",
      "Ep: 607, steps: 8, D loss: 0.229062, acc:  64%, G loss: 1.644815\n",
      "Ep: 607, steps: 9, D loss: 0.233744, acc:  61%, G loss: 1.579315\n",
      "Ep: 607, steps: 10, D loss: 0.182706, acc:  79%, G loss: 1.511124\n",
      "Ep: 607, steps: 11, D loss: 0.251420, acc:  54%, G loss: 1.778865\n",
      "Ep: 607, steps: 12, D loss: 0.278210, acc:  39%, G loss: 1.425513\n",
      "Ep: 607, steps: 13, D loss: 0.289559, acc:  34%, G loss: 1.440002\n",
      "Ep: 607, steps: 14, D loss: 0.274931, acc:  41%, G loss: 1.470576\n",
      "Ep: 607, steps: 15, D loss: 0.261531, acc:  46%, G loss: 1.587674\n",
      "Ep: 607, steps: 16, D loss: 0.253212, acc:  54%, G loss: 1.601905\n",
      "Ep: 607, steps: 17, D loss: 0.216084, acc:  67%, G loss: 1.506680\n",
      "Ep: 607, steps: 18, D loss: 0.235090, acc:  60%, G loss: 1.604236\n",
      "Ep: 607, steps: 19, D loss: 0.201922, acc:  70%, G loss: 1.623288\n",
      "Ep: 607, steps: 20, D loss: 0.182136, acc:  76%, G loss: 1.704856\n",
      "Ep: 607, steps: 21, D loss: 0.262392, acc:  45%, G loss: 1.481913\n",
      "Ep: 607, steps: 22, D loss: 0.200316, acc:  71%, G loss: 1.649523\n",
      "Ep: 607, steps: 23, D loss: 0.228096, acc:  64%, G loss: 1.867365\n",
      "Ep: 607, steps: 24, D loss: 0.212878, acc:  67%, G loss: 1.606558\n",
      "Ep: 607, steps: 25, D loss: 0.255883, acc:  53%, G loss: 1.625612\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 608, steps: 1, D loss: 0.247222, acc:  56%, G loss: 1.703220\n",
      "Ep: 608, steps: 2, D loss: 0.240710, acc:  58%, G loss: 1.531196\n",
      "Ep: 608, steps: 3, D loss: 0.174438, acc:  78%, G loss: 1.959814\n",
      "Ep: 608, steps: 4, D loss: 0.196713, acc:  79%, G loss: 1.740357\n",
      "Ep: 608, steps: 5, D loss: 0.281058, acc:  45%, G loss: 1.717356\n",
      "Ep: 608, steps: 6, D loss: 0.229259, acc:  57%, G loss: 1.567963\n",
      "Ep: 608, steps: 7, D loss: 0.283293, acc:  39%, G loss: 1.560913\n",
      "Ep: 608, steps: 8, D loss: 0.230283, acc:  62%, G loss: 1.659751\n",
      "Ep: 608, steps: 9, D loss: 0.230542, acc:  62%, G loss: 1.615220\n",
      "Ep: 608, steps: 10, D loss: 0.175829, acc:  79%, G loss: 1.656373\n",
      "Ep: 608, steps: 11, D loss: 0.236544, acc:  58%, G loss: 1.762287\n",
      "Ep: 608, steps: 12, D loss: 0.300200, acc:  35%, G loss: 1.393615\n",
      "Ep: 608, steps: 13, D loss: 0.272591, acc:  40%, G loss: 1.402201\n",
      "Ep: 608, steps: 14, D loss: 0.274214, acc:  41%, G loss: 1.454740\n",
      "Ep: 608, steps: 15, D loss: 0.240618, acc:  57%, G loss: 1.582491\n",
      "Ep: 608, steps: 16, D loss: 0.242656, acc:  59%, G loss: 1.595419\n",
      "Ep: 608, steps: 17, D loss: 0.204243, acc:  73%, G loss: 1.546235\n",
      "Ep: 608, steps: 18, D loss: 0.244424, acc:  56%, G loss: 1.619974\n",
      "Ep: 608, steps: 19, D loss: 0.213378, acc:  66%, G loss: 1.635762\n",
      "Ep: 608, steps: 20, D loss: 0.175246, acc:  79%, G loss: 1.725673\n",
      "Ep: 608, steps: 21, D loss: 0.264815, acc:  44%, G loss: 1.493635\n",
      "Ep: 608, steps: 22, D loss: 0.197192, acc:  69%, G loss: 1.673170\n",
      "Ep: 608, steps: 23, D loss: 0.240452, acc:  57%, G loss: 1.831367\n",
      "Ep: 608, steps: 24, D loss: 0.196780, acc:  75%, G loss: 1.538223\n",
      "Ep: 608, steps: 25, D loss: 0.255904, acc:  53%, G loss: 1.466621\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 609, steps: 1, D loss: 0.243122, acc:  57%, G loss: 1.720863\n",
      "Ep: 609, steps: 2, D loss: 0.248566, acc:  54%, G loss: 1.534568\n",
      "Ep: 609, steps: 3, D loss: 0.174102, acc:  78%, G loss: 1.943768\n",
      "Ep: 609, steps: 4, D loss: 0.191029, acc:  82%, G loss: 1.885804\n",
      "Ep: 609, steps: 5, D loss: 0.283791, acc:  44%, G loss: 1.620245\n",
      "Ep: 609, steps: 6, D loss: 0.241161, acc:  55%, G loss: 1.578374\n",
      "Ep: 609, steps: 7, D loss: 0.307242, acc:  29%, G loss: 1.587325\n",
      "Ep: 609, steps: 8, D loss: 0.230267, acc:  61%, G loss: 1.724362\n",
      "Ep: 609, steps: 9, D loss: 0.237506, acc:  61%, G loss: 1.671353\n",
      "Ep: 609, steps: 10, D loss: 0.180376, acc:  80%, G loss: 1.602675\n",
      "Ep: 609, steps: 11, D loss: 0.255917, acc:  49%, G loss: 1.790171\n",
      "Ep: 609, steps: 12, D loss: 0.274042, acc:  42%, G loss: 1.389775\n",
      "Saved Model\n",
      "Ep: 609, steps: 13, D loss: 0.276384, acc:  40%, G loss: 1.393387\n",
      "Ep: 609, steps: 14, D loss: 0.224065, acc:  64%, G loss: 1.562364\n",
      "Ep: 609, steps: 15, D loss: 0.244558, acc:  56%, G loss: 1.574144\n",
      "Ep: 609, steps: 16, D loss: 0.200524, acc:  74%, G loss: 1.577107\n",
      "Ep: 609, steps: 17, D loss: 0.228990, acc:  62%, G loss: 1.635658\n",
      "Ep: 609, steps: 18, D loss: 0.207010, acc:  68%, G loss: 1.632036\n",
      "Ep: 609, steps: 19, D loss: 0.184811, acc:  75%, G loss: 1.687215\n",
      "Ep: 609, steps: 20, D loss: 0.281731, acc:  37%, G loss: 1.459549\n",
      "Ep: 609, steps: 21, D loss: 0.177337, acc:  74%, G loss: 1.596381\n",
      "Ep: 609, steps: 22, D loss: 0.227618, acc:  63%, G loss: 1.842576\n",
      "Ep: 609, steps: 23, D loss: 0.216056, acc:  66%, G loss: 1.600786\n",
      "Ep: 609, steps: 24, D loss: 0.260366, acc:  53%, G loss: 1.556258\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 610, steps: 1, D loss: 0.239680, acc:  60%, G loss: 1.785494\n",
      "Ep: 610, steps: 2, D loss: 0.230814, acc:  62%, G loss: 1.472034\n",
      "Ep: 610, steps: 3, D loss: 0.171053, acc:  79%, G loss: 1.936771\n",
      "Ep: 610, steps: 4, D loss: 0.181674, acc:  83%, G loss: 1.778686\n",
      "Ep: 610, steps: 5, D loss: 0.291679, acc:  43%, G loss: 1.668090\n",
      "Ep: 610, steps: 6, D loss: 0.215656, acc:  60%, G loss: 1.611750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 610, steps: 7, D loss: 0.300971, acc:  36%, G loss: 1.655534\n",
      "Ep: 610, steps: 8, D loss: 0.242735, acc:  60%, G loss: 1.718097\n",
      "Ep: 610, steps: 9, D loss: 0.290240, acc:  42%, G loss: 1.601940\n",
      "Ep: 610, steps: 10, D loss: 0.200841, acc:  70%, G loss: 1.582405\n",
      "Ep: 610, steps: 11, D loss: 0.270305, acc:  47%, G loss: 1.769169\n",
      "Ep: 610, steps: 12, D loss: 0.304098, acc:  32%, G loss: 1.328429\n",
      "Ep: 610, steps: 13, D loss: 0.279672, acc:  37%, G loss: 1.430977\n",
      "Ep: 610, steps: 14, D loss: 0.282595, acc:  39%, G loss: 1.467784\n",
      "Ep: 610, steps: 15, D loss: 0.243635, acc:  54%, G loss: 1.545389\n",
      "Ep: 610, steps: 16, D loss: 0.261309, acc:  49%, G loss: 1.562311\n",
      "Ep: 610, steps: 17, D loss: 0.221751, acc:  67%, G loss: 1.650480\n",
      "Ep: 610, steps: 18, D loss: 0.227783, acc:  63%, G loss: 1.612794\n",
      "Ep: 610, steps: 19, D loss: 0.212238, acc:  66%, G loss: 1.626187\n",
      "Ep: 610, steps: 20, D loss: 0.179630, acc:  80%, G loss: 1.715174\n",
      "Ep: 610, steps: 21, D loss: 0.266787, acc:  41%, G loss: 1.477154\n",
      "Ep: 610, steps: 22, D loss: 0.193499, acc:  72%, G loss: 1.558713\n",
      "Ep: 610, steps: 23, D loss: 0.224693, acc:  65%, G loss: 1.792210\n",
      "Ep: 610, steps: 24, D loss: 0.215918, acc:  66%, G loss: 1.548997\n",
      "Ep: 610, steps: 25, D loss: 0.261268, acc:  52%, G loss: 1.572084\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 611, steps: 1, D loss: 0.234478, acc:  60%, G loss: 1.634744\n",
      "Ep: 611, steps: 2, D loss: 0.231757, acc:  60%, G loss: 1.494240\n",
      "Ep: 611, steps: 3, D loss: 0.169258, acc:  81%, G loss: 1.924517\n",
      "Ep: 611, steps: 4, D loss: 0.186158, acc:  83%, G loss: 1.739979\n",
      "Ep: 611, steps: 5, D loss: 0.278127, acc:  47%, G loss: 1.647233\n",
      "Ep: 611, steps: 6, D loss: 0.224819, acc:  60%, G loss: 1.570794\n",
      "Ep: 611, steps: 7, D loss: 0.292536, acc:  36%, G loss: 1.542209\n",
      "Ep: 611, steps: 8, D loss: 0.225427, acc:  64%, G loss: 1.666142\n",
      "Ep: 611, steps: 9, D loss: 0.243884, acc:  57%, G loss: 1.628001\n",
      "Ep: 611, steps: 10, D loss: 0.180321, acc:  78%, G loss: 1.604324\n",
      "Ep: 611, steps: 11, D loss: 0.265203, acc:  47%, G loss: 1.742203\n",
      "Ep: 611, steps: 12, D loss: 0.294057, acc:  36%, G loss: 1.449919\n",
      "Ep: 611, steps: 13, D loss: 0.273221, acc:  42%, G loss: 1.387486\n",
      "Ep: 611, steps: 14, D loss: 0.275725, acc:  41%, G loss: 1.465287\n",
      "Ep: 611, steps: 15, D loss: 0.243286, acc:  55%, G loss: 1.547666\n",
      "Ep: 611, steps: 16, D loss: 0.251506, acc:  53%, G loss: 1.588942\n",
      "Ep: 611, steps: 17, D loss: 0.218677, acc:  68%, G loss: 1.482317\n",
      "Ep: 611, steps: 18, D loss: 0.246771, acc:  55%, G loss: 1.591936\n",
      "Ep: 611, steps: 19, D loss: 0.221755, acc:  64%, G loss: 1.566776\n",
      "Ep: 611, steps: 20, D loss: 0.185602, acc:  75%, G loss: 1.726898\n",
      "Ep: 611, steps: 21, D loss: 0.267430, acc:  39%, G loss: 1.579068\n",
      "Ep: 611, steps: 22, D loss: 0.197951, acc:  70%, G loss: 1.601090\n",
      "Ep: 611, steps: 23, D loss: 0.212525, acc:  69%, G loss: 1.794291\n",
      "Ep: 611, steps: 24, D loss: 0.216728, acc:  67%, G loss: 1.464413\n",
      "Ep: 611, steps: 25, D loss: 0.263881, acc:  52%, G loss: 1.616918\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 612, steps: 1, D loss: 0.247401, acc:  56%, G loss: 1.622546\n",
      "Ep: 612, steps: 2, D loss: 0.236351, acc:  57%, G loss: 1.470674\n",
      "Ep: 612, steps: 3, D loss: 0.177997, acc:  77%, G loss: 1.975794\n",
      "Ep: 612, steps: 4, D loss: 0.195657, acc:  78%, G loss: 1.799468\n",
      "Ep: 612, steps: 5, D loss: 0.277911, acc:  47%, G loss: 1.655374\n",
      "Ep: 612, steps: 6, D loss: 0.239211, acc:  57%, G loss: 1.575387\n",
      "Ep: 612, steps: 7, D loss: 0.310052, acc:  30%, G loss: 1.528551\n",
      "Ep: 612, steps: 8, D loss: 0.241260, acc:  58%, G loss: 1.744380\n",
      "Ep: 612, steps: 9, D loss: 0.226164, acc:  64%, G loss: 1.618308\n",
      "Ep: 612, steps: 10, D loss: 0.176569, acc:  82%, G loss: 1.599181\n",
      "Saved Model\n",
      "Ep: 612, steps: 11, D loss: 0.270571, acc:  46%, G loss: 1.834487\n",
      "Ep: 612, steps: 12, D loss: 0.300946, acc:  32%, G loss: 1.347412\n",
      "Ep: 612, steps: 13, D loss: 0.293298, acc:  36%, G loss: 1.440816\n",
      "Ep: 612, steps: 14, D loss: 0.256805, acc:  49%, G loss: 1.595214\n",
      "Ep: 612, steps: 15, D loss: 0.243829, acc:  55%, G loss: 1.555814\n",
      "Ep: 612, steps: 16, D loss: 0.209813, acc:  72%, G loss: 1.507891\n",
      "Ep: 612, steps: 17, D loss: 0.236047, acc:  58%, G loss: 1.568922\n",
      "Ep: 612, steps: 18, D loss: 0.213166, acc:  67%, G loss: 1.608802\n",
      "Ep: 612, steps: 19, D loss: 0.200002, acc:  68%, G loss: 1.811498\n",
      "Ep: 612, steps: 20, D loss: 0.256584, acc:  45%, G loss: 1.583888\n",
      "Ep: 612, steps: 21, D loss: 0.193696, acc:  72%, G loss: 1.645049\n",
      "Ep: 612, steps: 22, D loss: 0.211074, acc:  70%, G loss: 1.838628\n",
      "Ep: 612, steps: 23, D loss: 0.209955, acc:  68%, G loss: 1.535594\n",
      "Ep: 612, steps: 24, D loss: 0.267647, acc:  51%, G loss: 1.583413\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 613, steps: 1, D loss: 0.256552, acc:  53%, G loss: 1.607036\n",
      "Ep: 613, steps: 2, D loss: 0.245961, acc:  55%, G loss: 1.477393\n",
      "Ep: 613, steps: 3, D loss: 0.170583, acc:  80%, G loss: 1.969495\n",
      "Ep: 613, steps: 4, D loss: 0.186239, acc:  83%, G loss: 1.845261\n",
      "Ep: 613, steps: 5, D loss: 0.278197, acc:  49%, G loss: 1.650176\n",
      "Ep: 613, steps: 6, D loss: 0.235694, acc:  57%, G loss: 1.575773\n",
      "Ep: 613, steps: 7, D loss: 0.314944, acc:  30%, G loss: 1.617183\n",
      "Ep: 613, steps: 8, D loss: 0.227454, acc:  62%, G loss: 1.713860\n",
      "Ep: 613, steps: 9, D loss: 0.239689, acc:  60%, G loss: 1.616668\n",
      "Ep: 613, steps: 10, D loss: 0.180810, acc:  77%, G loss: 1.553341\n",
      "Ep: 613, steps: 11, D loss: 0.265973, acc:  49%, G loss: 1.738194\n",
      "Ep: 613, steps: 12, D loss: 0.288851, acc:  36%, G loss: 1.396256\n",
      "Ep: 613, steps: 13, D loss: 0.281511, acc:  39%, G loss: 1.386424\n",
      "Ep: 613, steps: 14, D loss: 0.272335, acc:  43%, G loss: 1.432087\n",
      "Ep: 613, steps: 15, D loss: 0.244495, acc:  54%, G loss: 1.568083\n",
      "Ep: 613, steps: 16, D loss: 0.250199, acc:  55%, G loss: 1.587222\n",
      "Ep: 613, steps: 17, D loss: 0.232348, acc:  62%, G loss: 1.661866\n",
      "Ep: 613, steps: 18, D loss: 0.240499, acc:  57%, G loss: 1.544696\n",
      "Ep: 613, steps: 19, D loss: 0.207838, acc:  69%, G loss: 1.553767\n",
      "Ep: 613, steps: 20, D loss: 0.174155, acc:  79%, G loss: 1.775069\n",
      "Ep: 613, steps: 21, D loss: 0.252491, acc:  49%, G loss: 1.525809\n",
      "Ep: 613, steps: 22, D loss: 0.191636, acc:  74%, G loss: 1.733182\n",
      "Ep: 613, steps: 23, D loss: 0.209627, acc:  69%, G loss: 1.801565\n",
      "Ep: 613, steps: 24, D loss: 0.212601, acc:  67%, G loss: 1.579296\n",
      "Ep: 613, steps: 25, D loss: 0.251599, acc:  55%, G loss: 1.476120\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 614, steps: 1, D loss: 0.243214, acc:  55%, G loss: 1.670595\n",
      "Ep: 614, steps: 2, D loss: 0.258600, acc:  51%, G loss: 1.481343\n",
      "Ep: 614, steps: 3, D loss: 0.165471, acc:  83%, G loss: 1.932611\n",
      "Ep: 614, steps: 4, D loss: 0.187577, acc:  81%, G loss: 1.831893\n",
      "Ep: 614, steps: 5, D loss: 0.290407, acc:  43%, G loss: 1.624340\n",
      "Ep: 614, steps: 6, D loss: 0.231367, acc:  57%, G loss: 1.552922\n",
      "Ep: 614, steps: 7, D loss: 0.304567, acc:  33%, G loss: 1.428670\n",
      "Ep: 614, steps: 8, D loss: 0.229018, acc:  61%, G loss: 1.679613\n",
      "Ep: 614, steps: 9, D loss: 0.237559, acc:  61%, G loss: 1.581419\n",
      "Ep: 614, steps: 10, D loss: 0.190916, acc:  75%, G loss: 1.587201\n",
      "Ep: 614, steps: 11, D loss: 0.270589, acc:  46%, G loss: 1.775279\n",
      "Ep: 614, steps: 12, D loss: 0.284889, acc:  38%, G loss: 1.378825\n",
      "Ep: 614, steps: 13, D loss: 0.286429, acc:  36%, G loss: 1.397859\n",
      "Ep: 614, steps: 14, D loss: 0.273089, acc:  42%, G loss: 1.442736\n",
      "Ep: 614, steps: 15, D loss: 0.253797, acc:  50%, G loss: 1.690995\n",
      "Ep: 614, steps: 16, D loss: 0.244778, acc:  55%, G loss: 1.615307\n",
      "Ep: 614, steps: 17, D loss: 0.217584, acc:  70%, G loss: 1.543736\n",
      "Ep: 614, steps: 18, D loss: 0.241507, acc:  56%, G loss: 1.575283\n",
      "Ep: 614, steps: 19, D loss: 0.221197, acc:  64%, G loss: 1.599249\n",
      "Ep: 614, steps: 20, D loss: 0.186535, acc:  76%, G loss: 1.675579\n",
      "Ep: 614, steps: 21, D loss: 0.264454, acc:  42%, G loss: 1.548475\n",
      "Ep: 614, steps: 22, D loss: 0.176546, acc:  76%, G loss: 1.580834\n",
      "Ep: 614, steps: 23, D loss: 0.237755, acc:  57%, G loss: 1.800859\n",
      "Ep: 614, steps: 24, D loss: 0.206211, acc:  72%, G loss: 1.629032\n",
      "Ep: 614, steps: 25, D loss: 0.257487, acc:  53%, G loss: 1.547561\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 615, steps: 1, D loss: 0.237479, acc:  58%, G loss: 1.666808\n",
      "Ep: 615, steps: 2, D loss: 0.240301, acc:  58%, G loss: 1.478615\n",
      "Ep: 615, steps: 3, D loss: 0.168880, acc:  84%, G loss: 1.924433\n",
      "Ep: 615, steps: 4, D loss: 0.185077, acc:  83%, G loss: 1.796961\n",
      "Ep: 615, steps: 5, D loss: 0.280702, acc:  45%, G loss: 1.611539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 615, steps: 6, D loss: 0.234238, acc:  58%, G loss: 1.554840\n",
      "Ep: 615, steps: 7, D loss: 0.304291, acc:  32%, G loss: 1.642038\n",
      "Ep: 615, steps: 8, D loss: 0.226778, acc:  64%, G loss: 1.648034\n",
      "Saved Model\n",
      "Ep: 615, steps: 9, D loss: 0.249354, acc:  56%, G loss: 1.574107\n",
      "Ep: 615, steps: 10, D loss: 0.263874, acc:  47%, G loss: 1.686671\n",
      "Ep: 615, steps: 11, D loss: 0.284899, acc:  38%, G loss: 1.408409\n",
      "Ep: 615, steps: 12, D loss: 0.279951, acc:  37%, G loss: 1.417618\n",
      "Ep: 615, steps: 13, D loss: 0.259790, acc:  47%, G loss: 1.467441\n",
      "Ep: 615, steps: 14, D loss: 0.240209, acc:  56%, G loss: 1.551151\n",
      "Ep: 615, steps: 15, D loss: 0.253444, acc:  51%, G loss: 1.553843\n",
      "Ep: 615, steps: 16, D loss: 0.229670, acc:  63%, G loss: 1.569491\n",
      "Ep: 615, steps: 17, D loss: 0.237442, acc:  60%, G loss: 1.557989\n",
      "Ep: 615, steps: 18, D loss: 0.238751, acc:  58%, G loss: 1.549868\n",
      "Ep: 615, steps: 19, D loss: 0.198012, acc:  74%, G loss: 1.645211\n",
      "Ep: 615, steps: 20, D loss: 0.252719, acc:  46%, G loss: 1.518105\n",
      "Ep: 615, steps: 21, D loss: 0.199747, acc:  69%, G loss: 1.521107\n",
      "Ep: 615, steps: 22, D loss: 0.241500, acc:  58%, G loss: 1.803766\n",
      "Ep: 615, steps: 23, D loss: 0.217744, acc:  64%, G loss: 1.652356\n",
      "Ep: 615, steps: 24, D loss: 0.242290, acc:  58%, G loss: 1.703704\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 616, steps: 1, D loss: 0.230996, acc:  62%, G loss: 1.716251\n",
      "Ep: 616, steps: 2, D loss: 0.246163, acc:  56%, G loss: 1.499547\n",
      "Ep: 616, steps: 3, D loss: 0.172644, acc:  79%, G loss: 2.001365\n",
      "Ep: 616, steps: 4, D loss: 0.211054, acc:  73%, G loss: 1.775731\n",
      "Ep: 616, steps: 5, D loss: 0.278486, acc:  44%, G loss: 1.638343\n",
      "Ep: 616, steps: 6, D loss: 0.231441, acc:  58%, G loss: 1.592176\n",
      "Ep: 616, steps: 7, D loss: 0.298803, acc:  34%, G loss: 1.488674\n",
      "Ep: 616, steps: 8, D loss: 0.230339, acc:  64%, G loss: 1.687922\n",
      "Ep: 616, steps: 9, D loss: 0.251301, acc:  54%, G loss: 1.622201\n",
      "Ep: 616, steps: 10, D loss: 0.204056, acc:  71%, G loss: 1.592322\n",
      "Ep: 616, steps: 11, D loss: 0.252929, acc:  52%, G loss: 1.743313\n",
      "Ep: 616, steps: 12, D loss: 0.292850, acc:  35%, G loss: 1.367972\n",
      "Ep: 616, steps: 13, D loss: 0.280664, acc:  38%, G loss: 1.456456\n",
      "Ep: 616, steps: 14, D loss: 0.279643, acc:  40%, G loss: 1.443835\n",
      "Ep: 616, steps: 15, D loss: 0.254679, acc:  51%, G loss: 1.547071\n",
      "Ep: 616, steps: 16, D loss: 0.256418, acc:  52%, G loss: 1.586381\n",
      "Ep: 616, steps: 17, D loss: 0.216907, acc:  67%, G loss: 1.497510\n",
      "Ep: 616, steps: 18, D loss: 0.222629, acc:  66%, G loss: 1.551937\n",
      "Ep: 616, steps: 19, D loss: 0.219205, acc:  64%, G loss: 1.534255\n",
      "Ep: 616, steps: 20, D loss: 0.184253, acc:  77%, G loss: 1.765751\n",
      "Ep: 616, steps: 21, D loss: 0.261489, acc:  42%, G loss: 1.445970\n",
      "Ep: 616, steps: 22, D loss: 0.200977, acc:  67%, G loss: 1.569613\n",
      "Ep: 616, steps: 23, D loss: 0.213963, acc:  69%, G loss: 1.823267\n",
      "Ep: 616, steps: 24, D loss: 0.197931, acc:  73%, G loss: 1.513004\n",
      "Ep: 616, steps: 25, D loss: 0.281255, acc:  48%, G loss: 1.589232\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 617, steps: 1, D loss: 0.238874, acc:  60%, G loss: 1.621062\n",
      "Ep: 617, steps: 2, D loss: 0.262953, acc:  49%, G loss: 1.469661\n",
      "Ep: 617, steps: 3, D loss: 0.176040, acc:  79%, G loss: 1.862771\n",
      "Ep: 617, steps: 4, D loss: 0.192107, acc:  80%, G loss: 1.769270\n",
      "Ep: 617, steps: 5, D loss: 0.288311, acc:  43%, G loss: 1.649817\n",
      "Ep: 617, steps: 6, D loss: 0.231481, acc:  58%, G loss: 1.586320\n",
      "Ep: 617, steps: 7, D loss: 0.322474, acc:  27%, G loss: 1.499439\n",
      "Ep: 617, steps: 8, D loss: 0.228240, acc:  62%, G loss: 1.699728\n",
      "Ep: 617, steps: 9, D loss: 0.234950, acc:  62%, G loss: 1.642160\n",
      "Ep: 617, steps: 10, D loss: 0.188417, acc:  76%, G loss: 1.591212\n",
      "Ep: 617, steps: 11, D loss: 0.268669, acc:  46%, G loss: 1.773876\n",
      "Ep: 617, steps: 12, D loss: 0.287302, acc:  37%, G loss: 1.425420\n",
      "Ep: 617, steps: 13, D loss: 0.276858, acc:  41%, G loss: 1.399009\n",
      "Ep: 617, steps: 14, D loss: 0.275592, acc:  40%, G loss: 1.453276\n",
      "Ep: 617, steps: 15, D loss: 0.246021, acc:  54%, G loss: 1.644337\n",
      "Ep: 617, steps: 16, D loss: 0.252959, acc:  53%, G loss: 1.616835\n",
      "Ep: 617, steps: 17, D loss: 0.213867, acc:  70%, G loss: 1.587550\n",
      "Ep: 617, steps: 18, D loss: 0.233512, acc:  61%, G loss: 1.579521\n",
      "Ep: 617, steps: 19, D loss: 0.214070, acc:  65%, G loss: 1.596486\n",
      "Ep: 617, steps: 20, D loss: 0.202852, acc:  70%, G loss: 1.726880\n",
      "Ep: 617, steps: 21, D loss: 0.263357, acc:  43%, G loss: 1.575617\n",
      "Ep: 617, steps: 22, D loss: 0.192442, acc:  73%, G loss: 1.483840\n",
      "Ep: 617, steps: 23, D loss: 0.222134, acc:  65%, G loss: 1.882236\n",
      "Ep: 617, steps: 24, D loss: 0.207473, acc:  72%, G loss: 1.517580\n",
      "Ep: 617, steps: 25, D loss: 0.251451, acc:  55%, G loss: 1.671904\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 618, steps: 1, D loss: 0.231912, acc:  61%, G loss: 1.629914\n",
      "Ep: 618, steps: 2, D loss: 0.243850, acc:  55%, G loss: 1.488855\n",
      "Ep: 618, steps: 3, D loss: 0.171350, acc:  82%, G loss: 1.965567\n",
      "Ep: 618, steps: 4, D loss: 0.189259, acc:  82%, G loss: 1.819665\n",
      "Ep: 618, steps: 5, D loss: 0.286969, acc:  44%, G loss: 1.577117\n",
      "Ep: 618, steps: 6, D loss: 0.239308, acc:  55%, G loss: 1.562679\n",
      "Saved Model\n",
      "Ep: 618, steps: 7, D loss: 0.307617, acc:  33%, G loss: 1.522332\n",
      "Ep: 618, steps: 8, D loss: 0.202779, acc:  74%, G loss: 1.708241\n",
      "Ep: 618, steps: 9, D loss: 0.186435, acc:  76%, G loss: 1.668622\n",
      "Ep: 618, steps: 10, D loss: 0.222769, acc:  62%, G loss: 1.877281\n",
      "Ep: 618, steps: 11, D loss: 0.302162, acc:  33%, G loss: 1.366703\n",
      "Ep: 618, steps: 12, D loss: 0.286236, acc:  36%, G loss: 1.425548\n",
      "Ep: 618, steps: 13, D loss: 0.278309, acc:  38%, G loss: 1.505431\n",
      "Ep: 618, steps: 14, D loss: 0.254254, acc:  51%, G loss: 1.624341\n",
      "Ep: 618, steps: 15, D loss: 0.250746, acc:  55%, G loss: 1.604484\n",
      "Ep: 618, steps: 16, D loss: 0.215155, acc:  67%, G loss: 1.585467\n",
      "Ep: 618, steps: 17, D loss: 0.228487, acc:  62%, G loss: 1.595393\n",
      "Ep: 618, steps: 18, D loss: 0.224206, acc:  62%, G loss: 1.590641\n",
      "Ep: 618, steps: 19, D loss: 0.189911, acc:  73%, G loss: 1.670491\n",
      "Ep: 618, steps: 20, D loss: 0.274351, acc:  37%, G loss: 1.425819\n",
      "Ep: 618, steps: 21, D loss: 0.209022, acc:  68%, G loss: 1.615819\n",
      "Ep: 618, steps: 22, D loss: 0.217974, acc:  65%, G loss: 1.757993\n",
      "Ep: 618, steps: 23, D loss: 0.210691, acc:  68%, G loss: 1.578902\n",
      "Ep: 618, steps: 24, D loss: 0.253137, acc:  53%, G loss: 1.500496\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 619, steps: 1, D loss: 0.250914, acc:  55%, G loss: 1.693589\n",
      "Ep: 619, steps: 2, D loss: 0.235787, acc:  59%, G loss: 1.547678\n",
      "Ep: 619, steps: 3, D loss: 0.173718, acc:  79%, G loss: 1.936979\n",
      "Ep: 619, steps: 4, D loss: 0.183752, acc:  83%, G loss: 1.839603\n",
      "Ep: 619, steps: 5, D loss: 0.291634, acc:  42%, G loss: 1.677123\n",
      "Ep: 619, steps: 6, D loss: 0.243107, acc:  55%, G loss: 1.588831\n",
      "Ep: 619, steps: 7, D loss: 0.300172, acc:  34%, G loss: 1.482528\n",
      "Ep: 619, steps: 8, D loss: 0.231046, acc:  63%, G loss: 1.673260\n",
      "Ep: 619, steps: 9, D loss: 0.230268, acc:  63%, G loss: 1.579334\n",
      "Ep: 619, steps: 10, D loss: 0.182637, acc:  79%, G loss: 1.572154\n",
      "Ep: 619, steps: 11, D loss: 0.258570, acc:  50%, G loss: 1.752586\n",
      "Ep: 619, steps: 12, D loss: 0.279542, acc:  41%, G loss: 1.399737\n",
      "Ep: 619, steps: 13, D loss: 0.286266, acc:  34%, G loss: 1.478584\n",
      "Ep: 619, steps: 14, D loss: 0.268747, acc:  44%, G loss: 1.481682\n",
      "Ep: 619, steps: 15, D loss: 0.258651, acc:  51%, G loss: 1.616732\n",
      "Ep: 619, steps: 16, D loss: 0.243416, acc:  57%, G loss: 1.591493\n",
      "Ep: 619, steps: 17, D loss: 0.208903, acc:  71%, G loss: 1.601145\n",
      "Ep: 619, steps: 18, D loss: 0.231173, acc:  62%, G loss: 1.602524\n",
      "Ep: 619, steps: 19, D loss: 0.206435, acc:  68%, G loss: 1.589101\n",
      "Ep: 619, steps: 20, D loss: 0.179787, acc:  76%, G loss: 1.721672\n",
      "Ep: 619, steps: 21, D loss: 0.266575, acc:  42%, G loss: 1.439330\n",
      "Ep: 619, steps: 22, D loss: 0.211573, acc:  69%, G loss: 1.713177\n",
      "Ep: 619, steps: 23, D loss: 0.231223, acc:  63%, G loss: 1.830169\n",
      "Ep: 619, steps: 24, D loss: 0.207327, acc:  71%, G loss: 1.563116\n",
      "Ep: 619, steps: 25, D loss: 0.278399, acc:  49%, G loss: 1.565355\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 620, steps: 1, D loss: 0.242889, acc:  57%, G loss: 1.701567\n",
      "Ep: 620, steps: 2, D loss: 0.242222, acc:  57%, G loss: 1.511365\n",
      "Ep: 620, steps: 3, D loss: 0.178450, acc:  79%, G loss: 1.990014\n",
      "Ep: 620, steps: 4, D loss: 0.189392, acc:  82%, G loss: 1.778017\n",
      "Ep: 620, steps: 5, D loss: 0.292287, acc:  44%, G loss: 1.619633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 620, steps: 6, D loss: 0.223887, acc:  61%, G loss: 1.590606\n",
      "Ep: 620, steps: 7, D loss: 0.326724, acc:  27%, G loss: 1.496868\n",
      "Ep: 620, steps: 8, D loss: 0.230045, acc:  63%, G loss: 1.679925\n",
      "Ep: 620, steps: 9, D loss: 0.236604, acc:  60%, G loss: 1.740255\n",
      "Ep: 620, steps: 10, D loss: 0.194200, acc:  74%, G loss: 1.647594\n",
      "Ep: 620, steps: 11, D loss: 0.249430, acc:  54%, G loss: 1.810092\n",
      "Ep: 620, steps: 12, D loss: 0.280418, acc:  40%, G loss: 1.393900\n",
      "Ep: 620, steps: 13, D loss: 0.287016, acc:  37%, G loss: 1.407243\n",
      "Ep: 620, steps: 14, D loss: 0.278828, acc:  40%, G loss: 1.434909\n",
      "Ep: 620, steps: 15, D loss: 0.240260, acc:  55%, G loss: 1.568308\n",
      "Ep: 620, steps: 16, D loss: 0.252065, acc:  52%, G loss: 1.569639\n",
      "Ep: 620, steps: 17, D loss: 0.225097, acc:  63%, G loss: 1.645530\n",
      "Ep: 620, steps: 18, D loss: 0.231598, acc:  62%, G loss: 1.534721\n",
      "Ep: 620, steps: 19, D loss: 0.209907, acc:  69%, G loss: 1.590326\n",
      "Ep: 620, steps: 20, D loss: 0.194279, acc:  75%, G loss: 1.654402\n",
      "Ep: 620, steps: 21, D loss: 0.255574, acc:  45%, G loss: 1.470819\n",
      "Ep: 620, steps: 22, D loss: 0.191509, acc:  73%, G loss: 1.553957\n",
      "Ep: 620, steps: 23, D loss: 0.231978, acc:  62%, G loss: 1.782603\n",
      "Ep: 620, steps: 24, D loss: 0.200793, acc:  73%, G loss: 1.563767\n",
      "Ep: 620, steps: 25, D loss: 0.265519, acc:  50%, G loss: 1.608857\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 621, steps: 1, D loss: 0.247346, acc:  58%, G loss: 1.615197\n",
      "Ep: 621, steps: 2, D loss: 0.261037, acc:  50%, G loss: 1.466536\n",
      "Ep: 621, steps: 3, D loss: 0.169336, acc:  81%, G loss: 1.943268\n",
      "Ep: 621, steps: 4, D loss: 0.182371, acc:  84%, G loss: 1.788304\n",
      "Saved Model\n",
      "Ep: 621, steps: 5, D loss: 0.299396, acc:  44%, G loss: 1.682572\n",
      "Ep: 621, steps: 6, D loss: 0.329332, acc:  26%, G loss: 1.495480\n",
      "Ep: 621, steps: 7, D loss: 0.220685, acc:  66%, G loss: 1.708487\n",
      "Ep: 621, steps: 8, D loss: 0.257280, acc:  50%, G loss: 1.532761\n",
      "Ep: 621, steps: 9, D loss: 0.184544, acc:  78%, G loss: 1.654031\n",
      "Ep: 621, steps: 10, D loss: 0.272935, acc:  45%, G loss: 1.784168\n",
      "Ep: 621, steps: 11, D loss: 0.295047, acc:  32%, G loss: 1.420627\n",
      "Ep: 621, steps: 12, D loss: 0.280485, acc:  40%, G loss: 1.452940\n",
      "Ep: 621, steps: 13, D loss: 0.275440, acc:  42%, G loss: 1.462362\n",
      "Ep: 621, steps: 14, D loss: 0.255605, acc:  49%, G loss: 1.588194\n",
      "Ep: 621, steps: 15, D loss: 0.260996, acc:  49%, G loss: 1.556475\n",
      "Ep: 621, steps: 16, D loss: 0.205632, acc:  74%, G loss: 1.504857\n",
      "Ep: 621, steps: 17, D loss: 0.227903, acc:  63%, G loss: 1.586304\n",
      "Ep: 621, steps: 18, D loss: 0.231726, acc:  59%, G loss: 1.631770\n",
      "Ep: 621, steps: 19, D loss: 0.195444, acc:  75%, G loss: 1.676808\n",
      "Ep: 621, steps: 20, D loss: 0.266037, acc:  39%, G loss: 1.426035\n",
      "Ep: 621, steps: 21, D loss: 0.201705, acc:  71%, G loss: 1.534853\n",
      "Ep: 621, steps: 22, D loss: 0.222228, acc:  66%, G loss: 1.722524\n",
      "Ep: 621, steps: 23, D loss: 0.207459, acc:  71%, G loss: 1.589022\n",
      "Ep: 621, steps: 24, D loss: 0.241547, acc:  59%, G loss: 1.564859\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 622, steps: 1, D loss: 0.239556, acc:  60%, G loss: 1.613296\n",
      "Ep: 622, steps: 2, D loss: 0.243223, acc:  56%, G loss: 1.481157\n",
      "Ep: 622, steps: 3, D loss: 0.169297, acc:  82%, G loss: 2.037394\n",
      "Ep: 622, steps: 4, D loss: 0.175779, acc:  85%, G loss: 1.780340\n",
      "Ep: 622, steps: 5, D loss: 0.297650, acc:  45%, G loss: 1.636999\n",
      "Ep: 622, steps: 6, D loss: 0.235477, acc:  55%, G loss: 1.541734\n",
      "Ep: 622, steps: 7, D loss: 0.298078, acc:  33%, G loss: 1.558582\n",
      "Ep: 622, steps: 8, D loss: 0.235158, acc:  61%, G loss: 1.666584\n",
      "Ep: 622, steps: 9, D loss: 0.235791, acc:  61%, G loss: 1.605461\n",
      "Ep: 622, steps: 10, D loss: 0.173726, acc:  80%, G loss: 1.597261\n",
      "Ep: 622, steps: 11, D loss: 0.265653, acc:  47%, G loss: 1.905201\n",
      "Ep: 622, steps: 12, D loss: 0.302983, acc:  33%, G loss: 1.419597\n",
      "Ep: 622, steps: 13, D loss: 0.283182, acc:  39%, G loss: 1.378744\n",
      "Ep: 622, steps: 14, D loss: 0.278970, acc:  41%, G loss: 1.435635\n",
      "Ep: 622, steps: 15, D loss: 0.239896, acc:  55%, G loss: 1.608969\n",
      "Ep: 622, steps: 16, D loss: 0.242952, acc:  58%, G loss: 1.549155\n",
      "Ep: 622, steps: 17, D loss: 0.226891, acc:  64%, G loss: 1.505347\n",
      "Ep: 622, steps: 18, D loss: 0.236585, acc:  59%, G loss: 1.554430\n",
      "Ep: 622, steps: 19, D loss: 0.215452, acc:  65%, G loss: 1.567896\n",
      "Ep: 622, steps: 20, D loss: 0.177440, acc:  79%, G loss: 1.711030\n",
      "Ep: 622, steps: 21, D loss: 0.280346, acc:  35%, G loss: 1.538643\n",
      "Ep: 622, steps: 22, D loss: 0.220486, acc:  66%, G loss: 1.675610\n",
      "Ep: 622, steps: 23, D loss: 0.224356, acc:  65%, G loss: 1.860333\n",
      "Ep: 622, steps: 24, D loss: 0.205814, acc:  71%, G loss: 1.531475\n",
      "Ep: 622, steps: 25, D loss: 0.278093, acc:  50%, G loss: 1.625614\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 623, steps: 1, D loss: 0.240248, acc:  57%, G loss: 1.657958\n",
      "Ep: 623, steps: 2, D loss: 0.236305, acc:  59%, G loss: 1.483898\n",
      "Ep: 623, steps: 3, D loss: 0.163205, acc:  82%, G loss: 1.953480\n",
      "Ep: 623, steps: 4, D loss: 0.189009, acc:  80%, G loss: 1.760763\n",
      "Ep: 623, steps: 5, D loss: 0.283280, acc:  43%, G loss: 1.723825\n",
      "Ep: 623, steps: 6, D loss: 0.233760, acc:  55%, G loss: 1.552013\n",
      "Ep: 623, steps: 7, D loss: 0.305228, acc:  33%, G loss: 1.485508\n",
      "Ep: 623, steps: 8, D loss: 0.240704, acc:  60%, G loss: 1.777428\n",
      "Ep: 623, steps: 9, D loss: 0.229918, acc:  65%, G loss: 1.569826\n",
      "Ep: 623, steps: 10, D loss: 0.177704, acc:  79%, G loss: 1.601145\n",
      "Ep: 623, steps: 11, D loss: 0.242403, acc:  54%, G loss: 1.767323\n",
      "Ep: 623, steps: 12, D loss: 0.284333, acc:  39%, G loss: 1.321118\n",
      "Ep: 623, steps: 13, D loss: 0.278179, acc:  39%, G loss: 1.362645\n",
      "Ep: 623, steps: 14, D loss: 0.276937, acc:  41%, G loss: 1.493876\n",
      "Ep: 623, steps: 15, D loss: 0.251835, acc:  51%, G loss: 1.591661\n",
      "Ep: 623, steps: 16, D loss: 0.261600, acc:  49%, G loss: 1.563062\n",
      "Ep: 623, steps: 17, D loss: 0.217698, acc:  67%, G loss: 1.513202\n",
      "Ep: 623, steps: 18, D loss: 0.237393, acc:  61%, G loss: 1.577867\n",
      "Ep: 623, steps: 19, D loss: 0.203976, acc:  69%, G loss: 1.562577\n",
      "Ep: 623, steps: 20, D loss: 0.187023, acc:  74%, G loss: 1.674773\n",
      "Ep: 623, steps: 21, D loss: 0.264132, acc:  44%, G loss: 1.449286\n",
      "Ep: 623, steps: 22, D loss: 0.193596, acc:  71%, G loss: 1.750025\n",
      "Ep: 623, steps: 23, D loss: 0.223835, acc:  64%, G loss: 1.878061\n",
      "Ep: 623, steps: 24, D loss: 0.211879, acc:  67%, G loss: 1.522385\n",
      "Ep: 623, steps: 25, D loss: 0.230382, acc:  62%, G loss: 1.600607\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 624, steps: 1, D loss: 0.253089, acc:  54%, G loss: 1.712626\n",
      "Ep: 624, steps: 2, D loss: 0.257402, acc:  49%, G loss: 1.493028\n",
      "Saved Model\n",
      "Ep: 624, steps: 3, D loss: 0.178500, acc:  79%, G loss: 1.918833\n",
      "Ep: 624, steps: 4, D loss: 0.261317, acc:  50%, G loss: 1.660502\n",
      "Ep: 624, steps: 5, D loss: 0.239471, acc:  59%, G loss: 1.526843\n",
      "Ep: 624, steps: 6, D loss: 0.270844, acc:  41%, G loss: 1.581645\n",
      "Ep: 624, steps: 7, D loss: 0.234471, acc:  60%, G loss: 1.743484\n",
      "Ep: 624, steps: 8, D loss: 0.248773, acc:  56%, G loss: 1.605974\n",
      "Ep: 624, steps: 9, D loss: 0.179094, acc:  83%, G loss: 1.550850\n",
      "Ep: 624, steps: 10, D loss: 0.272655, acc:  44%, G loss: 1.768745\n",
      "Ep: 624, steps: 11, D loss: 0.291295, acc:  36%, G loss: 1.395088\n",
      "Ep: 624, steps: 12, D loss: 0.282010, acc:  38%, G loss: 1.433288\n",
      "Ep: 624, steps: 13, D loss: 0.269800, acc:  44%, G loss: 1.435915\n",
      "Ep: 624, steps: 14, D loss: 0.240726, acc:  56%, G loss: 1.594589\n",
      "Ep: 624, steps: 15, D loss: 0.253042, acc:  53%, G loss: 1.544872\n",
      "Ep: 624, steps: 16, D loss: 0.236634, acc:  57%, G loss: 1.533448\n",
      "Ep: 624, steps: 17, D loss: 0.233055, acc:  61%, G loss: 1.530146\n",
      "Ep: 624, steps: 18, D loss: 0.221701, acc:  64%, G loss: 1.581424\n",
      "Ep: 624, steps: 19, D loss: 0.177239, acc:  78%, G loss: 1.687349\n",
      "Ep: 624, steps: 20, D loss: 0.254553, acc:  45%, G loss: 1.512341\n",
      "Ep: 624, steps: 21, D loss: 0.214404, acc:  68%, G loss: 1.617615\n",
      "Ep: 624, steps: 22, D loss: 0.224916, acc:  63%, G loss: 1.825590\n",
      "Ep: 624, steps: 23, D loss: 0.213391, acc:  67%, G loss: 1.564278\n",
      "Ep: 624, steps: 24, D loss: 0.250245, acc:  56%, G loss: 1.562061\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 625, steps: 1, D loss: 0.241306, acc:  60%, G loss: 1.783368\n",
      "Ep: 625, steps: 2, D loss: 0.252358, acc:  53%, G loss: 1.434653\n",
      "Ep: 625, steps: 3, D loss: 0.183029, acc:  78%, G loss: 1.918709\n",
      "Ep: 625, steps: 4, D loss: 0.187016, acc:  83%, G loss: 1.771717\n",
      "Ep: 625, steps: 5, D loss: 0.291740, acc:  43%, G loss: 1.666684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 625, steps: 6, D loss: 0.247947, acc:  53%, G loss: 1.526795\n",
      "Ep: 625, steps: 7, D loss: 0.302430, acc:  32%, G loss: 1.525081\n",
      "Ep: 625, steps: 8, D loss: 0.229487, acc:  62%, G loss: 1.787326\n",
      "Ep: 625, steps: 9, D loss: 0.233174, acc:  63%, G loss: 1.609678\n",
      "Ep: 625, steps: 10, D loss: 0.186812, acc:  76%, G loss: 1.655285\n",
      "Ep: 625, steps: 11, D loss: 0.250071, acc:  52%, G loss: 1.831043\n",
      "Ep: 625, steps: 12, D loss: 0.287981, acc:  38%, G loss: 1.367825\n",
      "Ep: 625, steps: 13, D loss: 0.274382, acc:  40%, G loss: 1.353327\n",
      "Ep: 625, steps: 14, D loss: 0.270286, acc:  42%, G loss: 1.493681\n",
      "Ep: 625, steps: 15, D loss: 0.242342, acc:  56%, G loss: 1.588606\n",
      "Ep: 625, steps: 16, D loss: 0.244105, acc:  56%, G loss: 1.529099\n",
      "Ep: 625, steps: 17, D loss: 0.232389, acc:  63%, G loss: 1.546062\n",
      "Ep: 625, steps: 18, D loss: 0.232782, acc:  61%, G loss: 1.541460\n",
      "Ep: 625, steps: 19, D loss: 0.215714, acc:  65%, G loss: 1.615124\n",
      "Ep: 625, steps: 20, D loss: 0.184471, acc:  77%, G loss: 1.649844\n",
      "Ep: 625, steps: 21, D loss: 0.257305, acc:  45%, G loss: 1.379713\n",
      "Ep: 625, steps: 22, D loss: 0.189182, acc:  72%, G loss: 1.535001\n",
      "Ep: 625, steps: 23, D loss: 0.205244, acc:  73%, G loss: 1.800886\n",
      "Ep: 625, steps: 24, D loss: 0.211160, acc:  68%, G loss: 1.614221\n",
      "Ep: 625, steps: 25, D loss: 0.267053, acc:  50%, G loss: 1.542359\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 626, steps: 1, D loss: 0.229680, acc:  62%, G loss: 1.694919\n",
      "Ep: 626, steps: 2, D loss: 0.247763, acc:  54%, G loss: 1.481883\n",
      "Ep: 626, steps: 3, D loss: 0.164300, acc:  82%, G loss: 1.969752\n",
      "Ep: 626, steps: 4, D loss: 0.181936, acc:  81%, G loss: 1.762357\n",
      "Ep: 626, steps: 5, D loss: 0.300071, acc:  39%, G loss: 1.617931\n",
      "Ep: 626, steps: 6, D loss: 0.233496, acc:  56%, G loss: 1.587313\n",
      "Ep: 626, steps: 7, D loss: 0.306954, acc:  30%, G loss: 1.524684\n",
      "Ep: 626, steps: 8, D loss: 0.235585, acc:  60%, G loss: 1.733809\n",
      "Ep: 626, steps: 9, D loss: 0.243975, acc:  57%, G loss: 1.633538\n",
      "Ep: 626, steps: 10, D loss: 0.185428, acc:  76%, G loss: 1.580706\n",
      "Ep: 626, steps: 11, D loss: 0.252393, acc:  53%, G loss: 1.781948\n",
      "Ep: 626, steps: 12, D loss: 0.286216, acc:  37%, G loss: 1.375041\n",
      "Ep: 626, steps: 13, D loss: 0.285688, acc:  36%, G loss: 1.400812\n",
      "Ep: 626, steps: 14, D loss: 0.276185, acc:  40%, G loss: 1.495329\n",
      "Ep: 626, steps: 15, D loss: 0.258697, acc:  50%, G loss: 1.616305\n",
      "Ep: 626, steps: 16, D loss: 0.257389, acc:  50%, G loss: 1.547027\n",
      "Ep: 626, steps: 17, D loss: 0.225123, acc:  65%, G loss: 1.556299\n",
      "Ep: 626, steps: 18, D loss: 0.226363, acc:  63%, G loss: 1.598118\n",
      "Ep: 626, steps: 19, D loss: 0.216444, acc:  66%, G loss: 1.601467\n",
      "Ep: 626, steps: 20, D loss: 0.201447, acc:  71%, G loss: 1.805758\n",
      "Ep: 626, steps: 21, D loss: 0.265188, acc:  42%, G loss: 1.426494\n",
      "Ep: 626, steps: 22, D loss: 0.224283, acc:  68%, G loss: 1.823265\n",
      "Ep: 626, steps: 23, D loss: 0.217417, acc:  66%, G loss: 1.853797\n",
      "Ep: 626, steps: 24, D loss: 0.202109, acc:  73%, G loss: 1.558530\n",
      "Ep: 626, steps: 25, D loss: 0.285700, acc:  45%, G loss: 1.536242\n",
      "Data exhausted, Re Initialize\n",
      "Saved Model\n",
      "Ep: 627, steps: 1, D loss: 0.239832, acc:  59%, G loss: 1.655331\n",
      "Ep: 627, steps: 2, D loss: 0.164455, acc:  83%, G loss: 1.933740\n",
      "Ep: 627, steps: 3, D loss: 0.202653, acc:  75%, G loss: 1.690582\n",
      "Ep: 627, steps: 4, D loss: 0.278582, acc:  47%, G loss: 1.649726\n",
      "Ep: 627, steps: 5, D loss: 0.242125, acc:  57%, G loss: 1.571518\n",
      "Ep: 627, steps: 6, D loss: 0.300147, acc:  36%, G loss: 1.510699\n",
      "Ep: 627, steps: 7, D loss: 0.220112, acc:  67%, G loss: 1.770929\n",
      "Ep: 627, steps: 8, D loss: 0.243055, acc:  59%, G loss: 1.703273\n",
      "Ep: 627, steps: 9, D loss: 0.183191, acc:  78%, G loss: 1.595450\n",
      "Ep: 627, steps: 10, D loss: 0.243493, acc:  56%, G loss: 1.741907\n",
      "Ep: 627, steps: 11, D loss: 0.279073, acc:  41%, G loss: 1.396561\n",
      "Ep: 627, steps: 12, D loss: 0.272598, acc:  43%, G loss: 1.467661\n",
      "Ep: 627, steps: 13, D loss: 0.275376, acc:  44%, G loss: 1.461403\n",
      "Ep: 627, steps: 14, D loss: 0.264288, acc:  46%, G loss: 1.576392\n",
      "Ep: 627, steps: 15, D loss: 0.253755, acc:  53%, G loss: 1.597186\n",
      "Ep: 627, steps: 16, D loss: 0.230248, acc:  61%, G loss: 1.516336\n",
      "Ep: 627, steps: 17, D loss: 0.228924, acc:  63%, G loss: 1.543561\n",
      "Ep: 627, steps: 18, D loss: 0.218641, acc:  64%, G loss: 1.612589\n",
      "Ep: 627, steps: 19, D loss: 0.180316, acc:  79%, G loss: 1.665741\n",
      "Ep: 627, steps: 20, D loss: 0.267085, acc:  40%, G loss: 1.597337\n",
      "Ep: 627, steps: 21, D loss: 0.182901, acc:  73%, G loss: 1.616388\n",
      "Ep: 627, steps: 22, D loss: 0.219901, acc:  66%, G loss: 1.856487\n",
      "Ep: 627, steps: 23, D loss: 0.198913, acc:  74%, G loss: 1.603213\n",
      "Ep: 627, steps: 24, D loss: 0.246789, acc:  56%, G loss: 1.584618\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 628, steps: 1, D loss: 0.242519, acc:  57%, G loss: 1.675677\n",
      "Ep: 628, steps: 2, D loss: 0.248228, acc:  53%, G loss: 1.426907\n",
      "Ep: 628, steps: 3, D loss: 0.185837, acc:  78%, G loss: 1.932131\n",
      "Ep: 628, steps: 4, D loss: 0.175421, acc:  86%, G loss: 1.752648\n",
      "Ep: 628, steps: 5, D loss: 0.316548, acc:  40%, G loss: 1.634153\n",
      "Ep: 628, steps: 6, D loss: 0.229662, acc:  57%, G loss: 1.554720\n",
      "Ep: 628, steps: 7, D loss: 0.312176, acc:  32%, G loss: 1.463894\n",
      "Ep: 628, steps: 8, D loss: 0.231000, acc:  60%, G loss: 1.732366\n",
      "Ep: 628, steps: 9, D loss: 0.236824, acc:  62%, G loss: 1.571356\n",
      "Ep: 628, steps: 10, D loss: 0.195947, acc:  74%, G loss: 1.611404\n",
      "Ep: 628, steps: 11, D loss: 0.259370, acc:  49%, G loss: 1.821422\n",
      "Ep: 628, steps: 12, D loss: 0.294433, acc:  34%, G loss: 1.384000\n",
      "Ep: 628, steps: 13, D loss: 0.276327, acc:  42%, G loss: 1.341536\n",
      "Ep: 628, steps: 14, D loss: 0.285350, acc:  36%, G loss: 1.468909\n",
      "Ep: 628, steps: 15, D loss: 0.245836, acc:  53%, G loss: 1.534134\n",
      "Ep: 628, steps: 16, D loss: 0.243129, acc:  58%, G loss: 1.572778\n",
      "Ep: 628, steps: 17, D loss: 0.230164, acc:  63%, G loss: 1.588984\n",
      "Ep: 628, steps: 18, D loss: 0.228092, acc:  64%, G loss: 1.533782\n",
      "Ep: 628, steps: 19, D loss: 0.222667, acc:  63%, G loss: 1.595810\n",
      "Ep: 628, steps: 20, D loss: 0.179534, acc:  80%, G loss: 1.757867\n",
      "Ep: 628, steps: 21, D loss: 0.266737, acc:  40%, G loss: 1.429708\n",
      "Ep: 628, steps: 22, D loss: 0.199393, acc:  71%, G loss: 1.603257\n",
      "Ep: 628, steps: 23, D loss: 0.225200, acc:  63%, G loss: 1.816886\n",
      "Ep: 628, steps: 24, D loss: 0.213512, acc:  69%, G loss: 1.513476\n",
      "Ep: 628, steps: 25, D loss: 0.263835, acc:  52%, G loss: 1.554543\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 629, steps: 1, D loss: 0.247777, acc:  54%, G loss: 1.684062\n",
      "Ep: 629, steps: 2, D loss: 0.237129, acc:  57%, G loss: 1.461991\n",
      "Ep: 629, steps: 3, D loss: 0.162063, acc:  82%, G loss: 1.910458\n",
      "Ep: 629, steps: 4, D loss: 0.178654, acc:  84%, G loss: 1.775893\n",
      "Ep: 629, steps: 5, D loss: 0.294536, acc:  42%, G loss: 1.635275\n",
      "Ep: 629, steps: 6, D loss: 0.227677, acc:  59%, G loss: 1.569542\n",
      "Ep: 629, steps: 7, D loss: 0.325439, acc:  29%, G loss: 1.482283\n",
      "Ep: 629, steps: 8, D loss: 0.230360, acc:  62%, G loss: 1.734608\n",
      "Ep: 629, steps: 9, D loss: 0.234737, acc:  60%, G loss: 1.596387\n",
      "Ep: 629, steps: 10, D loss: 0.186762, acc:  76%, G loss: 1.562009\n",
      "Ep: 629, steps: 11, D loss: 0.232888, acc:  60%, G loss: 1.737886\n",
      "Ep: 629, steps: 12, D loss: 0.286336, acc:  40%, G loss: 1.365620\n",
      "Ep: 629, steps: 13, D loss: 0.284737, acc:  34%, G loss: 1.395965\n",
      "Ep: 629, steps: 14, D loss: 0.269108, acc:  44%, G loss: 1.454746\n",
      "Ep: 629, steps: 15, D loss: 0.257248, acc:  51%, G loss: 1.609882\n",
      "Ep: 629, steps: 16, D loss: 0.247168, acc:  56%, G loss: 1.559165\n",
      "Ep: 629, steps: 17, D loss: 0.216750, acc:  69%, G loss: 1.490150\n",
      "Ep: 629, steps: 18, D loss: 0.231485, acc:  61%, G loss: 1.597212\n",
      "Ep: 629, steps: 19, D loss: 0.212593, acc:  66%, G loss: 1.619255\n",
      "Ep: 629, steps: 20, D loss: 0.212079, acc:  69%, G loss: 1.664774\n",
      "Ep: 629, steps: 21, D loss: 0.265757, acc:  42%, G loss: 1.411672\n",
      "Ep: 629, steps: 22, D loss: 0.191054, acc:  73%, G loss: 1.551183\n",
      "Ep: 629, steps: 23, D loss: 0.209983, acc:  69%, G loss: 1.817299\n",
      "Saved Model\n",
      "Ep: 629, steps: 24, D loss: 0.220283, acc:  66%, G loss: 1.565981\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 630, steps: 1, D loss: 0.248565, acc:  56%, G loss: 1.629091\n",
      "Ep: 630, steps: 2, D loss: 0.247140, acc:  54%, G loss: 1.459353\n",
      "Ep: 630, steps: 3, D loss: 0.170788, acc:  78%, G loss: 1.912751\n",
      "Ep: 630, steps: 4, D loss: 0.185887, acc:  81%, G loss: 1.801999\n",
      "Ep: 630, steps: 5, D loss: 0.309164, acc:  39%, G loss: 1.604869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 630, steps: 6, D loss: 0.233555, acc:  60%, G loss: 1.586706\n",
      "Ep: 630, steps: 7, D loss: 0.302195, acc:  34%, G loss: 1.465974\n",
      "Ep: 630, steps: 8, D loss: 0.230138, acc:  64%, G loss: 1.738209\n",
      "Ep: 630, steps: 9, D loss: 0.231739, acc:  63%, G loss: 1.663493\n",
      "Ep: 630, steps: 10, D loss: 0.175260, acc:  79%, G loss: 1.630362\n",
      "Ep: 630, steps: 11, D loss: 0.259884, acc:  50%, G loss: 1.781345\n",
      "Ep: 630, steps: 12, D loss: 0.298220, acc:  34%, G loss: 1.364373\n",
      "Ep: 630, steps: 13, D loss: 0.287888, acc:  37%, G loss: 1.437680\n",
      "Ep: 630, steps: 14, D loss: 0.276678, acc:  40%, G loss: 1.463477\n",
      "Ep: 630, steps: 15, D loss: 0.254853, acc:  49%, G loss: 1.587906\n",
      "Ep: 630, steps: 16, D loss: 0.269704, acc:  44%, G loss: 1.583988\n",
      "Ep: 630, steps: 17, D loss: 0.226719, acc:  63%, G loss: 1.540274\n",
      "Ep: 630, steps: 18, D loss: 0.237093, acc:  61%, G loss: 1.533419\n",
      "Ep: 630, steps: 19, D loss: 0.201836, acc:  69%, G loss: 1.602070\n",
      "Ep: 630, steps: 20, D loss: 0.170574, acc:  81%, G loss: 1.648147\n",
      "Ep: 630, steps: 21, D loss: 0.272265, acc:  38%, G loss: 1.505369\n",
      "Ep: 630, steps: 22, D loss: 0.192789, acc:  71%, G loss: 1.694643\n",
      "Ep: 630, steps: 23, D loss: 0.213789, acc:  69%, G loss: 1.893540\n",
      "Ep: 630, steps: 24, D loss: 0.216053, acc:  66%, G loss: 1.615096\n",
      "Ep: 630, steps: 25, D loss: 0.249290, acc:  55%, G loss: 1.508013\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 631, steps: 1, D loss: 0.259481, acc:  52%, G loss: 1.789525\n",
      "Ep: 631, steps: 2, D loss: 0.253465, acc:  52%, G loss: 1.407311\n",
      "Ep: 631, steps: 3, D loss: 0.175760, acc:  80%, G loss: 1.884964\n",
      "Ep: 631, steps: 4, D loss: 0.189354, acc:  82%, G loss: 1.739804\n",
      "Ep: 631, steps: 5, D loss: 0.289863, acc:  44%, G loss: 1.609353\n",
      "Ep: 631, steps: 6, D loss: 0.248863, acc:  55%, G loss: 1.587781\n",
      "Ep: 631, steps: 7, D loss: 0.311646, acc:  32%, G loss: 1.543509\n",
      "Ep: 631, steps: 8, D loss: 0.235749, acc:  59%, G loss: 1.730484\n",
      "Ep: 631, steps: 9, D loss: 0.237239, acc:  61%, G loss: 1.560459\n",
      "Ep: 631, steps: 10, D loss: 0.197081, acc:  74%, G loss: 1.559535\n",
      "Ep: 631, steps: 11, D loss: 0.256163, acc:  50%, G loss: 1.714769\n",
      "Ep: 631, steps: 12, D loss: 0.288683, acc:  37%, G loss: 1.362759\n",
      "Ep: 631, steps: 13, D loss: 0.277855, acc:  40%, G loss: 1.392165\n",
      "Ep: 631, steps: 14, D loss: 0.263967, acc:  47%, G loss: 1.463182\n",
      "Ep: 631, steps: 15, D loss: 0.253164, acc:  53%, G loss: 1.626140\n",
      "Ep: 631, steps: 16, D loss: 0.246165, acc:  56%, G loss: 1.557273\n",
      "Ep: 631, steps: 17, D loss: 0.226523, acc:  63%, G loss: 1.570718\n",
      "Ep: 631, steps: 18, D loss: 0.230005, acc:  62%, G loss: 1.563271\n",
      "Ep: 631, steps: 19, D loss: 0.223190, acc:  65%, G loss: 1.590902\n",
      "Ep: 631, steps: 20, D loss: 0.202948, acc:  71%, G loss: 1.685214\n",
      "Ep: 631, steps: 21, D loss: 0.263883, acc:  40%, G loss: 1.429372\n",
      "Ep: 631, steps: 22, D loss: 0.202417, acc:  71%, G loss: 1.549651\n",
      "Ep: 631, steps: 23, D loss: 0.221534, acc:  65%, G loss: 1.814104\n",
      "Ep: 631, steps: 24, D loss: 0.214768, acc:  70%, G loss: 1.549530\n",
      "Ep: 631, steps: 25, D loss: 0.272031, acc:  48%, G loss: 1.538901\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 632, steps: 1, D loss: 0.230314, acc:  63%, G loss: 1.718001\n",
      "Ep: 632, steps: 2, D loss: 0.246922, acc:  55%, G loss: 1.460057\n",
      "Ep: 632, steps: 3, D loss: 0.159495, acc:  86%, G loss: 1.873777\n",
      "Ep: 632, steps: 4, D loss: 0.196184, acc:  79%, G loss: 1.754308\n",
      "Ep: 632, steps: 5, D loss: 0.276357, acc:  49%, G loss: 1.581163\n",
      "Ep: 632, steps: 6, D loss: 0.233893, acc:  56%, G loss: 1.555718\n",
      "Ep: 632, steps: 7, D loss: 0.309703, acc:  36%, G loss: 1.476721\n",
      "Ep: 632, steps: 8, D loss: 0.227905, acc:  63%, G loss: 1.677718\n",
      "Ep: 632, steps: 9, D loss: 0.248332, acc:  55%, G loss: 1.628913\n",
      "Ep: 632, steps: 10, D loss: 0.177620, acc:  79%, G loss: 1.598880\n",
      "Ep: 632, steps: 11, D loss: 0.264360, acc:  48%, G loss: 1.780498\n",
      "Ep: 632, steps: 12, D loss: 0.287067, acc:  38%, G loss: 1.420565\n",
      "Ep: 632, steps: 13, D loss: 0.289868, acc:  35%, G loss: 1.435406\n",
      "Ep: 632, steps: 14, D loss: 0.271531, acc:  43%, G loss: 1.484898\n",
      "Ep: 632, steps: 15, D loss: 0.241852, acc:  55%, G loss: 1.607620\n",
      "Ep: 632, steps: 16, D loss: 0.247618, acc:  56%, G loss: 1.551006\n",
      "Ep: 632, steps: 17, D loss: 0.215037, acc:  69%, G loss: 1.530734\n",
      "Ep: 632, steps: 18, D loss: 0.227544, acc:  63%, G loss: 1.562594\n",
      "Ep: 632, steps: 19, D loss: 0.216390, acc:  67%, G loss: 1.635188\n",
      "Ep: 632, steps: 20, D loss: 0.189156, acc:  77%, G loss: 1.662298\n",
      "Ep: 632, steps: 21, D loss: 0.269018, acc:  39%, G loss: 1.479370\n",
      "Saved Model\n",
      "Ep: 632, steps: 22, D loss: 0.211644, acc:  69%, G loss: 1.684347\n",
      "Ep: 632, steps: 23, D loss: 0.222198, acc:  64%, G loss: 1.594691\n",
      "Ep: 632, steps: 24, D loss: 0.255610, acc:  52%, G loss: 1.622254\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 633, steps: 1, D loss: 0.247337, acc:  58%, G loss: 1.646233\n",
      "Ep: 633, steps: 2, D loss: 0.239469, acc:  57%, G loss: 1.429920\n",
      "Ep: 633, steps: 3, D loss: 0.180528, acc:  78%, G loss: 1.888841\n",
      "Ep: 633, steps: 4, D loss: 0.194925, acc:  79%, G loss: 1.691322\n",
      "Ep: 633, steps: 5, D loss: 0.290750, acc:  49%, G loss: 1.634417\n",
      "Ep: 633, steps: 6, D loss: 0.228553, acc:  60%, G loss: 1.584485\n",
      "Ep: 633, steps: 7, D loss: 0.309362, acc:  33%, G loss: 1.479895\n",
      "Ep: 633, steps: 8, D loss: 0.229772, acc:  61%, G loss: 1.691833\n",
      "Ep: 633, steps: 9, D loss: 0.249764, acc:  54%, G loss: 1.581330\n",
      "Ep: 633, steps: 10, D loss: 0.188978, acc:  75%, G loss: 1.605438\n",
      "Ep: 633, steps: 11, D loss: 0.266215, acc:  46%, G loss: 1.699302\n",
      "Ep: 633, steps: 12, D loss: 0.293050, acc:  36%, G loss: 1.405966\n",
      "Ep: 633, steps: 13, D loss: 0.274563, acc:  40%, G loss: 1.436579\n",
      "Ep: 633, steps: 14, D loss: 0.276127, acc:  43%, G loss: 1.464096\n",
      "Ep: 633, steps: 15, D loss: 0.252094, acc:  51%, G loss: 1.576664\n",
      "Ep: 633, steps: 16, D loss: 0.257733, acc:  51%, G loss: 1.553623\n",
      "Ep: 633, steps: 17, D loss: 0.216363, acc:  67%, G loss: 1.520537\n",
      "Ep: 633, steps: 18, D loss: 0.230776, acc:  61%, G loss: 1.553973\n",
      "Ep: 633, steps: 19, D loss: 0.215795, acc:  66%, G loss: 1.545518\n",
      "Ep: 633, steps: 20, D loss: 0.186106, acc:  78%, G loss: 1.682037\n",
      "Ep: 633, steps: 21, D loss: 0.262200, acc:  42%, G loss: 1.565372\n",
      "Ep: 633, steps: 22, D loss: 0.195744, acc:  70%, G loss: 1.566506\n",
      "Ep: 633, steps: 23, D loss: 0.225144, acc:  64%, G loss: 1.816375\n",
      "Ep: 633, steps: 24, D loss: 0.208998, acc:  71%, G loss: 1.554772\n",
      "Ep: 633, steps: 25, D loss: 0.257060, acc:  53%, G loss: 1.594283\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 634, steps: 1, D loss: 0.251915, acc:  56%, G loss: 1.625258\n",
      "Ep: 634, steps: 2, D loss: 0.247225, acc:  54%, G loss: 1.362429\n",
      "Ep: 634, steps: 3, D loss: 0.177261, acc:  77%, G loss: 1.912925\n",
      "Ep: 634, steps: 4, D loss: 0.185959, acc:  82%, G loss: 1.769352\n",
      "Ep: 634, steps: 5, D loss: 0.297803, acc:  42%, G loss: 1.672470\n",
      "Ep: 634, steps: 6, D loss: 0.223648, acc:  59%, G loss: 1.619423\n",
      "Ep: 634, steps: 7, D loss: 0.341005, acc:  27%, G loss: 1.546622\n",
      "Ep: 634, steps: 8, D loss: 0.232603, acc:  62%, G loss: 1.697885\n",
      "Ep: 634, steps: 9, D loss: 0.257503, acc:  52%, G loss: 1.579720\n",
      "Ep: 634, steps: 10, D loss: 0.195769, acc:  71%, G loss: 1.629853\n",
      "Ep: 634, steps: 11, D loss: 0.240765, acc:  54%, G loss: 1.946649\n",
      "Ep: 634, steps: 12, D loss: 0.296425, acc:  35%, G loss: 1.418502\n",
      "Ep: 634, steps: 13, D loss: 0.292554, acc:  32%, G loss: 1.387731\n",
      "Ep: 634, steps: 14, D loss: 0.273170, acc:  44%, G loss: 1.477101\n",
      "Ep: 634, steps: 15, D loss: 0.232237, acc:  59%, G loss: 1.595033\n",
      "Ep: 634, steps: 16, D loss: 0.263086, acc:  48%, G loss: 1.527150\n",
      "Ep: 634, steps: 17, D loss: 0.208508, acc:  71%, G loss: 1.505577\n",
      "Ep: 634, steps: 18, D loss: 0.246739, acc:  55%, G loss: 1.565722\n",
      "Ep: 634, steps: 19, D loss: 0.213897, acc:  69%, G loss: 1.584813\n",
      "Ep: 634, steps: 20, D loss: 0.226409, acc:  67%, G loss: 1.680445\n",
      "Ep: 634, steps: 21, D loss: 0.283086, acc:  35%, G loss: 1.470824\n",
      "Ep: 634, steps: 22, D loss: 0.217414, acc:  64%, G loss: 1.635385\n",
      "Ep: 634, steps: 23, D loss: 0.215167, acc:  67%, G loss: 1.769047\n",
      "Ep: 634, steps: 24, D loss: 0.210459, acc:  70%, G loss: 1.526768\n",
      "Ep: 634, steps: 25, D loss: 0.268647, acc:  54%, G loss: 1.541251\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 635, steps: 1, D loss: 0.237992, acc:  60%, G loss: 1.638599\n",
      "Ep: 635, steps: 2, D loss: 0.249105, acc:  53%, G loss: 1.432380\n",
      "Ep: 635, steps: 3, D loss: 0.178704, acc:  76%, G loss: 1.842144\n",
      "Ep: 635, steps: 4, D loss: 0.200830, acc:  77%, G loss: 1.645500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 635, steps: 5, D loss: 0.266642, acc:  50%, G loss: 1.661011\n",
      "Ep: 635, steps: 6, D loss: 0.233024, acc:  56%, G loss: 1.518838\n",
      "Ep: 635, steps: 7, D loss: 0.294074, acc:  37%, G loss: 1.364608\n",
      "Ep: 635, steps: 8, D loss: 0.230434, acc:  62%, G loss: 1.626216\n",
      "Ep: 635, steps: 9, D loss: 0.227375, acc:  64%, G loss: 1.582552\n",
      "Ep: 635, steps: 10, D loss: 0.181752, acc:  80%, G loss: 1.571418\n",
      "Ep: 635, steps: 11, D loss: 0.240584, acc:  57%, G loss: 1.796216\n",
      "Ep: 635, steps: 12, D loss: 0.274209, acc:  44%, G loss: 1.424693\n",
      "Ep: 635, steps: 13, D loss: 0.284916, acc:  35%, G loss: 1.416203\n",
      "Ep: 635, steps: 14, D loss: 0.267075, acc:  45%, G loss: 1.434673\n",
      "Ep: 635, steps: 15, D loss: 0.260036, acc:  47%, G loss: 1.560595\n",
      "Ep: 635, steps: 16, D loss: 0.246194, acc:  55%, G loss: 1.547794\n",
      "Ep: 635, steps: 17, D loss: 0.219179, acc:  67%, G loss: 1.513838\n",
      "Ep: 635, steps: 18, D loss: 0.231296, acc:  62%, G loss: 1.524851\n",
      "Ep: 635, steps: 19, D loss: 0.225364, acc:  62%, G loss: 1.572559\n",
      "Saved Model\n",
      "Ep: 635, steps: 20, D loss: 0.186643, acc:  75%, G loss: 1.718413\n",
      "Ep: 635, steps: 21, D loss: 0.203031, acc:  66%, G loss: 1.589623\n",
      "Ep: 635, steps: 22, D loss: 0.212050, acc:  68%, G loss: 1.795212\n",
      "Ep: 635, steps: 23, D loss: 0.214290, acc:  68%, G loss: 1.529182\n",
      "Ep: 635, steps: 24, D loss: 0.261419, acc:  51%, G loss: 1.543508\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 636, steps: 1, D loss: 0.218403, acc:  66%, G loss: 1.750399\n",
      "Ep: 636, steps: 2, D loss: 0.228383, acc:  61%, G loss: 1.437595\n",
      "Ep: 636, steps: 3, D loss: 0.162954, acc:  83%, G loss: 1.942789\n",
      "Ep: 636, steps: 4, D loss: 0.169380, acc:  88%, G loss: 1.725364\n",
      "Ep: 636, steps: 5, D loss: 0.308482, acc:  37%, G loss: 1.631720\n",
      "Ep: 636, steps: 6, D loss: 0.232850, acc:  57%, G loss: 1.596918\n",
      "Ep: 636, steps: 7, D loss: 0.351026, acc:  26%, G loss: 1.529931\n",
      "Ep: 636, steps: 8, D loss: 0.232296, acc:  62%, G loss: 1.694983\n",
      "Ep: 636, steps: 9, D loss: 0.223509, acc:  66%, G loss: 1.635533\n",
      "Ep: 636, steps: 10, D loss: 0.169815, acc:  83%, G loss: 1.618401\n",
      "Ep: 636, steps: 11, D loss: 0.259506, acc:  52%, G loss: 1.763551\n",
      "Ep: 636, steps: 12, D loss: 0.300010, acc:  33%, G loss: 1.368879\n",
      "Ep: 636, steps: 13, D loss: 0.286521, acc:  36%, G loss: 1.357182\n",
      "Ep: 636, steps: 14, D loss: 0.274418, acc:  43%, G loss: 1.458918\n",
      "Ep: 636, steps: 15, D loss: 0.239699, acc:  56%, G loss: 1.586230\n",
      "Ep: 636, steps: 16, D loss: 0.259827, acc:  51%, G loss: 1.554727\n",
      "Ep: 636, steps: 17, D loss: 0.230116, acc:  61%, G loss: 1.550607\n",
      "Ep: 636, steps: 18, D loss: 0.235287, acc:  61%, G loss: 1.563879\n",
      "Ep: 636, steps: 19, D loss: 0.212122, acc:  68%, G loss: 1.628374\n",
      "Ep: 636, steps: 20, D loss: 0.189171, acc:  78%, G loss: 1.696608\n",
      "Ep: 636, steps: 21, D loss: 0.272203, acc:  38%, G loss: 1.489722\n",
      "Ep: 636, steps: 22, D loss: 0.201448, acc:  73%, G loss: 1.537006\n",
      "Ep: 636, steps: 23, D loss: 0.230991, acc:  60%, G loss: 1.795948\n",
      "Ep: 636, steps: 24, D loss: 0.215569, acc:  66%, G loss: 1.496900\n",
      "Ep: 636, steps: 25, D loss: 0.263967, acc:  53%, G loss: 1.576659\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 637, steps: 1, D loss: 0.244588, acc:  57%, G loss: 1.728355\n",
      "Ep: 637, steps: 2, D loss: 0.245492, acc:  54%, G loss: 1.457588\n",
      "Ep: 637, steps: 3, D loss: 0.181682, acc:  77%, G loss: 1.854584\n",
      "Ep: 637, steps: 4, D loss: 0.199318, acc:  78%, G loss: 1.665238\n",
      "Ep: 637, steps: 5, D loss: 0.273089, acc:  49%, G loss: 1.634226\n",
      "Ep: 637, steps: 6, D loss: 0.223773, acc:  60%, G loss: 1.539506\n",
      "Ep: 637, steps: 7, D loss: 0.307215, acc:  36%, G loss: 1.553306\n",
      "Ep: 637, steps: 8, D loss: 0.218822, acc:  66%, G loss: 1.661319\n",
      "Ep: 637, steps: 9, D loss: 0.248147, acc:  58%, G loss: 1.605426\n",
      "Ep: 637, steps: 10, D loss: 0.189779, acc:  76%, G loss: 1.681962\n",
      "Ep: 637, steps: 11, D loss: 0.242278, acc:  54%, G loss: 1.822701\n",
      "Ep: 637, steps: 12, D loss: 0.289940, acc:  37%, G loss: 1.402672\n",
      "Ep: 637, steps: 13, D loss: 0.283262, acc:  36%, G loss: 1.453525\n",
      "Ep: 637, steps: 14, D loss: 0.276514, acc:  41%, G loss: 1.441932\n",
      "Ep: 637, steps: 15, D loss: 0.238875, acc:  56%, G loss: 1.532902\n",
      "Ep: 637, steps: 16, D loss: 0.248462, acc:  55%, G loss: 1.531974\n",
      "Ep: 637, steps: 17, D loss: 0.208223, acc:  72%, G loss: 1.519646\n",
      "Ep: 637, steps: 18, D loss: 0.220244, acc:  65%, G loss: 1.553683\n",
      "Ep: 637, steps: 19, D loss: 0.217722, acc:  68%, G loss: 1.555599\n",
      "Ep: 637, steps: 20, D loss: 0.206644, acc:  71%, G loss: 1.708569\n",
      "Ep: 637, steps: 21, D loss: 0.269847, acc:  40%, G loss: 1.486258\n",
      "Ep: 637, steps: 22, D loss: 0.206697, acc:  69%, G loss: 1.560305\n",
      "Ep: 637, steps: 23, D loss: 0.232552, acc:  61%, G loss: 1.781245\n",
      "Ep: 637, steps: 24, D loss: 0.204190, acc:  73%, G loss: 1.560758\n",
      "Ep: 637, steps: 25, D loss: 0.247003, acc:  56%, G loss: 1.572608\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 638, steps: 1, D loss: 0.239347, acc:  60%, G loss: 1.680432\n",
      "Ep: 638, steps: 2, D loss: 0.245879, acc:  55%, G loss: 1.456805\n",
      "Ep: 638, steps: 3, D loss: 0.176214, acc:  78%, G loss: 1.948566\n",
      "Ep: 638, steps: 4, D loss: 0.185367, acc:  82%, G loss: 1.716121\n",
      "Ep: 638, steps: 5, D loss: 0.288211, acc:  45%, G loss: 1.680816\n",
      "Ep: 638, steps: 6, D loss: 0.246650, acc:  54%, G loss: 1.567618\n",
      "Ep: 638, steps: 7, D loss: 0.309427, acc:  31%, G loss: 1.512410\n",
      "Ep: 638, steps: 8, D loss: 0.234183, acc:  60%, G loss: 1.732547\n",
      "Ep: 638, steps: 9, D loss: 0.226956, acc:  65%, G loss: 1.589091\n",
      "Ep: 638, steps: 10, D loss: 0.167880, acc:  81%, G loss: 1.571162\n",
      "Ep: 638, steps: 11, D loss: 0.267802, acc:  46%, G loss: 1.781545\n",
      "Ep: 638, steps: 12, D loss: 0.282351, acc:  39%, G loss: 1.397351\n",
      "Ep: 638, steps: 13, D loss: 0.276644, acc:  40%, G loss: 1.401614\n",
      "Ep: 638, steps: 14, D loss: 0.286814, acc:  39%, G loss: 1.458516\n",
      "Ep: 638, steps: 15, D loss: 0.249535, acc:  51%, G loss: 1.603897\n",
      "Ep: 638, steps: 16, D loss: 0.252151, acc:  53%, G loss: 1.561623\n",
      "Ep: 638, steps: 17, D loss: 0.228562, acc:  64%, G loss: 1.478374\n",
      "Saved Model\n",
      "Ep: 638, steps: 18, D loss: 0.238893, acc:  59%, G loss: 1.548594\n",
      "Ep: 638, steps: 19, D loss: 0.183220, acc:  76%, G loss: 1.640611\n",
      "Ep: 638, steps: 20, D loss: 0.251246, acc:  48%, G loss: 1.644959\n",
      "Ep: 638, steps: 21, D loss: 0.225782, acc:  65%, G loss: 1.628778\n",
      "Ep: 638, steps: 22, D loss: 0.235622, acc:  60%, G loss: 1.774043\n",
      "Ep: 638, steps: 23, D loss: 0.207081, acc:  72%, G loss: 1.548919\n",
      "Ep: 638, steps: 24, D loss: 0.260952, acc:  53%, G loss: 1.825950\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 639, steps: 1, D loss: 0.237675, acc:  59%, G loss: 1.735845\n",
      "Ep: 639, steps: 2, D loss: 0.249015, acc:  54%, G loss: 1.428866\n",
      "Ep: 639, steps: 3, D loss: 0.172654, acc:  81%, G loss: 1.878822\n",
      "Ep: 639, steps: 4, D loss: 0.192285, acc:  79%, G loss: 1.677523\n",
      "Ep: 639, steps: 5, D loss: 0.282124, acc:  48%, G loss: 1.694234\n",
      "Ep: 639, steps: 6, D loss: 0.243765, acc:  55%, G loss: 1.596257\n",
      "Ep: 639, steps: 7, D loss: 0.298582, acc:  38%, G loss: 1.515365\n",
      "Ep: 639, steps: 8, D loss: 0.224180, acc:  66%, G loss: 1.764132\n",
      "Ep: 639, steps: 9, D loss: 0.252792, acc:  57%, G loss: 1.558489\n",
      "Ep: 639, steps: 10, D loss: 0.192184, acc:  75%, G loss: 1.580413\n",
      "Ep: 639, steps: 11, D loss: 0.254951, acc:  49%, G loss: 1.730466\n",
      "Ep: 639, steps: 12, D loss: 0.287525, acc:  36%, G loss: 1.433640\n",
      "Ep: 639, steps: 13, D loss: 0.282140, acc:  36%, G loss: 1.424166\n",
      "Ep: 639, steps: 14, D loss: 0.265389, acc:  46%, G loss: 1.545451\n",
      "Ep: 639, steps: 15, D loss: 0.264911, acc:  45%, G loss: 1.582474\n",
      "Ep: 639, steps: 16, D loss: 0.258038, acc:  51%, G loss: 1.545363\n",
      "Ep: 639, steps: 17, D loss: 0.227893, acc:  64%, G loss: 1.573857\n",
      "Ep: 639, steps: 18, D loss: 0.243564, acc:  57%, G loss: 1.587612\n",
      "Ep: 639, steps: 19, D loss: 0.216800, acc:  67%, G loss: 1.545592\n",
      "Ep: 639, steps: 20, D loss: 0.197461, acc:  72%, G loss: 1.649632\n",
      "Ep: 639, steps: 21, D loss: 0.258686, acc:  44%, G loss: 1.517721\n",
      "Ep: 639, steps: 22, D loss: 0.173773, acc:  77%, G loss: 1.542826\n",
      "Ep: 639, steps: 23, D loss: 0.211380, acc:  68%, G loss: 1.777499\n",
      "Ep: 639, steps: 24, D loss: 0.212135, acc:  70%, G loss: 1.562810\n",
      "Ep: 639, steps: 25, D loss: 0.230531, acc:  60%, G loss: 1.695928\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 640, steps: 1, D loss: 0.260903, acc:  53%, G loss: 1.764456\n",
      "Ep: 640, steps: 2, D loss: 0.256945, acc:  51%, G loss: 1.513160\n",
      "Ep: 640, steps: 3, D loss: 0.175787, acc:  79%, G loss: 1.889385\n",
      "Ep: 640, steps: 4, D loss: 0.199386, acc:  78%, G loss: 1.745311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 640, steps: 5, D loss: 0.277659, acc:  50%, G loss: 1.610705\n",
      "Ep: 640, steps: 6, D loss: 0.231248, acc:  57%, G loss: 1.636910\n",
      "Ep: 640, steps: 7, D loss: 0.318839, acc:  30%, G loss: 1.493478\n",
      "Ep: 640, steps: 8, D loss: 0.228148, acc:  63%, G loss: 1.706552\n",
      "Ep: 640, steps: 9, D loss: 0.242723, acc:  58%, G loss: 1.601093\n",
      "Ep: 640, steps: 10, D loss: 0.183606, acc:  77%, G loss: 1.627226\n",
      "Ep: 640, steps: 11, D loss: 0.255829, acc:  49%, G loss: 1.735735\n",
      "Ep: 640, steps: 12, D loss: 0.287636, acc:  35%, G loss: 1.468099\n",
      "Ep: 640, steps: 13, D loss: 0.274007, acc:  41%, G loss: 1.391495\n",
      "Ep: 640, steps: 14, D loss: 0.273823, acc:  43%, G loss: 1.437766\n",
      "Ep: 640, steps: 15, D loss: 0.251204, acc:  51%, G loss: 1.557253\n",
      "Ep: 640, steps: 16, D loss: 0.249672, acc:  54%, G loss: 1.538735\n",
      "Ep: 640, steps: 17, D loss: 0.214725, acc:  69%, G loss: 1.552968\n",
      "Ep: 640, steps: 18, D loss: 0.235523, acc:  59%, G loss: 1.562995\n",
      "Ep: 640, steps: 19, D loss: 0.222180, acc:  65%, G loss: 1.599990\n",
      "Ep: 640, steps: 20, D loss: 0.179687, acc:  79%, G loss: 1.769142\n",
      "Ep: 640, steps: 21, D loss: 0.265585, acc:  40%, G loss: 1.471794\n",
      "Ep: 640, steps: 22, D loss: 0.225991, acc:  65%, G loss: 1.636963\n",
      "Ep: 640, steps: 23, D loss: 0.220092, acc:  65%, G loss: 1.785775\n",
      "Ep: 640, steps: 24, D loss: 0.214915, acc:  67%, G loss: 1.577826\n",
      "Ep: 640, steps: 25, D loss: 0.260521, acc:  53%, G loss: 1.673232\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 641, steps: 1, D loss: 0.232002, acc:  61%, G loss: 1.776301\n",
      "Ep: 641, steps: 2, D loss: 0.238177, acc:  57%, G loss: 1.519067\n",
      "Ep: 641, steps: 3, D loss: 0.178993, acc:  79%, G loss: 1.944072\n",
      "Ep: 641, steps: 4, D loss: 0.190208, acc:  81%, G loss: 1.678033\n",
      "Ep: 641, steps: 5, D loss: 0.278280, acc:  50%, G loss: 1.598446\n",
      "Ep: 641, steps: 6, D loss: 0.240966, acc:  55%, G loss: 1.636306\n",
      "Ep: 641, steps: 7, D loss: 0.323183, acc:  29%, G loss: 1.450934\n",
      "Ep: 641, steps: 8, D loss: 0.232203, acc:  61%, G loss: 1.667463\n",
      "Ep: 641, steps: 9, D loss: 0.245091, acc:  57%, G loss: 1.583478\n",
      "Ep: 641, steps: 10, D loss: 0.182656, acc:  78%, G loss: 1.544846\n",
      "Ep: 641, steps: 11, D loss: 0.257751, acc:  51%, G loss: 1.729745\n",
      "Ep: 641, steps: 12, D loss: 0.290435, acc:  37%, G loss: 1.374833\n",
      "Ep: 641, steps: 13, D loss: 0.284064, acc:  36%, G loss: 1.423351\n",
      "Ep: 641, steps: 14, D loss: 0.272402, acc:  43%, G loss: 1.456282\n",
      "Saved Model\n",
      "Ep: 641, steps: 15, D loss: 0.249862, acc:  51%, G loss: 1.577299\n",
      "Ep: 641, steps: 16, D loss: 0.212775, acc:  69%, G loss: 1.625143\n",
      "Ep: 641, steps: 17, D loss: 0.228267, acc:  62%, G loss: 1.581032\n",
      "Ep: 641, steps: 18, D loss: 0.215710, acc:  67%, G loss: 1.587729\n",
      "Ep: 641, steps: 19, D loss: 0.187470, acc:  77%, G loss: 1.693431\n",
      "Ep: 641, steps: 20, D loss: 0.261359, acc:  44%, G loss: 1.617484\n",
      "Ep: 641, steps: 21, D loss: 0.198702, acc:  70%, G loss: 1.685833\n",
      "Ep: 641, steps: 22, D loss: 0.220019, acc:  67%, G loss: 1.840613\n",
      "Ep: 641, steps: 23, D loss: 0.209650, acc:  66%, G loss: 1.577112\n",
      "Ep: 641, steps: 24, D loss: 0.259046, acc:  51%, G loss: 1.488032\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 642, steps: 1, D loss: 0.246283, acc:  59%, G loss: 1.753377\n",
      "Ep: 642, steps: 2, D loss: 0.238398, acc:  57%, G loss: 1.468334\n",
      "Ep: 642, steps: 3, D loss: 0.178019, acc:  79%, G loss: 1.934392\n",
      "Ep: 642, steps: 4, D loss: 0.185225, acc:  80%, G loss: 1.724482\n",
      "Ep: 642, steps: 5, D loss: 0.309747, acc:  41%, G loss: 1.578450\n",
      "Ep: 642, steps: 6, D loss: 0.237856, acc:  55%, G loss: 1.587332\n",
      "Ep: 642, steps: 7, D loss: 0.319645, acc:  32%, G loss: 1.514686\n",
      "Ep: 642, steps: 8, D loss: 0.220498, acc:  67%, G loss: 1.681293\n",
      "Ep: 642, steps: 9, D loss: 0.250203, acc:  55%, G loss: 1.611184\n",
      "Ep: 642, steps: 10, D loss: 0.183564, acc:  79%, G loss: 1.592851\n",
      "Ep: 642, steps: 11, D loss: 0.256964, acc:  52%, G loss: 1.762898\n",
      "Ep: 642, steps: 12, D loss: 0.277407, acc:  41%, G loss: 1.360991\n",
      "Ep: 642, steps: 13, D loss: 0.297727, acc:  32%, G loss: 1.350104\n",
      "Ep: 642, steps: 14, D loss: 0.279463, acc:  40%, G loss: 1.448457\n",
      "Ep: 642, steps: 15, D loss: 0.247088, acc:  54%, G loss: 1.578904\n",
      "Ep: 642, steps: 16, D loss: 0.253586, acc:  54%, G loss: 1.551230\n",
      "Ep: 642, steps: 17, D loss: 0.226756, acc:  62%, G loss: 1.524459\n",
      "Ep: 642, steps: 18, D loss: 0.219872, acc:  65%, G loss: 1.633422\n",
      "Ep: 642, steps: 19, D loss: 0.222658, acc:  64%, G loss: 1.568988\n",
      "Ep: 642, steps: 20, D loss: 0.188287, acc:  75%, G loss: 1.671748\n",
      "Ep: 642, steps: 21, D loss: 0.255983, acc:  46%, G loss: 1.557378\n",
      "Ep: 642, steps: 22, D loss: 0.212371, acc:  66%, G loss: 1.628555\n",
      "Ep: 642, steps: 23, D loss: 0.230721, acc:  61%, G loss: 1.770590\n",
      "Ep: 642, steps: 24, D loss: 0.214597, acc:  67%, G loss: 1.554119\n",
      "Ep: 642, steps: 25, D loss: 0.252212, acc:  54%, G loss: 1.572459\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 643, steps: 1, D loss: 0.260585, acc:  52%, G loss: 1.717881\n",
      "Ep: 643, steps: 2, D loss: 0.246179, acc:  52%, G loss: 1.510276\n",
      "Ep: 643, steps: 3, D loss: 0.173385, acc:  81%, G loss: 1.894821\n",
      "Ep: 643, steps: 4, D loss: 0.190760, acc:  81%, G loss: 1.711684\n",
      "Ep: 643, steps: 5, D loss: 0.269299, acc:  51%, G loss: 1.617062\n",
      "Ep: 643, steps: 6, D loss: 0.228322, acc:  58%, G loss: 1.573049\n",
      "Ep: 643, steps: 7, D loss: 0.318350, acc:  30%, G loss: 1.586880\n",
      "Ep: 643, steps: 8, D loss: 0.220406, acc:  66%, G loss: 1.750257\n",
      "Ep: 643, steps: 9, D loss: 0.233209, acc:  62%, G loss: 1.619998\n",
      "Ep: 643, steps: 10, D loss: 0.196694, acc:  72%, G loss: 1.494582\n",
      "Ep: 643, steps: 11, D loss: 0.255544, acc:  51%, G loss: 1.703999\n",
      "Ep: 643, steps: 12, D loss: 0.297585, acc:  34%, G loss: 1.356375\n",
      "Ep: 643, steps: 13, D loss: 0.280351, acc:  37%, G loss: 1.378660\n",
      "Ep: 643, steps: 14, D loss: 0.271858, acc:  44%, G loss: 1.509192\n",
      "Ep: 643, steps: 15, D loss: 0.248120, acc:  53%, G loss: 1.620808\n",
      "Ep: 643, steps: 16, D loss: 0.250396, acc:  54%, G loss: 1.585152\n",
      "Ep: 643, steps: 17, D loss: 0.218437, acc:  66%, G loss: 1.513402\n",
      "Ep: 643, steps: 18, D loss: 0.229282, acc:  61%, G loss: 1.563862\n",
      "Ep: 643, steps: 19, D loss: 0.209175, acc:  69%, G loss: 1.572526\n",
      "Ep: 643, steps: 20, D loss: 0.185560, acc:  78%, G loss: 1.704569\n",
      "Ep: 643, steps: 21, D loss: 0.254623, acc:  48%, G loss: 1.462232\n",
      "Ep: 643, steps: 22, D loss: 0.197129, acc:  67%, G loss: 1.611321\n",
      "Ep: 643, steps: 23, D loss: 0.227960, acc:  62%, G loss: 1.787251\n",
      "Ep: 643, steps: 24, D loss: 0.209924, acc:  68%, G loss: 1.547226\n",
      "Ep: 643, steps: 25, D loss: 0.255424, acc:  55%, G loss: 1.767388\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 644, steps: 1, D loss: 0.239452, acc:  59%, G loss: 1.837473\n",
      "Ep: 644, steps: 2, D loss: 0.242610, acc:  55%, G loss: 1.568898\n",
      "Ep: 644, steps: 3, D loss: 0.176522, acc:  79%, G loss: 1.922206\n",
      "Ep: 644, steps: 4, D loss: 0.193620, acc:  81%, G loss: 1.730282\n",
      "Ep: 644, steps: 5, D loss: 0.265290, acc:  51%, G loss: 1.593036\n",
      "Ep: 644, steps: 6, D loss: 0.230989, acc:  56%, G loss: 1.559058\n",
      "Ep: 644, steps: 7, D loss: 0.312552, acc:  30%, G loss: 1.507456\n",
      "Ep: 644, steps: 8, D loss: 0.228141, acc:  63%, G loss: 1.700150\n",
      "Ep: 644, steps: 9, D loss: 0.253515, acc:  53%, G loss: 1.588974\n",
      "Ep: 644, steps: 10, D loss: 0.195304, acc:  74%, G loss: 1.551425\n",
      "Ep: 644, steps: 11, D loss: 0.258689, acc:  48%, G loss: 1.736331\n",
      "Ep: 644, steps: 12, D loss: 0.289131, acc:  38%, G loss: 1.384884\n",
      "Saved Model\n",
      "Ep: 644, steps: 13, D loss: 0.290848, acc:  34%, G loss: 1.388073\n",
      "Ep: 644, steps: 14, D loss: 0.220155, acc:  63%, G loss: 1.632704\n",
      "Ep: 644, steps: 15, D loss: 0.252124, acc:  54%, G loss: 1.573131\n",
      "Ep: 644, steps: 16, D loss: 0.233888, acc:  59%, G loss: 1.540060\n",
      "Ep: 644, steps: 17, D loss: 0.218712, acc:  65%, G loss: 1.636251\n",
      "Ep: 644, steps: 18, D loss: 0.215118, acc:  65%, G loss: 1.594393\n",
      "Ep: 644, steps: 19, D loss: 0.210219, acc:  69%, G loss: 1.689874\n",
      "Ep: 644, steps: 20, D loss: 0.277555, acc:  36%, G loss: 1.430464\n",
      "Ep: 644, steps: 21, D loss: 0.223997, acc:  67%, G loss: 1.676742\n",
      "Ep: 644, steps: 22, D loss: 0.220808, acc:  65%, G loss: 1.859823\n",
      "Ep: 644, steps: 23, D loss: 0.203599, acc:  70%, G loss: 1.557693\n",
      "Ep: 644, steps: 24, D loss: 0.253725, acc:  56%, G loss: 1.707545\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 645, steps: 1, D loss: 0.229447, acc:  63%, G loss: 1.754343\n",
      "Ep: 645, steps: 2, D loss: 0.246571, acc:  55%, G loss: 1.592103\n",
      "Ep: 645, steps: 3, D loss: 0.161775, acc:  84%, G loss: 1.924672\n",
      "Ep: 645, steps: 4, D loss: 0.177235, acc:  86%, G loss: 1.703875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 645, steps: 5, D loss: 0.288878, acc:  46%, G loss: 1.623460\n",
      "Ep: 645, steps: 6, D loss: 0.225357, acc:  57%, G loss: 1.562516\n",
      "Ep: 645, steps: 7, D loss: 0.320434, acc:  30%, G loss: 1.476159\n",
      "Ep: 645, steps: 8, D loss: 0.228343, acc:  63%, G loss: 1.706625\n",
      "Ep: 645, steps: 9, D loss: 0.229176, acc:  63%, G loss: 1.645913\n",
      "Ep: 645, steps: 10, D loss: 0.190162, acc:  74%, G loss: 1.573078\n",
      "Ep: 645, steps: 11, D loss: 0.251831, acc:  52%, G loss: 1.752199\n",
      "Ep: 645, steps: 12, D loss: 0.283215, acc:  40%, G loss: 1.331335\n",
      "Ep: 645, steps: 13, D loss: 0.283127, acc:  36%, G loss: 1.400516\n",
      "Ep: 645, steps: 14, D loss: 0.288606, acc:  37%, G loss: 1.460973\n",
      "Ep: 645, steps: 15, D loss: 0.234812, acc:  58%, G loss: 1.588035\n",
      "Ep: 645, steps: 16, D loss: 0.254527, acc:  53%, G loss: 1.533442\n",
      "Ep: 645, steps: 17, D loss: 0.223449, acc:  65%, G loss: 1.504452\n",
      "Ep: 645, steps: 18, D loss: 0.248006, acc:  54%, G loss: 1.636573\n",
      "Ep: 645, steps: 19, D loss: 0.214609, acc:  65%, G loss: 1.587563\n",
      "Ep: 645, steps: 20, D loss: 0.193957, acc:  75%, G loss: 1.772589\n",
      "Ep: 645, steps: 21, D loss: 0.268714, acc:  38%, G loss: 1.521823\n",
      "Ep: 645, steps: 22, D loss: 0.206480, acc:  70%, G loss: 1.576186\n",
      "Ep: 645, steps: 23, D loss: 0.222902, acc:  63%, G loss: 1.795915\n",
      "Ep: 645, steps: 24, D loss: 0.201909, acc:  70%, G loss: 1.503262\n",
      "Ep: 645, steps: 25, D loss: 0.262649, acc:  52%, G loss: 1.568603\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 646, steps: 1, D loss: 0.251959, acc:  54%, G loss: 1.743943\n",
      "Ep: 646, steps: 2, D loss: 0.242629, acc:  56%, G loss: 1.531149\n",
      "Ep: 646, steps: 3, D loss: 0.176421, acc:  80%, G loss: 1.917115\n",
      "Ep: 646, steps: 4, D loss: 0.190253, acc:  81%, G loss: 1.687894\n",
      "Ep: 646, steps: 5, D loss: 0.286460, acc:  46%, G loss: 1.613330\n",
      "Ep: 646, steps: 6, D loss: 0.234496, acc:  56%, G loss: 1.534877\n",
      "Ep: 646, steps: 7, D loss: 0.321626, acc:  31%, G loss: 1.472219\n",
      "Ep: 646, steps: 8, D loss: 0.231121, acc:  62%, G loss: 1.698014\n",
      "Ep: 646, steps: 9, D loss: 0.253136, acc:  53%, G loss: 1.565365\n",
      "Ep: 646, steps: 10, D loss: 0.212624, acc:  68%, G loss: 1.582201\n",
      "Ep: 646, steps: 11, D loss: 0.262700, acc:  46%, G loss: 1.746680\n",
      "Ep: 646, steps: 12, D loss: 0.297517, acc:  32%, G loss: 1.331823\n",
      "Ep: 646, steps: 13, D loss: 0.283350, acc:  36%, G loss: 1.380498\n",
      "Ep: 646, steps: 14, D loss: 0.268781, acc:  45%, G loss: 1.436831\n",
      "Ep: 646, steps: 15, D loss: 0.248647, acc:  52%, G loss: 1.583076\n",
      "Ep: 646, steps: 16, D loss: 0.246897, acc:  56%, G loss: 1.565233\n",
      "Ep: 646, steps: 17, D loss: 0.222249, acc:  64%, G loss: 1.597600\n",
      "Ep: 646, steps: 18, D loss: 0.230392, acc:  61%, G loss: 1.582823\n",
      "Ep: 646, steps: 19, D loss: 0.218041, acc:  66%, G loss: 1.606313\n",
      "Ep: 646, steps: 20, D loss: 0.180731, acc:  81%, G loss: 1.729989\n",
      "Ep: 646, steps: 21, D loss: 0.266361, acc:  41%, G loss: 1.451680\n",
      "Ep: 646, steps: 22, D loss: 0.224027, acc:  65%, G loss: 1.704311\n",
      "Ep: 646, steps: 23, D loss: 0.206952, acc:  70%, G loss: 1.751660\n",
      "Ep: 646, steps: 24, D loss: 0.202371, acc:  72%, G loss: 1.492346\n",
      "Ep: 646, steps: 25, D loss: 0.243641, acc:  59%, G loss: 1.532162\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 647, steps: 1, D loss: 0.247864, acc:  56%, G loss: 1.676077\n",
      "Ep: 647, steps: 2, D loss: 0.232310, acc:  58%, G loss: 1.444036\n",
      "Ep: 647, steps: 3, D loss: 0.163167, acc:  83%, G loss: 1.939877\n",
      "Ep: 647, steps: 4, D loss: 0.186431, acc:  82%, G loss: 1.694093\n",
      "Ep: 647, steps: 5, D loss: 0.305955, acc:  41%, G loss: 1.630437\n",
      "Ep: 647, steps: 6, D loss: 0.240523, acc:  56%, G loss: 1.555801\n",
      "Ep: 647, steps: 7, D loss: 0.324097, acc:  28%, G loss: 1.444126\n",
      "Ep: 647, steps: 8, D loss: 0.227726, acc:  64%, G loss: 1.668795\n",
      "Ep: 647, steps: 9, D loss: 0.230291, acc:  62%, G loss: 1.601604\n",
      "Ep: 647, steps: 10, D loss: 0.178253, acc:  80%, G loss: 1.572414\n",
      "Saved Model\n",
      "Ep: 647, steps: 11, D loss: 0.258571, acc:  52%, G loss: 1.716385\n",
      "Ep: 647, steps: 12, D loss: 0.299687, acc:  29%, G loss: 1.394117\n",
      "Ep: 647, steps: 13, D loss: 0.286009, acc:  39%, G loss: 1.477840\n",
      "Ep: 647, steps: 14, D loss: 0.237797, acc:  58%, G loss: 1.605985\n",
      "Ep: 647, steps: 15, D loss: 0.246782, acc:  57%, G loss: 1.593445\n",
      "Ep: 647, steps: 16, D loss: 0.213556, acc:  69%, G loss: 1.499247\n",
      "Ep: 647, steps: 17, D loss: 0.224566, acc:  63%, G loss: 1.606127\n",
      "Ep: 647, steps: 18, D loss: 0.212444, acc:  65%, G loss: 1.602211\n",
      "Ep: 647, steps: 19, D loss: 0.189741, acc:  74%, G loss: 1.767266\n",
      "Ep: 647, steps: 20, D loss: 0.276221, acc:  36%, G loss: 1.532356\n",
      "Ep: 647, steps: 21, D loss: 0.190844, acc:  75%, G loss: 1.643184\n",
      "Ep: 647, steps: 22, D loss: 0.235354, acc:  60%, G loss: 1.790656\n",
      "Ep: 647, steps: 23, D loss: 0.210241, acc:  67%, G loss: 1.488424\n",
      "Ep: 647, steps: 24, D loss: 0.253683, acc:  58%, G loss: 1.686633\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 648, steps: 1, D loss: 0.258626, acc:  53%, G loss: 1.688649\n",
      "Ep: 648, steps: 2, D loss: 0.251918, acc:  52%, G loss: 1.520280\n",
      "Ep: 648, steps: 3, D loss: 0.169440, acc:  83%, G loss: 1.914316\n",
      "Ep: 648, steps: 4, D loss: 0.185940, acc:  82%, G loss: 1.770430\n",
      "Ep: 648, steps: 5, D loss: 0.285256, acc:  47%, G loss: 1.636711\n",
      "Ep: 648, steps: 6, D loss: 0.241404, acc:  56%, G loss: 1.529214\n",
      "Ep: 648, steps: 7, D loss: 0.315841, acc:  30%, G loss: 1.567886\n",
      "Ep: 648, steps: 8, D loss: 0.216073, acc:  66%, G loss: 1.663371\n",
      "Ep: 648, steps: 9, D loss: 0.248341, acc:  54%, G loss: 1.665511\n",
      "Ep: 648, steps: 10, D loss: 0.189926, acc:  71%, G loss: 1.641187\n",
      "Ep: 648, steps: 11, D loss: 0.258395, acc:  49%, G loss: 1.703967\n",
      "Ep: 648, steps: 12, D loss: 0.300530, acc:  34%, G loss: 1.320220\n",
      "Ep: 648, steps: 13, D loss: 0.289522, acc:  34%, G loss: 1.371958\n",
      "Ep: 648, steps: 14, D loss: 0.281365, acc:  40%, G loss: 1.485715\n",
      "Ep: 648, steps: 15, D loss: 0.249171, acc:  51%, G loss: 1.590910\n",
      "Ep: 648, steps: 16, D loss: 0.259119, acc:  51%, G loss: 1.531087\n",
      "Ep: 648, steps: 17, D loss: 0.213493, acc:  72%, G loss: 1.518152\n",
      "Ep: 648, steps: 18, D loss: 0.229054, acc:  62%, G loss: 1.576969\n",
      "Ep: 648, steps: 19, D loss: 0.216025, acc:  64%, G loss: 1.611144\n",
      "Ep: 648, steps: 20, D loss: 0.183727, acc:  78%, G loss: 1.739738\n",
      "Ep: 648, steps: 21, D loss: 0.263875, acc:  43%, G loss: 1.563436\n",
      "Ep: 648, steps: 22, D loss: 0.210198, acc:  70%, G loss: 1.626319\n",
      "Ep: 648, steps: 23, D loss: 0.229978, acc:  62%, G loss: 1.856648\n",
      "Ep: 648, steps: 24, D loss: 0.215101, acc:  68%, G loss: 1.566486\n",
      "Ep: 648, steps: 25, D loss: 0.248252, acc:  55%, G loss: 1.591835\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 649, steps: 1, D loss: 0.249893, acc:  57%, G loss: 1.711484\n",
      "Ep: 649, steps: 2, D loss: 0.244279, acc:  55%, G loss: 1.512343\n",
      "Ep: 649, steps: 3, D loss: 0.174305, acc:  81%, G loss: 1.927906\n",
      "Ep: 649, steps: 4, D loss: 0.193979, acc:  80%, G loss: 1.698511\n",
      "Ep: 649, steps: 5, D loss: 0.289342, acc:  46%, G loss: 1.640704\n",
      "Ep: 649, steps: 6, D loss: 0.240924, acc:  54%, G loss: 1.578728\n",
      "Ep: 649, steps: 7, D loss: 0.320211, acc:  31%, G loss: 1.520221\n",
      "Ep: 649, steps: 8, D loss: 0.225685, acc:  65%, G loss: 1.707270\n",
      "Ep: 649, steps: 9, D loss: 0.244361, acc:  56%, G loss: 1.591856\n",
      "Ep: 649, steps: 10, D loss: 0.200045, acc:  71%, G loss: 1.602500\n",
      "Ep: 649, steps: 11, D loss: 0.259846, acc:  50%, G loss: 1.721257\n",
      "Ep: 649, steps: 12, D loss: 0.294770, acc:  35%, G loss: 1.371905\n",
      "Ep: 649, steps: 13, D loss: 0.290535, acc:  33%, G loss: 1.387930\n",
      "Ep: 649, steps: 14, D loss: 0.270391, acc:  44%, G loss: 1.447186\n",
      "Ep: 649, steps: 15, D loss: 0.250197, acc:  51%, G loss: 1.537727\n",
      "Ep: 649, steps: 16, D loss: 0.242300, acc:  58%, G loss: 1.554709\n",
      "Ep: 649, steps: 17, D loss: 0.223934, acc:  64%, G loss: 1.522317\n",
      "Ep: 649, steps: 18, D loss: 0.233299, acc:  61%, G loss: 1.596412\n",
      "Ep: 649, steps: 19, D loss: 0.197608, acc:  71%, G loss: 1.631584\n",
      "Ep: 649, steps: 20, D loss: 0.179861, acc:  80%, G loss: 1.740570\n",
      "Ep: 649, steps: 21, D loss: 0.270866, acc:  39%, G loss: 1.480673\n",
      "Ep: 649, steps: 22, D loss: 0.187480, acc:  72%, G loss: 1.674227\n",
      "Ep: 649, steps: 23, D loss: 0.234766, acc:  57%, G loss: 1.862509\n",
      "Ep: 649, steps: 24, D loss: 0.213607, acc:  67%, G loss: 1.567197\n",
      "Ep: 649, steps: 25, D loss: 0.247926, acc:  56%, G loss: 1.704554\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 650, steps: 1, D loss: 0.249617, acc:  57%, G loss: 1.670985\n",
      "Ep: 650, steps: 2, D loss: 0.241716, acc:  56%, G loss: 1.497981\n",
      "Ep: 650, steps: 3, D loss: 0.175348, acc:  80%, G loss: 2.048304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 650, steps: 4, D loss: 0.192160, acc:  81%, G loss: 1.738091\n",
      "Ep: 650, steps: 5, D loss: 0.280069, acc:  47%, G loss: 1.619669\n",
      "Ep: 650, steps: 6, D loss: 0.248943, acc:  55%, G loss: 1.559212\n",
      "Ep: 650, steps: 7, D loss: 0.293364, acc:  37%, G loss: 1.449170\n",
      "Ep: 650, steps: 8, D loss: 0.217521, acc:  68%, G loss: 1.678939\n",
      "Saved Model\n",
      "Ep: 650, steps: 9, D loss: 0.239817, acc:  58%, G loss: 1.577016\n",
      "Ep: 650, steps: 10, D loss: 0.276025, acc:  41%, G loss: 1.687327\n",
      "Ep: 650, steps: 11, D loss: 0.272513, acc:  40%, G loss: 1.343444\n",
      "Ep: 650, steps: 12, D loss: 0.284376, acc:  35%, G loss: 1.392075\n",
      "Ep: 650, steps: 13, D loss: 0.264279, acc:  48%, G loss: 1.481248\n",
      "Ep: 650, steps: 14, D loss: 0.266359, acc:  50%, G loss: 1.558268\n",
      "Ep: 650, steps: 15, D loss: 0.237365, acc:  60%, G loss: 1.590614\n",
      "Ep: 650, steps: 16, D loss: 0.230583, acc:  63%, G loss: 1.531538\n",
      "Ep: 650, steps: 17, D loss: 0.237043, acc:  58%, G loss: 1.574656\n",
      "Ep: 650, steps: 18, D loss: 0.225990, acc:  64%, G loss: 1.564196\n",
      "Ep: 650, steps: 19, D loss: 0.187590, acc:  77%, G loss: 1.705853\n",
      "Ep: 650, steps: 20, D loss: 0.256419, acc:  44%, G loss: 1.534725\n",
      "Ep: 650, steps: 21, D loss: 0.218920, acc:  67%, G loss: 1.625952\n",
      "Ep: 650, steps: 22, D loss: 0.213395, acc:  69%, G loss: 1.807588\n",
      "Ep: 650, steps: 23, D loss: 0.197873, acc:  74%, G loss: 1.501908\n",
      "Ep: 650, steps: 24, D loss: 0.251149, acc:  56%, G loss: 1.479144\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 651, steps: 1, D loss: 0.255097, acc:  51%, G loss: 1.648437\n",
      "Ep: 651, steps: 2, D loss: 0.243501, acc:  55%, G loss: 1.513889\n",
      "Ep: 651, steps: 3, D loss: 0.177401, acc:  79%, G loss: 1.903802\n",
      "Ep: 651, steps: 4, D loss: 0.196306, acc:  78%, G loss: 1.647212\n",
      "Ep: 651, steps: 5, D loss: 0.297339, acc:  42%, G loss: 1.563379\n",
      "Ep: 651, steps: 6, D loss: 0.233242, acc:  57%, G loss: 1.549287\n",
      "Ep: 651, steps: 7, D loss: 0.323237, acc:  29%, G loss: 1.477349\n",
      "Ep: 651, steps: 8, D loss: 0.220961, acc:  65%, G loss: 1.676185\n",
      "Ep: 651, steps: 9, D loss: 0.255618, acc:  53%, G loss: 1.646079\n",
      "Ep: 651, steps: 10, D loss: 0.189194, acc:  76%, G loss: 1.590494\n",
      "Ep: 651, steps: 11, D loss: 0.245612, acc:  56%, G loss: 1.782113\n",
      "Ep: 651, steps: 12, D loss: 0.279897, acc:  41%, G loss: 1.337742\n",
      "Ep: 651, steps: 13, D loss: 0.302048, acc:  29%, G loss: 1.372310\n",
      "Ep: 651, steps: 14, D loss: 0.272831, acc:  43%, G loss: 1.443354\n",
      "Ep: 651, steps: 15, D loss: 0.254327, acc:  52%, G loss: 1.668798\n",
      "Ep: 651, steps: 16, D loss: 0.242003, acc:  58%, G loss: 1.580283\n",
      "Ep: 651, steps: 17, D loss: 0.228989, acc:  64%, G loss: 1.556724\n",
      "Ep: 651, steps: 18, D loss: 0.221911, acc:  64%, G loss: 1.621932\n",
      "Ep: 651, steps: 19, D loss: 0.212130, acc:  68%, G loss: 1.602644\n",
      "Ep: 651, steps: 20, D loss: 0.188768, acc:  77%, G loss: 1.708119\n",
      "Ep: 651, steps: 21, D loss: 0.270900, acc:  39%, G loss: 1.572317\n",
      "Ep: 651, steps: 22, D loss: 0.219112, acc:  65%, G loss: 1.544415\n",
      "Ep: 651, steps: 23, D loss: 0.217125, acc:  67%, G loss: 1.820883\n",
      "Ep: 651, steps: 24, D loss: 0.198835, acc:  72%, G loss: 1.537349\n",
      "Ep: 651, steps: 25, D loss: 0.255072, acc:  55%, G loss: 1.582591\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 652, steps: 1, D loss: 0.248787, acc:  55%, G loss: 1.712970\n",
      "Ep: 652, steps: 2, D loss: 0.249761, acc:  53%, G loss: 1.528645\n",
      "Ep: 652, steps: 3, D loss: 0.173369, acc:  83%, G loss: 1.850062\n",
      "Ep: 652, steps: 4, D loss: 0.199019, acc:  77%, G loss: 1.648716\n",
      "Ep: 652, steps: 5, D loss: 0.287431, acc:  47%, G loss: 1.659040\n",
      "Ep: 652, steps: 6, D loss: 0.230527, acc:  55%, G loss: 1.564262\n",
      "Ep: 652, steps: 7, D loss: 0.310013, acc:  32%, G loss: 1.505718\n",
      "Ep: 652, steps: 8, D loss: 0.225754, acc:  64%, G loss: 1.775967\n",
      "Ep: 652, steps: 9, D loss: 0.238255, acc:  58%, G loss: 1.615512\n",
      "Ep: 652, steps: 10, D loss: 0.208480, acc:  68%, G loss: 1.612125\n",
      "Ep: 652, steps: 11, D loss: 0.261465, acc:  49%, G loss: 1.794829\n",
      "Ep: 652, steps: 12, D loss: 0.297456, acc:  34%, G loss: 1.314714\n",
      "Ep: 652, steps: 13, D loss: 0.294577, acc:  32%, G loss: 1.382233\n",
      "Ep: 652, steps: 14, D loss: 0.276726, acc:  41%, G loss: 1.480708\n",
      "Ep: 652, steps: 15, D loss: 0.243052, acc:  55%, G loss: 1.622148\n",
      "Ep: 652, steps: 16, D loss: 0.241226, acc:  59%, G loss: 1.558028\n",
      "Ep: 652, steps: 17, D loss: 0.215976, acc:  67%, G loss: 1.536260\n",
      "Ep: 652, steps: 18, D loss: 0.244509, acc:  58%, G loss: 1.659156\n",
      "Ep: 652, steps: 19, D loss: 0.209457, acc:  69%, G loss: 1.626910\n",
      "Ep: 652, steps: 20, D loss: 0.185658, acc:  75%, G loss: 1.779609\n",
      "Ep: 652, steps: 21, D loss: 0.270292, acc:  40%, G loss: 1.642308\n",
      "Ep: 652, steps: 22, D loss: 0.218426, acc:  66%, G loss: 1.607145\n",
      "Ep: 652, steps: 23, D loss: 0.225542, acc:  64%, G loss: 1.799876\n",
      "Ep: 652, steps: 24, D loss: 0.205017, acc:  71%, G loss: 1.558864\n",
      "Ep: 652, steps: 25, D loss: 0.241899, acc:  59%, G loss: 1.585426\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 653, steps: 1, D loss: 0.237753, acc:  61%, G loss: 1.813693\n",
      "Ep: 653, steps: 2, D loss: 0.239115, acc:  57%, G loss: 1.500818\n",
      "Ep: 653, steps: 3, D loss: 0.177565, acc:  80%, G loss: 1.851968\n",
      "Ep: 653, steps: 4, D loss: 0.183164, acc:  81%, G loss: 1.678256\n",
      "Ep: 653, steps: 5, D loss: 0.293954, acc:  45%, G loss: 1.582032\n",
      "Ep: 653, steps: 6, D loss: 0.242282, acc:  55%, G loss: 1.554739\n",
      "Saved Model\n",
      "Ep: 653, steps: 7, D loss: 0.318697, acc:  34%, G loss: 1.636223\n",
      "Ep: 653, steps: 8, D loss: 0.224692, acc:  65%, G loss: 1.671603\n",
      "Ep: 653, steps: 9, D loss: 0.194753, acc:  73%, G loss: 1.705932\n",
      "Ep: 653, steps: 10, D loss: 0.232089, acc:  59%, G loss: 1.835491\n",
      "Ep: 653, steps: 11, D loss: 0.298149, acc:  33%, G loss: 1.390881\n",
      "Ep: 653, steps: 12, D loss: 0.288695, acc:  32%, G loss: 1.321424\n",
      "Ep: 653, steps: 13, D loss: 0.271218, acc:  42%, G loss: 1.432543\n",
      "Ep: 653, steps: 14, D loss: 0.236604, acc:  58%, G loss: 1.569071\n",
      "Ep: 653, steps: 15, D loss: 0.245088, acc:  58%, G loss: 1.526743\n",
      "Ep: 653, steps: 16, D loss: 0.223089, acc:  66%, G loss: 1.512975\n",
      "Ep: 653, steps: 17, D loss: 0.214227, acc:  68%, G loss: 1.675989\n",
      "Ep: 653, steps: 18, D loss: 0.211668, acc:  67%, G loss: 1.627566\n",
      "Ep: 653, steps: 19, D loss: 0.191552, acc:  73%, G loss: 1.678798\n",
      "Ep: 653, steps: 20, D loss: 0.264028, acc:  42%, G loss: 1.597923\n",
      "Ep: 653, steps: 21, D loss: 0.222839, acc:  65%, G loss: 1.672230\n",
      "Ep: 653, steps: 22, D loss: 0.214970, acc:  67%, G loss: 1.781615\n",
      "Ep: 653, steps: 23, D loss: 0.199681, acc:  72%, G loss: 1.513162\n",
      "Ep: 653, steps: 24, D loss: 0.257916, acc:  53%, G loss: 1.615750\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 654, steps: 1, D loss: 0.264184, acc:  48%, G loss: 1.713246\n",
      "Ep: 654, steps: 2, D loss: 0.243971, acc:  55%, G loss: 1.465591\n",
      "Ep: 654, steps: 3, D loss: 0.169555, acc:  81%, G loss: 1.870983\n",
      "Ep: 654, steps: 4, D loss: 0.194081, acc:  79%, G loss: 1.651687\n",
      "Ep: 654, steps: 5, D loss: 0.281009, acc:  48%, G loss: 1.573026\n",
      "Ep: 654, steps: 6, D loss: 0.235708, acc:  54%, G loss: 1.531324\n",
      "Ep: 654, steps: 7, D loss: 0.328324, acc:  29%, G loss: 1.492759\n",
      "Ep: 654, steps: 8, D loss: 0.234578, acc:  61%, G loss: 1.613795\n",
      "Ep: 654, steps: 9, D loss: 0.244127, acc:  58%, G loss: 1.546515\n",
      "Ep: 654, steps: 10, D loss: 0.182518, acc:  78%, G loss: 1.584398\n",
      "Ep: 654, steps: 11, D loss: 0.255580, acc:  50%, G loss: 1.738498\n",
      "Ep: 654, steps: 12, D loss: 0.290994, acc:  36%, G loss: 1.369474\n",
      "Ep: 654, steps: 13, D loss: 0.282259, acc:  38%, G loss: 1.396724\n",
      "Ep: 654, steps: 14, D loss: 0.278403, acc:  43%, G loss: 1.474392\n",
      "Ep: 654, steps: 15, D loss: 0.263716, acc:  48%, G loss: 1.594566\n",
      "Ep: 654, steps: 16, D loss: 0.245456, acc:  56%, G loss: 1.599337\n",
      "Ep: 654, steps: 17, D loss: 0.219794, acc:  67%, G loss: 1.528383\n",
      "Ep: 654, steps: 18, D loss: 0.234023, acc:  59%, G loss: 1.610173\n",
      "Ep: 654, steps: 19, D loss: 0.216057, acc:  66%, G loss: 1.628559\n",
      "Ep: 654, steps: 20, D loss: 0.182868, acc:  79%, G loss: 1.775937\n",
      "Ep: 654, steps: 21, D loss: 0.277258, acc:  36%, G loss: 1.559092\n",
      "Ep: 654, steps: 22, D loss: 0.202385, acc:  69%, G loss: 1.709190\n",
      "Ep: 654, steps: 23, D loss: 0.226432, acc:  64%, G loss: 1.841655\n",
      "Ep: 654, steps: 24, D loss: 0.204918, acc:  72%, G loss: 1.532465\n",
      "Ep: 654, steps: 25, D loss: 0.249967, acc:  57%, G loss: 1.483392\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 655, steps: 1, D loss: 0.250017, acc:  56%, G loss: 1.705805\n",
      "Ep: 655, steps: 2, D loss: 0.235453, acc:  58%, G loss: 1.477440\n",
      "Ep: 655, steps: 3, D loss: 0.170003, acc:  82%, G loss: 1.868542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 655, steps: 4, D loss: 0.191196, acc:  81%, G loss: 1.707603\n",
      "Ep: 655, steps: 5, D loss: 0.270770, acc:  52%, G loss: 1.568447\n",
      "Ep: 655, steps: 6, D loss: 0.252891, acc:  51%, G loss: 1.543200\n",
      "Ep: 655, steps: 7, D loss: 0.320928, acc:  29%, G loss: 1.503466\n",
      "Ep: 655, steps: 8, D loss: 0.225540, acc:  63%, G loss: 1.615271\n",
      "Ep: 655, steps: 9, D loss: 0.240109, acc:  59%, G loss: 1.598271\n",
      "Ep: 655, steps: 10, D loss: 0.204257, acc:  69%, G loss: 1.572593\n",
      "Ep: 655, steps: 11, D loss: 0.246874, acc:  54%, G loss: 1.750718\n",
      "Ep: 655, steps: 12, D loss: 0.283334, acc:  36%, G loss: 1.362544\n",
      "Ep: 655, steps: 13, D loss: 0.279193, acc:  39%, G loss: 1.422579\n",
      "Ep: 655, steps: 14, D loss: 0.293597, acc:  35%, G loss: 1.481182\n",
      "Ep: 655, steps: 15, D loss: 0.236382, acc:  58%, G loss: 1.551843\n",
      "Ep: 655, steps: 16, D loss: 0.256864, acc:  51%, G loss: 1.597214\n",
      "Ep: 655, steps: 17, D loss: 0.209896, acc:  71%, G loss: 1.514996\n",
      "Ep: 655, steps: 18, D loss: 0.244663, acc:  56%, G loss: 1.707423\n",
      "Ep: 655, steps: 19, D loss: 0.207608, acc:  70%, G loss: 1.632485\n",
      "Ep: 655, steps: 20, D loss: 0.199543, acc:  73%, G loss: 1.782100\n",
      "Ep: 655, steps: 21, D loss: 0.268071, acc:  39%, G loss: 1.472276\n",
      "Ep: 655, steps: 22, D loss: 0.206607, acc:  71%, G loss: 1.512191\n",
      "Ep: 655, steps: 23, D loss: 0.232348, acc:  60%, G loss: 1.804631\n",
      "Ep: 655, steps: 24, D loss: 0.204637, acc:  71%, G loss: 1.521550\n",
      "Ep: 655, steps: 25, D loss: 0.256003, acc:  54%, G loss: 1.565816\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 656, steps: 1, D loss: 0.233136, acc:  62%, G loss: 1.693763\n",
      "Ep: 656, steps: 2, D loss: 0.243195, acc:  58%, G loss: 1.464011\n",
      "Ep: 656, steps: 3, D loss: 0.149856, acc:  87%, G loss: 1.870764\n",
      "Ep: 656, steps: 4, D loss: 0.201580, acc:  75%, G loss: 1.633963\n",
      "Saved Model\n",
      "Ep: 656, steps: 5, D loss: 0.284142, acc:  49%, G loss: 1.675170\n",
      "Ep: 656, steps: 6, D loss: 0.328706, acc:  28%, G loss: 1.585869\n",
      "Ep: 656, steps: 7, D loss: 0.237710, acc:  58%, G loss: 1.659041\n",
      "Ep: 656, steps: 8, D loss: 0.267971, acc:  49%, G loss: 1.569685\n",
      "Ep: 656, steps: 9, D loss: 0.190458, acc:  77%, G loss: 1.557914\n",
      "Ep: 656, steps: 10, D loss: 0.269467, acc:  45%, G loss: 1.774367\n",
      "Ep: 656, steps: 11, D loss: 0.293809, acc:  33%, G loss: 1.400730\n",
      "Ep: 656, steps: 12, D loss: 0.275846, acc:  40%, G loss: 1.410820\n",
      "Ep: 656, steps: 13, D loss: 0.274450, acc:  41%, G loss: 1.422838\n",
      "Ep: 656, steps: 14, D loss: 0.253654, acc:  49%, G loss: 1.578939\n",
      "Ep: 656, steps: 15, D loss: 0.254098, acc:  53%, G loss: 1.579681\n",
      "Ep: 656, steps: 16, D loss: 0.214758, acc:  70%, G loss: 1.460977\n",
      "Ep: 656, steps: 17, D loss: 0.229579, acc:  62%, G loss: 1.698806\n",
      "Ep: 656, steps: 18, D loss: 0.213723, acc:  66%, G loss: 1.597159\n",
      "Ep: 656, steps: 19, D loss: 0.184398, acc:  77%, G loss: 1.717369\n",
      "Ep: 656, steps: 20, D loss: 0.276563, acc:  36%, G loss: 1.590445\n",
      "Ep: 656, steps: 21, D loss: 0.197377, acc:  74%, G loss: 1.594846\n",
      "Ep: 656, steps: 22, D loss: 0.221331, acc:  66%, G loss: 1.788866\n",
      "Ep: 656, steps: 23, D loss: 0.211528, acc:  68%, G loss: 1.485530\n",
      "Ep: 656, steps: 24, D loss: 0.252974, acc:  54%, G loss: 1.586373\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 657, steps: 1, D loss: 0.244852, acc:  56%, G loss: 1.705204\n",
      "Ep: 657, steps: 2, D loss: 0.245239, acc:  54%, G loss: 1.440637\n",
      "Ep: 657, steps: 3, D loss: 0.161857, acc:  85%, G loss: 1.838035\n",
      "Ep: 657, steps: 4, D loss: 0.172257, acc:  87%, G loss: 1.629037\n",
      "Ep: 657, steps: 5, D loss: 0.285098, acc:  46%, G loss: 1.631445\n",
      "Ep: 657, steps: 6, D loss: 0.249138, acc:  53%, G loss: 1.585450\n",
      "Ep: 657, steps: 7, D loss: 0.290946, acc:  37%, G loss: 1.601729\n",
      "Ep: 657, steps: 8, D loss: 0.219158, acc:  66%, G loss: 1.716803\n",
      "Ep: 657, steps: 9, D loss: 0.238601, acc:  59%, G loss: 1.608875\n",
      "Ep: 657, steps: 10, D loss: 0.187874, acc:  76%, G loss: 1.580717\n",
      "Ep: 657, steps: 11, D loss: 0.253686, acc:  53%, G loss: 1.724530\n",
      "Ep: 657, steps: 12, D loss: 0.296744, acc:  34%, G loss: 1.371842\n",
      "Ep: 657, steps: 13, D loss: 0.291666, acc:  33%, G loss: 1.351082\n",
      "Ep: 657, steps: 14, D loss: 0.268237, acc:  45%, G loss: 1.443907\n",
      "Ep: 657, steps: 15, D loss: 0.249366, acc:  51%, G loss: 1.609716\n",
      "Ep: 657, steps: 16, D loss: 0.246057, acc:  55%, G loss: 1.598369\n",
      "Ep: 657, steps: 17, D loss: 0.217099, acc:  66%, G loss: 1.526322\n",
      "Ep: 657, steps: 18, D loss: 0.230297, acc:  62%, G loss: 1.674186\n",
      "Ep: 657, steps: 19, D loss: 0.206863, acc:  66%, G loss: 1.604354\n",
      "Ep: 657, steps: 20, D loss: 0.175003, acc:  79%, G loss: 1.725225\n",
      "Ep: 657, steps: 21, D loss: 0.263653, acc:  43%, G loss: 1.511072\n",
      "Ep: 657, steps: 22, D loss: 0.208669, acc:  68%, G loss: 1.689720\n",
      "Ep: 657, steps: 23, D loss: 0.225396, acc:  65%, G loss: 1.764702\n",
      "Ep: 657, steps: 24, D loss: 0.201393, acc:  73%, G loss: 1.517980\n",
      "Ep: 657, steps: 25, D loss: 0.259254, acc:  54%, G loss: 1.689945\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 658, steps: 1, D loss: 0.256943, acc:  51%, G loss: 1.733610\n",
      "Ep: 658, steps: 2, D loss: 0.254352, acc:  52%, G loss: 1.531661\n",
      "Ep: 658, steps: 3, D loss: 0.174819, acc:  83%, G loss: 1.853066\n",
      "Ep: 658, steps: 4, D loss: 0.181398, acc:  83%, G loss: 1.641692\n",
      "Ep: 658, steps: 5, D loss: 0.270053, acc:  50%, G loss: 1.563646\n",
      "Ep: 658, steps: 6, D loss: 0.252465, acc:  52%, G loss: 1.523452\n",
      "Ep: 658, steps: 7, D loss: 0.296188, acc:  36%, G loss: 1.456081\n",
      "Ep: 658, steps: 8, D loss: 0.224464, acc:  63%, G loss: 1.727042\n",
      "Ep: 658, steps: 9, D loss: 0.231155, acc:  61%, G loss: 1.614110\n",
      "Ep: 658, steps: 10, D loss: 0.188457, acc:  75%, G loss: 1.558474\n",
      "Ep: 658, steps: 11, D loss: 0.243781, acc:  53%, G loss: 1.787979\n",
      "Ep: 658, steps: 12, D loss: 0.293882, acc:  34%, G loss: 1.339590\n",
      "Ep: 658, steps: 13, D loss: 0.290306, acc:  34%, G loss: 1.385977\n",
      "Ep: 658, steps: 14, D loss: 0.285073, acc:  41%, G loss: 1.459036\n",
      "Ep: 658, steps: 15, D loss: 0.260074, acc:  46%, G loss: 1.556155\n",
      "Ep: 658, steps: 16, D loss: 0.240344, acc:  60%, G loss: 1.581855\n",
      "Ep: 658, steps: 17, D loss: 0.218484, acc:  66%, G loss: 1.553726\n",
      "Ep: 658, steps: 18, D loss: 0.234426, acc:  61%, G loss: 1.622901\n",
      "Ep: 658, steps: 19, D loss: 0.210551, acc:  66%, G loss: 1.611637\n",
      "Ep: 658, steps: 20, D loss: 0.183806, acc:  77%, G loss: 1.741792\n",
      "Ep: 658, steps: 21, D loss: 0.281809, acc:  36%, G loss: 1.491231\n",
      "Ep: 658, steps: 22, D loss: 0.171077, acc:  77%, G loss: 1.563653\n",
      "Ep: 658, steps: 23, D loss: 0.229258, acc:  61%, G loss: 1.838016\n",
      "Ep: 658, steps: 24, D loss: 0.207221, acc:  70%, G loss: 1.583265\n",
      "Ep: 658, steps: 25, D loss: 0.235022, acc:  61%, G loss: 1.740927\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 659, steps: 1, D loss: 0.228760, acc:  63%, G loss: 1.669127\n",
      "Ep: 659, steps: 2, D loss: 0.241210, acc:  57%, G loss: 1.447983\n",
      "Saved Model\n",
      "Ep: 659, steps: 3, D loss: 0.168167, acc:  82%, G loss: 2.048562\n",
      "Ep: 659, steps: 4, D loss: 0.278673, acc:  47%, G loss: 1.660249\n",
      "Ep: 659, steps: 5, D loss: 0.249910, acc:  54%, G loss: 1.491494\n",
      "Ep: 659, steps: 6, D loss: 0.277888, acc:  41%, G loss: 1.546514\n",
      "Ep: 659, steps: 7, D loss: 0.221196, acc:  65%, G loss: 1.780316\n",
      "Ep: 659, steps: 8, D loss: 0.253332, acc:  54%, G loss: 1.620500\n",
      "Ep: 659, steps: 9, D loss: 0.192688, acc:  73%, G loss: 1.599099\n",
      "Ep: 659, steps: 10, D loss: 0.254630, acc:  51%, G loss: 1.800632\n",
      "Ep: 659, steps: 11, D loss: 0.293583, acc:  35%, G loss: 1.381648\n",
      "Ep: 659, steps: 12, D loss: 0.281767, acc:  38%, G loss: 1.379473\n",
      "Ep: 659, steps: 13, D loss: 0.260091, acc:  46%, G loss: 1.477024\n",
      "Ep: 659, steps: 14, D loss: 0.240034, acc:  55%, G loss: 1.551555\n",
      "Ep: 659, steps: 15, D loss: 0.254863, acc:  51%, G loss: 1.538515\n",
      "Ep: 659, steps: 16, D loss: 0.217030, acc:  69%, G loss: 1.546101\n",
      "Ep: 659, steps: 17, D loss: 0.231652, acc:  60%, G loss: 1.623716\n",
      "Ep: 659, steps: 18, D loss: 0.229207, acc:  62%, G loss: 1.641242\n",
      "Ep: 659, steps: 19, D loss: 0.181306, acc:  78%, G loss: 1.722491\n",
      "Ep: 659, steps: 20, D loss: 0.272138, acc:  38%, G loss: 1.495129\n",
      "Ep: 659, steps: 21, D loss: 0.203434, acc:  68%, G loss: 1.662348\n",
      "Ep: 659, steps: 22, D loss: 0.217802, acc:  67%, G loss: 1.795397\n",
      "Ep: 659, steps: 23, D loss: 0.201559, acc:  71%, G loss: 1.625012\n",
      "Ep: 659, steps: 24, D loss: 0.261661, acc:  54%, G loss: 1.580962\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 660, steps: 1, D loss: 0.257532, acc:  50%, G loss: 1.640470\n",
      "Ep: 660, steps: 2, D loss: 0.248329, acc:  54%, G loss: 1.463649\n",
      "Ep: 660, steps: 3, D loss: 0.165558, acc:  84%, G loss: 1.871687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 660, steps: 4, D loss: 0.198163, acc:  79%, G loss: 1.628194\n",
      "Ep: 660, steps: 5, D loss: 0.277823, acc:  48%, G loss: 1.669608\n",
      "Ep: 660, steps: 6, D loss: 0.238444, acc:  55%, G loss: 1.506141\n",
      "Ep: 660, steps: 7, D loss: 0.315618, acc:  31%, G loss: 1.566639\n",
      "Ep: 660, steps: 8, D loss: 0.237445, acc:  59%, G loss: 1.662017\n",
      "Ep: 660, steps: 9, D loss: 0.230306, acc:  62%, G loss: 1.623180\n",
      "Ep: 660, steps: 10, D loss: 0.199081, acc:  73%, G loss: 1.557245\n",
      "Ep: 660, steps: 11, D loss: 0.258180, acc:  50%, G loss: 1.748851\n",
      "Ep: 660, steps: 12, D loss: 0.287361, acc:  37%, G loss: 1.387998\n",
      "Ep: 660, steps: 13, D loss: 0.297647, acc:  29%, G loss: 1.396895\n",
      "Ep: 660, steps: 14, D loss: 0.265606, acc:  46%, G loss: 1.469561\n",
      "Ep: 660, steps: 15, D loss: 0.249560, acc:  51%, G loss: 1.559522\n",
      "Ep: 660, steps: 16, D loss: 0.253489, acc:  52%, G loss: 1.554149\n",
      "Ep: 660, steps: 17, D loss: 0.212343, acc:  71%, G loss: 1.596268\n",
      "Ep: 660, steps: 18, D loss: 0.237123, acc:  59%, G loss: 1.627855\n",
      "Ep: 660, steps: 19, D loss: 0.219528, acc:  63%, G loss: 1.605680\n",
      "Ep: 660, steps: 20, D loss: 0.188727, acc:  77%, G loss: 1.659913\n",
      "Ep: 660, steps: 21, D loss: 0.272665, acc:  39%, G loss: 1.457252\n",
      "Ep: 660, steps: 22, D loss: 0.205662, acc:  69%, G loss: 1.570566\n",
      "Ep: 660, steps: 23, D loss: 0.218621, acc:  68%, G loss: 1.845207\n",
      "Ep: 660, steps: 24, D loss: 0.201220, acc:  72%, G loss: 1.561676\n",
      "Ep: 660, steps: 25, D loss: 0.230872, acc:  62%, G loss: 1.663862\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 661, steps: 1, D loss: 0.248050, acc:  56%, G loss: 1.638603\n",
      "Ep: 661, steps: 2, D loss: 0.242492, acc:  58%, G loss: 1.415715\n",
      "Ep: 661, steps: 3, D loss: 0.162844, acc:  84%, G loss: 1.879377\n",
      "Ep: 661, steps: 4, D loss: 0.188370, acc:  81%, G loss: 1.689711\n",
      "Ep: 661, steps: 5, D loss: 0.278242, acc:  48%, G loss: 1.625287\n",
      "Ep: 661, steps: 6, D loss: 0.246590, acc:  54%, G loss: 1.536347\n",
      "Ep: 661, steps: 7, D loss: 0.322599, acc:  29%, G loss: 1.621286\n",
      "Ep: 661, steps: 8, D loss: 0.241717, acc:  60%, G loss: 1.762558\n",
      "Ep: 661, steps: 9, D loss: 0.241033, acc:  58%, G loss: 1.592984\n",
      "Ep: 661, steps: 10, D loss: 0.184480, acc:  79%, G loss: 1.607692\n",
      "Ep: 661, steps: 11, D loss: 0.245543, acc:  53%, G loss: 1.695513\n",
      "Ep: 661, steps: 12, D loss: 0.292269, acc:  32%, G loss: 1.315965\n",
      "Ep: 661, steps: 13, D loss: 0.288664, acc:  34%, G loss: 1.359971\n",
      "Ep: 661, steps: 14, D loss: 0.261617, acc:  47%, G loss: 1.438494\n",
      "Ep: 661, steps: 15, D loss: 0.250590, acc:  54%, G loss: 1.570823\n",
      "Ep: 661, steps: 16, D loss: 0.235580, acc:  61%, G loss: 1.615232\n",
      "Ep: 661, steps: 17, D loss: 0.218339, acc:  67%, G loss: 1.472084\n",
      "Ep: 661, steps: 18, D loss: 0.243220, acc:  55%, G loss: 1.658069\n",
      "Ep: 661, steps: 19, D loss: 0.214908, acc:  67%, G loss: 1.598459\n",
      "Ep: 661, steps: 20, D loss: 0.190265, acc:  76%, G loss: 1.752063\n",
      "Ep: 661, steps: 21, D loss: 0.261212, acc:  43%, G loss: 1.504380\n",
      "Ep: 661, steps: 22, D loss: 0.209297, acc:  70%, G loss: 1.646663\n",
      "Ep: 661, steps: 23, D loss: 0.218744, acc:  66%, G loss: 1.815170\n",
      "Ep: 661, steps: 24, D loss: 0.207181, acc:  70%, G loss: 1.603216\n",
      "Ep: 661, steps: 25, D loss: 0.249099, acc:  56%, G loss: 1.642491\n",
      "Data exhausted, Re Initialize\n",
      "Saved Model\n",
      "Ep: 662, steps: 1, D loss: 0.245548, acc:  56%, G loss: 1.659806\n",
      "Ep: 662, steps: 2, D loss: 0.175751, acc:  80%, G loss: 1.931363\n",
      "Ep: 662, steps: 3, D loss: 0.208200, acc:  72%, G loss: 1.599410\n",
      "Ep: 662, steps: 4, D loss: 0.277599, acc:  50%, G loss: 1.642182\n",
      "Ep: 662, steps: 5, D loss: 0.249282, acc:  54%, G loss: 1.558090\n",
      "Ep: 662, steps: 6, D loss: 0.297250, acc:  41%, G loss: 1.537411\n",
      "Ep: 662, steps: 7, D loss: 0.224193, acc:  64%, G loss: 1.790454\n",
      "Ep: 662, steps: 8, D loss: 0.251872, acc:  56%, G loss: 1.566527\n",
      "Ep: 662, steps: 9, D loss: 0.188962, acc:  74%, G loss: 1.526676\n",
      "Ep: 662, steps: 10, D loss: 0.250315, acc:  54%, G loss: 1.715305\n",
      "Ep: 662, steps: 11, D loss: 0.294196, acc:  31%, G loss: 1.358182\n",
      "Ep: 662, steps: 12, D loss: 0.285166, acc:  34%, G loss: 1.398597\n",
      "Ep: 662, steps: 13, D loss: 0.280887, acc:  39%, G loss: 1.457416\n",
      "Ep: 662, steps: 14, D loss: 0.256002, acc:  49%, G loss: 1.586522\n",
      "Ep: 662, steps: 15, D loss: 0.248074, acc:  54%, G loss: 1.583945\n",
      "Ep: 662, steps: 16, D loss: 0.224502, acc:  66%, G loss: 1.530691\n",
      "Ep: 662, steps: 17, D loss: 0.236530, acc:  60%, G loss: 1.641847\n",
      "Ep: 662, steps: 18, D loss: 0.208913, acc:  69%, G loss: 1.578449\n",
      "Ep: 662, steps: 19, D loss: 0.180948, acc:  78%, G loss: 1.776689\n",
      "Ep: 662, steps: 20, D loss: 0.260992, acc:  42%, G loss: 1.504967\n",
      "Ep: 662, steps: 21, D loss: 0.237659, acc:  63%, G loss: 1.663248\n",
      "Ep: 662, steps: 22, D loss: 0.214346, acc:  68%, G loss: 1.858371\n",
      "Ep: 662, steps: 23, D loss: 0.203303, acc:  70%, G loss: 1.558417\n",
      "Ep: 662, steps: 24, D loss: 0.260736, acc:  50%, G loss: 1.621473\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 663, steps: 1, D loss: 0.247604, acc:  56%, G loss: 1.695599\n",
      "Ep: 663, steps: 2, D loss: 0.248672, acc:  54%, G loss: 1.490039\n",
      "Ep: 663, steps: 3, D loss: 0.158931, acc:  85%, G loss: 1.801515\n",
      "Ep: 663, steps: 4, D loss: 0.199840, acc:  79%, G loss: 1.624257\n",
      "Ep: 663, steps: 5, D loss: 0.278834, acc:  47%, G loss: 1.672417\n",
      "Ep: 663, steps: 6, D loss: 0.248859, acc:  51%, G loss: 1.507993\n",
      "Ep: 663, steps: 7, D loss: 0.319504, acc:  29%, G loss: 1.536185\n",
      "Ep: 663, steps: 8, D loss: 0.226240, acc:  64%, G loss: 1.782787\n",
      "Ep: 663, steps: 9, D loss: 0.235864, acc:  61%, G loss: 1.535595\n",
      "Ep: 663, steps: 10, D loss: 0.200116, acc:  71%, G loss: 1.582055\n",
      "Ep: 663, steps: 11, D loss: 0.238028, acc:  55%, G loss: 1.779855\n",
      "Ep: 663, steps: 12, D loss: 0.289684, acc:  35%, G loss: 1.297752\n",
      "Ep: 663, steps: 13, D loss: 0.289738, acc:  34%, G loss: 1.362589\n",
      "Ep: 663, steps: 14, D loss: 0.271276, acc:  43%, G loss: 1.429649\n",
      "Ep: 663, steps: 15, D loss: 0.244750, acc:  55%, G loss: 1.577583\n",
      "Ep: 663, steps: 16, D loss: 0.234274, acc:  61%, G loss: 1.551225\n",
      "Ep: 663, steps: 17, D loss: 0.219605, acc:  67%, G loss: 1.592991\n",
      "Ep: 663, steps: 18, D loss: 0.226341, acc:  62%, G loss: 1.681847\n",
      "Ep: 663, steps: 19, D loss: 0.211521, acc:  67%, G loss: 1.620962\n",
      "Ep: 663, steps: 20, D loss: 0.185196, acc:  77%, G loss: 1.688691\n",
      "Ep: 663, steps: 21, D loss: 0.267903, acc:  40%, G loss: 1.442628\n",
      "Ep: 663, steps: 22, D loss: 0.197335, acc:  72%, G loss: 1.549990\n",
      "Ep: 663, steps: 23, D loss: 0.239671, acc:  58%, G loss: 1.751477\n",
      "Ep: 663, steps: 24, D loss: 0.213219, acc:  67%, G loss: 1.532301\n",
      "Ep: 663, steps: 25, D loss: 0.249481, acc:  55%, G loss: 1.540819\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 664, steps: 1, D loss: 0.242666, acc:  58%, G loss: 1.724251\n",
      "Ep: 664, steps: 2, D loss: 0.239494, acc:  56%, G loss: 1.498275\n",
      "Ep: 664, steps: 3, D loss: 0.158374, acc:  86%, G loss: 1.836030\n",
      "Ep: 664, steps: 4, D loss: 0.187901, acc:  82%, G loss: 1.650226\n",
      "Ep: 664, steps: 5, D loss: 0.283861, acc:  47%, G loss: 1.540292\n",
      "Ep: 664, steps: 6, D loss: 0.248828, acc:  53%, G loss: 1.479382\n",
      "Ep: 664, steps: 7, D loss: 0.339900, acc:  26%, G loss: 1.486896\n",
      "Ep: 664, steps: 8, D loss: 0.236881, acc:  60%, G loss: 1.646092\n",
      "Ep: 664, steps: 9, D loss: 0.242335, acc:  57%, G loss: 1.627573\n",
      "Ep: 664, steps: 10, D loss: 0.185325, acc:  78%, G loss: 1.530268\n",
      "Ep: 664, steps: 11, D loss: 0.238954, acc:  56%, G loss: 1.791136\n",
      "Ep: 664, steps: 12, D loss: 0.303908, acc:  30%, G loss: 1.346225\n",
      "Ep: 664, steps: 13, D loss: 0.301530, acc:  29%, G loss: 1.432253\n",
      "Ep: 664, steps: 14, D loss: 0.280421, acc:  40%, G loss: 1.451676\n",
      "Ep: 664, steps: 15, D loss: 0.234329, acc:  58%, G loss: 1.575257\n",
      "Ep: 664, steps: 16, D loss: 0.231414, acc:  62%, G loss: 1.582334\n",
      "Ep: 664, steps: 17, D loss: 0.223667, acc:  65%, G loss: 1.500751\n",
      "Ep: 664, steps: 18, D loss: 0.236015, acc:  61%, G loss: 1.618424\n",
      "Ep: 664, steps: 19, D loss: 0.212801, acc:  68%, G loss: 1.608179\n",
      "Ep: 664, steps: 20, D loss: 0.189312, acc:  77%, G loss: 1.716052\n",
      "Ep: 664, steps: 21, D loss: 0.265029, acc:  42%, G loss: 1.490770\n",
      "Ep: 664, steps: 22, D loss: 0.200421, acc:  72%, G loss: 1.587677\n",
      "Ep: 664, steps: 23, D loss: 0.227734, acc:  63%, G loss: 1.808074\n",
      "Saved Model\n",
      "Ep: 664, steps: 24, D loss: 0.205722, acc:  72%, G loss: 1.480450\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 665, steps: 1, D loss: 0.226538, acc:  66%, G loss: 1.618590\n",
      "Ep: 665, steps: 2, D loss: 0.240773, acc:  57%, G loss: 1.455620\n",
      "Ep: 665, steps: 3, D loss: 0.170516, acc:  81%, G loss: 1.872486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 665, steps: 4, D loss: 0.173743, acc:  86%, G loss: 1.673803\n",
      "Ep: 665, steps: 5, D loss: 0.292966, acc:  51%, G loss: 1.617467\n",
      "Ep: 665, steps: 6, D loss: 0.250612, acc:  55%, G loss: 1.548113\n",
      "Ep: 665, steps: 7, D loss: 0.322740, acc:  32%, G loss: 1.409001\n",
      "Ep: 665, steps: 8, D loss: 0.234889, acc:  60%, G loss: 1.708851\n",
      "Ep: 665, steps: 9, D loss: 0.231284, acc:  63%, G loss: 1.614574\n",
      "Ep: 665, steps: 10, D loss: 0.186534, acc:  75%, G loss: 1.496384\n",
      "Ep: 665, steps: 11, D loss: 0.266470, acc:  49%, G loss: 1.707158\n",
      "Ep: 665, steps: 12, D loss: 0.308058, acc:  30%, G loss: 1.386054\n",
      "Ep: 665, steps: 13, D loss: 0.288664, acc:  35%, G loss: 1.403394\n",
      "Ep: 665, steps: 14, D loss: 0.275654, acc:  42%, G loss: 1.461097\n",
      "Ep: 665, steps: 15, D loss: 0.254102, acc:  48%, G loss: 1.591323\n",
      "Ep: 665, steps: 16, D loss: 0.245059, acc:  53%, G loss: 1.549349\n",
      "Ep: 665, steps: 17, D loss: 0.211846, acc:  69%, G loss: 1.515935\n",
      "Ep: 665, steps: 18, D loss: 0.225868, acc:  63%, G loss: 1.687331\n",
      "Ep: 665, steps: 19, D loss: 0.210603, acc:  68%, G loss: 1.612080\n",
      "Ep: 665, steps: 20, D loss: 0.177098, acc:  81%, G loss: 1.846592\n",
      "Ep: 665, steps: 21, D loss: 0.271220, acc:  38%, G loss: 1.437483\n",
      "Ep: 665, steps: 22, D loss: 0.199037, acc:  69%, G loss: 1.622220\n",
      "Ep: 665, steps: 23, D loss: 0.220914, acc:  67%, G loss: 1.854383\n",
      "Ep: 665, steps: 24, D loss: 0.209425, acc:  69%, G loss: 1.568096\n",
      "Ep: 665, steps: 25, D loss: 0.265093, acc:  51%, G loss: 1.545866\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 666, steps: 1, D loss: 0.241397, acc:  58%, G loss: 1.728046\n",
      "Ep: 666, steps: 2, D loss: 0.233354, acc:  58%, G loss: 1.478791\n",
      "Ep: 666, steps: 3, D loss: 0.171567, acc:  83%, G loss: 2.077300\n",
      "Ep: 666, steps: 4, D loss: 0.191608, acc:  82%, G loss: 1.611244\n",
      "Ep: 666, steps: 5, D loss: 0.267518, acc:  52%, G loss: 1.645955\n",
      "Ep: 666, steps: 6, D loss: 0.252934, acc:  53%, G loss: 1.517023\n",
      "Ep: 666, steps: 7, D loss: 0.308228, acc:  34%, G loss: 1.459449\n",
      "Ep: 666, steps: 8, D loss: 0.240543, acc:  59%, G loss: 1.677786\n",
      "Ep: 666, steps: 9, D loss: 0.257207, acc:  52%, G loss: 1.646303\n",
      "Ep: 666, steps: 10, D loss: 0.195921, acc:  73%, G loss: 1.591580\n",
      "Ep: 666, steps: 11, D loss: 0.243744, acc:  54%, G loss: 1.807009\n",
      "Ep: 666, steps: 12, D loss: 0.293039, acc:  36%, G loss: 1.411265\n",
      "Ep: 666, steps: 13, D loss: 0.287209, acc:  36%, G loss: 1.360017\n",
      "Ep: 666, steps: 14, D loss: 0.268211, acc:  46%, G loss: 1.490646\n",
      "Ep: 666, steps: 15, D loss: 0.259318, acc:  47%, G loss: 1.607915\n",
      "Ep: 666, steps: 16, D loss: 0.259277, acc:  50%, G loss: 1.581395\n",
      "Ep: 666, steps: 17, D loss: 0.212230, acc:  70%, G loss: 1.531219\n",
      "Ep: 666, steps: 18, D loss: 0.242680, acc:  57%, G loss: 1.608817\n",
      "Ep: 666, steps: 19, D loss: 0.212774, acc:  64%, G loss: 1.612714\n",
      "Ep: 666, steps: 20, D loss: 0.191621, acc:  75%, G loss: 1.667183\n",
      "Ep: 666, steps: 21, D loss: 0.273227, acc:  38%, G loss: 1.433755\n",
      "Ep: 666, steps: 22, D loss: 0.215816, acc:  66%, G loss: 1.597142\n",
      "Ep: 666, steps: 23, D loss: 0.221286, acc:  64%, G loss: 1.783445\n",
      "Ep: 666, steps: 24, D loss: 0.205438, acc:  70%, G loss: 1.555466\n",
      "Ep: 666, steps: 25, D loss: 0.257024, acc:  56%, G loss: 1.723839\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 667, steps: 1, D loss: 0.254212, acc:  52%, G loss: 1.757440\n",
      "Ep: 667, steps: 2, D loss: 0.246712, acc:  55%, G loss: 1.511430\n",
      "Ep: 667, steps: 3, D loss: 0.165109, acc:  86%, G loss: 1.872149\n",
      "Ep: 667, steps: 4, D loss: 0.196820, acc:  78%, G loss: 1.640036\n",
      "Ep: 667, steps: 5, D loss: 0.289120, acc:  46%, G loss: 1.618156\n",
      "Ep: 667, steps: 6, D loss: 0.244972, acc:  54%, G loss: 1.526767\n",
      "Ep: 667, steps: 7, D loss: 0.333598, acc:  25%, G loss: 1.494033\n",
      "Ep: 667, steps: 8, D loss: 0.220102, acc:  66%, G loss: 1.624589\n",
      "Ep: 667, steps: 9, D loss: 0.234404, acc:  61%, G loss: 1.563876\n",
      "Ep: 667, steps: 10, D loss: 0.189420, acc:  75%, G loss: 1.570070\n",
      "Ep: 667, steps: 11, D loss: 0.247846, acc:  56%, G loss: 1.697219\n",
      "Ep: 667, steps: 12, D loss: 0.293105, acc:  37%, G loss: 1.335457\n",
      "Ep: 667, steps: 13, D loss: 0.290270, acc:  32%, G loss: 1.339888\n",
      "Ep: 667, steps: 14, D loss: 0.280056, acc:  40%, G loss: 1.486314\n",
      "Ep: 667, steps: 15, D loss: 0.244582, acc:  54%, G loss: 1.564453\n",
      "Ep: 667, steps: 16, D loss: 0.238542, acc:  59%, G loss: 1.625567\n",
      "Ep: 667, steps: 17, D loss: 0.218067, acc:  67%, G loss: 1.500168\n",
      "Ep: 667, steps: 18, D loss: 0.221260, acc:  65%, G loss: 1.732321\n",
      "Ep: 667, steps: 19, D loss: 0.222313, acc:  64%, G loss: 1.614411\n",
      "Ep: 667, steps: 20, D loss: 0.185353, acc:  79%, G loss: 1.686632\n",
      "Ep: 667, steps: 21, D loss: 0.279973, acc:  36%, G loss: 1.490356\n",
      "Saved Model\n",
      "Ep: 667, steps: 22, D loss: 0.215929, acc:  67%, G loss: 1.543421\n",
      "Ep: 667, steps: 23, D loss: 0.223669, acc:  64%, G loss: 1.492108\n",
      "Ep: 667, steps: 24, D loss: 0.261557, acc:  52%, G loss: 1.713970\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 668, steps: 1, D loss: 0.241903, acc:  59%, G loss: 1.597436\n",
      "Ep: 668, steps: 2, D loss: 0.244337, acc:  56%, G loss: 1.446049\n",
      "Ep: 668, steps: 3, D loss: 0.167814, acc:  82%, G loss: 1.873443\n",
      "Ep: 668, steps: 4, D loss: 0.206974, acc:  75%, G loss: 1.644295\n",
      "Ep: 668, steps: 5, D loss: 0.248645, acc:  57%, G loss: 1.610475\n",
      "Ep: 668, steps: 6, D loss: 0.244601, acc:  53%, G loss: 1.502800\n",
      "Ep: 668, steps: 7, D loss: 0.304106, acc:  33%, G loss: 1.394258\n",
      "Ep: 668, steps: 8, D loss: 0.220033, acc:  65%, G loss: 1.686723\n",
      "Ep: 668, steps: 9, D loss: 0.255971, acc:  52%, G loss: 1.559891\n",
      "Ep: 668, steps: 10, D loss: 0.190221, acc:  74%, G loss: 1.499931\n",
      "Ep: 668, steps: 11, D loss: 0.253566, acc:  50%, G loss: 1.678605\n",
      "Ep: 668, steps: 12, D loss: 0.290156, acc:  36%, G loss: 1.395981\n",
      "Ep: 668, steps: 13, D loss: 0.287436, acc:  34%, G loss: 1.428764\n",
      "Ep: 668, steps: 14, D loss: 0.269694, acc:  44%, G loss: 1.432378\n",
      "Ep: 668, steps: 15, D loss: 0.258528, acc:  51%, G loss: 1.615060\n",
      "Ep: 668, steps: 16, D loss: 0.251335, acc:  54%, G loss: 1.574297\n",
      "Ep: 668, steps: 17, D loss: 0.206953, acc:  73%, G loss: 1.576579\n",
      "Ep: 668, steps: 18, D loss: 0.224362, acc:  64%, G loss: 1.587307\n",
      "Ep: 668, steps: 19, D loss: 0.230950, acc:  61%, G loss: 1.619528\n",
      "Ep: 668, steps: 20, D loss: 0.189713, acc:  75%, G loss: 1.692248\n",
      "Ep: 668, steps: 21, D loss: 0.256523, acc:  45%, G loss: 1.455424\n",
      "Ep: 668, steps: 22, D loss: 0.196929, acc:  74%, G loss: 1.633220\n",
      "Ep: 668, steps: 23, D loss: 0.229849, acc:  62%, G loss: 1.822038\n",
      "Ep: 668, steps: 24, D loss: 0.208440, acc:  69%, G loss: 1.603719\n",
      "Ep: 668, steps: 25, D loss: 0.245182, acc:  57%, G loss: 1.593511\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 669, steps: 1, D loss: 0.241749, acc:  57%, G loss: 1.679857\n",
      "Ep: 669, steps: 2, D loss: 0.242989, acc:  55%, G loss: 1.427370\n",
      "Ep: 669, steps: 3, D loss: 0.184343, acc:  78%, G loss: 1.874107\n",
      "Ep: 669, steps: 4, D loss: 0.184205, acc:  83%, G loss: 1.659556\n",
      "Ep: 669, steps: 5, D loss: 0.280409, acc:  49%, G loss: 1.595162\n",
      "Ep: 669, steps: 6, D loss: 0.243575, acc:  55%, G loss: 1.529110\n",
      "Ep: 669, steps: 7, D loss: 0.321530, acc:  30%, G loss: 1.472026\n",
      "Ep: 669, steps: 8, D loss: 0.227131, acc:  63%, G loss: 1.655387\n",
      "Ep: 669, steps: 9, D loss: 0.241339, acc:  59%, G loss: 1.558958\n",
      "Ep: 669, steps: 10, D loss: 0.191793, acc:  74%, G loss: 1.472621\n",
      "Ep: 669, steps: 11, D loss: 0.252973, acc:  51%, G loss: 1.753446\n",
      "Ep: 669, steps: 12, D loss: 0.292105, acc:  34%, G loss: 1.368998\n",
      "Ep: 669, steps: 13, D loss: 0.292310, acc:  32%, G loss: 1.416144\n",
      "Ep: 669, steps: 14, D loss: 0.266322, acc:  46%, G loss: 1.437392\n",
      "Ep: 669, steps: 15, D loss: 0.235478, acc:  58%, G loss: 1.582735\n",
      "Ep: 669, steps: 16, D loss: 0.242372, acc:  58%, G loss: 1.585864\n",
      "Ep: 669, steps: 17, D loss: 0.233053, acc:  62%, G loss: 1.572637\n",
      "Ep: 669, steps: 18, D loss: 0.229091, acc:  62%, G loss: 1.662068\n",
      "Ep: 669, steps: 19, D loss: 0.224477, acc:  62%, G loss: 1.599455\n",
      "Ep: 669, steps: 20, D loss: 0.179073, acc:  80%, G loss: 1.726663\n",
      "Ep: 669, steps: 21, D loss: 0.270646, acc:  40%, G loss: 1.476519\n",
      "Ep: 669, steps: 22, D loss: 0.218735, acc:  67%, G loss: 1.613674\n",
      "Ep: 669, steps: 23, D loss: 0.236219, acc:  59%, G loss: 1.805058\n",
      "Ep: 669, steps: 24, D loss: 0.212742, acc:  67%, G loss: 1.508562\n",
      "Ep: 669, steps: 25, D loss: 0.256728, acc:  55%, G loss: 1.499709\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 670, steps: 1, D loss: 0.242732, acc:  58%, G loss: 1.742097\n",
      "Ep: 670, steps: 2, D loss: 0.241284, acc:  57%, G loss: 1.504525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 670, steps: 3, D loss: 0.178148, acc:  80%, G loss: 1.868886\n",
      "Ep: 670, steps: 4, D loss: 0.187439, acc:  82%, G loss: 1.718194\n",
      "Ep: 670, steps: 5, D loss: 0.277024, acc:  49%, G loss: 1.581725\n",
      "Ep: 670, steps: 6, D loss: 0.257724, acc:  52%, G loss: 1.509735\n",
      "Ep: 670, steps: 7, D loss: 0.310622, acc:  29%, G loss: 1.472040\n",
      "Ep: 670, steps: 8, D loss: 0.223472, acc:  64%, G loss: 1.649079\n",
      "Ep: 670, steps: 9, D loss: 0.240782, acc:  58%, G loss: 1.640793\n",
      "Ep: 670, steps: 10, D loss: 0.192672, acc:  76%, G loss: 1.491814\n",
      "Ep: 670, steps: 11, D loss: 0.248861, acc:  53%, G loss: 1.745817\n",
      "Ep: 670, steps: 12, D loss: 0.281792, acc:  38%, G loss: 1.342667\n",
      "Ep: 670, steps: 13, D loss: 0.284292, acc:  36%, G loss: 1.322085\n",
      "Ep: 670, steps: 14, D loss: 0.276599, acc:  42%, G loss: 1.402118\n",
      "Ep: 670, steps: 15, D loss: 0.259540, acc:  51%, G loss: 1.540270\n",
      "Ep: 670, steps: 16, D loss: 0.248871, acc:  55%, G loss: 1.545702\n",
      "Ep: 670, steps: 17, D loss: 0.216685, acc:  71%, G loss: 1.574825\n",
      "Ep: 670, steps: 18, D loss: 0.238446, acc:  56%, G loss: 1.681498\n",
      "Ep: 670, steps: 19, D loss: 0.214626, acc:  66%, G loss: 1.612504\n",
      "Saved Model\n",
      "Ep: 670, steps: 20, D loss: 0.185805, acc:  77%, G loss: 1.688199\n",
      "Ep: 670, steps: 21, D loss: 0.212415, acc:  65%, G loss: 1.597308\n",
      "Ep: 670, steps: 22, D loss: 0.210874, acc:  68%, G loss: 1.867263\n",
      "Ep: 670, steps: 23, D loss: 0.203489, acc:  73%, G loss: 1.579874\n",
      "Ep: 670, steps: 24, D loss: 0.257051, acc:  54%, G loss: 1.607041\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 671, steps: 1, D loss: 0.236471, acc:  59%, G loss: 1.612640\n",
      "Ep: 671, steps: 2, D loss: 0.231154, acc:  62%, G loss: 1.490437\n",
      "Ep: 671, steps: 3, D loss: 0.173899, acc:  81%, G loss: 1.821736\n",
      "Ep: 671, steps: 4, D loss: 0.176210, acc:  87%, G loss: 1.654957\n",
      "Ep: 671, steps: 5, D loss: 0.271409, acc:  50%, G loss: 1.608970\n",
      "Ep: 671, steps: 6, D loss: 0.235332, acc:  56%, G loss: 1.536811\n",
      "Ep: 671, steps: 7, D loss: 0.325314, acc:  30%, G loss: 1.699965\n",
      "Ep: 671, steps: 8, D loss: 0.219454, acc:  66%, G loss: 1.678549\n",
      "Ep: 671, steps: 9, D loss: 0.245173, acc:  61%, G loss: 1.555441\n",
      "Ep: 671, steps: 10, D loss: 0.178517, acc:  78%, G loss: 1.549124\n",
      "Ep: 671, steps: 11, D loss: 0.263981, acc:  48%, G loss: 1.792625\n",
      "Ep: 671, steps: 12, D loss: 0.288023, acc:  35%, G loss: 1.314863\n",
      "Ep: 671, steps: 13, D loss: 0.277102, acc:  39%, G loss: 1.326703\n",
      "Ep: 671, steps: 14, D loss: 0.276785, acc:  42%, G loss: 1.428923\n",
      "Ep: 671, steps: 15, D loss: 0.239696, acc:  54%, G loss: 1.588944\n",
      "Ep: 671, steps: 16, D loss: 0.239386, acc:  60%, G loss: 1.553181\n",
      "Ep: 671, steps: 17, D loss: 0.214191, acc:  69%, G loss: 1.555945\n",
      "Ep: 671, steps: 18, D loss: 0.225032, acc:  64%, G loss: 1.672801\n",
      "Ep: 671, steps: 19, D loss: 0.209637, acc:  68%, G loss: 1.656184\n",
      "Ep: 671, steps: 20, D loss: 0.183910, acc:  78%, G loss: 1.715028\n",
      "Ep: 671, steps: 21, D loss: 0.286357, acc:  35%, G loss: 1.588331\n",
      "Ep: 671, steps: 22, D loss: 0.209293, acc:  68%, G loss: 1.565832\n",
      "Ep: 671, steps: 23, D loss: 0.229018, acc:  61%, G loss: 1.816252\n",
      "Ep: 671, steps: 24, D loss: 0.209057, acc:  69%, G loss: 1.525535\n",
      "Ep: 671, steps: 25, D loss: 0.247328, acc:  56%, G loss: 1.600483\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 672, steps: 1, D loss: 0.235451, acc:  60%, G loss: 1.655540\n",
      "Ep: 672, steps: 2, D loss: 0.248562, acc:  53%, G loss: 1.443281\n",
      "Ep: 672, steps: 3, D loss: 0.174150, acc:  82%, G loss: 1.850714\n",
      "Ep: 672, steps: 4, D loss: 0.184452, acc:  84%, G loss: 1.661556\n",
      "Ep: 672, steps: 5, D loss: 0.285044, acc:  49%, G loss: 1.637647\n",
      "Ep: 672, steps: 6, D loss: 0.244693, acc:  53%, G loss: 1.566541\n",
      "Ep: 672, steps: 7, D loss: 0.301188, acc:  36%, G loss: 1.456839\n",
      "Ep: 672, steps: 8, D loss: 0.226720, acc:  63%, G loss: 1.766204\n",
      "Ep: 672, steps: 9, D loss: 0.221679, acc:  67%, G loss: 1.638530\n",
      "Ep: 672, steps: 10, D loss: 0.187971, acc:  74%, G loss: 1.636360\n",
      "Ep: 672, steps: 11, D loss: 0.245880, acc:  55%, G loss: 1.742756\n",
      "Ep: 672, steps: 12, D loss: 0.295317, acc:  36%, G loss: 1.369679\n",
      "Ep: 672, steps: 13, D loss: 0.290322, acc:  33%, G loss: 1.413738\n",
      "Ep: 672, steps: 14, D loss: 0.275235, acc:  43%, G loss: 1.461903\n",
      "Ep: 672, steps: 15, D loss: 0.280108, acc:  40%, G loss: 1.564720\n",
      "Ep: 672, steps: 16, D loss: 0.247558, acc:  53%, G loss: 1.554059\n",
      "Ep: 672, steps: 17, D loss: 0.212513, acc:  68%, G loss: 1.651719\n",
      "Ep: 672, steps: 18, D loss: 0.246330, acc:  56%, G loss: 1.565198\n",
      "Ep: 672, steps: 19, D loss: 0.213630, acc:  66%, G loss: 1.591356\n",
      "Ep: 672, steps: 20, D loss: 0.178353, acc:  78%, G loss: 1.732984\n",
      "Ep: 672, steps: 21, D loss: 0.281637, acc:  34%, G loss: 1.478133\n",
      "Ep: 672, steps: 22, D loss: 0.204751, acc:  69%, G loss: 1.583493\n",
      "Ep: 672, steps: 23, D loss: 0.230565, acc:  61%, G loss: 1.782202\n",
      "Ep: 672, steps: 24, D loss: 0.205911, acc:  68%, G loss: 1.540208\n",
      "Ep: 672, steps: 25, D loss: 0.252224, acc:  56%, G loss: 1.550517\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 673, steps: 1, D loss: 0.247289, acc:  56%, G loss: 1.688442\n",
      "Ep: 673, steps: 2, D loss: 0.255144, acc:  52%, G loss: 1.494245\n",
      "Ep: 673, steps: 3, D loss: 0.179629, acc:  79%, G loss: 1.840715\n",
      "Ep: 673, steps: 4, D loss: 0.196003, acc:  78%, G loss: 1.634880\n",
      "Ep: 673, steps: 5, D loss: 0.273561, acc:  48%, G loss: 1.575365\n",
      "Ep: 673, steps: 6, D loss: 0.253282, acc:  51%, G loss: 1.549259\n",
      "Ep: 673, steps: 7, D loss: 0.295590, acc:  36%, G loss: 1.553153\n",
      "Ep: 673, steps: 8, D loss: 0.217320, acc:  67%, G loss: 1.663140\n",
      "Ep: 673, steps: 9, D loss: 0.223828, acc:  67%, G loss: 1.616936\n",
      "Ep: 673, steps: 10, D loss: 0.188556, acc:  74%, G loss: 1.578447\n",
      "Ep: 673, steps: 11, D loss: 0.246858, acc:  53%, G loss: 1.724905\n",
      "Ep: 673, steps: 12, D loss: 0.295594, acc:  33%, G loss: 1.343504\n",
      "Ep: 673, steps: 13, D loss: 0.288764, acc:  33%, G loss: 1.406071\n",
      "Ep: 673, steps: 14, D loss: 0.270376, acc:  46%, G loss: 1.455250\n",
      "Ep: 673, steps: 15, D loss: 0.240743, acc:  57%, G loss: 1.593397\n",
      "Ep: 673, steps: 16, D loss: 0.248948, acc:  54%, G loss: 1.583220\n",
      "Ep: 673, steps: 17, D loss: 0.218130, acc:  67%, G loss: 1.494056\n",
      "Saved Model\n",
      "Ep: 673, steps: 18, D loss: 0.236677, acc:  59%, G loss: 1.691175\n",
      "Ep: 673, steps: 19, D loss: 0.179011, acc:  77%, G loss: 1.749729\n",
      "Ep: 673, steps: 20, D loss: 0.243847, acc:  52%, G loss: 1.480417\n",
      "Ep: 673, steps: 21, D loss: 0.199558, acc:  71%, G loss: 1.577457\n",
      "Ep: 673, steps: 22, D loss: 0.246204, acc:  53%, G loss: 1.825008\n",
      "Ep: 673, steps: 23, D loss: 0.200469, acc:  73%, G loss: 1.552534\n",
      "Ep: 673, steps: 24, D loss: 0.240701, acc:  58%, G loss: 1.625355\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 674, steps: 1, D loss: 0.239313, acc:  57%, G loss: 1.696796\n",
      "Ep: 674, steps: 2, D loss: 0.249969, acc:  53%, G loss: 1.430906\n",
      "Ep: 674, steps: 3, D loss: 0.177477, acc:  79%, G loss: 1.882075\n",
      "Ep: 674, steps: 4, D loss: 0.197538, acc:  79%, G loss: 1.594024\n",
      "Ep: 674, steps: 5, D loss: 0.274643, acc:  50%, G loss: 1.643896\n",
      "Ep: 674, steps: 6, D loss: 0.252535, acc:  53%, G loss: 1.498522\n",
      "Ep: 674, steps: 7, D loss: 0.288936, acc:  39%, G loss: 1.489569\n",
      "Ep: 674, steps: 8, D loss: 0.223403, acc:  67%, G loss: 1.744064\n",
      "Ep: 674, steps: 9, D loss: 0.253412, acc:  55%, G loss: 1.602570\n",
      "Ep: 674, steps: 10, D loss: 0.191187, acc:  76%, G loss: 1.667218\n",
      "Ep: 674, steps: 11, D loss: 0.255897, acc:  50%, G loss: 1.729734\n",
      "Ep: 674, steps: 12, D loss: 0.297135, acc:  32%, G loss: 1.389163\n",
      "Ep: 674, steps: 13, D loss: 0.289996, acc:  33%, G loss: 1.377963\n",
      "Ep: 674, steps: 14, D loss: 0.278404, acc:  40%, G loss: 1.492274\n",
      "Ep: 674, steps: 15, D loss: 0.255861, acc:  50%, G loss: 1.666294\n",
      "Ep: 674, steps: 16, D loss: 0.243122, acc:  58%, G loss: 1.617500\n",
      "Ep: 674, steps: 17, D loss: 0.213573, acc:  68%, G loss: 1.533325\n",
      "Ep: 674, steps: 18, D loss: 0.228447, acc:  64%, G loss: 1.589102\n",
      "Ep: 674, steps: 19, D loss: 0.223168, acc:  62%, G loss: 1.611358\n",
      "Ep: 674, steps: 20, D loss: 0.186741, acc:  75%, G loss: 1.700266\n",
      "Ep: 674, steps: 21, D loss: 0.268476, acc:  40%, G loss: 1.462683\n",
      "Ep: 674, steps: 22, D loss: 0.189003, acc:  72%, G loss: 1.585256\n",
      "Ep: 674, steps: 23, D loss: 0.223614, acc:  63%, G loss: 1.797514\n",
      "Ep: 674, steps: 24, D loss: 0.213516, acc:  68%, G loss: 1.517582\n",
      "Ep: 674, steps: 25, D loss: 0.252159, acc:  56%, G loss: 1.664915\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 675, steps: 1, D loss: 0.262969, acc:  52%, G loss: 1.711077\n",
      "Ep: 675, steps: 2, D loss: 0.261625, acc:  47%, G loss: 1.472554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 675, steps: 3, D loss: 0.171367, acc:  84%, G loss: 1.912035\n",
      "Ep: 675, steps: 4, D loss: 0.192397, acc:  80%, G loss: 1.663815\n",
      "Ep: 675, steps: 5, D loss: 0.286881, acc:  45%, G loss: 1.584086\n",
      "Ep: 675, steps: 6, D loss: 0.262162, acc:  50%, G loss: 1.518792\n",
      "Ep: 675, steps: 7, D loss: 0.310068, acc:  32%, G loss: 1.465303\n",
      "Ep: 675, steps: 8, D loss: 0.231148, acc:  63%, G loss: 1.613158\n",
      "Ep: 675, steps: 9, D loss: 0.231653, acc:  62%, G loss: 1.605731\n",
      "Ep: 675, steps: 10, D loss: 0.192504, acc:  77%, G loss: 1.500996\n",
      "Ep: 675, steps: 11, D loss: 0.256493, acc:  51%, G loss: 1.698226\n",
      "Ep: 675, steps: 12, D loss: 0.304284, acc:  28%, G loss: 1.307496\n",
      "Ep: 675, steps: 13, D loss: 0.278504, acc:  37%, G loss: 1.305413\n",
      "Ep: 675, steps: 14, D loss: 0.271220, acc:  44%, G loss: 1.455122\n",
      "Ep: 675, steps: 15, D loss: 0.238417, acc:  57%, G loss: 1.580702\n",
      "Ep: 675, steps: 16, D loss: 0.245768, acc:  54%, G loss: 1.585526\n",
      "Ep: 675, steps: 17, D loss: 0.220834, acc:  68%, G loss: 1.507861\n",
      "Ep: 675, steps: 18, D loss: 0.229992, acc:  61%, G loss: 1.652138\n",
      "Ep: 675, steps: 19, D loss: 0.219900, acc:  64%, G loss: 1.638817\n",
      "Ep: 675, steps: 20, D loss: 0.190662, acc:  76%, G loss: 1.745952\n",
      "Ep: 675, steps: 21, D loss: 0.267677, acc:  42%, G loss: 1.409111\n",
      "Ep: 675, steps: 22, D loss: 0.218002, acc:  64%, G loss: 1.562550\n",
      "Ep: 675, steps: 23, D loss: 0.220268, acc:  65%, G loss: 1.796201\n",
      "Ep: 675, steps: 24, D loss: 0.208170, acc:  71%, G loss: 1.482613\n",
      "Ep: 675, steps: 25, D loss: 0.253329, acc:  54%, G loss: 1.732097\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 676, steps: 1, D loss: 0.236946, acc:  61%, G loss: 1.702306\n",
      "Ep: 676, steps: 2, D loss: 0.252965, acc:  53%, G loss: 1.448600\n",
      "Ep: 676, steps: 3, D loss: 0.175410, acc:  81%, G loss: 1.871974\n",
      "Ep: 676, steps: 4, D loss: 0.206180, acc:  76%, G loss: 1.623104\n",
      "Ep: 676, steps: 5, D loss: 0.275069, acc:  48%, G loss: 1.601152\n",
      "Ep: 676, steps: 6, D loss: 0.241728, acc:  54%, G loss: 1.620865\n",
      "Ep: 676, steps: 7, D loss: 0.312760, acc:  29%, G loss: 1.452531\n",
      "Ep: 676, steps: 8, D loss: 0.225369, acc:  65%, G loss: 1.662598\n",
      "Ep: 676, steps: 9, D loss: 0.236201, acc:  62%, G loss: 1.592617\n",
      "Ep: 676, steps: 10, D loss: 0.177222, acc:  81%, G loss: 1.625426\n",
      "Ep: 676, steps: 11, D loss: 0.252647, acc:  53%, G loss: 1.727165\n",
      "Ep: 676, steps: 12, D loss: 0.280464, acc:  42%, G loss: 1.415322\n",
      "Ep: 676, steps: 13, D loss: 0.284980, acc:  35%, G loss: 1.354556\n",
      "Ep: 676, steps: 14, D loss: 0.259417, acc:  48%, G loss: 1.468704\n",
      "Ep: 676, steps: 15, D loss: 0.235494, acc:  58%, G loss: 1.539319\n",
      "Saved Model\n",
      "Ep: 676, steps: 16, D loss: 0.238985, acc:  59%, G loss: 1.598361\n",
      "Ep: 676, steps: 17, D loss: 0.214114, acc:  66%, G loss: 1.713245\n",
      "Ep: 676, steps: 18, D loss: 0.218888, acc:  64%, G loss: 1.630367\n",
      "Ep: 676, steps: 19, D loss: 0.174843, acc:  80%, G loss: 1.719880\n",
      "Ep: 676, steps: 20, D loss: 0.274041, acc:  38%, G loss: 1.551600\n",
      "Ep: 676, steps: 21, D loss: 0.212923, acc:  65%, G loss: 1.550193\n",
      "Ep: 676, steps: 22, D loss: 0.236855, acc:  60%, G loss: 1.807331\n",
      "Ep: 676, steps: 23, D loss: 0.220257, acc:  64%, G loss: 1.505313\n",
      "Ep: 676, steps: 24, D loss: 0.252558, acc:  55%, G loss: 1.627586\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 677, steps: 1, D loss: 0.233876, acc:  60%, G loss: 1.652331\n",
      "Ep: 677, steps: 2, D loss: 0.243061, acc:  56%, G loss: 1.455193\n",
      "Ep: 677, steps: 3, D loss: 0.174893, acc:  82%, G loss: 1.893944\n",
      "Ep: 677, steps: 4, D loss: 0.206794, acc:  73%, G loss: 1.693033\n",
      "Ep: 677, steps: 5, D loss: 0.273777, acc:  46%, G loss: 1.625556\n",
      "Ep: 677, steps: 6, D loss: 0.254932, acc:  52%, G loss: 1.544279\n",
      "Ep: 677, steps: 7, D loss: 0.297203, acc:  33%, G loss: 1.530617\n",
      "Ep: 677, steps: 8, D loss: 0.221960, acc:  65%, G loss: 1.722787\n",
      "Ep: 677, steps: 9, D loss: 0.238940, acc:  60%, G loss: 1.631016\n",
      "Ep: 677, steps: 10, D loss: 0.183820, acc:  76%, G loss: 1.656774\n",
      "Ep: 677, steps: 11, D loss: 0.245642, acc:  54%, G loss: 1.721713\n",
      "Ep: 677, steps: 12, D loss: 0.296814, acc:  34%, G loss: 1.410050\n",
      "Ep: 677, steps: 13, D loss: 0.276742, acc:  41%, G loss: 1.365373\n",
      "Ep: 677, steps: 14, D loss: 0.272753, acc:  45%, G loss: 1.439247\n",
      "Ep: 677, steps: 15, D loss: 0.243623, acc:  55%, G loss: 1.562949\n",
      "Ep: 677, steps: 16, D loss: 0.246712, acc:  56%, G loss: 1.565957\n",
      "Ep: 677, steps: 17, D loss: 0.221564, acc:  65%, G loss: 1.502514\n",
      "Ep: 677, steps: 18, D loss: 0.242670, acc:  58%, G loss: 1.600775\n",
      "Ep: 677, steps: 19, D loss: 0.217356, acc:  65%, G loss: 1.612273\n",
      "Ep: 677, steps: 20, D loss: 0.188666, acc:  77%, G loss: 1.740948\n",
      "Ep: 677, steps: 21, D loss: 0.272623, acc:  39%, G loss: 1.440454\n",
      "Ep: 677, steps: 22, D loss: 0.190655, acc:  72%, G loss: 1.712319\n",
      "Ep: 677, steps: 23, D loss: 0.206609, acc:  71%, G loss: 1.778538\n",
      "Ep: 677, steps: 24, D loss: 0.214069, acc:  68%, G loss: 1.520338\n",
      "Ep: 677, steps: 25, D loss: 0.247593, acc:  55%, G loss: 1.727643\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 678, steps: 1, D loss: 0.246096, acc:  57%, G loss: 1.658621\n",
      "Ep: 678, steps: 2, D loss: 0.276449, acc:  43%, G loss: 1.490929\n",
      "Ep: 678, steps: 3, D loss: 0.156570, acc:  88%, G loss: 1.849280\n",
      "Ep: 678, steps: 4, D loss: 0.192091, acc:  80%, G loss: 1.640056\n",
      "Ep: 678, steps: 5, D loss: 0.281981, acc:  45%, G loss: 1.601767\n",
      "Ep: 678, steps: 6, D loss: 0.243597, acc:  53%, G loss: 1.565203\n",
      "Ep: 678, steps: 7, D loss: 0.318515, acc:  26%, G loss: 1.469332\n",
      "Ep: 678, steps: 8, D loss: 0.224191, acc:  63%, G loss: 1.620858\n",
      "Ep: 678, steps: 9, D loss: 0.239722, acc:  60%, G loss: 1.637189\n",
      "Ep: 678, steps: 10, D loss: 0.184683, acc:  80%, G loss: 1.596970\n",
      "Ep: 678, steps: 11, D loss: 0.261446, acc:  51%, G loss: 1.702958\n",
      "Ep: 678, steps: 12, D loss: 0.289740, acc:  33%, G loss: 1.388648\n",
      "Ep: 678, steps: 13, D loss: 0.281143, acc:  35%, G loss: 1.399862\n",
      "Ep: 678, steps: 14, D loss: 0.272924, acc:  43%, G loss: 1.480412\n",
      "Ep: 678, steps: 15, D loss: 0.247391, acc:  53%, G loss: 1.571880\n",
      "Ep: 678, steps: 16, D loss: 0.245754, acc:  54%, G loss: 1.553886\n",
      "Ep: 678, steps: 17, D loss: 0.226925, acc:  63%, G loss: 1.491976\n",
      "Ep: 678, steps: 18, D loss: 0.227131, acc:  61%, G loss: 1.599754\n",
      "Ep: 678, steps: 19, D loss: 0.216832, acc:  65%, G loss: 1.609511\n",
      "Ep: 678, steps: 20, D loss: 0.186641, acc:  78%, G loss: 1.679324\n",
      "Ep: 678, steps: 21, D loss: 0.271630, acc:  38%, G loss: 1.506078\n",
      "Ep: 678, steps: 22, D loss: 0.201436, acc:  70%, G loss: 1.591617\n",
      "Ep: 678, steps: 23, D loss: 0.219509, acc:  66%, G loss: 1.829684\n",
      "Ep: 678, steps: 24, D loss: 0.201403, acc:  72%, G loss: 1.528256\n",
      "Ep: 678, steps: 25, D loss: 0.241732, acc:  59%, G loss: 1.533868\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 679, steps: 1, D loss: 0.254919, acc:  52%, G loss: 1.712433\n",
      "Ep: 679, steps: 2, D loss: 0.246824, acc:  56%, G loss: 1.445620\n",
      "Ep: 679, steps: 3, D loss: 0.173673, acc:  80%, G loss: 1.805945\n",
      "Ep: 679, steps: 4, D loss: 0.199368, acc:  76%, G loss: 1.624277\n",
      "Ep: 679, steps: 5, D loss: 0.272339, acc:  51%, G loss: 1.577368\n",
      "Ep: 679, steps: 6, D loss: 0.242416, acc:  54%, G loss: 1.545990\n",
      "Ep: 679, steps: 7, D loss: 0.313604, acc:  35%, G loss: 1.419526\n",
      "Ep: 679, steps: 8, D loss: 0.220080, acc:  64%, G loss: 1.718036\n",
      "Ep: 679, steps: 9, D loss: 0.239030, acc:  61%, G loss: 1.583702\n",
      "Ep: 679, steps: 10, D loss: 0.178808, acc:  77%, G loss: 1.563680\n",
      "Ep: 679, steps: 11, D loss: 0.235127, acc:  57%, G loss: 1.729627\n",
      "Ep: 679, steps: 12, D loss: 0.295519, acc:  34%, G loss: 1.380321\n",
      "Ep: 679, steps: 13, D loss: 0.294483, acc:  33%, G loss: 1.390144\n",
      "Saved Model\n",
      "Ep: 679, steps: 14, D loss: 0.272866, acc:  43%, G loss: 1.479075\n",
      "Ep: 679, steps: 15, D loss: 0.224832, acc:  64%, G loss: 1.565108\n",
      "Ep: 679, steps: 16, D loss: 0.240902, acc:  55%, G loss: 1.479649\n",
      "Ep: 679, steps: 17, D loss: 0.246167, acc:  57%, G loss: 1.574535\n",
      "Ep: 679, steps: 18, D loss: 0.216125, acc:  66%, G loss: 1.585233\n",
      "Ep: 679, steps: 19, D loss: 0.159623, acc:  85%, G loss: 1.716972\n",
      "Ep: 679, steps: 20, D loss: 0.259683, acc:  44%, G loss: 1.674740\n",
      "Ep: 679, steps: 21, D loss: 0.203789, acc:  68%, G loss: 1.603346\n",
      "Ep: 679, steps: 22, D loss: 0.224890, acc:  63%, G loss: 1.880629\n",
      "Ep: 679, steps: 23, D loss: 0.197183, acc:  74%, G loss: 1.584135\n",
      "Ep: 679, steps: 24, D loss: 0.253103, acc:  53%, G loss: 1.575601\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 680, steps: 1, D loss: 0.247209, acc:  53%, G loss: 1.621644\n",
      "Ep: 680, steps: 2, D loss: 0.252563, acc:  53%, G loss: 1.514274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 680, steps: 3, D loss: 0.176253, acc:  80%, G loss: 1.811458\n",
      "Ep: 680, steps: 4, D loss: 0.189372, acc:  81%, G loss: 1.676735\n",
      "Ep: 680, steps: 5, D loss: 0.268048, acc:  49%, G loss: 1.632853\n",
      "Ep: 680, steps: 6, D loss: 0.257466, acc:  51%, G loss: 1.556625\n",
      "Ep: 680, steps: 7, D loss: 0.314547, acc:  32%, G loss: 1.486512\n",
      "Ep: 680, steps: 8, D loss: 0.223139, acc:  66%, G loss: 1.664092\n",
      "Ep: 680, steps: 9, D loss: 0.233474, acc:  61%, G loss: 1.671528\n",
      "Ep: 680, steps: 10, D loss: 0.186653, acc:  75%, G loss: 1.579616\n",
      "Ep: 680, steps: 11, D loss: 0.250209, acc:  55%, G loss: 1.697551\n",
      "Ep: 680, steps: 12, D loss: 0.291136, acc:  37%, G loss: 1.348500\n",
      "Ep: 680, steps: 13, D loss: 0.283028, acc:  36%, G loss: 1.380223\n",
      "Ep: 680, steps: 14, D loss: 0.273391, acc:  44%, G loss: 1.494903\n",
      "Ep: 680, steps: 15, D loss: 0.258942, acc:  48%, G loss: 1.538635\n",
      "Ep: 680, steps: 16, D loss: 0.244009, acc:  56%, G loss: 1.583657\n",
      "Ep: 680, steps: 17, D loss: 0.212935, acc:  69%, G loss: 1.504383\n",
      "Ep: 680, steps: 18, D loss: 0.225873, acc:  63%, G loss: 1.595418\n",
      "Ep: 680, steps: 19, D loss: 0.219046, acc:  64%, G loss: 1.567538\n",
      "Ep: 680, steps: 20, D loss: 0.190371, acc:  76%, G loss: 1.686220\n",
      "Ep: 680, steps: 21, D loss: 0.259814, acc:  43%, G loss: 1.503361\n",
      "Ep: 680, steps: 22, D loss: 0.195856, acc:  70%, G loss: 1.671558\n",
      "Ep: 680, steps: 23, D loss: 0.216810, acc:  66%, G loss: 1.844581\n",
      "Ep: 680, steps: 24, D loss: 0.206164, acc:  71%, G loss: 1.638192\n",
      "Ep: 680, steps: 25, D loss: 0.247585, acc:  58%, G loss: 1.600561\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 681, steps: 1, D loss: 0.222204, acc:  66%, G loss: 1.735205\n",
      "Ep: 681, steps: 2, D loss: 0.246983, acc:  54%, G loss: 1.455942\n",
      "Ep: 681, steps: 3, D loss: 0.171321, acc:  82%, G loss: 1.863674\n",
      "Ep: 681, steps: 4, D loss: 0.183248, acc:  82%, G loss: 1.642768\n",
      "Ep: 681, steps: 5, D loss: 0.280416, acc:  51%, G loss: 1.570441\n",
      "Ep: 681, steps: 6, D loss: 0.247852, acc:  52%, G loss: 1.524960\n",
      "Ep: 681, steps: 7, D loss: 0.326320, acc:  29%, G loss: 1.433508\n",
      "Ep: 681, steps: 8, D loss: 0.220279, acc:  65%, G loss: 1.670436\n",
      "Ep: 681, steps: 9, D loss: 0.235879, acc:  61%, G loss: 1.624864\n",
      "Ep: 681, steps: 10, D loss: 0.175233, acc:  79%, G loss: 1.575998\n",
      "Ep: 681, steps: 11, D loss: 0.244896, acc:  55%, G loss: 1.725865\n",
      "Ep: 681, steps: 12, D loss: 0.295702, acc:  36%, G loss: 1.356712\n",
      "Ep: 681, steps: 13, D loss: 0.284949, acc:  37%, G loss: 1.411855\n",
      "Ep: 681, steps: 14, D loss: 0.271883, acc:  44%, G loss: 1.452487\n",
      "Ep: 681, steps: 15, D loss: 0.250130, acc:  52%, G loss: 1.590548\n",
      "Ep: 681, steps: 16, D loss: 0.248773, acc:  55%, G loss: 1.590692\n",
      "Ep: 681, steps: 17, D loss: 0.222914, acc:  65%, G loss: 1.549788\n",
      "Ep: 681, steps: 18, D loss: 0.218103, acc:  67%, G loss: 1.613702\n",
      "Ep: 681, steps: 19, D loss: 0.212715, acc:  66%, G loss: 1.647279\n",
      "Ep: 681, steps: 20, D loss: 0.180856, acc:  79%, G loss: 1.701861\n",
      "Ep: 681, steps: 21, D loss: 0.271561, acc:  39%, G loss: 1.454232\n",
      "Ep: 681, steps: 22, D loss: 0.206004, acc:  70%, G loss: 1.554915\n",
      "Ep: 681, steps: 23, D loss: 0.221900, acc:  66%, G loss: 1.846071\n",
      "Ep: 681, steps: 24, D loss: 0.188851, acc:  77%, G loss: 1.619538\n",
      "Ep: 681, steps: 25, D loss: 0.232806, acc:  61%, G loss: 1.504694\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 682, steps: 1, D loss: 0.240906, acc:  59%, G loss: 1.624701\n",
      "Ep: 682, steps: 2, D loss: 0.257592, acc:  50%, G loss: 1.418957\n",
      "Ep: 682, steps: 3, D loss: 0.171350, acc:  82%, G loss: 1.843330\n",
      "Ep: 682, steps: 4, D loss: 0.188301, acc:  82%, G loss: 1.650681\n",
      "Ep: 682, steps: 5, D loss: 0.306302, acc:  46%, G loss: 1.684284\n",
      "Ep: 682, steps: 6, D loss: 0.230570, acc:  57%, G loss: 1.583781\n",
      "Ep: 682, steps: 7, D loss: 0.322027, acc:  29%, G loss: 1.446215\n",
      "Ep: 682, steps: 8, D loss: 0.225871, acc:  63%, G loss: 1.727960\n",
      "Ep: 682, steps: 9, D loss: 0.251523, acc:  56%, G loss: 1.584848\n",
      "Ep: 682, steps: 10, D loss: 0.191188, acc:  73%, G loss: 1.556574\n",
      "Ep: 682, steps: 11, D loss: 0.255393, acc:  54%, G loss: 1.740802\n",
      "Saved Model\n",
      "Ep: 682, steps: 12, D loss: 0.290677, acc:  36%, G loss: 1.350196\n",
      "Ep: 682, steps: 13, D loss: 0.281816, acc:  39%, G loss: 1.432906\n",
      "Ep: 682, steps: 14, D loss: 0.238053, acc:  58%, G loss: 1.685167\n",
      "Ep: 682, steps: 15, D loss: 0.249534, acc:  54%, G loss: 1.654598\n",
      "Ep: 682, steps: 16, D loss: 0.210035, acc:  73%, G loss: 1.599164\n",
      "Ep: 682, steps: 17, D loss: 0.219646, acc:  66%, G loss: 1.575277\n",
      "Ep: 682, steps: 18, D loss: 0.200673, acc:  69%, G loss: 1.663988\n",
      "Ep: 682, steps: 19, D loss: 0.197015, acc:  74%, G loss: 1.627344\n",
      "Ep: 682, steps: 20, D loss: 0.276907, acc:  38%, G loss: 1.383352\n",
      "Ep: 682, steps: 21, D loss: 0.209436, acc:  68%, G loss: 1.542471\n",
      "Ep: 682, steps: 22, D loss: 0.234423, acc:  59%, G loss: 1.816145\n",
      "Ep: 682, steps: 23, D loss: 0.215015, acc:  66%, G loss: 1.577024\n",
      "Ep: 682, steps: 24, D loss: 0.258207, acc:  53%, G loss: 1.577851\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 683, steps: 1, D loss: 0.242567, acc:  58%, G loss: 1.692315\n",
      "Ep: 683, steps: 2, D loss: 0.225792, acc:  63%, G loss: 1.371976\n",
      "Ep: 683, steps: 3, D loss: 0.168907, acc:  83%, G loss: 1.834087\n",
      "Ep: 683, steps: 4, D loss: 0.184069, acc:  82%, G loss: 1.623136\n",
      "Ep: 683, steps: 5, D loss: 0.281102, acc:  50%, G loss: 1.586633\n",
      "Ep: 683, steps: 6, D loss: 0.249614, acc:  53%, G loss: 1.545598\n",
      "Ep: 683, steps: 7, D loss: 0.346256, acc:  24%, G loss: 1.514533\n",
      "Ep: 683, steps: 8, D loss: 0.222910, acc:  65%, G loss: 1.641315\n",
      "Ep: 683, steps: 9, D loss: 0.233864, acc:  60%, G loss: 1.614758\n",
      "Ep: 683, steps: 10, D loss: 0.183120, acc:  76%, G loss: 1.549576\n",
      "Ep: 683, steps: 11, D loss: 0.254598, acc:  52%, G loss: 1.673165\n",
      "Ep: 683, steps: 12, D loss: 0.290462, acc:  35%, G loss: 1.323323\n",
      "Ep: 683, steps: 13, D loss: 0.301622, acc:  29%, G loss: 1.384976\n",
      "Ep: 683, steps: 14, D loss: 0.271307, acc:  44%, G loss: 1.460164\n",
      "Ep: 683, steps: 15, D loss: 0.245523, acc:  54%, G loss: 1.567990\n",
      "Ep: 683, steps: 16, D loss: 0.245237, acc:  56%, G loss: 1.546952\n",
      "Ep: 683, steps: 17, D loss: 0.221553, acc:  66%, G loss: 1.566620\n",
      "Ep: 683, steps: 18, D loss: 0.243536, acc:  57%, G loss: 1.709590\n",
      "Ep: 683, steps: 19, D loss: 0.213073, acc:  67%, G loss: 1.598028\n",
      "Ep: 683, steps: 20, D loss: 0.181858, acc:  80%, G loss: 1.705360\n",
      "Ep: 683, steps: 21, D loss: 0.269815, acc:  41%, G loss: 1.569921\n",
      "Ep: 683, steps: 22, D loss: 0.227840, acc:  62%, G loss: 1.642260\n",
      "Ep: 683, steps: 23, D loss: 0.221247, acc:  63%, G loss: 1.837920\n",
      "Ep: 683, steps: 24, D loss: 0.206029, acc:  72%, G loss: 1.537253\n",
      "Ep: 683, steps: 25, D loss: 0.246880, acc:  58%, G loss: 1.619053\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 684, steps: 1, D loss: 0.249904, acc:  55%, G loss: 1.707254\n",
      "Ep: 684, steps: 2, D loss: 0.260779, acc:  50%, G loss: 1.447890\n",
      "Ep: 684, steps: 3, D loss: 0.168537, acc:  82%, G loss: 1.832317\n",
      "Ep: 684, steps: 4, D loss: 0.188387, acc:  82%, G loss: 1.621627\n",
      "Ep: 684, steps: 5, D loss: 0.281159, acc:  51%, G loss: 1.572478\n",
      "Ep: 684, steps: 6, D loss: 0.244037, acc:  54%, G loss: 1.547481\n",
      "Ep: 684, steps: 7, D loss: 0.312782, acc:  29%, G loss: 1.478564\n",
      "Ep: 684, steps: 8, D loss: 0.227209, acc:  64%, G loss: 1.687715\n",
      "Ep: 684, steps: 9, D loss: 0.233711, acc:  62%, G loss: 1.592241\n",
      "Ep: 684, steps: 10, D loss: 0.170786, acc:  80%, G loss: 1.559917\n",
      "Ep: 684, steps: 11, D loss: 0.256324, acc:  51%, G loss: 1.792765\n",
      "Ep: 684, steps: 12, D loss: 0.289342, acc:  36%, G loss: 1.379209\n",
      "Ep: 684, steps: 13, D loss: 0.290324, acc:  32%, G loss: 1.356239\n",
      "Ep: 684, steps: 14, D loss: 0.275880, acc:  44%, G loss: 1.497539\n",
      "Ep: 684, steps: 15, D loss: 0.260022, acc:  48%, G loss: 1.533711\n",
      "Ep: 684, steps: 16, D loss: 0.240619, acc:  59%, G loss: 1.566411\n",
      "Ep: 684, steps: 17, D loss: 0.223461, acc:  65%, G loss: 1.550119\n",
      "Ep: 684, steps: 18, D loss: 0.224107, acc:  63%, G loss: 1.653613\n",
      "Ep: 684, steps: 19, D loss: 0.215650, acc:  66%, G loss: 1.556410\n",
      "Ep: 684, steps: 20, D loss: 0.179644, acc:  77%, G loss: 1.743760\n",
      "Ep: 684, steps: 21, D loss: 0.260445, acc:  43%, G loss: 1.444815\n",
      "Ep: 684, steps: 22, D loss: 0.185103, acc:  72%, G loss: 1.600382\n",
      "Ep: 684, steps: 23, D loss: 0.217627, acc:  65%, G loss: 1.810349\n",
      "Ep: 684, steps: 24, D loss: 0.215307, acc:  67%, G loss: 1.512775\n",
      "Ep: 684, steps: 25, D loss: 0.268029, acc:  49%, G loss: 1.710993\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 685, steps: 1, D loss: 0.248749, acc:  56%, G loss: 1.727661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 685, steps: 2, D loss: 0.244692, acc:  55%, G loss: 1.398028\n",
      "Ep: 685, steps: 3, D loss: 0.171078, acc:  82%, G loss: 1.858803\n",
      "Ep: 685, steps: 4, D loss: 0.193285, acc:  82%, G loss: 1.687180\n",
      "Ep: 685, steps: 5, D loss: 0.288702, acc:  46%, G loss: 1.614876\n",
      "Ep: 685, steps: 6, D loss: 0.253347, acc:  52%, G loss: 1.551808\n",
      "Ep: 685, steps: 7, D loss: 0.323686, acc:  31%, G loss: 1.507957\n",
      "Ep: 685, steps: 8, D loss: 0.233522, acc:  62%, G loss: 1.780734\n",
      "Ep: 685, steps: 9, D loss: 0.240447, acc:  61%, G loss: 1.659464\n",
      "Saved Model\n",
      "Ep: 685, steps: 10, D loss: 0.183273, acc:  79%, G loss: 1.578358\n",
      "Ep: 685, steps: 11, D loss: 0.287839, acc:  34%, G loss: 1.350811\n",
      "Ep: 685, steps: 12, D loss: 0.282618, acc:  37%, G loss: 1.400649\n",
      "Ep: 685, steps: 13, D loss: 0.262504, acc:  47%, G loss: 1.451108\n",
      "Ep: 685, steps: 14, D loss: 0.289491, acc:  39%, G loss: 1.540247\n",
      "Ep: 685, steps: 15, D loss: 0.240997, acc:  58%, G loss: 1.557751\n",
      "Ep: 685, steps: 16, D loss: 0.198766, acc:  74%, G loss: 1.588615\n",
      "Ep: 685, steps: 17, D loss: 0.237340, acc:  60%, G loss: 1.674180\n",
      "Ep: 685, steps: 18, D loss: 0.210530, acc:  68%, G loss: 1.628617\n",
      "Ep: 685, steps: 19, D loss: 0.165376, acc:  84%, G loss: 1.704523\n",
      "Ep: 685, steps: 20, D loss: 0.266632, acc:  41%, G loss: 1.538718\n",
      "Ep: 685, steps: 21, D loss: 0.193014, acc:  72%, G loss: 1.647812\n",
      "Ep: 685, steps: 22, D loss: 0.223399, acc:  63%, G loss: 1.789634\n",
      "Ep: 685, steps: 23, D loss: 0.205791, acc:  70%, G loss: 1.543512\n",
      "Ep: 685, steps: 24, D loss: 0.239972, acc:  60%, G loss: 1.721376\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 686, steps: 1, D loss: 0.249912, acc:  53%, G loss: 1.679429\n",
      "Ep: 686, steps: 2, D loss: 0.256738, acc:  51%, G loss: 1.501519\n",
      "Ep: 686, steps: 3, D loss: 0.159417, acc:  84%, G loss: 1.890390\n",
      "Ep: 686, steps: 4, D loss: 0.191980, acc:  80%, G loss: 1.673388\n",
      "Ep: 686, steps: 5, D loss: 0.274619, acc:  51%, G loss: 1.564347\n",
      "Ep: 686, steps: 6, D loss: 0.239005, acc:  57%, G loss: 1.543227\n",
      "Ep: 686, steps: 7, D loss: 0.306045, acc:  34%, G loss: 1.452882\n",
      "Ep: 686, steps: 8, D loss: 0.227500, acc:  63%, G loss: 1.660412\n",
      "Ep: 686, steps: 9, D loss: 0.248023, acc:  56%, G loss: 1.637233\n",
      "Ep: 686, steps: 10, D loss: 0.181012, acc:  79%, G loss: 1.595847\n",
      "Ep: 686, steps: 11, D loss: 0.268594, acc:  47%, G loss: 1.697952\n",
      "Ep: 686, steps: 12, D loss: 0.305665, acc:  31%, G loss: 1.420274\n",
      "Ep: 686, steps: 13, D loss: 0.298275, acc:  32%, G loss: 1.394259\n",
      "Ep: 686, steps: 14, D loss: 0.267275, acc:  44%, G loss: 1.461013\n",
      "Ep: 686, steps: 15, D loss: 0.257837, acc:  50%, G loss: 1.621242\n",
      "Ep: 686, steps: 16, D loss: 0.248563, acc:  55%, G loss: 1.587573\n",
      "Ep: 686, steps: 17, D loss: 0.212008, acc:  70%, G loss: 1.507040\n",
      "Ep: 686, steps: 18, D loss: 0.245283, acc:  55%, G loss: 1.706874\n",
      "Ep: 686, steps: 19, D loss: 0.212511, acc:  67%, G loss: 1.657950\n",
      "Ep: 686, steps: 20, D loss: 0.190947, acc:  76%, G loss: 1.659367\n",
      "Ep: 686, steps: 21, D loss: 0.268144, acc:  39%, G loss: 1.522601\n",
      "Ep: 686, steps: 22, D loss: 0.193749, acc:  74%, G loss: 1.594163\n",
      "Ep: 686, steps: 23, D loss: 0.218123, acc:  67%, G loss: 1.804758\n",
      "Ep: 686, steps: 24, D loss: 0.194617, acc:  76%, G loss: 1.490513\n",
      "Ep: 686, steps: 25, D loss: 0.244609, acc:  58%, G loss: 1.523906\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 687, steps: 1, D loss: 0.222760, acc:  66%, G loss: 1.615415\n",
      "Ep: 687, steps: 2, D loss: 0.258430, acc:  50%, G loss: 1.521770\n",
      "Ep: 687, steps: 3, D loss: 0.178483, acc:  79%, G loss: 1.851857\n",
      "Ep: 687, steps: 4, D loss: 0.194984, acc:  79%, G loss: 1.712102\n",
      "Ep: 687, steps: 5, D loss: 0.284685, acc:  51%, G loss: 1.631947\n",
      "Ep: 687, steps: 6, D loss: 0.236409, acc:  55%, G loss: 1.572239\n",
      "Ep: 687, steps: 7, D loss: 0.324769, acc:  27%, G loss: 1.537381\n",
      "Ep: 687, steps: 8, D loss: 0.220555, acc:  64%, G loss: 1.779206\n",
      "Ep: 687, steps: 9, D loss: 0.242162, acc:  59%, G loss: 1.604111\n",
      "Ep: 687, steps: 10, D loss: 0.170626, acc:  82%, G loss: 1.576753\n",
      "Ep: 687, steps: 11, D loss: 0.257288, acc:  50%, G loss: 1.723273\n",
      "Ep: 687, steps: 12, D loss: 0.309106, acc:  30%, G loss: 1.357770\n",
      "Ep: 687, steps: 13, D loss: 0.281266, acc:  39%, G loss: 1.369748\n",
      "Ep: 687, steps: 14, D loss: 0.272552, acc:  41%, G loss: 1.384756\n",
      "Ep: 687, steps: 15, D loss: 0.259876, acc:  48%, G loss: 1.561464\n",
      "Ep: 687, steps: 16, D loss: 0.240989, acc:  59%, G loss: 1.576612\n",
      "Ep: 687, steps: 17, D loss: 0.228388, acc:  65%, G loss: 1.586509\n",
      "Ep: 687, steps: 18, D loss: 0.221931, acc:  64%, G loss: 1.614276\n",
      "Ep: 687, steps: 19, D loss: 0.217742, acc:  67%, G loss: 1.596357\n",
      "Ep: 687, steps: 20, D loss: 0.194708, acc:  75%, G loss: 1.656537\n",
      "Ep: 687, steps: 21, D loss: 0.281215, acc:  35%, G loss: 1.466964\n",
      "Ep: 687, steps: 22, D loss: 0.197272, acc:  71%, G loss: 1.602047\n",
      "Ep: 687, steps: 23, D loss: 0.225608, acc:  64%, G loss: 1.800261\n",
      "Ep: 687, steps: 24, D loss: 0.207252, acc:  72%, G loss: 1.546646\n",
      "Ep: 687, steps: 25, D loss: 0.261959, acc:  52%, G loss: 1.520825\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 688, steps: 1, D loss: 0.235871, acc:  60%, G loss: 1.592997\n",
      "Ep: 688, steps: 2, D loss: 0.237069, acc:  59%, G loss: 1.434170\n",
      "Ep: 688, steps: 3, D loss: 0.173895, acc:  83%, G loss: 1.903992\n",
      "Ep: 688, steps: 4, D loss: 0.192485, acc:  81%, G loss: 1.653278\n",
      "Ep: 688, steps: 5, D loss: 0.280326, acc:  50%, G loss: 1.603966\n",
      "Ep: 688, steps: 6, D loss: 0.242385, acc:  54%, G loss: 1.545260\n",
      "Ep: 688, steps: 7, D loss: 0.299548, acc:  34%, G loss: 1.550962\n",
      "Saved Model\n",
      "Ep: 688, steps: 8, D loss: 0.225148, acc:  66%, G loss: 1.683483\n",
      "Ep: 688, steps: 9, D loss: 0.193021, acc:  74%, G loss: 1.508030\n",
      "Ep: 688, steps: 10, D loss: 0.289412, acc:  40%, G loss: 1.678917\n",
      "Ep: 688, steps: 11, D loss: 0.289296, acc:  38%, G loss: 1.332311\n",
      "Ep: 688, steps: 12, D loss: 0.275030, acc:  39%, G loss: 1.363625\n",
      "Ep: 688, steps: 13, D loss: 0.260849, acc:  49%, G loss: 1.499516\n",
      "Ep: 688, steps: 14, D loss: 0.267608, acc:  47%, G loss: 1.518661\n",
      "Ep: 688, steps: 15, D loss: 0.251347, acc:  53%, G loss: 1.577403\n",
      "Ep: 688, steps: 16, D loss: 0.218901, acc:  67%, G loss: 1.569759\n",
      "Ep: 688, steps: 17, D loss: 0.235407, acc:  59%, G loss: 1.674164\n",
      "Ep: 688, steps: 18, D loss: 0.224257, acc:  63%, G loss: 1.592065\n",
      "Ep: 688, steps: 19, D loss: 0.176522, acc:  80%, G loss: 1.749473\n",
      "Ep: 688, steps: 20, D loss: 0.264250, acc:  43%, G loss: 1.460848\n",
      "Ep: 688, steps: 21, D loss: 0.183786, acc:  72%, G loss: 1.555791\n",
      "Ep: 688, steps: 22, D loss: 0.218761, acc:  67%, G loss: 1.846292\n",
      "Ep: 688, steps: 23, D loss: 0.202995, acc:  73%, G loss: 1.503212\n",
      "Ep: 688, steps: 24, D loss: 0.250735, acc:  56%, G loss: 1.601234\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 689, steps: 1, D loss: 0.252856, acc:  54%, G loss: 1.648946\n",
      "Ep: 689, steps: 2, D loss: 0.243655, acc:  56%, G loss: 1.426921\n",
      "Ep: 689, steps: 3, D loss: 0.167014, acc:  83%, G loss: 1.857875\n",
      "Ep: 689, steps: 4, D loss: 0.196456, acc:  78%, G loss: 1.671890\n",
      "Ep: 689, steps: 5, D loss: 0.293574, acc:  48%, G loss: 1.637293\n",
      "Ep: 689, steps: 6, D loss: 0.233453, acc:  56%, G loss: 1.531384\n",
      "Ep: 689, steps: 7, D loss: 0.325923, acc:  27%, G loss: 1.498809\n",
      "Ep: 689, steps: 8, D loss: 0.220119, acc:  65%, G loss: 1.713728\n",
      "Ep: 689, steps: 9, D loss: 0.234865, acc:  62%, G loss: 1.558632\n",
      "Ep: 689, steps: 10, D loss: 0.179663, acc:  77%, G loss: 1.532261\n",
      "Ep: 689, steps: 11, D loss: 0.246755, acc:  54%, G loss: 1.715953\n",
      "Ep: 689, steps: 12, D loss: 0.295235, acc:  34%, G loss: 1.306555\n",
      "Ep: 689, steps: 13, D loss: 0.288634, acc:  34%, G loss: 1.396441\n",
      "Ep: 689, steps: 14, D loss: 0.273139, acc:  45%, G loss: 1.448791\n",
      "Ep: 689, steps: 15, D loss: 0.263388, acc:  49%, G loss: 1.584958\n",
      "Ep: 689, steps: 16, D loss: 0.248353, acc:  56%, G loss: 1.556791\n",
      "Ep: 689, steps: 17, D loss: 0.228469, acc:  64%, G loss: 1.518340\n",
      "Ep: 689, steps: 18, D loss: 0.230228, acc:  63%, G loss: 1.610794\n",
      "Ep: 689, steps: 19, D loss: 0.234285, acc:  61%, G loss: 1.603668\n",
      "Ep: 689, steps: 20, D loss: 0.192586, acc:  78%, G loss: 1.754672\n",
      "Ep: 689, steps: 21, D loss: 0.272497, acc:  39%, G loss: 1.421665\n",
      "Ep: 689, steps: 22, D loss: 0.198491, acc:  71%, G loss: 1.533073\n",
      "Ep: 689, steps: 23, D loss: 0.221790, acc:  64%, G loss: 1.825953\n",
      "Ep: 689, steps: 24, D loss: 0.206994, acc:  71%, G loss: 1.478353\n",
      "Ep: 689, steps: 25, D loss: 0.247265, acc:  59%, G loss: 1.532231\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 690, steps: 1, D loss: 0.245224, acc:  56%, G loss: 1.667175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 690, steps: 2, D loss: 0.249814, acc:  54%, G loss: 1.435396\n",
      "Ep: 690, steps: 3, D loss: 0.167501, acc:  85%, G loss: 1.929308\n",
      "Ep: 690, steps: 4, D loss: 0.195472, acc:  79%, G loss: 1.675503\n",
      "Ep: 690, steps: 5, D loss: 0.286042, acc:  50%, G loss: 1.578369\n",
      "Ep: 690, steps: 6, D loss: 0.245697, acc:  54%, G loss: 1.524582\n",
      "Ep: 690, steps: 7, D loss: 0.299342, acc:  35%, G loss: 1.481126\n",
      "Ep: 690, steps: 8, D loss: 0.225805, acc:  64%, G loss: 1.701896\n",
      "Ep: 690, steps: 9, D loss: 0.235796, acc:  63%, G loss: 1.603136\n",
      "Ep: 690, steps: 10, D loss: 0.187737, acc:  76%, G loss: 1.591091\n",
      "Ep: 690, steps: 11, D loss: 0.261911, acc:  50%, G loss: 1.763204\n",
      "Ep: 690, steps: 12, D loss: 0.293804, acc:  35%, G loss: 1.385972\n",
      "Ep: 690, steps: 13, D loss: 0.295078, acc:  29%, G loss: 1.363364\n",
      "Ep: 690, steps: 14, D loss: 0.277200, acc:  42%, G loss: 1.465428\n",
      "Ep: 690, steps: 15, D loss: 0.257024, acc:  48%, G loss: 1.517066\n",
      "Ep: 690, steps: 16, D loss: 0.246747, acc:  56%, G loss: 1.526850\n",
      "Ep: 690, steps: 17, D loss: 0.225519, acc:  65%, G loss: 1.516265\n",
      "Ep: 690, steps: 18, D loss: 0.233368, acc:  60%, G loss: 1.602741\n",
      "Ep: 690, steps: 19, D loss: 0.225212, acc:  62%, G loss: 1.578382\n",
      "Ep: 690, steps: 20, D loss: 0.181994, acc:  77%, G loss: 1.738020\n",
      "Ep: 690, steps: 21, D loss: 0.271418, acc:  38%, G loss: 1.460126\n",
      "Ep: 690, steps: 22, D loss: 0.207483, acc:  66%, G loss: 1.563918\n",
      "Ep: 690, steps: 23, D loss: 0.215254, acc:  67%, G loss: 1.832977\n",
      "Ep: 690, steps: 24, D loss: 0.209370, acc:  68%, G loss: 1.568994\n",
      "Ep: 690, steps: 25, D loss: 0.252483, acc:  55%, G loss: 1.568489\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 691, steps: 1, D loss: 0.246982, acc:  54%, G loss: 1.785565\n",
      "Ep: 691, steps: 2, D loss: 0.244079, acc:  57%, G loss: 1.463071\n",
      "Ep: 691, steps: 3, D loss: 0.167107, acc:  85%, G loss: 1.841302\n",
      "Ep: 691, steps: 4, D loss: 0.177628, acc:  86%, G loss: 1.634174\n",
      "Ep: 691, steps: 5, D loss: 0.286255, acc:  48%, G loss: 1.608270\n",
      "Saved Model\n",
      "Ep: 691, steps: 6, D loss: 0.240021, acc:  53%, G loss: 1.483154\n",
      "Ep: 691, steps: 7, D loss: 0.245798, acc:  58%, G loss: 1.718628\n",
      "Ep: 691, steps: 8, D loss: 0.199984, acc:  74%, G loss: 1.731614\n",
      "Ep: 691, steps: 9, D loss: 0.145963, acc:  90%, G loss: 1.624783\n",
      "Ep: 691, steps: 10, D loss: 0.248685, acc:  55%, G loss: 1.796786\n",
      "Ep: 691, steps: 11, D loss: 0.318326, acc:  29%, G loss: 1.405245\n",
      "Ep: 691, steps: 12, D loss: 0.311429, acc:  25%, G loss: 1.359368\n",
      "Ep: 691, steps: 13, D loss: 0.298956, acc:  32%, G loss: 1.466359\n",
      "Ep: 691, steps: 14, D loss: 0.279067, acc:  40%, G loss: 1.560677\n",
      "Ep: 691, steps: 15, D loss: 0.246009, acc:  55%, G loss: 1.579580\n",
      "Ep: 691, steps: 16, D loss: 0.218864, acc:  65%, G loss: 1.504751\n",
      "Ep: 691, steps: 17, D loss: 0.242483, acc:  54%, G loss: 1.563679\n",
      "Ep: 691, steps: 18, D loss: 0.187234, acc:  72%, G loss: 1.612612\n",
      "Ep: 691, steps: 19, D loss: 0.181198, acc:  76%, G loss: 1.743681\n",
      "Ep: 691, steps: 20, D loss: 0.272776, acc:  37%, G loss: 1.447428\n",
      "Ep: 691, steps: 21, D loss: 0.213096, acc:  68%, G loss: 1.622714\n",
      "Ep: 691, steps: 22, D loss: 0.212520, acc:  70%, G loss: 1.777632\n",
      "Ep: 691, steps: 23, D loss: 0.207325, acc:  71%, G loss: 1.601157\n",
      "Ep: 691, steps: 24, D loss: 0.248932, acc:  56%, G loss: 1.501396\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 692, steps: 1, D loss: 0.239647, acc:  58%, G loss: 1.630126\n",
      "Ep: 692, steps: 2, D loss: 0.238406, acc:  57%, G loss: 1.406857\n",
      "Ep: 692, steps: 3, D loss: 0.173610, acc:  83%, G loss: 1.856456\n",
      "Ep: 692, steps: 4, D loss: 0.186697, acc:  81%, G loss: 1.658545\n",
      "Ep: 692, steps: 5, D loss: 0.267352, acc:  53%, G loss: 1.578328\n",
      "Ep: 692, steps: 6, D loss: 0.260932, acc:  51%, G loss: 1.562129\n",
      "Ep: 692, steps: 7, D loss: 0.339184, acc:  26%, G loss: 1.411841\n",
      "Ep: 692, steps: 8, D loss: 0.225085, acc:  65%, G loss: 1.661014\n",
      "Ep: 692, steps: 9, D loss: 0.237038, acc:  61%, G loss: 1.658599\n",
      "Ep: 692, steps: 10, D loss: 0.179697, acc:  78%, G loss: 1.560295\n",
      "Ep: 692, steps: 11, D loss: 0.256726, acc:  50%, G loss: 1.866342\n",
      "Ep: 692, steps: 12, D loss: 0.305807, acc:  31%, G loss: 1.359988\n",
      "Ep: 692, steps: 13, D loss: 0.282039, acc:  35%, G loss: 1.381512\n",
      "Ep: 692, steps: 14, D loss: 0.273371, acc:  45%, G loss: 1.420577\n",
      "Ep: 692, steps: 15, D loss: 0.255307, acc:  50%, G loss: 1.634648\n",
      "Ep: 692, steps: 16, D loss: 0.245298, acc:  56%, G loss: 1.549775\n",
      "Ep: 692, steps: 17, D loss: 0.214837, acc:  69%, G loss: 1.514500\n",
      "Ep: 692, steps: 18, D loss: 0.238935, acc:  58%, G loss: 1.587071\n",
      "Ep: 692, steps: 19, D loss: 0.221486, acc:  64%, G loss: 1.524401\n",
      "Ep: 692, steps: 20, D loss: 0.184624, acc:  76%, G loss: 1.720443\n",
      "Ep: 692, steps: 21, D loss: 0.263699, acc:  41%, G loss: 1.424851\n",
      "Ep: 692, steps: 22, D loss: 0.205661, acc:  69%, G loss: 1.722503\n",
      "Ep: 692, steps: 23, D loss: 0.216999, acc:  66%, G loss: 1.764181\n",
      "Ep: 692, steps: 24, D loss: 0.206638, acc:  72%, G loss: 1.586223\n",
      "Ep: 692, steps: 25, D loss: 0.259290, acc:  55%, G loss: 1.658472\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 693, steps: 1, D loss: 0.222291, acc:  66%, G loss: 1.705503\n",
      "Ep: 693, steps: 2, D loss: 0.248033, acc:  53%, G loss: 1.420531\n",
      "Ep: 693, steps: 3, D loss: 0.176355, acc:  81%, G loss: 1.873045\n",
      "Ep: 693, steps: 4, D loss: 0.186817, acc:  83%, G loss: 1.644260\n",
      "Ep: 693, steps: 5, D loss: 0.271528, acc:  51%, G loss: 1.562137\n",
      "Ep: 693, steps: 6, D loss: 0.231226, acc:  56%, G loss: 1.578450\n",
      "Ep: 693, steps: 7, D loss: 0.319202, acc:  33%, G loss: 1.452391\n",
      "Ep: 693, steps: 8, D loss: 0.234775, acc:  62%, G loss: 1.647183\n",
      "Ep: 693, steps: 9, D loss: 0.248828, acc:  57%, G loss: 1.584635\n",
      "Ep: 693, steps: 10, D loss: 0.186491, acc:  77%, G loss: 1.593701\n",
      "Ep: 693, steps: 11, D loss: 0.246382, acc:  54%, G loss: 1.793380\n",
      "Ep: 693, steps: 12, D loss: 0.295735, acc:  32%, G loss: 1.357873\n",
      "Ep: 693, steps: 13, D loss: 0.280322, acc:  36%, G loss: 1.335220\n",
      "Ep: 693, steps: 14, D loss: 0.270895, acc:  44%, G loss: 1.467523\n",
      "Ep: 693, steps: 15, D loss: 0.251725, acc:  53%, G loss: 1.565561\n",
      "Ep: 693, steps: 16, D loss: 0.249829, acc:  54%, G loss: 1.532766\n",
      "Ep: 693, steps: 17, D loss: 0.220084, acc:  67%, G loss: 1.492951\n",
      "Ep: 693, steps: 18, D loss: 0.239537, acc:  58%, G loss: 1.633925\n",
      "Ep: 693, steps: 19, D loss: 0.219748, acc:  64%, G loss: 1.554075\n",
      "Ep: 693, steps: 20, D loss: 0.191732, acc:  73%, G loss: 1.698618\n",
      "Ep: 693, steps: 21, D loss: 0.255052, acc:  44%, G loss: 1.479315\n",
      "Ep: 693, steps: 22, D loss: 0.193150, acc:  71%, G loss: 1.575812\n",
      "Ep: 693, steps: 23, D loss: 0.217130, acc:  66%, G loss: 1.788914\n",
      "Ep: 693, steps: 24, D loss: 0.209166, acc:  70%, G loss: 1.512403\n",
      "Ep: 693, steps: 25, D loss: 0.238090, acc:  60%, G loss: 1.661531\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 694, steps: 1, D loss: 0.232588, acc:  61%, G loss: 1.686326\n",
      "Ep: 694, steps: 2, D loss: 0.256831, acc:  53%, G loss: 1.459270\n",
      "Ep: 694, steps: 3, D loss: 0.163651, acc:  85%, G loss: 1.929664\n",
      "Saved Model\n",
      "Ep: 694, steps: 4, D loss: 0.194971, acc:  80%, G loss: 1.688031\n",
      "Ep: 694, steps: 5, D loss: 0.243164, acc:  54%, G loss: 1.723163\n",
      "Ep: 694, steps: 6, D loss: 0.356338, acc:  21%, G loss: 1.484439\n",
      "Ep: 694, steps: 7, D loss: 0.221455, acc:  67%, G loss: 1.681321\n",
      "Ep: 694, steps: 8, D loss: 0.230474, acc:  64%, G loss: 1.595717\n",
      "Ep: 694, steps: 9, D loss: 0.191612, acc:  72%, G loss: 1.532579\n",
      "Ep: 694, steps: 10, D loss: 0.260350, acc:  50%, G loss: 1.870814\n",
      "Ep: 694, steps: 11, D loss: 0.302497, acc:  31%, G loss: 1.317626\n",
      "Ep: 694, steps: 12, D loss: 0.293850, acc:  32%, G loss: 1.365387\n",
      "Ep: 694, steps: 13, D loss: 0.280191, acc:  42%, G loss: 1.459562\n",
      "Ep: 694, steps: 14, D loss: 0.241008, acc:  56%, G loss: 1.618281\n",
      "Ep: 694, steps: 15, D loss: 0.234636, acc:  61%, G loss: 1.557297\n",
      "Ep: 694, steps: 16, D loss: 0.225458, acc:  65%, G loss: 1.514732\n",
      "Ep: 694, steps: 17, D loss: 0.229933, acc:  62%, G loss: 1.591939\n",
      "Ep: 694, steps: 18, D loss: 0.222537, acc:  64%, G loss: 1.601900\n",
      "Ep: 694, steps: 19, D loss: 0.203805, acc:  73%, G loss: 1.675668\n",
      "Ep: 694, steps: 20, D loss: 0.278556, acc:  36%, G loss: 1.442570\n",
      "Ep: 694, steps: 21, D loss: 0.207392, acc:  66%, G loss: 1.715004\n",
      "Ep: 694, steps: 22, D loss: 0.239037, acc:  57%, G loss: 1.767507\n",
      "Ep: 694, steps: 23, D loss: 0.206513, acc:  71%, G loss: 1.538471\n",
      "Ep: 694, steps: 24, D loss: 0.246722, acc:  56%, G loss: 1.517849\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 695, steps: 1, D loss: 0.219687, acc:  65%, G loss: 1.660717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 695, steps: 2, D loss: 0.238367, acc:  57%, G loss: 1.431636\n",
      "Ep: 695, steps: 3, D loss: 0.167914, acc:  83%, G loss: 1.941355\n",
      "Ep: 695, steps: 4, D loss: 0.196410, acc:  79%, G loss: 1.680027\n",
      "Ep: 695, steps: 5, D loss: 0.277122, acc:  48%, G loss: 1.620439\n",
      "Ep: 695, steps: 6, D loss: 0.251577, acc:  52%, G loss: 1.517049\n",
      "Ep: 695, steps: 7, D loss: 0.300764, acc:  33%, G loss: 1.505269\n",
      "Ep: 695, steps: 8, D loss: 0.234162, acc:  60%, G loss: 1.633382\n",
      "Ep: 695, steps: 9, D loss: 0.250542, acc:  55%, G loss: 1.622314\n",
      "Ep: 695, steps: 10, D loss: 0.182083, acc:  78%, G loss: 1.482002\n",
      "Ep: 695, steps: 11, D loss: 0.239248, acc:  56%, G loss: 1.780813\n",
      "Ep: 695, steps: 12, D loss: 0.294743, acc:  32%, G loss: 1.418005\n",
      "Ep: 695, steps: 13, D loss: 0.277420, acc:  40%, G loss: 1.338180\n",
      "Ep: 695, steps: 14, D loss: 0.265979, acc:  45%, G loss: 1.447723\n",
      "Ep: 695, steps: 15, D loss: 0.270230, acc:  49%, G loss: 1.555721\n",
      "Ep: 695, steps: 16, D loss: 0.239190, acc:  59%, G loss: 1.586131\n",
      "Ep: 695, steps: 17, D loss: 0.210445, acc:  70%, G loss: 1.439643\n",
      "Ep: 695, steps: 18, D loss: 0.229753, acc:  62%, G loss: 1.582908\n",
      "Ep: 695, steps: 19, D loss: 0.219679, acc:  65%, G loss: 1.561716\n",
      "Ep: 695, steps: 20, D loss: 0.174607, acc:  80%, G loss: 1.697794\n",
      "Ep: 695, steps: 21, D loss: 0.273644, acc:  38%, G loss: 1.648620\n",
      "Ep: 695, steps: 22, D loss: 0.197206, acc:  70%, G loss: 1.583571\n",
      "Ep: 695, steps: 23, D loss: 0.215210, acc:  65%, G loss: 1.845569\n",
      "Ep: 695, steps: 24, D loss: 0.207572, acc:  70%, G loss: 1.568025\n",
      "Ep: 695, steps: 25, D loss: 0.257263, acc:  55%, G loss: 1.725532\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 696, steps: 1, D loss: 0.240501, acc:  59%, G loss: 1.745890\n",
      "Ep: 696, steps: 2, D loss: 0.251201, acc:  52%, G loss: 1.422876\n",
      "Ep: 696, steps: 3, D loss: 0.163959, acc:  83%, G loss: 1.883802\n",
      "Ep: 696, steps: 4, D loss: 0.201096, acc:  78%, G loss: 1.648098\n",
      "Ep: 696, steps: 5, D loss: 0.294247, acc:  45%, G loss: 1.601191\n",
      "Ep: 696, steps: 6, D loss: 0.241774, acc:  53%, G loss: 1.563511\n",
      "Ep: 696, steps: 7, D loss: 0.320216, acc:  30%, G loss: 1.564243\n",
      "Ep: 696, steps: 8, D loss: 0.226119, acc:  65%, G loss: 1.677723\n",
      "Ep: 696, steps: 9, D loss: 0.258893, acc:  51%, G loss: 1.605733\n",
      "Ep: 696, steps: 10, D loss: 0.183171, acc:  76%, G loss: 1.557252\n",
      "Ep: 696, steps: 11, D loss: 0.267144, acc:  49%, G loss: 1.718091\n",
      "Ep: 696, steps: 12, D loss: 0.309361, acc:  27%, G loss: 1.345882\n",
      "Ep: 696, steps: 13, D loss: 0.301288, acc:  30%, G loss: 1.344654\n",
      "Ep: 696, steps: 14, D loss: 0.280230, acc:  40%, G loss: 1.468249\n",
      "Ep: 696, steps: 15, D loss: 0.229848, acc:  61%, G loss: 1.601815\n",
      "Ep: 696, steps: 16, D loss: 0.239008, acc:  60%, G loss: 1.550293\n",
      "Ep: 696, steps: 17, D loss: 0.224854, acc:  66%, G loss: 1.513170\n",
      "Ep: 696, steps: 18, D loss: 0.232105, acc:  61%, G loss: 1.612389\n",
      "Ep: 696, steps: 19, D loss: 0.215666, acc:  66%, G loss: 1.597122\n",
      "Ep: 696, steps: 20, D loss: 0.192825, acc:  76%, G loss: 1.661658\n",
      "Ep: 696, steps: 21, D loss: 0.264920, acc:  42%, G loss: 1.415113\n",
      "Ep: 696, steps: 22, D loss: 0.200125, acc:  70%, G loss: 1.505535\n",
      "Ep: 696, steps: 23, D loss: 0.233633, acc:  60%, G loss: 1.792104\n",
      "Ep: 696, steps: 24, D loss: 0.220336, acc:  67%, G loss: 1.533654\n",
      "Ep: 696, steps: 25, D loss: 0.247912, acc:  55%, G loss: 1.630571\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 697, steps: 1, D loss: 0.234111, acc:  61%, G loss: 1.647433\n",
      "Saved Model\n",
      "Ep: 697, steps: 2, D loss: 0.229215, acc:  61%, G loss: 1.460904\n",
      "Ep: 697, steps: 3, D loss: 0.198793, acc:  77%, G loss: 1.507479\n",
      "Ep: 697, steps: 4, D loss: 0.277286, acc:  48%, G loss: 1.589184\n",
      "Ep: 697, steps: 5, D loss: 0.226396, acc:  56%, G loss: 1.550549\n",
      "Ep: 697, steps: 6, D loss: 0.316489, acc:  32%, G loss: 1.511453\n",
      "Ep: 697, steps: 7, D loss: 0.236735, acc:  60%, G loss: 1.706937\n",
      "Ep: 697, steps: 8, D loss: 0.228038, acc:  63%, G loss: 1.634155\n",
      "Ep: 697, steps: 9, D loss: 0.184820, acc:  77%, G loss: 1.514865\n",
      "Ep: 697, steps: 10, D loss: 0.234661, acc:  59%, G loss: 1.748147\n",
      "Ep: 697, steps: 11, D loss: 0.300530, acc:  31%, G loss: 1.375304\n",
      "Ep: 697, steps: 12, D loss: 0.293933, acc:  32%, G loss: 1.375510\n",
      "Ep: 697, steps: 13, D loss: 0.270138, acc:  45%, G loss: 1.489411\n",
      "Ep: 697, steps: 14, D loss: 0.251991, acc:  53%, G loss: 1.613196\n",
      "Ep: 697, steps: 15, D loss: 0.252232, acc:  52%, G loss: 1.555420\n",
      "Ep: 697, steps: 16, D loss: 0.229888, acc:  62%, G loss: 1.485829\n",
      "Ep: 697, steps: 17, D loss: 0.234261, acc:  61%, G loss: 1.648701\n",
      "Ep: 697, steps: 18, D loss: 0.219723, acc:  64%, G loss: 1.594948\n",
      "Ep: 697, steps: 19, D loss: 0.181652, acc:  78%, G loss: 1.667561\n",
      "Ep: 697, steps: 20, D loss: 0.269706, acc:  40%, G loss: 1.486488\n",
      "Ep: 697, steps: 21, D loss: 0.196138, acc:  72%, G loss: 1.580754\n",
      "Ep: 697, steps: 22, D loss: 0.228166, acc:  60%, G loss: 1.821881\n",
      "Ep: 697, steps: 23, D loss: 0.210713, acc:  70%, G loss: 1.578451\n",
      "Ep: 697, steps: 24, D loss: 0.246469, acc:  58%, G loss: 1.513893\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 698, steps: 1, D loss: 0.237267, acc:  58%, G loss: 1.657021\n",
      "Ep: 698, steps: 2, D loss: 0.255191, acc:  52%, G loss: 1.443171\n",
      "Ep: 698, steps: 3, D loss: 0.190256, acc:  76%, G loss: 1.829064\n",
      "Ep: 698, steps: 4, D loss: 0.192219, acc:  80%, G loss: 1.692719\n",
      "Ep: 698, steps: 5, D loss: 0.263023, acc:  54%, G loss: 1.571538\n",
      "Ep: 698, steps: 6, D loss: 0.254359, acc:  53%, G loss: 1.532570\n",
      "Ep: 698, steps: 7, D loss: 0.330952, acc:  28%, G loss: 1.427298\n",
      "Ep: 698, steps: 8, D loss: 0.218719, acc:  67%, G loss: 1.653067\n",
      "Ep: 698, steps: 9, D loss: 0.239098, acc:  59%, G loss: 1.567406\n",
      "Ep: 698, steps: 10, D loss: 0.182704, acc:  77%, G loss: 1.626499\n",
      "Ep: 698, steps: 11, D loss: 0.252095, acc:  51%, G loss: 1.715564\n",
      "Ep: 698, steps: 12, D loss: 0.289640, acc:  35%, G loss: 1.361132\n",
      "Ep: 698, steps: 13, D loss: 0.275202, acc:  38%, G loss: 1.367724\n",
      "Ep: 698, steps: 14, D loss: 0.273444, acc:  43%, G loss: 1.449296\n",
      "Ep: 698, steps: 15, D loss: 0.253384, acc:  50%, G loss: 1.568608\n",
      "Ep: 698, steps: 16, D loss: 0.248411, acc:  55%, G loss: 1.559283\n",
      "Ep: 698, steps: 17, D loss: 0.216764, acc:  67%, G loss: 1.492361\n",
      "Ep: 698, steps: 18, D loss: 0.241827, acc:  57%, G loss: 1.597857\n",
      "Ep: 698, steps: 19, D loss: 0.214876, acc:  64%, G loss: 1.605201\n",
      "Ep: 698, steps: 20, D loss: 0.188183, acc:  76%, G loss: 1.699374\n",
      "Ep: 698, steps: 21, D loss: 0.266001, acc:  40%, G loss: 1.460839\n",
      "Ep: 698, steps: 22, D loss: 0.186180, acc:  74%, G loss: 1.549955\n",
      "Ep: 698, steps: 23, D loss: 0.231055, acc:  60%, G loss: 1.833255\n",
      "Ep: 698, steps: 24, D loss: 0.205119, acc:  71%, G loss: 1.545460\n",
      "Ep: 698, steps: 25, D loss: 0.258289, acc:  53%, G loss: 1.524464\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 699, steps: 1, D loss: 0.236943, acc:  59%, G loss: 1.694676\n",
      "Ep: 699, steps: 2, D loss: 0.247497, acc:  55%, G loss: 1.424308\n",
      "Ep: 699, steps: 3, D loss: 0.175660, acc:  82%, G loss: 1.947939\n",
      "Ep: 699, steps: 4, D loss: 0.190318, acc:  80%, G loss: 1.591120\n",
      "Ep: 699, steps: 5, D loss: 0.278182, acc:  51%, G loss: 1.598514\n",
      "Ep: 699, steps: 6, D loss: 0.232999, acc:  55%, G loss: 1.521572\n",
      "Ep: 699, steps: 7, D loss: 0.327290, acc:  25%, G loss: 1.526549\n",
      "Ep: 699, steps: 8, D loss: 0.222766, acc:  63%, G loss: 1.747606\n",
      "Ep: 699, steps: 9, D loss: 0.248068, acc:  55%, G loss: 1.658309\n",
      "Ep: 699, steps: 10, D loss: 0.189883, acc:  76%, G loss: 1.607903\n",
      "Ep: 699, steps: 11, D loss: 0.258640, acc:  50%, G loss: 1.816654\n",
      "Ep: 699, steps: 12, D loss: 0.297933, acc:  33%, G loss: 1.445523\n",
      "Ep: 699, steps: 13, D loss: 0.290165, acc:  33%, G loss: 1.438036\n",
      "Ep: 699, steps: 14, D loss: 0.284788, acc:  40%, G loss: 1.501202\n",
      "Ep: 699, steps: 15, D loss: 0.253336, acc:  51%, G loss: 1.586359\n",
      "Ep: 699, steps: 16, D loss: 0.251327, acc:  52%, G loss: 1.557974\n",
      "Ep: 699, steps: 17, D loss: 0.225079, acc:  65%, G loss: 1.482768\n",
      "Ep: 699, steps: 18, D loss: 0.232344, acc:  61%, G loss: 1.643737\n",
      "Ep: 699, steps: 19, D loss: 0.207592, acc:  70%, G loss: 1.612706\n",
      "Ep: 699, steps: 20, D loss: 0.194660, acc:  75%, G loss: 1.675651\n",
      "Ep: 699, steps: 21, D loss: 0.271260, acc:  37%, G loss: 1.458882\n",
      "Ep: 699, steps: 22, D loss: 0.215044, acc:  65%, G loss: 1.554744\n",
      "Ep: 699, steps: 23, D loss: 0.217305, acc:  66%, G loss: 1.786555\n",
      "Ep: 699, steps: 24, D loss: 0.209016, acc:  71%, G loss: 1.517112\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 699, steps: 25, D loss: 0.240683, acc:  59%, G loss: 1.664853\n",
      "Ep: 699, steps: 26, D loss: 0.260602, acc:  50%, G loss: 1.389512\n",
      "Ep: 699, steps: 27, D loss: 0.165643, acc:  85%, G loss: 1.885161\n",
      "Ep: 699, steps: 28, D loss: 0.208328, acc:  74%, G loss: 1.566380\n",
      "Ep: 699, steps: 29, D loss: 0.252117, acc:  56%, G loss: 1.642941\n",
      "Ep: 699, steps: 30, D loss: 0.246128, acc:  55%, G loss: 1.536783\n",
      "Ep: 699, steps: 31, D loss: 0.281772, acc:  39%, G loss: 1.448763\n",
      "Ep: 699, steps: 32, D loss: 0.217703, acc:  66%, G loss: 1.747402\n",
      "Ep: 699, steps: 33, D loss: 0.228745, acc:  63%, G loss: 1.599555\n",
      "Ep: 699, steps: 34, D loss: 0.187060, acc:  75%, G loss: 1.550632\n",
      "Ep: 699, steps: 35, D loss: 0.278341, acc:  42%, G loss: 1.664938\n",
      "Ep: 699, steps: 36, D loss: 0.281973, acc:  39%, G loss: 1.357537\n",
      "Ep: 699, steps: 37, D loss: 0.283325, acc:  35%, G loss: 1.432367\n",
      "Ep: 699, steps: 38, D loss: 0.269905, acc:  43%, G loss: 1.466254\n",
      "Ep: 699, steps: 39, D loss: 0.270840, acc:  46%, G loss: 1.604587\n",
      "Ep: 699, steps: 40, D loss: 0.232316, acc:  62%, G loss: 1.593895\n",
      "Ep: 699, steps: 41, D loss: 0.213024, acc:  69%, G loss: 1.572965\n",
      "Ep: 699, steps: 42, D loss: 0.228790, acc:  63%, G loss: 1.631033\n",
      "Ep: 699, steps: 43, D loss: 0.215106, acc:  66%, G loss: 1.595998\n",
      "Ep: 699, steps: 44, D loss: 0.173546, acc:  81%, G loss: 1.732991\n",
      "Ep: 699, steps: 45, D loss: 0.263413, acc:  43%, G loss: 1.493405\n",
      "Ep: 699, steps: 46, D loss: 0.230472, acc:  61%, G loss: 1.664863\n",
      "Ep: 699, steps: 47, D loss: 0.224275, acc:  64%, G loss: 1.859139\n",
      "Ep: 699, steps: 48, D loss: 0.205512, acc:  70%, G loss: 1.645753\n",
      "Ep: 699, steps: 49, D loss: 0.268359, acc:  51%, G loss: 1.790828\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 700, steps: 1, D loss: 0.274500, acc:  46%, G loss: 1.658950\n",
      "Ep: 700, steps: 2, D loss: 0.238591, acc:  58%, G loss: 1.423067\n",
      "Ep: 700, steps: 3, D loss: 0.173971, acc:  83%, G loss: 1.861336\n",
      "Ep: 700, steps: 4, D loss: 0.189277, acc:  83%, G loss: 1.610873\n",
      "Ep: 700, steps: 5, D loss: 0.268491, acc:  48%, G loss: 1.617208\n",
      "Ep: 700, steps: 6, D loss: 0.257007, acc:  50%, G loss: 1.580312\n",
      "Ep: 700, steps: 7, D loss: 0.297599, acc:  34%, G loss: 1.434779\n",
      "Ep: 700, steps: 8, D loss: 0.212048, acc:  69%, G loss: 1.777319\n",
      "Ep: 700, steps: 9, D loss: 0.241872, acc:  58%, G loss: 1.585757\n",
      "Ep: 700, steps: 10, D loss: 0.184517, acc:  75%, G loss: 1.517272\n",
      "Ep: 700, steps: 11, D loss: 0.262553, acc:  48%, G loss: 1.856323\n",
      "Ep: 700, steps: 12, D loss: 0.295133, acc:  34%, G loss: 1.402812\n",
      "Ep: 700, steps: 13, D loss: 0.287032, acc:  34%, G loss: 1.398493\n",
      "Ep: 700, steps: 14, D loss: 0.264347, acc:  47%, G loss: 1.488856\n",
      "Ep: 700, steps: 15, D loss: 0.255285, acc:  50%, G loss: 1.620999\n",
      "Ep: 700, steps: 16, D loss: 0.254205, acc:  51%, G loss: 1.542047\n",
      "Ep: 700, steps: 17, D loss: 0.226273, acc:  64%, G loss: 1.442165\n",
      "Ep: 700, steps: 18, D loss: 0.235909, acc:  61%, G loss: 1.632110\n",
      "Ep: 700, steps: 19, D loss: 0.213503, acc:  66%, G loss: 1.582149\n",
      "Ep: 700, steps: 20, D loss: 0.193129, acc:  73%, G loss: 1.711583\n",
      "Ep: 700, steps: 21, D loss: 0.259506, acc:  43%, G loss: 1.462767\n",
      "Ep: 700, steps: 22, D loss: 0.179802, acc:  78%, G loss: 1.549863\n",
      "Ep: 700, steps: 23, D loss: 0.230035, acc:  62%, G loss: 1.824790\n",
      "Ep: 700, steps: 24, D loss: 0.211229, acc:  69%, G loss: 1.498432\n",
      "Ep: 700, steps: 25, D loss: 0.237121, acc:  58%, G loss: 1.569645\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 701, steps: 1, D loss: 0.238927, acc:  59%, G loss: 1.658820\n",
      "Ep: 701, steps: 2, D loss: 0.260652, acc:  50%, G loss: 1.370919\n",
      "Ep: 701, steps: 3, D loss: 0.186932, acc:  76%, G loss: 1.844077\n",
      "Ep: 701, steps: 4, D loss: 0.190473, acc:  81%, G loss: 1.613671\n",
      "Ep: 701, steps: 5, D loss: 0.274435, acc:  48%, G loss: 1.607534\n",
      "Ep: 701, steps: 6, D loss: 0.243730, acc:  53%, G loss: 1.615302\n",
      "Ep: 701, steps: 7, D loss: 0.318124, acc:  30%, G loss: 1.471212\n",
      "Ep: 701, steps: 8, D loss: 0.225168, acc:  64%, G loss: 1.786939\n",
      "Ep: 701, steps: 9, D loss: 0.255582, acc:  54%, G loss: 1.601653\n",
      "Ep: 701, steps: 10, D loss: 0.191776, acc:  74%, G loss: 1.593669\n",
      "Ep: 701, steps: 11, D loss: 0.249650, acc:  54%, G loss: 1.775937\n",
      "Ep: 701, steps: 12, D loss: 0.293019, acc:  34%, G loss: 1.402572\n",
      "Ep: 701, steps: 13, D loss: 0.306694, acc:  27%, G loss: 1.393594\n",
      "Ep: 701, steps: 14, D loss: 0.276952, acc:  43%, G loss: 1.466001\n",
      "Ep: 701, steps: 15, D loss: 0.239081, acc:  56%, G loss: 1.534448\n",
      "Ep: 701, steps: 16, D loss: 0.248560, acc:  55%, G loss: 1.555163\n",
      "Ep: 701, steps: 17, D loss: 0.219734, acc:  65%, G loss: 1.565871\n",
      "Ep: 701, steps: 18, D loss: 0.245781, acc:  55%, G loss: 1.556530\n",
      "Ep: 701, steps: 19, D loss: 0.218285, acc:  66%, G loss: 1.549929\n",
      "Ep: 701, steps: 20, D loss: 0.176559, acc:  81%, G loss: 1.642505\n",
      "Ep: 701, steps: 21, D loss: 0.270712, acc:  40%, G loss: 1.415555\n",
      "Ep: 701, steps: 22, D loss: 0.202488, acc:  69%, G loss: 1.577915\n",
      "Saved Model\n",
      "Ep: 701, steps: 23, D loss: 0.233015, acc:  60%, G loss: 1.797032\n",
      "Ep: 701, steps: 24, D loss: 0.258226, acc:  52%, G loss: 1.546035\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 702, steps: 1, D loss: 0.227261, acc:  64%, G loss: 1.687415\n",
      "Ep: 702, steps: 2, D loss: 0.240785, acc:  60%, G loss: 1.440452\n",
      "Ep: 702, steps: 3, D loss: 0.180996, acc:  80%, G loss: 1.787769\n",
      "Ep: 702, steps: 4, D loss: 0.191711, acc:  82%, G loss: 1.649251\n",
      "Ep: 702, steps: 5, D loss: 0.284281, acc:  48%, G loss: 1.649536\n",
      "Ep: 702, steps: 6, D loss: 0.237747, acc:  56%, G loss: 1.590109\n",
      "Ep: 702, steps: 7, D loss: 0.305622, acc:  33%, G loss: 1.497897\n",
      "Ep: 702, steps: 8, D loss: 0.230054, acc:  62%, G loss: 1.670504\n",
      "Ep: 702, steps: 9, D loss: 0.227928, acc:  66%, G loss: 1.668387\n",
      "Ep: 702, steps: 10, D loss: 0.201222, acc:  73%, G loss: 1.515487\n",
      "Ep: 702, steps: 11, D loss: 0.267230, acc:  47%, G loss: 1.732706\n",
      "Ep: 702, steps: 12, D loss: 0.287201, acc:  36%, G loss: 1.433853\n",
      "Ep: 702, steps: 13, D loss: 0.299156, acc:  27%, G loss: 1.435481\n",
      "Ep: 702, steps: 14, D loss: 0.260326, acc:  48%, G loss: 1.462620\n",
      "Ep: 702, steps: 15, D loss: 0.243871, acc:  55%, G loss: 1.493920\n",
      "Ep: 702, steps: 16, D loss: 0.237461, acc:  58%, G loss: 1.538585\n",
      "Ep: 702, steps: 17, D loss: 0.208267, acc:  73%, G loss: 1.506746\n",
      "Ep: 702, steps: 18, D loss: 0.228750, acc:  61%, G loss: 1.652518\n",
      "Ep: 702, steps: 19, D loss: 0.205721, acc:  69%, G loss: 1.568593\n",
      "Ep: 702, steps: 20, D loss: 0.178930, acc:  77%, G loss: 1.669910\n",
      "Ep: 702, steps: 21, D loss: 0.264391, acc:  41%, G loss: 1.503972\n",
      "Ep: 702, steps: 22, D loss: 0.224296, acc:  63%, G loss: 1.719819\n",
      "Ep: 702, steps: 23, D loss: 0.219567, acc:  66%, G loss: 1.796107\n",
      "Ep: 702, steps: 24, D loss: 0.208136, acc:  69%, G loss: 1.552335\n",
      "Ep: 702, steps: 25, D loss: 0.247302, acc:  55%, G loss: 1.714182\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 703, steps: 1, D loss: 0.257150, acc:  53%, G loss: 1.659741\n",
      "Ep: 703, steps: 2, D loss: 0.258760, acc:  50%, G loss: 1.437612\n",
      "Ep: 703, steps: 3, D loss: 0.167819, acc:  86%, G loss: 1.883455\n",
      "Ep: 703, steps: 4, D loss: 0.192116, acc:  82%, G loss: 1.577297\n",
      "Ep: 703, steps: 5, D loss: 0.264863, acc:  49%, G loss: 1.644002\n",
      "Ep: 703, steps: 6, D loss: 0.243230, acc:  54%, G loss: 1.574696\n",
      "Ep: 703, steps: 7, D loss: 0.294394, acc:  36%, G loss: 1.468259\n",
      "Ep: 703, steps: 8, D loss: 0.226041, acc:  64%, G loss: 1.842439\n",
      "Ep: 703, steps: 9, D loss: 0.232222, acc:  62%, G loss: 1.596205\n",
      "Ep: 703, steps: 10, D loss: 0.178792, acc:  81%, G loss: 1.552689\n",
      "Ep: 703, steps: 11, D loss: 0.262583, acc:  49%, G loss: 1.750309\n",
      "Ep: 703, steps: 12, D loss: 0.291101, acc:  34%, G loss: 1.384542\n",
      "Ep: 703, steps: 13, D loss: 0.283530, acc:  36%, G loss: 1.376970\n",
      "Ep: 703, steps: 14, D loss: 0.273312, acc:  42%, G loss: 1.520017\n",
      "Ep: 703, steps: 15, D loss: 0.244767, acc:  55%, G loss: 1.534251\n",
      "Ep: 703, steps: 16, D loss: 0.248579, acc:  53%, G loss: 1.552593\n",
      "Ep: 703, steps: 17, D loss: 0.229378, acc:  61%, G loss: 1.508922\n",
      "Ep: 703, steps: 18, D loss: 0.236074, acc:  59%, G loss: 1.666972\n",
      "Ep: 703, steps: 19, D loss: 0.227436, acc:  61%, G loss: 1.568923\n",
      "Ep: 703, steps: 20, D loss: 0.189748, acc:  75%, G loss: 1.743852\n",
      "Ep: 703, steps: 21, D loss: 0.259156, acc:  44%, G loss: 1.475876\n",
      "Ep: 703, steps: 22, D loss: 0.184724, acc:  73%, G loss: 1.555928\n",
      "Ep: 703, steps: 23, D loss: 0.219738, acc:  65%, G loss: 1.767293\n",
      "Ep: 703, steps: 24, D loss: 0.221301, acc:  65%, G loss: 1.524096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 703, steps: 25, D loss: 0.252638, acc:  55%, G loss: 1.553923\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 704, steps: 1, D loss: 0.239937, acc:  59%, G loss: 1.738564\n",
      "Ep: 704, steps: 2, D loss: 0.244628, acc:  56%, G loss: 1.419604\n",
      "Ep: 704, steps: 3, D loss: 0.190555, acc:  78%, G loss: 1.761401\n",
      "Ep: 704, steps: 4, D loss: 0.188344, acc:  83%, G loss: 1.632965\n",
      "Ep: 704, steps: 5, D loss: 0.300614, acc:  42%, G loss: 1.577234\n",
      "Ep: 704, steps: 6, D loss: 0.243844, acc:  54%, G loss: 1.623200\n",
      "Ep: 704, steps: 7, D loss: 0.310863, acc:  31%, G loss: 1.643697\n",
      "Ep: 704, steps: 8, D loss: 0.212543, acc:  69%, G loss: 1.706504\n",
      "Ep: 704, steps: 9, D loss: 0.255653, acc:  55%, G loss: 1.631783\n",
      "Ep: 704, steps: 10, D loss: 0.194445, acc:  75%, G loss: 1.557463\n",
      "Ep: 704, steps: 11, D loss: 0.243329, acc:  54%, G loss: 1.703238\n",
      "Ep: 704, steps: 12, D loss: 0.294556, acc:  32%, G loss: 1.362055\n",
      "Ep: 704, steps: 13, D loss: 0.278533, acc:  39%, G loss: 1.365337\n",
      "Ep: 704, steps: 14, D loss: 0.266068, acc:  46%, G loss: 1.445108\n",
      "Ep: 704, steps: 15, D loss: 0.255282, acc:  52%, G loss: 1.475833\n",
      "Ep: 704, steps: 16, D loss: 0.251584, acc:  52%, G loss: 1.579735\n",
      "Ep: 704, steps: 17, D loss: 0.221010, acc:  68%, G loss: 1.494135\n",
      "Ep: 704, steps: 18, D loss: 0.226678, acc:  64%, G loss: 1.624870\n",
      "Ep: 704, steps: 19, D loss: 0.206674, acc:  69%, G loss: 1.606124\n",
      "Saved Model\n",
      "Ep: 704, steps: 20, D loss: 0.196130, acc:  72%, G loss: 1.682729\n",
      "Ep: 704, steps: 21, D loss: 0.205675, acc:  68%, G loss: 1.610633\n",
      "Ep: 704, steps: 22, D loss: 0.221234, acc:  67%, G loss: 1.810352\n",
      "Ep: 704, steps: 23, D loss: 0.209269, acc:  70%, G loss: 1.485111\n",
      "Ep: 704, steps: 24, D loss: 0.257126, acc:  54%, G loss: 1.583644\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 705, steps: 1, D loss: 0.226925, acc:  62%, G loss: 1.710210\n",
      "Ep: 705, steps: 2, D loss: 0.232172, acc:  60%, G loss: 1.410802\n",
      "Ep: 705, steps: 3, D loss: 0.152153, acc:  88%, G loss: 1.875343\n",
      "Ep: 705, steps: 4, D loss: 0.173326, acc:  87%, G loss: 1.616373\n",
      "Ep: 705, steps: 5, D loss: 0.301468, acc:  44%, G loss: 1.622353\n",
      "Ep: 705, steps: 6, D loss: 0.244058, acc:  53%, G loss: 1.632432\n",
      "Ep: 705, steps: 7, D loss: 0.322173, acc:  31%, G loss: 1.661838\n",
      "Ep: 705, steps: 8, D loss: 0.227288, acc:  63%, G loss: 1.751026\n",
      "Ep: 705, steps: 9, D loss: 0.233544, acc:  61%, G loss: 1.577719\n",
      "Ep: 705, steps: 10, D loss: 0.172254, acc:  80%, G loss: 1.549869\n",
      "Ep: 705, steps: 11, D loss: 0.268641, acc:  48%, G loss: 1.784353\n",
      "Ep: 705, steps: 12, D loss: 0.297475, acc:  35%, G loss: 1.384056\n",
      "Ep: 705, steps: 13, D loss: 0.296738, acc:  30%, G loss: 1.389338\n",
      "Ep: 705, steps: 14, D loss: 0.270001, acc:  45%, G loss: 1.477101\n",
      "Ep: 705, steps: 15, D loss: 0.257970, acc:  50%, G loss: 1.574086\n",
      "Ep: 705, steps: 16, D loss: 0.244783, acc:  56%, G loss: 1.587953\n",
      "Ep: 705, steps: 17, D loss: 0.231191, acc:  60%, G loss: 1.537030\n",
      "Ep: 705, steps: 18, D loss: 0.237298, acc:  60%, G loss: 1.665952\n",
      "Ep: 705, steps: 19, D loss: 0.200976, acc:  68%, G loss: 1.614639\n",
      "Ep: 705, steps: 20, D loss: 0.176778, acc:  80%, G loss: 1.721212\n",
      "Ep: 705, steps: 21, D loss: 0.272796, acc:  40%, G loss: 1.513839\n",
      "Ep: 705, steps: 22, D loss: 0.200910, acc:  67%, G loss: 1.562735\n",
      "Ep: 705, steps: 23, D loss: 0.224614, acc:  64%, G loss: 1.826967\n",
      "Ep: 705, steps: 24, D loss: 0.219068, acc:  65%, G loss: 1.604266\n",
      "Ep: 705, steps: 25, D loss: 0.242744, acc:  57%, G loss: 1.724442\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 706, steps: 1, D loss: 0.239218, acc:  59%, G loss: 1.574288\n",
      "Ep: 706, steps: 2, D loss: 0.262995, acc:  50%, G loss: 1.398915\n",
      "Ep: 706, steps: 3, D loss: 0.179151, acc:  80%, G loss: 1.813745\n",
      "Ep: 706, steps: 4, D loss: 0.191321, acc:  81%, G loss: 1.601554\n",
      "Ep: 706, steps: 5, D loss: 0.283544, acc:  49%, G loss: 1.608844\n",
      "Ep: 706, steps: 6, D loss: 0.251664, acc:  53%, G loss: 1.598486\n",
      "Ep: 706, steps: 7, D loss: 0.291065, acc:  37%, G loss: 1.519701\n",
      "Ep: 706, steps: 8, D loss: 0.229172, acc:  63%, G loss: 1.749281\n",
      "Ep: 706, steps: 9, D loss: 0.238285, acc:  62%, G loss: 1.629671\n",
      "Ep: 706, steps: 10, D loss: 0.183933, acc:  78%, G loss: 1.597955\n",
      "Ep: 706, steps: 11, D loss: 0.247498, acc:  54%, G loss: 1.707627\n",
      "Ep: 706, steps: 12, D loss: 0.289482, acc:  37%, G loss: 1.323573\n",
      "Ep: 706, steps: 13, D loss: 0.283322, acc:  35%, G loss: 1.386700\n",
      "Ep: 706, steps: 14, D loss: 0.264648, acc:  45%, G loss: 1.439903\n",
      "Ep: 706, steps: 15, D loss: 0.252501, acc:  52%, G loss: 1.519876\n",
      "Ep: 706, steps: 16, D loss: 0.252193, acc:  52%, G loss: 1.555695\n",
      "Ep: 706, steps: 17, D loss: 0.226470, acc:  63%, G loss: 1.530815\n",
      "Ep: 706, steps: 18, D loss: 0.226493, acc:  62%, G loss: 1.641546\n",
      "Ep: 706, steps: 19, D loss: 0.214101, acc:  66%, G loss: 1.547730\n",
      "Ep: 706, steps: 20, D loss: 0.178196, acc:  79%, G loss: 1.711540\n",
      "Ep: 706, steps: 21, D loss: 0.277455, acc:  37%, G loss: 1.492589\n",
      "Ep: 706, steps: 22, D loss: 0.218526, acc:  66%, G loss: 1.584939\n",
      "Ep: 706, steps: 23, D loss: 0.222767, acc:  63%, G loss: 1.804176\n",
      "Ep: 706, steps: 24, D loss: 0.211775, acc:  68%, G loss: 1.558099\n",
      "Ep: 706, steps: 25, D loss: 0.246260, acc:  56%, G loss: 1.607492\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 707, steps: 1, D loss: 0.240450, acc:  58%, G loss: 1.601184\n",
      "Ep: 707, steps: 2, D loss: 0.245791, acc:  56%, G loss: 1.448342\n",
      "Ep: 707, steps: 3, D loss: 0.175830, acc:  77%, G loss: 1.838195\n",
      "Ep: 707, steps: 4, D loss: 0.192972, acc:  80%, G loss: 1.602327\n",
      "Ep: 707, steps: 5, D loss: 0.261038, acc:  52%, G loss: 1.638040\n",
      "Ep: 707, steps: 6, D loss: 0.254982, acc:  53%, G loss: 1.600790\n",
      "Ep: 707, steps: 7, D loss: 0.305650, acc:  35%, G loss: 1.531270\n",
      "Ep: 707, steps: 8, D loss: 0.224778, acc:  66%, G loss: 1.708436\n",
      "Ep: 707, steps: 9, D loss: 0.244920, acc:  58%, G loss: 1.609772\n",
      "Ep: 707, steps: 10, D loss: 0.175130, acc:  79%, G loss: 1.530046\n",
      "Ep: 707, steps: 11, D loss: 0.245382, acc:  55%, G loss: 1.659896\n",
      "Ep: 707, steps: 12, D loss: 0.287397, acc:  35%, G loss: 1.356912\n",
      "Ep: 707, steps: 13, D loss: 0.288828, acc:  33%, G loss: 1.362220\n",
      "Ep: 707, steps: 14, D loss: 0.265122, acc:  47%, G loss: 1.475576\n",
      "Ep: 707, steps: 15, D loss: 0.262845, acc:  49%, G loss: 1.501706\n",
      "Ep: 707, steps: 16, D loss: 0.244509, acc:  56%, G loss: 1.575055\n",
      "Ep: 707, steps: 17, D loss: 0.213029, acc:  66%, G loss: 1.565724\n",
      "Saved Model\n",
      "Ep: 707, steps: 18, D loss: 0.229183, acc:  62%, G loss: 1.669674\n",
      "Ep: 707, steps: 19, D loss: 0.179207, acc:  77%, G loss: 1.680160\n",
      "Ep: 707, steps: 20, D loss: 0.244349, acc:  50%, G loss: 1.529717\n",
      "Ep: 707, steps: 21, D loss: 0.205733, acc:  64%, G loss: 1.635008\n",
      "Ep: 707, steps: 22, D loss: 0.229193, acc:  61%, G loss: 1.824400\n",
      "Ep: 707, steps: 23, D loss: 0.209422, acc:  72%, G loss: 1.555645\n",
      "Ep: 707, steps: 24, D loss: 0.244744, acc:  56%, G loss: 1.687457\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 708, steps: 1, D loss: 0.239795, acc:  57%, G loss: 1.636984\n",
      "Ep: 708, steps: 2, D loss: 0.261751, acc:  51%, G loss: 1.408075\n",
      "Ep: 708, steps: 3, D loss: 0.177093, acc:  81%, G loss: 1.832541\n",
      "Ep: 708, steps: 4, D loss: 0.189616, acc:  79%, G loss: 1.642537\n",
      "Ep: 708, steps: 5, D loss: 0.285132, acc:  48%, G loss: 1.572569\n",
      "Ep: 708, steps: 6, D loss: 0.251344, acc:  53%, G loss: 1.564586\n",
      "Ep: 708, steps: 7, D loss: 0.313023, acc:  30%, G loss: 1.501283\n",
      "Ep: 708, steps: 8, D loss: 0.219901, acc:  67%, G loss: 1.824081\n",
      "Ep: 708, steps: 9, D loss: 0.263207, acc:  50%, G loss: 1.592774\n",
      "Ep: 708, steps: 10, D loss: 0.180145, acc:  78%, G loss: 1.574280\n",
      "Ep: 708, steps: 11, D loss: 0.262513, acc:  50%, G loss: 1.752535\n",
      "Ep: 708, steps: 12, D loss: 0.295775, acc:  33%, G loss: 1.306587\n",
      "Ep: 708, steps: 13, D loss: 0.284718, acc:  35%, G loss: 1.355102\n",
      "Ep: 708, steps: 14, D loss: 0.263082, acc:  48%, G loss: 1.430283\n",
      "Ep: 708, steps: 15, D loss: 0.237591, acc:  56%, G loss: 1.544077\n",
      "Ep: 708, steps: 16, D loss: 0.237820, acc:  60%, G loss: 1.614384\n",
      "Ep: 708, steps: 17, D loss: 0.208238, acc:  74%, G loss: 1.485860\n",
      "Ep: 708, steps: 18, D loss: 0.235326, acc:  59%, G loss: 1.632893\n",
      "Ep: 708, steps: 19, D loss: 0.227388, acc:  63%, G loss: 1.586901\n",
      "Ep: 708, steps: 20, D loss: 0.192279, acc:  77%, G loss: 1.694266\n",
      "Ep: 708, steps: 21, D loss: 0.259643, acc:  44%, G loss: 1.535507\n",
      "Ep: 708, steps: 22, D loss: 0.196050, acc:  72%, G loss: 1.578768\n",
      "Ep: 708, steps: 23, D loss: 0.234156, acc:  58%, G loss: 1.744719\n",
      "Ep: 708, steps: 24, D loss: 0.213388, acc:  69%, G loss: 1.499216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 708, steps: 25, D loss: 0.265200, acc:  51%, G loss: 1.661751\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 709, steps: 1, D loss: 0.244407, acc:  56%, G loss: 1.668302\n",
      "Ep: 709, steps: 2, D loss: 0.242493, acc:  55%, G loss: 1.386629\n",
      "Ep: 709, steps: 3, D loss: 0.183400, acc:  79%, G loss: 1.831147\n",
      "Ep: 709, steps: 4, D loss: 0.197093, acc:  78%, G loss: 1.626394\n",
      "Ep: 709, steps: 5, D loss: 0.276114, acc:  49%, G loss: 1.546936\n",
      "Ep: 709, steps: 6, D loss: 0.239162, acc:  56%, G loss: 1.588830\n",
      "Ep: 709, steps: 7, D loss: 0.281039, acc:  40%, G loss: 1.431816\n",
      "Ep: 709, steps: 8, D loss: 0.226406, acc:  63%, G loss: 1.741679\n",
      "Ep: 709, steps: 9, D loss: 0.233759, acc:  62%, G loss: 1.577550\n",
      "Ep: 709, steps: 10, D loss: 0.196140, acc:  73%, G loss: 1.584859\n",
      "Ep: 709, steps: 11, D loss: 0.244723, acc:  55%, G loss: 1.687155\n",
      "Ep: 709, steps: 12, D loss: 0.303667, acc:  31%, G loss: 1.351916\n",
      "Ep: 709, steps: 13, D loss: 0.294613, acc:  32%, G loss: 1.419438\n",
      "Ep: 709, steps: 14, D loss: 0.263578, acc:  46%, G loss: 1.458043\n",
      "Ep: 709, steps: 15, D loss: 0.267546, acc:  46%, G loss: 1.492233\n",
      "Ep: 709, steps: 16, D loss: 0.238393, acc:  58%, G loss: 1.552357\n",
      "Ep: 709, steps: 17, D loss: 0.219521, acc:  66%, G loss: 1.533075\n",
      "Ep: 709, steps: 18, D loss: 0.236690, acc:  59%, G loss: 1.651498\n",
      "Ep: 709, steps: 19, D loss: 0.200445, acc:  68%, G loss: 1.577562\n",
      "Ep: 709, steps: 20, D loss: 0.188296, acc:  73%, G loss: 1.776286\n",
      "Ep: 709, steps: 21, D loss: 0.268101, acc:  42%, G loss: 1.448745\n",
      "Ep: 709, steps: 22, D loss: 0.199737, acc:  68%, G loss: 1.625535\n",
      "Ep: 709, steps: 23, D loss: 0.212897, acc:  67%, G loss: 1.760254\n",
      "Ep: 709, steps: 24, D loss: 0.216920, acc:  64%, G loss: 1.545954\n",
      "Ep: 709, steps: 25, D loss: 0.248712, acc:  56%, G loss: 1.656031\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 710, steps: 1, D loss: 0.240222, acc:  58%, G loss: 1.700457\n",
      "Ep: 710, steps: 2, D loss: 0.247845, acc:  53%, G loss: 1.437497\n",
      "Ep: 710, steps: 3, D loss: 0.180562, acc:  80%, G loss: 1.931965\n",
      "Ep: 710, steps: 4, D loss: 0.182641, acc:  82%, G loss: 1.617000\n",
      "Ep: 710, steps: 5, D loss: 0.277282, acc:  47%, G loss: 1.573438\n",
      "Ep: 710, steps: 6, D loss: 0.253481, acc:  53%, G loss: 1.585945\n",
      "Ep: 710, steps: 7, D loss: 0.315998, acc:  34%, G loss: 1.578843\n",
      "Ep: 710, steps: 8, D loss: 0.221706, acc:  68%, G loss: 1.785557\n",
      "Ep: 710, steps: 9, D loss: 0.237209, acc:  61%, G loss: 1.633193\n",
      "Ep: 710, steps: 10, D loss: 0.179827, acc:  77%, G loss: 1.581941\n",
      "Ep: 710, steps: 11, D loss: 0.267138, acc:  47%, G loss: 1.720431\n",
      "Ep: 710, steps: 12, D loss: 0.304125, acc:  29%, G loss: 1.368100\n",
      "Ep: 710, steps: 13, D loss: 0.284833, acc:  34%, G loss: 1.411596\n",
      "Ep: 710, steps: 14, D loss: 0.262180, acc:  47%, G loss: 1.492086\n",
      "Ep: 710, steps: 15, D loss: 0.272695, acc:  43%, G loss: 1.507650\n",
      "Saved Model\n",
      "Ep: 710, steps: 16, D loss: 0.226618, acc:  65%, G loss: 1.535122\n",
      "Ep: 710, steps: 17, D loss: 0.218831, acc:  67%, G loss: 1.627124\n",
      "Ep: 710, steps: 18, D loss: 0.218286, acc:  64%, G loss: 1.537204\n",
      "Ep: 710, steps: 19, D loss: 0.176448, acc:  80%, G loss: 1.694857\n",
      "Ep: 710, steps: 20, D loss: 0.271229, acc:  38%, G loss: 1.424437\n",
      "Ep: 710, steps: 21, D loss: 0.207605, acc:  67%, G loss: 1.658004\n",
      "Ep: 710, steps: 22, D loss: 0.223317, acc:  65%, G loss: 1.809812\n",
      "Ep: 710, steps: 23, D loss: 0.205084, acc:  71%, G loss: 1.525916\n",
      "Ep: 710, steps: 24, D loss: 0.241064, acc:  58%, G loss: 1.580505\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 711, steps: 1, D loss: 0.243459, acc:  56%, G loss: 1.607010\n",
      "Ep: 711, steps: 2, D loss: 0.262710, acc:  49%, G loss: 1.410304\n",
      "Ep: 711, steps: 3, D loss: 0.177612, acc:  80%, G loss: 1.856201\n",
      "Ep: 711, steps: 4, D loss: 0.197658, acc:  78%, G loss: 1.600953\n",
      "Ep: 711, steps: 5, D loss: 0.251406, acc:  57%, G loss: 1.576898\n",
      "Ep: 711, steps: 6, D loss: 0.244889, acc:  53%, G loss: 1.564942\n",
      "Ep: 711, steps: 7, D loss: 0.307084, acc:  32%, G loss: 1.461916\n",
      "Ep: 711, steps: 8, D loss: 0.235029, acc:  61%, G loss: 1.731480\n",
      "Ep: 711, steps: 9, D loss: 0.239714, acc:  60%, G loss: 1.605385\n",
      "Ep: 711, steps: 10, D loss: 0.193340, acc:  75%, G loss: 1.547562\n",
      "Ep: 711, steps: 11, D loss: 0.243410, acc:  53%, G loss: 1.714585\n",
      "Ep: 711, steps: 12, D loss: 0.286519, acc:  37%, G loss: 1.335211\n",
      "Ep: 711, steps: 13, D loss: 0.285440, acc:  33%, G loss: 1.392903\n",
      "Ep: 711, steps: 14, D loss: 0.259209, acc:  50%, G loss: 1.501442\n",
      "Ep: 711, steps: 15, D loss: 0.260995, acc:  48%, G loss: 1.564101\n",
      "Ep: 711, steps: 16, D loss: 0.248313, acc:  53%, G loss: 1.527448\n",
      "Ep: 711, steps: 17, D loss: 0.224926, acc:  62%, G loss: 1.600811\n",
      "Ep: 711, steps: 18, D loss: 0.230209, acc:  61%, G loss: 1.646185\n",
      "Ep: 711, steps: 19, D loss: 0.216172, acc:  66%, G loss: 1.617505\n",
      "Ep: 711, steps: 20, D loss: 0.179495, acc:  78%, G loss: 1.752599\n",
      "Ep: 711, steps: 21, D loss: 0.265108, acc:  39%, G loss: 1.567528\n",
      "Ep: 711, steps: 22, D loss: 0.201409, acc:  70%, G loss: 1.630324\n",
      "Ep: 711, steps: 23, D loss: 0.221972, acc:  64%, G loss: 1.776326\n",
      "Ep: 711, steps: 24, D loss: 0.198960, acc:  73%, G loss: 1.528476\n",
      "Ep: 711, steps: 25, D loss: 0.234150, acc:  60%, G loss: 1.545936\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 712, steps: 1, D loss: 0.234931, acc:  59%, G loss: 1.700353\n",
      "Ep: 712, steps: 2, D loss: 0.261993, acc:  48%, G loss: 1.399739\n",
      "Ep: 712, steps: 3, D loss: 0.159739, acc:  85%, G loss: 1.853269\n",
      "Ep: 712, steps: 4, D loss: 0.195605, acc:  76%, G loss: 1.661886\n",
      "Ep: 712, steps: 5, D loss: 0.282583, acc:  46%, G loss: 1.558586\n",
      "Ep: 712, steps: 6, D loss: 0.257284, acc:  50%, G loss: 1.576639\n",
      "Ep: 712, steps: 7, D loss: 0.307521, acc:  33%, G loss: 1.497979\n",
      "Ep: 712, steps: 8, D loss: 0.218432, acc:  67%, G loss: 1.684165\n",
      "Ep: 712, steps: 9, D loss: 0.220669, acc:  67%, G loss: 1.600030\n",
      "Ep: 712, steps: 10, D loss: 0.182288, acc:  78%, G loss: 1.530835\n",
      "Ep: 712, steps: 11, D loss: 0.253809, acc:  49%, G loss: 1.710744\n",
      "Ep: 712, steps: 12, D loss: 0.289396, acc:  37%, G loss: 1.347531\n",
      "Ep: 712, steps: 13, D loss: 0.293808, acc:  31%, G loss: 1.355431\n",
      "Ep: 712, steps: 14, D loss: 0.275665, acc:  41%, G loss: 1.450645\n",
      "Ep: 712, steps: 15, D loss: 0.263361, acc:  48%, G loss: 1.542076\n",
      "Ep: 712, steps: 16, D loss: 0.247186, acc:  57%, G loss: 1.603622\n",
      "Ep: 712, steps: 17, D loss: 0.217523, acc:  68%, G loss: 1.530961\n",
      "Ep: 712, steps: 18, D loss: 0.226360, acc:  64%, G loss: 1.632984\n",
      "Ep: 712, steps: 19, D loss: 0.205914, acc:  67%, G loss: 1.625280\n",
      "Ep: 712, steps: 20, D loss: 0.169933, acc:  82%, G loss: 1.730523\n",
      "Ep: 712, steps: 21, D loss: 0.264940, acc:  42%, G loss: 1.448357\n",
      "Ep: 712, steps: 22, D loss: 0.201439, acc:  71%, G loss: 1.653967\n",
      "Ep: 712, steps: 23, D loss: 0.231654, acc:  60%, G loss: 1.843940\n",
      "Ep: 712, steps: 24, D loss: 0.210051, acc:  68%, G loss: 1.587486\n",
      "Ep: 712, steps: 25, D loss: 0.254999, acc:  54%, G loss: 1.715529\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 713, steps: 1, D loss: 0.220802, acc:  65%, G loss: 1.584533\n",
      "Ep: 713, steps: 2, D loss: 0.249173, acc:  54%, G loss: 1.392084\n",
      "Ep: 713, steps: 3, D loss: 0.174090, acc:  80%, G loss: 1.886147\n",
      "Ep: 713, steps: 4, D loss: 0.192476, acc:  80%, G loss: 1.613845\n",
      "Ep: 713, steps: 5, D loss: 0.271874, acc:  51%, G loss: 1.571403\n",
      "Ep: 713, steps: 6, D loss: 0.242946, acc:  56%, G loss: 1.652444\n",
      "Ep: 713, steps: 7, D loss: 0.296664, acc:  35%, G loss: 1.500004\n",
      "Ep: 713, steps: 8, D loss: 0.227910, acc:  64%, G loss: 1.666198\n",
      "Ep: 713, steps: 9, D loss: 0.241266, acc:  60%, G loss: 1.548539\n",
      "Ep: 713, steps: 10, D loss: 0.199913, acc:  71%, G loss: 1.533338\n",
      "Ep: 713, steps: 11, D loss: 0.264247, acc:  49%, G loss: 1.739814\n",
      "Ep: 713, steps: 12, D loss: 0.281265, acc:  40%, G loss: 1.370908\n",
      "Ep: 713, steps: 13, D loss: 0.285417, acc:  33%, G loss: 1.373402\n",
      "Saved Model\n",
      "Ep: 713, steps: 14, D loss: 0.273557, acc:  45%, G loss: 1.503947\n",
      "Ep: 713, steps: 15, D loss: 0.229477, acc:  64%, G loss: 1.603446\n",
      "Ep: 713, steps: 16, D loss: 0.215817, acc:  67%, G loss: 1.553156\n",
      "Ep: 713, steps: 17, D loss: 0.245692, acc:  57%, G loss: 1.605944\n",
      "Ep: 713, steps: 18, D loss: 0.219453, acc:  64%, G loss: 1.528996\n",
      "Ep: 713, steps: 19, D loss: 0.157318, acc:  84%, G loss: 1.801977\n",
      "Ep: 713, steps: 20, D loss: 0.252650, acc:  48%, G loss: 1.586677\n",
      "Ep: 713, steps: 21, D loss: 0.197552, acc:  68%, G loss: 1.583273\n",
      "Ep: 713, steps: 22, D loss: 0.229770, acc:  62%, G loss: 1.808924\n",
      "Ep: 713, steps: 23, D loss: 0.206020, acc:  69%, G loss: 1.545588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 713, steps: 24, D loss: 0.227609, acc:  64%, G loss: 1.534337\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 714, steps: 1, D loss: 0.254145, acc:  51%, G loss: 1.665178\n",
      "Ep: 714, steps: 2, D loss: 0.248655, acc:  54%, G loss: 1.438613\n",
      "Ep: 714, steps: 3, D loss: 0.176724, acc:  80%, G loss: 1.885709\n",
      "Ep: 714, steps: 4, D loss: 0.193413, acc:  79%, G loss: 1.699629\n",
      "Ep: 714, steps: 5, D loss: 0.282574, acc:  46%, G loss: 1.575599\n",
      "Ep: 714, steps: 6, D loss: 0.254988, acc:  53%, G loss: 1.594076\n",
      "Ep: 714, steps: 7, D loss: 0.299076, acc:  36%, G loss: 1.459882\n",
      "Ep: 714, steps: 8, D loss: 0.235754, acc:  61%, G loss: 1.707500\n",
      "Ep: 714, steps: 9, D loss: 0.245589, acc:  57%, G loss: 1.581167\n",
      "Ep: 714, steps: 10, D loss: 0.184038, acc:  76%, G loss: 1.561738\n",
      "Ep: 714, steps: 11, D loss: 0.262869, acc:  47%, G loss: 1.761261\n",
      "Ep: 714, steps: 12, D loss: 0.290593, acc:  37%, G loss: 1.337987\n",
      "Ep: 714, steps: 13, D loss: 0.289225, acc:  32%, G loss: 1.382408\n",
      "Ep: 714, steps: 14, D loss: 0.262184, acc:  48%, G loss: 1.453915\n",
      "Ep: 714, steps: 15, D loss: 0.267067, acc:  47%, G loss: 1.498596\n",
      "Ep: 714, steps: 16, D loss: 0.257098, acc:  50%, G loss: 1.537740\n",
      "Ep: 714, steps: 17, D loss: 0.226188, acc:  64%, G loss: 1.520530\n",
      "Ep: 714, steps: 18, D loss: 0.233923, acc:  59%, G loss: 1.645453\n",
      "Ep: 714, steps: 19, D loss: 0.217626, acc:  66%, G loss: 1.602491\n",
      "Ep: 714, steps: 20, D loss: 0.186210, acc:  76%, G loss: 1.733706\n",
      "Ep: 714, steps: 21, D loss: 0.265080, acc:  41%, G loss: 1.441496\n",
      "Ep: 714, steps: 22, D loss: 0.214412, acc:  64%, G loss: 1.621784\n",
      "Ep: 714, steps: 23, D loss: 0.219526, acc:  65%, G loss: 1.756148\n",
      "Ep: 714, steps: 24, D loss: 0.225475, acc:  63%, G loss: 1.511153\n",
      "Ep: 714, steps: 25, D loss: 0.249265, acc:  55%, G loss: 1.610744\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 715, steps: 1, D loss: 0.224478, acc:  64%, G loss: 1.596971\n",
      "Ep: 715, steps: 2, D loss: 0.238218, acc:  58%, G loss: 1.441791\n",
      "Ep: 715, steps: 3, D loss: 0.174163, acc:  80%, G loss: 1.886839\n",
      "Ep: 715, steps: 4, D loss: 0.187444, acc:  80%, G loss: 1.663067\n",
      "Ep: 715, steps: 5, D loss: 0.295210, acc:  46%, G loss: 1.576144\n",
      "Ep: 715, steps: 6, D loss: 0.237588, acc:  55%, G loss: 1.591946\n",
      "Ep: 715, steps: 7, D loss: 0.324537, acc:  30%, G loss: 1.554092\n",
      "Ep: 715, steps: 8, D loss: 0.225815, acc:  65%, G loss: 1.776527\n",
      "Ep: 715, steps: 9, D loss: 0.257750, acc:  53%, G loss: 1.630808\n",
      "Ep: 715, steps: 10, D loss: 0.182827, acc:  79%, G loss: 1.630419\n",
      "Ep: 715, steps: 11, D loss: 0.263580, acc:  48%, G loss: 1.729406\n",
      "Ep: 715, steps: 12, D loss: 0.289688, acc:  34%, G loss: 1.265810\n",
      "Ep: 715, steps: 13, D loss: 0.279939, acc:  38%, G loss: 1.377739\n",
      "Ep: 715, steps: 14, D loss: 0.281121, acc:  40%, G loss: 1.485107\n",
      "Ep: 715, steps: 15, D loss: 0.246866, acc:  51%, G loss: 1.530262\n",
      "Ep: 715, steps: 16, D loss: 0.244407, acc:  57%, G loss: 1.526432\n",
      "Ep: 715, steps: 17, D loss: 0.217857, acc:  70%, G loss: 1.458143\n",
      "Ep: 715, steps: 18, D loss: 0.231417, acc:  61%, G loss: 1.618987\n",
      "Ep: 715, steps: 19, D loss: 0.216926, acc:  66%, G loss: 1.628782\n",
      "Ep: 715, steps: 20, D loss: 0.179629, acc:  79%, G loss: 1.684957\n",
      "Ep: 715, steps: 21, D loss: 0.271318, acc:  39%, G loss: 1.496139\n",
      "Ep: 715, steps: 22, D loss: 0.206582, acc:  67%, G loss: 1.620865\n",
      "Ep: 715, steps: 23, D loss: 0.220465, acc:  65%, G loss: 1.804774\n",
      "Ep: 715, steps: 24, D loss: 0.216147, acc:  66%, G loss: 1.491872\n",
      "Ep: 715, steps: 25, D loss: 0.248785, acc:  58%, G loss: 1.572550\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 716, steps: 1, D loss: 0.230006, acc:  63%, G loss: 1.536184\n",
      "Ep: 716, steps: 2, D loss: 0.259513, acc:  49%, G loss: 1.420006\n",
      "Ep: 716, steps: 3, D loss: 0.157698, acc:  86%, G loss: 1.902725\n",
      "Ep: 716, steps: 4, D loss: 0.195945, acc:  77%, G loss: 1.668670\n",
      "Ep: 716, steps: 5, D loss: 0.277088, acc:  53%, G loss: 1.552152\n",
      "Ep: 716, steps: 6, D loss: 0.249675, acc:  53%, G loss: 1.587429\n",
      "Ep: 716, steps: 7, D loss: 0.315801, acc:  31%, G loss: 1.556921\n",
      "Ep: 716, steps: 8, D loss: 0.233031, acc:  60%, G loss: 1.717386\n",
      "Ep: 716, steps: 9, D loss: 0.230519, acc:  63%, G loss: 1.557888\n",
      "Ep: 716, steps: 10, D loss: 0.175154, acc:  81%, G loss: 1.551450\n",
      "Ep: 716, steps: 11, D loss: 0.263013, acc:  49%, G loss: 1.733514\n",
      "Saved Model\n",
      "Ep: 716, steps: 12, D loss: 0.295670, acc:  31%, G loss: 1.367737\n",
      "Ep: 716, steps: 13, D loss: 0.285013, acc:  41%, G loss: 1.509444\n",
      "Ep: 716, steps: 14, D loss: 0.251917, acc:  52%, G loss: 1.567566\n",
      "Ep: 716, steps: 15, D loss: 0.236927, acc:  58%, G loss: 1.520335\n",
      "Ep: 716, steps: 16, D loss: 0.231127, acc:  64%, G loss: 1.496620\n",
      "Ep: 716, steps: 17, D loss: 0.217823, acc:  66%, G loss: 1.639917\n",
      "Ep: 716, steps: 18, D loss: 0.204882, acc:  69%, G loss: 1.564345\n",
      "Ep: 716, steps: 19, D loss: 0.194760, acc:  72%, G loss: 1.685458\n",
      "Ep: 716, steps: 20, D loss: 0.273742, acc:  37%, G loss: 1.384191\n",
      "Ep: 716, steps: 21, D loss: 0.195700, acc:  69%, G loss: 1.603961\n",
      "Ep: 716, steps: 22, D loss: 0.237187, acc:  59%, G loss: 1.806502\n",
      "Ep: 716, steps: 23, D loss: 0.224278, acc:  64%, G loss: 1.584890\n",
      "Ep: 716, steps: 24, D loss: 0.241597, acc:  58%, G loss: 1.578077\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 717, steps: 1, D loss: 0.240374, acc:  61%, G loss: 1.602814\n",
      "Ep: 717, steps: 2, D loss: 0.232284, acc:  60%, G loss: 1.429691\n",
      "Ep: 717, steps: 3, D loss: 0.192059, acc:  76%, G loss: 1.889203\n",
      "Ep: 717, steps: 4, D loss: 0.175663, acc:  85%, G loss: 1.678070\n",
      "Ep: 717, steps: 5, D loss: 0.261199, acc:  53%, G loss: 1.622764\n",
      "Ep: 717, steps: 6, D loss: 0.248976, acc:  53%, G loss: 1.564816\n",
      "Ep: 717, steps: 7, D loss: 0.307375, acc:  32%, G loss: 1.474794\n",
      "Ep: 717, steps: 8, D loss: 0.218350, acc:  65%, G loss: 1.688016\n",
      "Ep: 717, steps: 9, D loss: 0.242530, acc:  58%, G loss: 1.581996\n",
      "Ep: 717, steps: 10, D loss: 0.175587, acc:  76%, G loss: 1.591154\n",
      "Ep: 717, steps: 11, D loss: 0.270583, acc:  44%, G loss: 1.693825\n",
      "Ep: 717, steps: 12, D loss: 0.300555, acc:  30%, G loss: 1.355687\n",
      "Ep: 717, steps: 13, D loss: 0.292651, acc:  32%, G loss: 1.386378\n",
      "Ep: 717, steps: 14, D loss: 0.273078, acc:  42%, G loss: 1.478320\n",
      "Ep: 717, steps: 15, D loss: 0.257069, acc:  47%, G loss: 1.542660\n",
      "Ep: 717, steps: 16, D loss: 0.241002, acc:  58%, G loss: 1.580692\n",
      "Ep: 717, steps: 17, D loss: 0.225378, acc:  64%, G loss: 1.607073\n",
      "Ep: 717, steps: 18, D loss: 0.243412, acc:  56%, G loss: 1.623706\n",
      "Ep: 717, steps: 19, D loss: 0.205957, acc:  68%, G loss: 1.594493\n",
      "Ep: 717, steps: 20, D loss: 0.172427, acc:  82%, G loss: 1.659829\n",
      "Ep: 717, steps: 21, D loss: 0.261173, acc:  41%, G loss: 1.559792\n",
      "Ep: 717, steps: 22, D loss: 0.235539, acc:  60%, G loss: 1.637283\n",
      "Ep: 717, steps: 23, D loss: 0.217670, acc:  66%, G loss: 1.783460\n",
      "Ep: 717, steps: 24, D loss: 0.215888, acc:  67%, G loss: 1.508127\n",
      "Ep: 717, steps: 25, D loss: 0.255887, acc:  55%, G loss: 1.605906\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 718, steps: 1, D loss: 0.232275, acc:  60%, G loss: 1.676925\n",
      "Ep: 718, steps: 2, D loss: 0.237841, acc:  58%, G loss: 1.463736\n",
      "Ep: 718, steps: 3, D loss: 0.172221, acc:  81%, G loss: 1.937442\n",
      "Ep: 718, steps: 4, D loss: 0.199530, acc:  76%, G loss: 1.644393\n",
      "Ep: 718, steps: 5, D loss: 0.282849, acc:  47%, G loss: 1.622748\n",
      "Ep: 718, steps: 6, D loss: 0.246654, acc:  54%, G loss: 1.553997\n",
      "Ep: 718, steps: 7, D loss: 0.308082, acc:  32%, G loss: 1.482114\n",
      "Ep: 718, steps: 8, D loss: 0.235442, acc:  60%, G loss: 1.707789\n",
      "Ep: 718, steps: 9, D loss: 0.228348, acc:  63%, G loss: 1.551311\n",
      "Ep: 718, steps: 10, D loss: 0.192861, acc:  74%, G loss: 1.567506\n",
      "Ep: 718, steps: 11, D loss: 0.278687, acc:  43%, G loss: 1.773135\n",
      "Ep: 718, steps: 12, D loss: 0.280144, acc:  38%, G loss: 1.350708\n",
      "Ep: 718, steps: 13, D loss: 0.287706, acc:  34%, G loss: 1.438182\n",
      "Ep: 718, steps: 14, D loss: 0.265762, acc:  48%, G loss: 1.498479\n",
      "Ep: 718, steps: 15, D loss: 0.252507, acc:  53%, G loss: 1.518636\n",
      "Ep: 718, steps: 16, D loss: 0.244206, acc:  57%, G loss: 1.576361\n",
      "Ep: 718, steps: 17, D loss: 0.219428, acc:  65%, G loss: 1.520801\n",
      "Ep: 718, steps: 18, D loss: 0.239058, acc:  57%, G loss: 1.669862\n",
      "Ep: 718, steps: 19, D loss: 0.215617, acc:  66%, G loss: 1.565118\n",
      "Ep: 718, steps: 20, D loss: 0.185583, acc:  77%, G loss: 1.750211\n",
      "Ep: 718, steps: 21, D loss: 0.262472, acc:  42%, G loss: 1.429302\n",
      "Ep: 718, steps: 22, D loss: 0.203371, acc:  66%, G loss: 1.678461\n",
      "Ep: 718, steps: 23, D loss: 0.221445, acc:  64%, G loss: 1.806249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 718, steps: 24, D loss: 0.201689, acc:  74%, G loss: 1.507488\n",
      "Ep: 718, steps: 25, D loss: 0.239067, acc:  59%, G loss: 1.555535\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 719, steps: 1, D loss: 0.230970, acc:  60%, G loss: 1.662990\n",
      "Ep: 719, steps: 2, D loss: 0.245765, acc:  54%, G loss: 1.427285\n",
      "Ep: 719, steps: 3, D loss: 0.173171, acc:  80%, G loss: 1.911597\n",
      "Ep: 719, steps: 4, D loss: 0.170898, acc:  85%, G loss: 1.671898\n",
      "Ep: 719, steps: 5, D loss: 0.285927, acc:  47%, G loss: 1.599848\n",
      "Ep: 719, steps: 6, D loss: 0.245040, acc:  53%, G loss: 1.578838\n",
      "Ep: 719, steps: 7, D loss: 0.328497, acc:  28%, G loss: 1.472505\n",
      "Ep: 719, steps: 8, D loss: 0.235512, acc:  62%, G loss: 1.737288\n",
      "Ep: 719, steps: 9, D loss: 0.234360, acc:  62%, G loss: 1.579153\n",
      "Saved Model\n",
      "Ep: 719, steps: 10, D loss: 0.190388, acc:  72%, G loss: 1.535904\n",
      "Ep: 719, steps: 11, D loss: 0.285159, acc:  37%, G loss: 1.403083\n",
      "Ep: 719, steps: 12, D loss: 0.285193, acc:  37%, G loss: 1.369737\n",
      "Ep: 719, steps: 13, D loss: 0.266062, acc:  45%, G loss: 1.507718\n",
      "Ep: 719, steps: 14, D loss: 0.316611, acc:  32%, G loss: 1.436564\n",
      "Ep: 719, steps: 15, D loss: 0.238136, acc:  59%, G loss: 1.559374\n",
      "Ep: 719, steps: 16, D loss: 0.215425, acc:  68%, G loss: 1.604696\n",
      "Ep: 719, steps: 17, D loss: 0.256847, acc:  50%, G loss: 1.593182\n",
      "Ep: 719, steps: 18, D loss: 0.223898, acc:  63%, G loss: 1.588325\n",
      "Ep: 719, steps: 19, D loss: 0.175209, acc:  82%, G loss: 1.676719\n",
      "Ep: 719, steps: 20, D loss: 0.269687, acc:  39%, G loss: 1.485421\n",
      "Ep: 719, steps: 21, D loss: 0.210256, acc:  67%, G loss: 1.616333\n",
      "Ep: 719, steps: 22, D loss: 0.229526, acc:  60%, G loss: 1.782947\n",
      "Ep: 719, steps: 23, D loss: 0.213617, acc:  68%, G loss: 1.467474\n",
      "Ep: 719, steps: 24, D loss: 0.243711, acc:  57%, G loss: 1.766967\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 720, steps: 1, D loss: 0.248280, acc:  56%, G loss: 1.701338\n",
      "Ep: 720, steps: 2, D loss: 0.247188, acc:  54%, G loss: 1.424168\n",
      "Ep: 720, steps: 3, D loss: 0.180166, acc:  80%, G loss: 1.863336\n",
      "Ep: 720, steps: 4, D loss: 0.194728, acc:  78%, G loss: 1.601179\n",
      "Ep: 720, steps: 5, D loss: 0.283124, acc:  46%, G loss: 1.608340\n",
      "Ep: 720, steps: 6, D loss: 0.255756, acc:  52%, G loss: 1.566105\n",
      "Ep: 720, steps: 7, D loss: 0.290727, acc:  35%, G loss: 1.465662\n",
      "Ep: 720, steps: 8, D loss: 0.223521, acc:  66%, G loss: 1.804041\n",
      "Ep: 720, steps: 9, D loss: 0.229081, acc:  64%, G loss: 1.610813\n",
      "Ep: 720, steps: 10, D loss: 0.177483, acc:  81%, G loss: 1.545809\n",
      "Ep: 720, steps: 11, D loss: 0.271690, acc:  45%, G loss: 1.775399\n",
      "Ep: 720, steps: 12, D loss: 0.279769, acc:  38%, G loss: 1.378493\n",
      "Ep: 720, steps: 13, D loss: 0.289904, acc:  32%, G loss: 1.386798\n",
      "Ep: 720, steps: 14, D loss: 0.268277, acc:  45%, G loss: 1.480268\n",
      "Ep: 720, steps: 15, D loss: 0.257469, acc:  46%, G loss: 1.494394\n",
      "Ep: 720, steps: 16, D loss: 0.243082, acc:  55%, G loss: 1.536332\n",
      "Ep: 720, steps: 17, D loss: 0.219616, acc:  65%, G loss: 1.488212\n",
      "Ep: 720, steps: 18, D loss: 0.221421, acc:  64%, G loss: 1.652683\n",
      "Ep: 720, steps: 19, D loss: 0.201037, acc:  71%, G loss: 1.615870\n",
      "Ep: 720, steps: 20, D loss: 0.183238, acc:  79%, G loss: 1.707875\n",
      "Ep: 720, steps: 21, D loss: 0.270640, acc:  38%, G loss: 1.456501\n",
      "Ep: 720, steps: 22, D loss: 0.224043, acc:  63%, G loss: 1.601883\n",
      "Ep: 720, steps: 23, D loss: 0.222923, acc:  64%, G loss: 1.792765\n",
      "Ep: 720, steps: 24, D loss: 0.209747, acc:  70%, G loss: 1.592600\n",
      "Ep: 720, steps: 25, D loss: 0.248423, acc:  56%, G loss: 1.548443\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 721, steps: 1, D loss: 0.227940, acc:  60%, G loss: 1.667939\n",
      "Ep: 721, steps: 2, D loss: 0.254209, acc:  51%, G loss: 1.474530\n",
      "Ep: 721, steps: 3, D loss: 0.162262, acc:  84%, G loss: 1.854156\n",
      "Ep: 721, steps: 4, D loss: 0.191291, acc:  80%, G loss: 1.716145\n",
      "Ep: 721, steps: 5, D loss: 0.269515, acc:  51%, G loss: 1.616492\n",
      "Ep: 721, steps: 6, D loss: 0.238837, acc:  55%, G loss: 1.559009\n",
      "Ep: 721, steps: 7, D loss: 0.309890, acc:  35%, G loss: 1.557550\n",
      "Ep: 721, steps: 8, D loss: 0.235393, acc:  61%, G loss: 1.800144\n",
      "Ep: 721, steps: 9, D loss: 0.251699, acc:  54%, G loss: 1.626196\n",
      "Ep: 721, steps: 10, D loss: 0.180019, acc:  79%, G loss: 1.595506\n",
      "Ep: 721, steps: 11, D loss: 0.264018, acc:  47%, G loss: 1.700789\n",
      "Ep: 721, steps: 12, D loss: 0.291356, acc:  34%, G loss: 1.354617\n",
      "Ep: 721, steps: 13, D loss: 0.269482, acc:  43%, G loss: 1.382363\n",
      "Ep: 721, steps: 14, D loss: 0.264816, acc:  46%, G loss: 1.474729\n",
      "Ep: 721, steps: 15, D loss: 0.273614, acc:  45%, G loss: 1.563986\n",
      "Ep: 721, steps: 16, D loss: 0.239063, acc:  59%, G loss: 1.574939\n",
      "Ep: 721, steps: 17, D loss: 0.232906, acc:  60%, G loss: 1.615602\n",
      "Ep: 721, steps: 18, D loss: 0.223953, acc:  63%, G loss: 1.637207\n",
      "Ep: 721, steps: 19, D loss: 0.224062, acc:  63%, G loss: 1.604813\n",
      "Ep: 721, steps: 20, D loss: 0.190083, acc:  74%, G loss: 1.635818\n",
      "Ep: 721, steps: 21, D loss: 0.264103, acc:  41%, G loss: 1.506471\n",
      "Ep: 721, steps: 22, D loss: 0.198074, acc:  69%, G loss: 1.580465\n",
      "Ep: 721, steps: 23, D loss: 0.218444, acc:  65%, G loss: 1.772287\n",
      "Ep: 721, steps: 24, D loss: 0.206900, acc:  72%, G loss: 1.534661\n",
      "Ep: 721, steps: 25, D loss: 0.265491, acc:  52%, G loss: 1.510474\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 722, steps: 1, D loss: 0.225709, acc:  59%, G loss: 1.779285\n",
      "Ep: 722, steps: 2, D loss: 0.246660, acc:  54%, G loss: 1.462319\n",
      "Ep: 722, steps: 3, D loss: 0.185177, acc:  77%, G loss: 1.864388\n",
      "Ep: 722, steps: 4, D loss: 0.180225, acc:  81%, G loss: 1.684837\n",
      "Ep: 722, steps: 5, D loss: 0.295756, acc:  45%, G loss: 1.592197\n",
      "Ep: 722, steps: 6, D loss: 0.251165, acc:  52%, G loss: 1.636199\n",
      "Ep: 722, steps: 7, D loss: 0.327124, acc:  28%, G loss: 1.501571\n",
      "Saved Model\n",
      "Ep: 722, steps: 8, D loss: 0.217174, acc:  66%, G loss: 1.805197\n",
      "Ep: 722, steps: 9, D loss: 0.205579, acc:  69%, G loss: 1.479128\n",
      "Ep: 722, steps: 10, D loss: 0.294185, acc:  38%, G loss: 1.632340\n",
      "Ep: 722, steps: 11, D loss: 0.282135, acc:  36%, G loss: 1.392106\n",
      "Ep: 722, steps: 12, D loss: 0.290543, acc:  33%, G loss: 1.459577\n",
      "Ep: 722, steps: 13, D loss: 0.251842, acc:  50%, G loss: 1.443353\n",
      "Ep: 722, steps: 14, D loss: 0.257865, acc:  48%, G loss: 1.565524\n",
      "Ep: 722, steps: 15, D loss: 0.255072, acc:  51%, G loss: 1.593971\n",
      "Ep: 722, steps: 16, D loss: 0.206434, acc:  73%, G loss: 1.627553\n",
      "Ep: 722, steps: 17, D loss: 0.240165, acc:  57%, G loss: 1.664181\n",
      "Ep: 722, steps: 18, D loss: 0.217970, acc:  67%, G loss: 1.620346\n",
      "Ep: 722, steps: 19, D loss: 0.185345, acc:  79%, G loss: 1.638318\n",
      "Ep: 722, steps: 20, D loss: 0.274594, acc:  39%, G loss: 1.436914\n",
      "Ep: 722, steps: 21, D loss: 0.220378, acc:  64%, G loss: 1.664788\n",
      "Ep: 722, steps: 22, D loss: 0.220886, acc:  65%, G loss: 1.743662\n",
      "Ep: 722, steps: 23, D loss: 0.216380, acc:  67%, G loss: 1.519949\n",
      "Ep: 722, steps: 24, D loss: 0.255797, acc:  54%, G loss: 1.538195\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 723, steps: 1, D loss: 0.239071, acc:  56%, G loss: 1.621059\n",
      "Ep: 723, steps: 2, D loss: 0.240894, acc:  57%, G loss: 1.402182\n",
      "Ep: 723, steps: 3, D loss: 0.178207, acc:  79%, G loss: 1.829987\n",
      "Ep: 723, steps: 4, D loss: 0.194189, acc:  78%, G loss: 1.581761\n",
      "Ep: 723, steps: 5, D loss: 0.262793, acc:  52%, G loss: 1.613516\n",
      "Ep: 723, steps: 6, D loss: 0.247355, acc:  53%, G loss: 1.554733\n",
      "Ep: 723, steps: 7, D loss: 0.289855, acc:  37%, G loss: 1.506032\n",
      "Ep: 723, steps: 8, D loss: 0.229928, acc:  63%, G loss: 1.664854\n",
      "Ep: 723, steps: 9, D loss: 0.250926, acc:  53%, G loss: 1.578740\n",
      "Ep: 723, steps: 10, D loss: 0.187997, acc:  74%, G loss: 1.525994\n",
      "Ep: 723, steps: 11, D loss: 0.254972, acc:  49%, G loss: 1.839097\n",
      "Ep: 723, steps: 12, D loss: 0.289857, acc:  36%, G loss: 1.364002\n",
      "Ep: 723, steps: 13, D loss: 0.289145, acc:  33%, G loss: 1.379005\n",
      "Ep: 723, steps: 14, D loss: 0.275527, acc:  41%, G loss: 1.472263\n",
      "Ep: 723, steps: 15, D loss: 0.267660, acc:  44%, G loss: 1.546351\n",
      "Ep: 723, steps: 16, D loss: 0.239543, acc:  58%, G loss: 1.545446\n",
      "Ep: 723, steps: 17, D loss: 0.212292, acc:  68%, G loss: 1.579466\n",
      "Ep: 723, steps: 18, D loss: 0.237425, acc:  58%, G loss: 1.616450\n",
      "Ep: 723, steps: 19, D loss: 0.194325, acc:  73%, G loss: 1.582157\n",
      "Ep: 723, steps: 20, D loss: 0.185680, acc:  77%, G loss: 1.730924\n",
      "Ep: 723, steps: 21, D loss: 0.256831, acc:  44%, G loss: 1.500906\n",
      "Ep: 723, steps: 22, D loss: 0.202401, acc:  70%, G loss: 1.622302\n",
      "Ep: 723, steps: 23, D loss: 0.231736, acc:  60%, G loss: 1.780512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 723, steps: 24, D loss: 0.206798, acc:  68%, G loss: 1.516152\n",
      "Ep: 723, steps: 25, D loss: 0.245912, acc:  59%, G loss: 1.548580\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 724, steps: 1, D loss: 0.235570, acc:  59%, G loss: 1.792218\n",
      "Ep: 724, steps: 2, D loss: 0.254607, acc:  53%, G loss: 1.418741\n",
      "Ep: 724, steps: 3, D loss: 0.161236, acc:  86%, G loss: 1.963437\n",
      "Ep: 724, steps: 4, D loss: 0.184609, acc:  82%, G loss: 1.652906\n",
      "Ep: 724, steps: 5, D loss: 0.281161, acc:  46%, G loss: 1.579863\n",
      "Ep: 724, steps: 6, D loss: 0.249360, acc:  53%, G loss: 1.532464\n",
      "Ep: 724, steps: 7, D loss: 0.297016, acc:  36%, G loss: 1.502939\n",
      "Ep: 724, steps: 8, D loss: 0.239646, acc:  62%, G loss: 1.723431\n",
      "Ep: 724, steps: 9, D loss: 0.243344, acc:  56%, G loss: 1.644840\n",
      "Ep: 724, steps: 10, D loss: 0.178925, acc:  79%, G loss: 1.527057\n",
      "Ep: 724, steps: 11, D loss: 0.246304, acc:  53%, G loss: 1.833712\n",
      "Ep: 724, steps: 12, D loss: 0.291601, acc:  36%, G loss: 1.398977\n",
      "Ep: 724, steps: 13, D loss: 0.296178, acc:  32%, G loss: 1.438847\n",
      "Ep: 724, steps: 14, D loss: 0.271376, acc:  45%, G loss: 1.427957\n",
      "Ep: 724, steps: 15, D loss: 0.268540, acc:  45%, G loss: 1.540791\n",
      "Ep: 724, steps: 16, D loss: 0.244462, acc:  57%, G loss: 1.601950\n",
      "Ep: 724, steps: 17, D loss: 0.220562, acc:  66%, G loss: 1.602637\n",
      "Ep: 724, steps: 18, D loss: 0.233046, acc:  60%, G loss: 1.622802\n",
      "Ep: 724, steps: 19, D loss: 0.222933, acc:  62%, G loss: 1.598344\n",
      "Ep: 724, steps: 20, D loss: 0.182876, acc:  79%, G loss: 1.678061\n",
      "Ep: 724, steps: 21, D loss: 0.271240, acc:  37%, G loss: 1.491956\n",
      "Ep: 724, steps: 22, D loss: 0.203855, acc:  68%, G loss: 1.682051\n",
      "Ep: 724, steps: 23, D loss: 0.212156, acc:  71%, G loss: 1.768312\n",
      "Ep: 724, steps: 24, D loss: 0.213245, acc:  67%, G loss: 1.538354\n",
      "Ep: 724, steps: 25, D loss: 0.242494, acc:  58%, G loss: 1.754298\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 725, steps: 1, D loss: 0.229633, acc:  61%, G loss: 1.597405\n",
      "Ep: 725, steps: 2, D loss: 0.248310, acc:  56%, G loss: 1.450344\n",
      "Ep: 725, steps: 3, D loss: 0.167946, acc:  82%, G loss: 1.912299\n",
      "Ep: 725, steps: 4, D loss: 0.181970, acc:  82%, G loss: 1.654599\n",
      "Ep: 725, steps: 5, D loss: 0.291137, acc:  44%, G loss: 1.686787\n",
      "Saved Model\n",
      "Ep: 725, steps: 6, D loss: 0.252925, acc:  53%, G loss: 1.579041\n",
      "Ep: 725, steps: 7, D loss: 0.243863, acc:  58%, G loss: 1.693490\n",
      "Ep: 725, steps: 8, D loss: 0.204654, acc:  74%, G loss: 1.677945\n",
      "Ep: 725, steps: 9, D loss: 0.159718, acc:  86%, G loss: 1.628792\n",
      "Ep: 725, steps: 10, D loss: 0.254491, acc:  53%, G loss: 1.876886\n",
      "Ep: 725, steps: 11, D loss: 0.311731, acc:  30%, G loss: 1.375899\n",
      "Ep: 725, steps: 12, D loss: 0.296037, acc:  29%, G loss: 1.358881\n",
      "Ep: 725, steps: 13, D loss: 0.286262, acc:  39%, G loss: 1.447364\n",
      "Ep: 725, steps: 14, D loss: 0.255844, acc:  50%, G loss: 1.515037\n",
      "Ep: 725, steps: 15, D loss: 0.237329, acc:  59%, G loss: 1.595529\n",
      "Ep: 725, steps: 16, D loss: 0.206274, acc:  73%, G loss: 1.492032\n",
      "Ep: 725, steps: 17, D loss: 0.249811, acc:  54%, G loss: 1.575310\n",
      "Ep: 725, steps: 18, D loss: 0.184810, acc:  72%, G loss: 1.627566\n",
      "Ep: 725, steps: 19, D loss: 0.172920, acc:  80%, G loss: 1.714003\n",
      "Ep: 725, steps: 20, D loss: 0.286221, acc:  33%, G loss: 1.460715\n",
      "Ep: 725, steps: 21, D loss: 0.210581, acc:  67%, G loss: 1.574720\n",
      "Ep: 725, steps: 22, D loss: 0.204402, acc:  70%, G loss: 1.805339\n",
      "Ep: 725, steps: 23, D loss: 0.224334, acc:  62%, G loss: 1.487029\n",
      "Ep: 725, steps: 24, D loss: 0.245025, acc:  58%, G loss: 1.540002\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 726, steps: 1, D loss: 0.226480, acc:  63%, G loss: 1.700215\n",
      "Ep: 726, steps: 2, D loss: 0.256806, acc:  51%, G loss: 1.542873\n",
      "Ep: 726, steps: 3, D loss: 0.171203, acc:  84%, G loss: 1.961514\n",
      "Ep: 726, steps: 4, D loss: 0.182020, acc:  82%, G loss: 1.684857\n",
      "Ep: 726, steps: 5, D loss: 0.272932, acc:  49%, G loss: 1.626290\n",
      "Ep: 726, steps: 6, D loss: 0.255263, acc:  50%, G loss: 1.540324\n",
      "Ep: 726, steps: 7, D loss: 0.331466, acc:  27%, G loss: 1.519386\n",
      "Ep: 726, steps: 8, D loss: 0.229428, acc:  63%, G loss: 1.647880\n",
      "Ep: 726, steps: 9, D loss: 0.241902, acc:  58%, G loss: 1.583173\n",
      "Ep: 726, steps: 10, D loss: 0.173858, acc:  78%, G loss: 1.577224\n",
      "Ep: 726, steps: 11, D loss: 0.262515, acc:  47%, G loss: 1.692622\n",
      "Ep: 726, steps: 12, D loss: 0.283251, acc:  38%, G loss: 1.365624\n",
      "Ep: 726, steps: 13, D loss: 0.281949, acc:  35%, G loss: 1.370689\n",
      "Ep: 726, steps: 14, D loss: 0.269315, acc:  43%, G loss: 1.443602\n",
      "Ep: 726, steps: 15, D loss: 0.275357, acc:  40%, G loss: 1.537609\n",
      "Ep: 726, steps: 16, D loss: 0.250159, acc:  54%, G loss: 1.672101\n",
      "Ep: 726, steps: 17, D loss: 0.219513, acc:  65%, G loss: 1.527526\n",
      "Ep: 726, steps: 18, D loss: 0.239135, acc:  57%, G loss: 1.631853\n",
      "Ep: 726, steps: 19, D loss: 0.221009, acc:  64%, G loss: 1.566147\n",
      "Ep: 726, steps: 20, D loss: 0.184346, acc:  78%, G loss: 1.758887\n",
      "Ep: 726, steps: 21, D loss: 0.260739, acc:  43%, G loss: 1.436150\n",
      "Ep: 726, steps: 22, D loss: 0.204189, acc:  68%, G loss: 1.579449\n",
      "Ep: 726, steps: 23, D loss: 0.228017, acc:  61%, G loss: 1.738638\n",
      "Ep: 726, steps: 24, D loss: 0.217835, acc:  66%, G loss: 1.567781\n",
      "Ep: 726, steps: 25, D loss: 0.257700, acc:  53%, G loss: 1.532796\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 727, steps: 1, D loss: 0.250350, acc:  55%, G loss: 1.633246\n",
      "Ep: 727, steps: 2, D loss: 0.245752, acc:  55%, G loss: 1.460635\n",
      "Ep: 727, steps: 3, D loss: 0.172327, acc:  81%, G loss: 1.923018\n",
      "Ep: 727, steps: 4, D loss: 0.177785, acc:  82%, G loss: 1.701980\n",
      "Ep: 727, steps: 5, D loss: 0.279507, acc:  48%, G loss: 1.646860\n",
      "Ep: 727, steps: 6, D loss: 0.248572, acc:  54%, G loss: 1.541502\n",
      "Ep: 727, steps: 7, D loss: 0.302851, acc:  34%, G loss: 1.476080\n",
      "Ep: 727, steps: 8, D loss: 0.222770, acc:  66%, G loss: 1.684703\n",
      "Ep: 727, steps: 9, D loss: 0.247295, acc:  54%, G loss: 1.546653\n",
      "Ep: 727, steps: 10, D loss: 0.182124, acc:  78%, G loss: 1.588093\n",
      "Ep: 727, steps: 11, D loss: 0.247918, acc:  53%, G loss: 1.725267\n",
      "Ep: 727, steps: 12, D loss: 0.280296, acc:  39%, G loss: 1.449549\n",
      "Ep: 727, steps: 13, D loss: 0.277075, acc:  38%, G loss: 1.394809\n",
      "Ep: 727, steps: 14, D loss: 0.274557, acc:  43%, G loss: 1.438109\n",
      "Ep: 727, steps: 15, D loss: 0.266365, acc:  46%, G loss: 1.512555\n",
      "Ep: 727, steps: 16, D loss: 0.237815, acc:  60%, G loss: 1.573889\n",
      "Ep: 727, steps: 17, D loss: 0.227969, acc:  62%, G loss: 1.636420\n",
      "Ep: 727, steps: 18, D loss: 0.233060, acc:  59%, G loss: 1.687217\n",
      "Ep: 727, steps: 19, D loss: 0.216619, acc:  66%, G loss: 1.581847\n",
      "Ep: 727, steps: 20, D loss: 0.174868, acc:  81%, G loss: 1.680009\n",
      "Ep: 727, steps: 21, D loss: 0.264846, acc:  42%, G loss: 1.444475\n",
      "Ep: 727, steps: 22, D loss: 0.233078, acc:  61%, G loss: 1.581083\n",
      "Ep: 727, steps: 23, D loss: 0.214623, acc:  66%, G loss: 1.778622\n",
      "Ep: 727, steps: 24, D loss: 0.216031, acc:  65%, G loss: 1.479943\n",
      "Ep: 727, steps: 25, D loss: 0.260526, acc:  53%, G loss: 1.699571\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 728, steps: 1, D loss: 0.223157, acc:  63%, G loss: 1.628140\n",
      "Ep: 728, steps: 2, D loss: 0.259664, acc:  49%, G loss: 1.466012\n",
      "Ep: 728, steps: 3, D loss: 0.167113, acc:  82%, G loss: 1.861607\n",
      "Saved Model\n",
      "Ep: 728, steps: 4, D loss: 0.201251, acc:  77%, G loss: 1.644315\n",
      "Ep: 728, steps: 5, D loss: 0.254649, acc:  52%, G loss: 1.635134\n",
      "Ep: 728, steps: 6, D loss: 0.326863, acc:  27%, G loss: 1.417153\n",
      "Ep: 728, steps: 7, D loss: 0.238373, acc:  60%, G loss: 1.666833\n",
      "Ep: 728, steps: 8, D loss: 0.227777, acc:  65%, G loss: 1.571691\n",
      "Ep: 728, steps: 9, D loss: 0.190805, acc:  73%, G loss: 1.526954\n",
      "Ep: 728, steps: 10, D loss: 0.257778, acc:  51%, G loss: 1.727220\n",
      "Ep: 728, steps: 11, D loss: 0.291914, acc:  34%, G loss: 1.395240\n",
      "Ep: 728, steps: 12, D loss: 0.292871, acc:  31%, G loss: 1.393910\n",
      "Ep: 728, steps: 13, D loss: 0.275084, acc:  40%, G loss: 1.469317\n",
      "Ep: 728, steps: 14, D loss: 0.242075, acc:  57%, G loss: 1.562196\n",
      "Ep: 728, steps: 15, D loss: 0.242665, acc:  58%, G loss: 1.605100\n",
      "Ep: 728, steps: 16, D loss: 0.219963, acc:  66%, G loss: 1.577907\n",
      "Ep: 728, steps: 17, D loss: 0.230975, acc:  62%, G loss: 1.635857\n",
      "Ep: 728, steps: 18, D loss: 0.205687, acc:  68%, G loss: 1.581800\n",
      "Ep: 728, steps: 19, D loss: 0.177067, acc:  80%, G loss: 1.728219\n",
      "Ep: 728, steps: 20, D loss: 0.270504, acc:  41%, G loss: 1.488225\n",
      "Ep: 728, steps: 21, D loss: 0.197557, acc:  68%, G loss: 1.564839\n",
      "Ep: 728, steps: 22, D loss: 0.223722, acc:  62%, G loss: 1.738269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 728, steps: 23, D loss: 0.215053, acc:  66%, G loss: 1.504849\n",
      "Ep: 728, steps: 24, D loss: 0.235762, acc:  60%, G loss: 1.552577\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 729, steps: 1, D loss: 0.220056, acc:  67%, G loss: 1.630865\n",
      "Ep: 729, steps: 2, D loss: 0.258020, acc:  50%, G loss: 1.441585\n",
      "Ep: 729, steps: 3, D loss: 0.160459, acc:  85%, G loss: 1.985705\n",
      "Ep: 729, steps: 4, D loss: 0.184435, acc:  80%, G loss: 1.715601\n",
      "Ep: 729, steps: 5, D loss: 0.297856, acc:  42%, G loss: 1.516249\n",
      "Ep: 729, steps: 6, D loss: 0.244594, acc:  52%, G loss: 1.547806\n",
      "Ep: 729, steps: 7, D loss: 0.323589, acc:  27%, G loss: 1.565160\n",
      "Ep: 729, steps: 8, D loss: 0.225210, acc:  65%, G loss: 1.691899\n",
      "Ep: 729, steps: 9, D loss: 0.263256, acc:  50%, G loss: 1.582242\n",
      "Ep: 729, steps: 10, D loss: 0.186203, acc:  75%, G loss: 1.519975\n",
      "Ep: 729, steps: 11, D loss: 0.263990, acc:  50%, G loss: 1.686541\n",
      "Ep: 729, steps: 12, D loss: 0.282129, acc:  39%, G loss: 1.359648\n",
      "Ep: 729, steps: 13, D loss: 0.276621, acc:  38%, G loss: 1.408380\n",
      "Ep: 729, steps: 14, D loss: 0.280638, acc:  38%, G loss: 1.500866\n",
      "Ep: 729, steps: 15, D loss: 0.261711, acc:  49%, G loss: 1.574182\n",
      "Ep: 729, steps: 16, D loss: 0.241161, acc:  58%, G loss: 1.591633\n",
      "Ep: 729, steps: 17, D loss: 0.207598, acc:  71%, G loss: 1.539260\n",
      "Ep: 729, steps: 18, D loss: 0.242897, acc:  56%, G loss: 1.608557\n",
      "Ep: 729, steps: 19, D loss: 0.206470, acc:  69%, G loss: 1.578868\n",
      "Ep: 729, steps: 20, D loss: 0.192002, acc:  76%, G loss: 1.675499\n",
      "Ep: 729, steps: 21, D loss: 0.260471, acc:  44%, G loss: 1.416752\n",
      "Ep: 729, steps: 22, D loss: 0.219901, acc:  64%, G loss: 1.564028\n",
      "Ep: 729, steps: 23, D loss: 0.238501, acc:  59%, G loss: 1.768011\n",
      "Ep: 729, steps: 24, D loss: 0.213124, acc:  68%, G loss: 1.555670\n",
      "Ep: 729, steps: 25, D loss: 0.245108, acc:  57%, G loss: 1.573762\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 730, steps: 1, D loss: 0.229028, acc:  61%, G loss: 1.698482\n",
      "Ep: 730, steps: 2, D loss: 0.240732, acc:  56%, G loss: 1.473005\n",
      "Ep: 730, steps: 3, D loss: 0.176975, acc:  80%, G loss: 1.925615\n",
      "Ep: 730, steps: 4, D loss: 0.187052, acc:  80%, G loss: 1.681507\n",
      "Ep: 730, steps: 5, D loss: 0.281794, acc:  48%, G loss: 1.547316\n",
      "Ep: 730, steps: 6, D loss: 0.234077, acc:  58%, G loss: 1.598611\n",
      "Ep: 730, steps: 7, D loss: 0.300787, acc:  36%, G loss: 1.528618\n",
      "Ep: 730, steps: 8, D loss: 0.226282, acc:  65%, G loss: 1.776863\n",
      "Ep: 730, steps: 9, D loss: 0.229416, acc:  63%, G loss: 1.578947\n",
      "Ep: 730, steps: 10, D loss: 0.180415, acc:  80%, G loss: 1.519487\n",
      "Ep: 730, steps: 11, D loss: 0.256575, acc:  52%, G loss: 1.714110\n",
      "Ep: 730, steps: 12, D loss: 0.289578, acc:  36%, G loss: 1.360507\n",
      "Ep: 730, steps: 13, D loss: 0.285551, acc:  33%, G loss: 1.390619\n",
      "Ep: 730, steps: 14, D loss: 0.270145, acc:  46%, G loss: 1.503232\n",
      "Ep: 730, steps: 15, D loss: 0.272470, acc:  44%, G loss: 1.595305\n",
      "Ep: 730, steps: 16, D loss: 0.247750, acc:  54%, G loss: 1.570140\n",
      "Ep: 730, steps: 17, D loss: 0.219777, acc:  67%, G loss: 1.536065\n",
      "Ep: 730, steps: 18, D loss: 0.241795, acc:  57%, G loss: 1.665040\n",
      "Ep: 730, steps: 19, D loss: 0.208089, acc:  67%, G loss: 1.586623\n",
      "Ep: 730, steps: 20, D loss: 0.170276, acc:  83%, G loss: 1.688598\n",
      "Ep: 730, steps: 21, D loss: 0.265000, acc:  41%, G loss: 1.496918\n",
      "Ep: 730, steps: 22, D loss: 0.200988, acc:  68%, G loss: 1.639110\n",
      "Ep: 730, steps: 23, D loss: 0.225047, acc:  64%, G loss: 1.819125\n",
      "Ep: 730, steps: 24, D loss: 0.211887, acc:  68%, G loss: 1.480424\n",
      "Ep: 730, steps: 25, D loss: 0.255625, acc:  55%, G loss: 1.490704\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 731, steps: 1, D loss: 0.226764, acc:  62%, G loss: 1.751476\n",
      "Saved Model\n",
      "Ep: 731, steps: 2, D loss: 0.253151, acc:  52%, G loss: 1.472761\n",
      "Ep: 731, steps: 3, D loss: 0.207834, acc:  73%, G loss: 1.524060\n",
      "Ep: 731, steps: 4, D loss: 0.255477, acc:  54%, G loss: 1.572348\n",
      "Ep: 731, steps: 5, D loss: 0.232256, acc:  57%, G loss: 1.594349\n",
      "Ep: 731, steps: 6, D loss: 0.278962, acc:  39%, G loss: 1.401262\n",
      "Ep: 731, steps: 7, D loss: 0.244374, acc:  58%, G loss: 1.765542\n",
      "Ep: 731, steps: 8, D loss: 0.236727, acc:  59%, G loss: 1.596294\n",
      "Ep: 731, steps: 9, D loss: 0.176282, acc:  81%, G loss: 1.563959\n",
      "Ep: 731, steps: 10, D loss: 0.247106, acc:  52%, G loss: 1.718291\n",
      "Ep: 731, steps: 11, D loss: 0.287288, acc:  37%, G loss: 1.356312\n",
      "Ep: 731, steps: 12, D loss: 0.289609, acc:  33%, G loss: 1.399046\n",
      "Ep: 731, steps: 13, D loss: 0.264582, acc:  45%, G loss: 1.462386\n",
      "Ep: 731, steps: 14, D loss: 0.265461, acc:  47%, G loss: 1.589689\n",
      "Ep: 731, steps: 15, D loss: 0.256046, acc:  50%, G loss: 1.565526\n",
      "Ep: 731, steps: 16, D loss: 0.221834, acc:  65%, G loss: 1.635705\n",
      "Ep: 731, steps: 17, D loss: 0.224881, acc:  65%, G loss: 1.630439\n",
      "Ep: 731, steps: 18, D loss: 0.209811, acc:  68%, G loss: 1.596370\n",
      "Ep: 731, steps: 19, D loss: 0.183532, acc:  78%, G loss: 1.789588\n",
      "Ep: 731, steps: 20, D loss: 0.266716, acc:  41%, G loss: 1.617918\n",
      "Ep: 731, steps: 21, D loss: 0.219719, acc:  66%, G loss: 1.651036\n",
      "Ep: 731, steps: 22, D loss: 0.213472, acc:  68%, G loss: 1.839756\n",
      "Ep: 731, steps: 23, D loss: 0.218178, acc:  66%, G loss: 1.564810\n",
      "Ep: 731, steps: 24, D loss: 0.248807, acc:  57%, G loss: 1.637478\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 732, steps: 1, D loss: 0.238132, acc:  59%, G loss: 1.648427\n",
      "Ep: 732, steps: 2, D loss: 0.250812, acc:  53%, G loss: 1.502049\n",
      "Ep: 732, steps: 3, D loss: 0.172479, acc:  84%, G loss: 1.864959\n",
      "Ep: 732, steps: 4, D loss: 0.186576, acc:  81%, G loss: 1.608117\n",
      "Ep: 732, steps: 5, D loss: 0.267074, acc:  50%, G loss: 1.604434\n",
      "Ep: 732, steps: 6, D loss: 0.259012, acc:  51%, G loss: 1.588615\n",
      "Ep: 732, steps: 7, D loss: 0.281647, acc:  40%, G loss: 1.485631\n",
      "Ep: 732, steps: 8, D loss: 0.234305, acc:  61%, G loss: 1.676639\n",
      "Ep: 732, steps: 9, D loss: 0.244684, acc:  57%, G loss: 1.588620\n",
      "Ep: 732, steps: 10, D loss: 0.168732, acc:  81%, G loss: 1.606116\n",
      "Ep: 732, steps: 11, D loss: 0.257923, acc:  49%, G loss: 1.691754\n",
      "Ep: 732, steps: 12, D loss: 0.294056, acc:  34%, G loss: 1.377534\n",
      "Ep: 732, steps: 13, D loss: 0.284517, acc:  35%, G loss: 1.398162\n",
      "Ep: 732, steps: 14, D loss: 0.271449, acc:  45%, G loss: 1.461232\n",
      "Ep: 732, steps: 15, D loss: 0.266222, acc:  46%, G loss: 1.594009\n",
      "Ep: 732, steps: 16, D loss: 0.239458, acc:  59%, G loss: 1.621083\n",
      "Ep: 732, steps: 17, D loss: 0.225102, acc:  65%, G loss: 1.526632\n",
      "Ep: 732, steps: 18, D loss: 0.238921, acc:  57%, G loss: 1.631910\n",
      "Ep: 732, steps: 19, D loss: 0.208694, acc:  68%, G loss: 1.567551\n",
      "Ep: 732, steps: 20, D loss: 0.173521, acc:  83%, G loss: 1.714228\n",
      "Ep: 732, steps: 21, D loss: 0.270380, acc:  36%, G loss: 1.479678\n",
      "Ep: 732, steps: 22, D loss: 0.209146, acc:  65%, G loss: 1.817248\n",
      "Ep: 732, steps: 23, D loss: 0.220971, acc:  66%, G loss: 1.807855\n",
      "Ep: 732, steps: 24, D loss: 0.210160, acc:  71%, G loss: 1.511564\n",
      "Ep: 732, steps: 25, D loss: 0.267516, acc:  53%, G loss: 1.701419\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 733, steps: 1, D loss: 0.226392, acc:  63%, G loss: 1.673890\n",
      "Ep: 733, steps: 2, D loss: 0.256764, acc:  51%, G loss: 1.444110\n",
      "Ep: 733, steps: 3, D loss: 0.158462, acc:  86%, G loss: 1.933063\n",
      "Ep: 733, steps: 4, D loss: 0.177330, acc:  84%, G loss: 1.634395\n",
      "Ep: 733, steps: 5, D loss: 0.266535, acc:  52%, G loss: 1.594546\n",
      "Ep: 733, steps: 6, D loss: 0.246600, acc:  54%, G loss: 1.624522\n",
      "Ep: 733, steps: 7, D loss: 0.310246, acc:  33%, G loss: 1.507235\n",
      "Ep: 733, steps: 8, D loss: 0.221722, acc:  64%, G loss: 1.673932\n",
      "Ep: 733, steps: 9, D loss: 0.250022, acc:  55%, G loss: 1.601014\n",
      "Ep: 733, steps: 10, D loss: 0.179227, acc:  80%, G loss: 1.539270\n",
      "Ep: 733, steps: 11, D loss: 0.268129, acc:  47%, G loss: 1.726964\n",
      "Ep: 733, steps: 12, D loss: 0.294998, acc:  33%, G loss: 1.376592\n",
      "Ep: 733, steps: 13, D loss: 0.290426, acc:  31%, G loss: 1.401760\n",
      "Ep: 733, steps: 14, D loss: 0.275651, acc:  41%, G loss: 1.466729\n",
      "Ep: 733, steps: 15, D loss: 0.251277, acc:  50%, G loss: 1.591452\n",
      "Ep: 733, steps: 16, D loss: 0.235392, acc:  61%, G loss: 1.608521\n",
      "Ep: 733, steps: 17, D loss: 0.228162, acc:  62%, G loss: 1.534541\n",
      "Ep: 733, steps: 18, D loss: 0.242790, acc:  56%, G loss: 1.580498\n",
      "Ep: 733, steps: 19, D loss: 0.211643, acc:  68%, G loss: 1.584021\n",
      "Ep: 733, steps: 20, D loss: 0.166488, acc:  85%, G loss: 1.658350\n",
      "Ep: 733, steps: 21, D loss: 0.260929, acc:  43%, G loss: 1.455098\n",
      "Ep: 733, steps: 22, D loss: 0.197860, acc:  69%, G loss: 1.532727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 733, steps: 23, D loss: 0.220673, acc:  65%, G loss: 1.762485\n",
      "Ep: 733, steps: 24, D loss: 0.216806, acc:  66%, G loss: 1.497675\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 733, steps: 25, D loss: 0.249337, acc:  56%, G loss: 1.467191\n",
      "Ep: 733, steps: 26, D loss: 0.248708, acc:  55%, G loss: 1.410642\n",
      "Ep: 733, steps: 27, D loss: 0.160235, acc:  84%, G loss: 1.895606\n",
      "Ep: 733, steps: 28, D loss: 0.213973, acc:  70%, G loss: 1.572308\n",
      "Ep: 733, steps: 29, D loss: 0.269123, acc:  50%, G loss: 1.655295\n",
      "Ep: 733, steps: 30, D loss: 0.242236, acc:  54%, G loss: 1.661825\n",
      "Ep: 733, steps: 31, D loss: 0.292569, acc:  34%, G loss: 1.541150\n",
      "Ep: 733, steps: 32, D loss: 0.216647, acc:  66%, G loss: 1.719532\n",
      "Ep: 733, steps: 33, D loss: 0.237499, acc:  60%, G loss: 1.576409\n",
      "Ep: 733, steps: 34, D loss: 0.186070, acc:  74%, G loss: 1.547843\n",
      "Ep: 733, steps: 35, D loss: 0.261312, acc:  47%, G loss: 1.687995\n",
      "Ep: 733, steps: 36, D loss: 0.282029, acc:  38%, G loss: 1.363450\n",
      "Ep: 733, steps: 37, D loss: 0.282709, acc:  38%, G loss: 1.345624\n",
      "Ep: 733, steps: 38, D loss: 0.268453, acc:  46%, G loss: 1.540132\n",
      "Ep: 733, steps: 39, D loss: 0.259578, acc:  48%, G loss: 1.591492\n",
      "Ep: 733, steps: 40, D loss: 0.248320, acc:  54%, G loss: 1.626645\n",
      "Ep: 733, steps: 41, D loss: 0.221827, acc:  64%, G loss: 1.566648\n",
      "Ep: 733, steps: 42, D loss: 0.247523, acc:  54%, G loss: 1.630034\n",
      "Ep: 733, steps: 43, D loss: 0.213806, acc:  68%, G loss: 1.589811\n",
      "Ep: 733, steps: 44, D loss: 0.177119, acc:  81%, G loss: 1.686863\n",
      "Ep: 733, steps: 45, D loss: 0.257737, acc:  44%, G loss: 1.484394\n",
      "Ep: 733, steps: 46, D loss: 0.199225, acc:  69%, G loss: 1.578183\n",
      "Ep: 733, steps: 47, D loss: 0.234736, acc:  59%, G loss: 1.750202\n",
      "Ep: 733, steps: 48, D loss: 0.212063, acc:  68%, G loss: 1.539661\n",
      "Ep: 733, steps: 49, D loss: 0.254712, acc:  55%, G loss: 1.510945\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 734, steps: 1, D loss: 0.235226, acc:  59%, G loss: 1.631520\n",
      "Ep: 734, steps: 2, D loss: 0.253368, acc:  52%, G loss: 1.435220\n",
      "Ep: 734, steps: 3, D loss: 0.182356, acc:  76%, G loss: 1.894817\n",
      "Ep: 734, steps: 4, D loss: 0.181278, acc:  81%, G loss: 1.605510\n",
      "Ep: 734, steps: 5, D loss: 0.292980, acc:  44%, G loss: 1.622214\n",
      "Ep: 734, steps: 6, D loss: 0.262144, acc:  51%, G loss: 1.577397\n",
      "Ep: 734, steps: 7, D loss: 0.296138, acc:  35%, G loss: 1.468323\n",
      "Ep: 734, steps: 8, D loss: 0.237718, acc:  60%, G loss: 1.789832\n",
      "Ep: 734, steps: 9, D loss: 0.252055, acc:  54%, G loss: 1.631581\n",
      "Ep: 734, steps: 10, D loss: 0.185448, acc:  75%, G loss: 1.593064\n",
      "Ep: 734, steps: 11, D loss: 0.259284, acc:  48%, G loss: 1.758500\n",
      "Ep: 734, steps: 12, D loss: 0.281774, acc:  39%, G loss: 1.373078\n",
      "Ep: 734, steps: 13, D loss: 0.280259, acc:  38%, G loss: 1.397373\n",
      "Ep: 734, steps: 14, D loss: 0.271439, acc:  43%, G loss: 1.454209\n",
      "Ep: 734, steps: 15, D loss: 0.265937, acc:  45%, G loss: 1.518970\n",
      "Ep: 734, steps: 16, D loss: 0.250950, acc:  53%, G loss: 1.562464\n",
      "Ep: 734, steps: 17, D loss: 0.218639, acc:  65%, G loss: 1.534037\n",
      "Ep: 734, steps: 18, D loss: 0.236734, acc:  58%, G loss: 1.637167\n",
      "Ep: 734, steps: 19, D loss: 0.205671, acc:  69%, G loss: 1.629472\n",
      "Ep: 734, steps: 20, D loss: 0.175192, acc:  81%, G loss: 1.797266\n",
      "Ep: 734, steps: 21, D loss: 0.259931, acc:  44%, G loss: 1.603315\n",
      "Ep: 734, steps: 22, D loss: 0.187941, acc:  71%, G loss: 1.540718\n",
      "Ep: 734, steps: 23, D loss: 0.218275, acc:  66%, G loss: 1.875198\n",
      "Ep: 734, steps: 24, D loss: 0.216767, acc:  65%, G loss: 1.542899\n",
      "Ep: 734, steps: 25, D loss: 0.249475, acc:  56%, G loss: 1.568803\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 735, steps: 1, D loss: 0.238597, acc:  58%, G loss: 1.738372\n",
      "Ep: 735, steps: 2, D loss: 0.244883, acc:  55%, G loss: 1.495973\n",
      "Ep: 735, steps: 3, D loss: 0.165826, acc:  82%, G loss: 1.839117\n",
      "Ep: 735, steps: 4, D loss: 0.183100, acc:  83%, G loss: 1.648120\n",
      "Ep: 735, steps: 5, D loss: 0.288898, acc:  45%, G loss: 1.626242\n",
      "Ep: 735, steps: 6, D loss: 0.240834, acc:  55%, G loss: 1.629711\n",
      "Ep: 735, steps: 7, D loss: 0.315714, acc:  31%, G loss: 1.521227\n",
      "Ep: 735, steps: 8, D loss: 0.219955, acc:  68%, G loss: 1.693289\n",
      "Ep: 735, steps: 9, D loss: 0.236132, acc:  59%, G loss: 1.547026\n",
      "Ep: 735, steps: 10, D loss: 0.190929, acc:  72%, G loss: 1.515403\n",
      "Ep: 735, steps: 11, D loss: 0.262514, acc:  48%, G loss: 1.703898\n",
      "Ep: 735, steps: 12, D loss: 0.287756, acc:  37%, G loss: 1.345620\n",
      "Ep: 735, steps: 13, D loss: 0.286844, acc:  33%, G loss: 1.371742\n",
      "Ep: 735, steps: 14, D loss: 0.273758, acc:  42%, G loss: 1.501698\n",
      "Ep: 735, steps: 15, D loss: 0.255937, acc:  51%, G loss: 1.514107\n",
      "Ep: 735, steps: 16, D loss: 0.236548, acc:  60%, G loss: 1.536538\n",
      "Ep: 735, steps: 17, D loss: 0.217514, acc:  67%, G loss: 1.489780\n",
      "Ep: 735, steps: 18, D loss: 0.236806, acc:  59%, G loss: 1.572258\n",
      "Ep: 735, steps: 19, D loss: 0.209501, acc:  69%, G loss: 1.600401\n",
      "Ep: 735, steps: 20, D loss: 0.178861, acc:  79%, G loss: 1.697510\n",
      "Ep: 735, steps: 21, D loss: 0.263375, acc:  41%, G loss: 1.444582\n",
      "Ep: 735, steps: 22, D loss: 0.229581, acc:  64%, G loss: 1.661750\n",
      "Saved Model\n",
      "Ep: 735, steps: 23, D loss: 0.222753, acc:  63%, G loss: 1.802643\n",
      "Ep: 735, steps: 24, D loss: 0.269401, acc:  51%, G loss: 1.807719\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 736, steps: 1, D loss: 0.210526, acc:  69%, G loss: 1.732468\n",
      "Ep: 736, steps: 2, D loss: 0.245618, acc:  56%, G loss: 1.533579\n",
      "Ep: 736, steps: 3, D loss: 0.169971, acc:  80%, G loss: 1.866058\n",
      "Ep: 736, steps: 4, D loss: 0.176197, acc:  86%, G loss: 1.622759\n",
      "Ep: 736, steps: 5, D loss: 0.272828, acc:  47%, G loss: 1.659379\n",
      "Ep: 736, steps: 6, D loss: 0.255915, acc:  52%, G loss: 1.502291\n",
      "Ep: 736, steps: 7, D loss: 0.308218, acc:  30%, G loss: 1.386039\n",
      "Ep: 736, steps: 8, D loss: 0.241774, acc:  57%, G loss: 1.684684\n",
      "Ep: 736, steps: 9, D loss: 0.219136, acc:  67%, G loss: 1.586011\n",
      "Ep: 736, steps: 10, D loss: 0.176226, acc:  80%, G loss: 1.684420\n",
      "Ep: 736, steps: 11, D loss: 0.258291, acc:  48%, G loss: 1.713533\n",
      "Ep: 736, steps: 12, D loss: 0.284481, acc:  38%, G loss: 1.386148\n",
      "Ep: 736, steps: 13, D loss: 0.285943, acc:  34%, G loss: 1.349937\n",
      "Ep: 736, steps: 14, D loss: 0.266509, acc:  45%, G loss: 1.464364\n",
      "Ep: 736, steps: 15, D loss: 0.261721, acc:  47%, G loss: 1.478059\n",
      "Ep: 736, steps: 16, D loss: 0.237798, acc:  61%, G loss: 1.566594\n",
      "Ep: 736, steps: 17, D loss: 0.222426, acc:  64%, G loss: 1.508992\n",
      "Ep: 736, steps: 18, D loss: 0.224311, acc:  64%, G loss: 1.597372\n",
      "Ep: 736, steps: 19, D loss: 0.204932, acc:  70%, G loss: 1.583680\n",
      "Ep: 736, steps: 20, D loss: 0.174632, acc:  80%, G loss: 1.704360\n",
      "Ep: 736, steps: 21, D loss: 0.265828, acc:  40%, G loss: 1.554191\n",
      "Ep: 736, steps: 22, D loss: 0.184500, acc:  73%, G loss: 1.581536\n",
      "Ep: 736, steps: 23, D loss: 0.219992, acc:  65%, G loss: 1.803907\n",
      "Ep: 736, steps: 24, D loss: 0.213452, acc:  65%, G loss: 1.532789\n",
      "Ep: 736, steps: 25, D loss: 0.253608, acc:  56%, G loss: 1.584839\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 737, steps: 1, D loss: 0.229623, acc:  59%, G loss: 1.687941\n",
      "Ep: 737, steps: 2, D loss: 0.252163, acc:  53%, G loss: 1.503119\n",
      "Ep: 737, steps: 3, D loss: 0.169308, acc:  83%, G loss: 1.866427\n",
      "Ep: 737, steps: 4, D loss: 0.175465, acc:  83%, G loss: 1.641354\n",
      "Ep: 737, steps: 5, D loss: 0.287430, acc:  47%, G loss: 1.541551\n",
      "Ep: 737, steps: 6, D loss: 0.252638, acc:  53%, G loss: 1.549256\n",
      "Ep: 737, steps: 7, D loss: 0.306485, acc:  34%, G loss: 1.489717\n",
      "Ep: 737, steps: 8, D loss: 0.212795, acc:  70%, G loss: 1.748177\n",
      "Ep: 737, steps: 9, D loss: 0.249852, acc:  55%, G loss: 1.583150\n",
      "Ep: 737, steps: 10, D loss: 0.176231, acc:  80%, G loss: 1.604481\n",
      "Ep: 737, steps: 11, D loss: 0.255332, acc:  52%, G loss: 1.706858\n",
      "Ep: 737, steps: 12, D loss: 0.296994, acc:  34%, G loss: 1.422319\n",
      "Ep: 737, steps: 13, D loss: 0.281779, acc:  37%, G loss: 1.400804\n",
      "Ep: 737, steps: 14, D loss: 0.277240, acc:  42%, G loss: 1.486798\n",
      "Ep: 737, steps: 15, D loss: 0.272707, acc:  44%, G loss: 1.530077\n",
      "Ep: 737, steps: 16, D loss: 0.242885, acc:  58%, G loss: 1.573309\n",
      "Ep: 737, steps: 17, D loss: 0.224613, acc:  61%, G loss: 1.488390\n",
      "Ep: 737, steps: 18, D loss: 0.231036, acc:  60%, G loss: 1.566921\n",
      "Ep: 737, steps: 19, D loss: 0.209308, acc:  67%, G loss: 1.564895\n",
      "Ep: 737, steps: 20, D loss: 0.180995, acc:  76%, G loss: 1.691180\n",
      "Ep: 737, steps: 21, D loss: 0.263706, acc:  41%, G loss: 1.497364\n",
      "Ep: 737, steps: 22, D loss: 0.218119, acc:  63%, G loss: 1.513210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 737, steps: 23, D loss: 0.219776, acc:  67%, G loss: 1.842849\n",
      "Ep: 737, steps: 24, D loss: 0.220574, acc:  63%, G loss: 1.561687\n",
      "Ep: 737, steps: 25, D loss: 0.254714, acc:  55%, G loss: 1.474826\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 738, steps: 1, D loss: 0.249226, acc:  53%, G loss: 1.604847\n",
      "Ep: 738, steps: 2, D loss: 0.251629, acc:  53%, G loss: 1.454046\n",
      "Ep: 738, steps: 3, D loss: 0.164651, acc:  83%, G loss: 1.812262\n",
      "Ep: 738, steps: 4, D loss: 0.187697, acc:  82%, G loss: 1.597421\n",
      "Ep: 738, steps: 5, D loss: 0.280780, acc:  47%, G loss: 1.610257\n",
      "Ep: 738, steps: 6, D loss: 0.239973, acc:  54%, G loss: 1.547279\n",
      "Ep: 738, steps: 7, D loss: 0.312823, acc:  32%, G loss: 1.430273\n",
      "Ep: 738, steps: 8, D loss: 0.220227, acc:  67%, G loss: 1.748652\n",
      "Ep: 738, steps: 9, D loss: 0.229394, acc:  63%, G loss: 1.656982\n",
      "Ep: 738, steps: 10, D loss: 0.175771, acc:  82%, G loss: 1.525220\n",
      "Ep: 738, steps: 11, D loss: 0.260051, acc:  49%, G loss: 1.726732\n",
      "Ep: 738, steps: 12, D loss: 0.286505, acc:  34%, G loss: 1.334365\n",
      "Ep: 738, steps: 13, D loss: 0.287373, acc:  33%, G loss: 1.375040\n",
      "Ep: 738, steps: 14, D loss: 0.272359, acc:  43%, G loss: 1.459693\n",
      "Ep: 738, steps: 15, D loss: 0.247277, acc:  53%, G loss: 1.521602\n",
      "Ep: 738, steps: 16, D loss: 0.246913, acc:  55%, G loss: 1.585660\n",
      "Ep: 738, steps: 17, D loss: 0.220259, acc:  67%, G loss: 1.492337\n",
      "Ep: 738, steps: 18, D loss: 0.226919, acc:  62%, G loss: 1.580632\n",
      "Ep: 738, steps: 19, D loss: 0.216926, acc:  65%, G loss: 1.610896\n",
      "Ep: 738, steps: 20, D loss: 0.186656, acc:  76%, G loss: 1.743480\n",
      "Saved Model\n",
      "Ep: 738, steps: 21, D loss: 0.253992, acc:  46%, G loss: 1.480411\n",
      "Ep: 738, steps: 22, D loss: 0.234215, acc:  59%, G loss: 1.699281\n",
      "Ep: 738, steps: 23, D loss: 0.215528, acc:  70%, G loss: 1.482154\n",
      "Ep: 738, steps: 24, D loss: 0.245433, acc:  58%, G loss: 1.454314\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 739, steps: 1, D loss: 0.236029, acc:  60%, G loss: 1.662247\n",
      "Ep: 739, steps: 2, D loss: 0.252389, acc:  54%, G loss: 1.474100\n",
      "Ep: 739, steps: 3, D loss: 0.173342, acc:  81%, G loss: 1.841571\n",
      "Ep: 739, steps: 4, D loss: 0.189915, acc:  81%, G loss: 1.628939\n",
      "Ep: 739, steps: 5, D loss: 0.257729, acc:  54%, G loss: 1.623546\n",
      "Ep: 739, steps: 6, D loss: 0.223461, acc:  58%, G loss: 1.526504\n",
      "Ep: 739, steps: 7, D loss: 0.300331, acc:  34%, G loss: 1.550035\n",
      "Ep: 739, steps: 8, D loss: 0.227158, acc:  64%, G loss: 1.731580\n",
      "Ep: 739, steps: 9, D loss: 0.265202, acc:  51%, G loss: 1.653740\n",
      "Ep: 739, steps: 10, D loss: 0.194528, acc:  73%, G loss: 1.513040\n",
      "Ep: 739, steps: 11, D loss: 0.246044, acc:  51%, G loss: 1.735613\n",
      "Ep: 739, steps: 12, D loss: 0.278633, acc:  37%, G loss: 1.392037\n",
      "Ep: 739, steps: 13, D loss: 0.277623, acc:  38%, G loss: 1.418424\n",
      "Ep: 739, steps: 14, D loss: 0.273229, acc:  44%, G loss: 1.437301\n",
      "Ep: 739, steps: 15, D loss: 0.258923, acc:  49%, G loss: 1.589978\n",
      "Ep: 739, steps: 16, D loss: 0.249844, acc:  53%, G loss: 1.580859\n",
      "Ep: 739, steps: 17, D loss: 0.234737, acc:  58%, G loss: 1.554457\n",
      "Ep: 739, steps: 18, D loss: 0.236663, acc:  60%, G loss: 1.579281\n",
      "Ep: 739, steps: 19, D loss: 0.199834, acc:  73%, G loss: 1.603765\n",
      "Ep: 739, steps: 20, D loss: 0.173812, acc:  82%, G loss: 1.794227\n",
      "Ep: 739, steps: 21, D loss: 0.275161, acc:  37%, G loss: 1.458813\n",
      "Ep: 739, steps: 22, D loss: 0.224590, acc:  60%, G loss: 1.617360\n",
      "Ep: 739, steps: 23, D loss: 0.234616, acc:  60%, G loss: 1.778199\n",
      "Ep: 739, steps: 24, D loss: 0.209195, acc:  69%, G loss: 1.520156\n",
      "Ep: 739, steps: 25, D loss: 0.263635, acc:  51%, G loss: 1.483989\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 740, steps: 1, D loss: 0.228482, acc:  62%, G loss: 1.604295\n",
      "Ep: 740, steps: 2, D loss: 0.241015, acc:  57%, G loss: 1.458537\n",
      "Ep: 740, steps: 3, D loss: 0.171917, acc:  81%, G loss: 1.920747\n",
      "Ep: 740, steps: 4, D loss: 0.184807, acc:  80%, G loss: 1.607542\n",
      "Ep: 740, steps: 5, D loss: 0.281487, acc:  48%, G loss: 1.641851\n",
      "Ep: 740, steps: 6, D loss: 0.246964, acc:  54%, G loss: 1.632591\n",
      "Ep: 740, steps: 7, D loss: 0.311288, acc:  33%, G loss: 1.481679\n",
      "Ep: 740, steps: 8, D loss: 0.224164, acc:  65%, G loss: 1.721134\n",
      "Ep: 740, steps: 9, D loss: 0.248958, acc:  55%, G loss: 1.614299\n",
      "Ep: 740, steps: 10, D loss: 0.183545, acc:  75%, G loss: 1.543197\n",
      "Ep: 740, steps: 11, D loss: 0.258829, acc:  49%, G loss: 1.712860\n",
      "Ep: 740, steps: 12, D loss: 0.279545, acc:  38%, G loss: 1.381896\n",
      "Ep: 740, steps: 13, D loss: 0.286704, acc:  36%, G loss: 1.417096\n",
      "Ep: 740, steps: 14, D loss: 0.275507, acc:  41%, G loss: 1.486922\n",
      "Ep: 740, steps: 15, D loss: 0.265856, acc:  46%, G loss: 1.587134\n",
      "Ep: 740, steps: 16, D loss: 0.252541, acc:  53%, G loss: 1.634154\n",
      "Ep: 740, steps: 17, D loss: 0.211913, acc:  70%, G loss: 1.467190\n",
      "Ep: 740, steps: 18, D loss: 0.244269, acc:  56%, G loss: 1.566431\n",
      "Ep: 740, steps: 19, D loss: 0.216626, acc:  65%, G loss: 1.577819\n",
      "Ep: 740, steps: 20, D loss: 0.172327, acc:  81%, G loss: 1.690496\n",
      "Ep: 740, steps: 21, D loss: 0.265120, acc:  40%, G loss: 1.531394\n",
      "Ep: 740, steps: 22, D loss: 0.219288, acc:  61%, G loss: 1.613983\n",
      "Ep: 740, steps: 23, D loss: 0.224592, acc:  63%, G loss: 1.783115\n",
      "Ep: 740, steps: 24, D loss: 0.227188, acc:  62%, G loss: 1.538612\n",
      "Ep: 740, steps: 25, D loss: 0.261576, acc:  53%, G loss: 1.534323\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 741, steps: 1, D loss: 0.218886, acc:  65%, G loss: 1.688130\n",
      "Ep: 741, steps: 2, D loss: 0.260016, acc:  51%, G loss: 1.476869\n",
      "Ep: 741, steps: 3, D loss: 0.174585, acc:  79%, G loss: 1.903025\n",
      "Ep: 741, steps: 4, D loss: 0.183864, acc:  82%, G loss: 1.674665\n",
      "Ep: 741, steps: 5, D loss: 0.270055, acc:  47%, G loss: 1.618522\n",
      "Ep: 741, steps: 6, D loss: 0.250577, acc:  55%, G loss: 1.562229\n",
      "Ep: 741, steps: 7, D loss: 0.296357, acc:  35%, G loss: 1.519798\n",
      "Ep: 741, steps: 8, D loss: 0.228954, acc:  65%, G loss: 1.776520\n",
      "Ep: 741, steps: 9, D loss: 0.239008, acc:  58%, G loss: 1.601125\n",
      "Ep: 741, steps: 10, D loss: 0.184404, acc:  76%, G loss: 1.544271\n",
      "Ep: 741, steps: 11, D loss: 0.267527, acc:  49%, G loss: 1.785269\n",
      "Ep: 741, steps: 12, D loss: 0.281081, acc:  37%, G loss: 1.399884\n",
      "Ep: 741, steps: 13, D loss: 0.281300, acc:  37%, G loss: 1.464935\n",
      "Ep: 741, steps: 14, D loss: 0.271240, acc:  45%, G loss: 1.531057\n",
      "Ep: 741, steps: 15, D loss: 0.267390, acc:  48%, G loss: 1.535771\n",
      "Ep: 741, steps: 16, D loss: 0.244650, acc:  57%, G loss: 1.619000\n",
      "Ep: 741, steps: 17, D loss: 0.215026, acc:  70%, G loss: 1.529787\n",
      "Ep: 741, steps: 18, D loss: 0.233738, acc:  59%, G loss: 1.616043\n",
      "Saved Model\n",
      "Ep: 741, steps: 19, D loss: 0.207428, acc:  68%, G loss: 1.618016\n",
      "Ep: 741, steps: 20, D loss: 0.260254, acc:  45%, G loss: 1.474273\n",
      "Ep: 741, steps: 21, D loss: 0.217587, acc:  65%, G loss: 1.521353\n",
      "Ep: 741, steps: 22, D loss: 0.214577, acc:  67%, G loss: 1.712730\n",
      "Ep: 741, steps: 23, D loss: 0.223499, acc:  64%, G loss: 1.538118\n",
      "Ep: 741, steps: 24, D loss: 0.256491, acc:  55%, G loss: 1.522910\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 742, steps: 1, D loss: 0.217847, acc:  66%, G loss: 1.678220\n",
      "Ep: 742, steps: 2, D loss: 0.258566, acc:  50%, G loss: 1.480933\n",
      "Ep: 742, steps: 3, D loss: 0.168339, acc:  83%, G loss: 1.874162\n",
      "Ep: 742, steps: 4, D loss: 0.180872, acc:  82%, G loss: 1.651268\n",
      "Ep: 742, steps: 5, D loss: 0.297052, acc:  43%, G loss: 1.557734\n",
      "Ep: 742, steps: 6, D loss: 0.237337, acc:  57%, G loss: 1.515545\n",
      "Ep: 742, steps: 7, D loss: 0.302929, acc:  34%, G loss: 1.450961\n",
      "Ep: 742, steps: 8, D loss: 0.229941, acc:  63%, G loss: 1.673853\n",
      "Ep: 742, steps: 9, D loss: 0.244711, acc:  57%, G loss: 1.632349\n",
      "Ep: 742, steps: 10, D loss: 0.198022, acc:  73%, G loss: 1.506563\n",
      "Ep: 742, steps: 11, D loss: 0.239557, acc:  57%, G loss: 1.680528\n",
      "Ep: 742, steps: 12, D loss: 0.288200, acc:  35%, G loss: 1.402126\n",
      "Ep: 742, steps: 13, D loss: 0.287424, acc:  35%, G loss: 1.424656\n",
      "Ep: 742, steps: 14, D loss: 0.278547, acc:  42%, G loss: 1.517865\n",
      "Ep: 742, steps: 15, D loss: 0.248383, acc:  53%, G loss: 1.556834\n",
      "Ep: 742, steps: 16, D loss: 0.255064, acc:  53%, G loss: 1.578151\n",
      "Ep: 742, steps: 17, D loss: 0.220275, acc:  67%, G loss: 1.533602\n",
      "Ep: 742, steps: 18, D loss: 0.238904, acc:  58%, G loss: 1.612974\n",
      "Ep: 742, steps: 19, D loss: 0.216588, acc:  66%, G loss: 1.574810\n",
      "Ep: 742, steps: 20, D loss: 0.184016, acc:  78%, G loss: 1.696434\n",
      "Ep: 742, steps: 21, D loss: 0.264909, acc:  39%, G loss: 1.487538\n",
      "Ep: 742, steps: 22, D loss: 0.241386, acc:  60%, G loss: 1.640585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 742, steps: 23, D loss: 0.225279, acc:  64%, G loss: 1.777379\n",
      "Ep: 742, steps: 24, D loss: 0.213040, acc:  68%, G loss: 1.536505\n",
      "Ep: 742, steps: 25, D loss: 0.256413, acc:  55%, G loss: 1.585145\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 743, steps: 1, D loss: 0.230787, acc:  60%, G loss: 1.647671\n",
      "Ep: 743, steps: 2, D loss: 0.258001, acc:  51%, G loss: 1.446807\n",
      "Ep: 743, steps: 3, D loss: 0.168833, acc:  84%, G loss: 1.888868\n",
      "Ep: 743, steps: 4, D loss: 0.179674, acc:  83%, G loss: 1.643184\n",
      "Ep: 743, steps: 5, D loss: 0.263064, acc:  51%, G loss: 1.616457\n",
      "Ep: 743, steps: 6, D loss: 0.244158, acc:  53%, G loss: 1.559144\n",
      "Ep: 743, steps: 7, D loss: 0.308885, acc:  33%, G loss: 1.461559\n",
      "Ep: 743, steps: 8, D loss: 0.238439, acc:  61%, G loss: 1.678239\n",
      "Ep: 743, steps: 9, D loss: 0.238253, acc:  60%, G loss: 1.597601\n",
      "Ep: 743, steps: 10, D loss: 0.176345, acc:  79%, G loss: 1.556937\n",
      "Ep: 743, steps: 11, D loss: 0.258805, acc:  49%, G loss: 1.720230\n",
      "Ep: 743, steps: 12, D loss: 0.285006, acc:  37%, G loss: 1.389806\n",
      "Ep: 743, steps: 13, D loss: 0.293773, acc:  32%, G loss: 1.377368\n",
      "Ep: 743, steps: 14, D loss: 0.263262, acc:  47%, G loss: 1.525872\n",
      "Ep: 743, steps: 15, D loss: 0.262591, acc:  48%, G loss: 1.628587\n",
      "Ep: 743, steps: 16, D loss: 0.235136, acc:  61%, G loss: 1.588996\n",
      "Ep: 743, steps: 17, D loss: 0.238682, acc:  57%, G loss: 1.564263\n",
      "Ep: 743, steps: 18, D loss: 0.224513, acc:  64%, G loss: 1.630777\n",
      "Ep: 743, steps: 19, D loss: 0.214576, acc:  68%, G loss: 1.544197\n",
      "Ep: 743, steps: 20, D loss: 0.186591, acc:  78%, G loss: 1.666424\n",
      "Ep: 743, steps: 21, D loss: 0.261145, acc:  42%, G loss: 1.495308\n",
      "Ep: 743, steps: 22, D loss: 0.191272, acc:  73%, G loss: 1.541460\n",
      "Ep: 743, steps: 23, D loss: 0.213020, acc:  68%, G loss: 1.774623\n",
      "Ep: 743, steps: 24, D loss: 0.219450, acc:  67%, G loss: 1.510047\n",
      "Ep: 743, steps: 25, D loss: 0.256246, acc:  53%, G loss: 1.699527\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 744, steps: 1, D loss: 0.218229, acc:  65%, G loss: 1.725345\n",
      "Ep: 744, steps: 2, D loss: 0.248401, acc:  54%, G loss: 1.419136\n",
      "Ep: 744, steps: 3, D loss: 0.178117, acc:  79%, G loss: 1.939683\n",
      "Ep: 744, steps: 4, D loss: 0.183170, acc:  81%, G loss: 1.631747\n",
      "Ep: 744, steps: 5, D loss: 0.282877, acc:  50%, G loss: 1.578004\n",
      "Ep: 744, steps: 6, D loss: 0.235218, acc:  56%, G loss: 1.521215\n",
      "Ep: 744, steps: 7, D loss: 0.299422, acc:  36%, G loss: 1.545406\n",
      "Ep: 744, steps: 8, D loss: 0.229494, acc:  64%, G loss: 1.746852\n",
      "Ep: 744, steps: 9, D loss: 0.244113, acc:  57%, G loss: 1.619073\n",
      "Ep: 744, steps: 10, D loss: 0.187244, acc:  78%, G loss: 1.533494\n",
      "Ep: 744, steps: 11, D loss: 0.247780, acc:  53%, G loss: 1.707866\n",
      "Ep: 744, steps: 12, D loss: 0.286565, acc:  37%, G loss: 1.386027\n",
      "Ep: 744, steps: 13, D loss: 0.281902, acc:  39%, G loss: 1.402302\n",
      "Ep: 744, steps: 14, D loss: 0.259114, acc:  50%, G loss: 1.538415\n",
      "Ep: 744, steps: 15, D loss: 0.254091, acc:  53%, G loss: 1.551590\n",
      "Ep: 744, steps: 16, D loss: 0.251783, acc:  56%, G loss: 1.588558\n",
      "Saved Model\n",
      "Ep: 744, steps: 17, D loss: 0.220068, acc:  66%, G loss: 1.488535\n",
      "Ep: 744, steps: 18, D loss: 0.224298, acc:  64%, G loss: 1.551451\n",
      "Ep: 744, steps: 19, D loss: 0.194985, acc:  74%, G loss: 1.716653\n",
      "Ep: 744, steps: 20, D loss: 0.257748, acc:  47%, G loss: 1.455252\n",
      "Ep: 744, steps: 21, D loss: 0.219871, acc:  65%, G loss: 1.582778\n",
      "Ep: 744, steps: 22, D loss: 0.217054, acc:  66%, G loss: 1.785397\n",
      "Ep: 744, steps: 23, D loss: 0.211171, acc:  68%, G loss: 1.575450\n",
      "Ep: 744, steps: 24, D loss: 0.219792, acc:  68%, G loss: 1.575158\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 745, steps: 1, D loss: 0.234130, acc:  58%, G loss: 1.603723\n",
      "Ep: 745, steps: 2, D loss: 0.263066, acc:  49%, G loss: 1.446882\n",
      "Ep: 745, steps: 3, D loss: 0.157569, acc:  87%, G loss: 1.940734\n",
      "Ep: 745, steps: 4, D loss: 0.190770, acc:  78%, G loss: 1.712108\n",
      "Ep: 745, steps: 5, D loss: 0.285235, acc:  47%, G loss: 1.685494\n",
      "Ep: 745, steps: 6, D loss: 0.250252, acc:  55%, G loss: 1.522601\n",
      "Ep: 745, steps: 7, D loss: 0.303478, acc:  34%, G loss: 1.453125\n",
      "Ep: 745, steps: 8, D loss: 0.224763, acc:  67%, G loss: 1.710952\n",
      "Ep: 745, steps: 9, D loss: 0.228478, acc:  64%, G loss: 1.604313\n",
      "Ep: 745, steps: 10, D loss: 0.180690, acc:  77%, G loss: 1.558214\n",
      "Ep: 745, steps: 11, D loss: 0.253415, acc:  53%, G loss: 1.731410\n",
      "Ep: 745, steps: 12, D loss: 0.284772, acc:  39%, G loss: 1.390083\n",
      "Ep: 745, steps: 13, D loss: 0.296838, acc:  30%, G loss: 1.367260\n",
      "Ep: 745, steps: 14, D loss: 0.256268, acc:  51%, G loss: 1.502080\n",
      "Ep: 745, steps: 15, D loss: 0.278686, acc:  44%, G loss: 1.625480\n",
      "Ep: 745, steps: 16, D loss: 0.236457, acc:  59%, G loss: 1.612789\n",
      "Ep: 745, steps: 17, D loss: 0.234119, acc:  60%, G loss: 1.468691\n",
      "Ep: 745, steps: 18, D loss: 0.253207, acc:  52%, G loss: 1.567440\n",
      "Ep: 745, steps: 19, D loss: 0.224827, acc:  65%, G loss: 1.583177\n",
      "Ep: 745, steps: 20, D loss: 0.194416, acc:  74%, G loss: 1.747670\n",
      "Ep: 745, steps: 21, D loss: 0.255115, acc:  46%, G loss: 1.434210\n",
      "Ep: 745, steps: 22, D loss: 0.194025, acc:  69%, G loss: 1.562959\n",
      "Ep: 745, steps: 23, D loss: 0.223488, acc:  65%, G loss: 1.737099\n",
      "Ep: 745, steps: 24, D loss: 0.215225, acc:  68%, G loss: 1.531906\n",
      "Ep: 745, steps: 25, D loss: 0.252666, acc:  56%, G loss: 1.686113\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 746, steps: 1, D loss: 0.226742, acc:  61%, G loss: 1.650899\n",
      "Ep: 746, steps: 2, D loss: 0.241892, acc:  57%, G loss: 1.451301\n",
      "Ep: 746, steps: 3, D loss: 0.160623, acc:  86%, G loss: 1.889533\n",
      "Ep: 746, steps: 4, D loss: 0.183189, acc:  81%, G loss: 1.638072\n",
      "Ep: 746, steps: 5, D loss: 0.263791, acc:  51%, G loss: 1.628363\n",
      "Ep: 746, steps: 6, D loss: 0.243430, acc:  54%, G loss: 1.578312\n",
      "Ep: 746, steps: 7, D loss: 0.312101, acc:  32%, G loss: 1.491915\n",
      "Ep: 746, steps: 8, D loss: 0.233503, acc:  63%, G loss: 1.705010\n",
      "Ep: 746, steps: 9, D loss: 0.240321, acc:  58%, G loss: 1.596655\n",
      "Ep: 746, steps: 10, D loss: 0.177292, acc:  78%, G loss: 1.575630\n",
      "Ep: 746, steps: 11, D loss: 0.266559, acc:  47%, G loss: 1.717111\n",
      "Ep: 746, steps: 12, D loss: 0.298934, acc:  32%, G loss: 1.404263\n",
      "Ep: 746, steps: 13, D loss: 0.282425, acc:  38%, G loss: 1.380239\n",
      "Ep: 746, steps: 14, D loss: 0.276562, acc:  44%, G loss: 1.477163\n",
      "Ep: 746, steps: 15, D loss: 0.246210, acc:  54%, G loss: 1.588438\n",
      "Ep: 746, steps: 16, D loss: 0.243607, acc:  57%, G loss: 1.653944\n",
      "Ep: 746, steps: 17, D loss: 0.218724, acc:  66%, G loss: 1.514483\n",
      "Ep: 746, steps: 18, D loss: 0.239060, acc:  58%, G loss: 1.569558\n",
      "Ep: 746, steps: 19, D loss: 0.230260, acc:  61%, G loss: 1.594598\n",
      "Ep: 746, steps: 20, D loss: 0.181743, acc:  79%, G loss: 1.650840\n",
      "Ep: 746, steps: 21, D loss: 0.258175, acc:  45%, G loss: 1.502398\n",
      "Ep: 746, steps: 22, D loss: 0.223121, acc:  63%, G loss: 1.530131\n",
      "Ep: 746, steps: 23, D loss: 0.213054, acc:  67%, G loss: 1.760862\n",
      "Ep: 746, steps: 24, D loss: 0.198696, acc:  74%, G loss: 1.518689\n",
      "Ep: 746, steps: 25, D loss: 0.242872, acc:  59%, G loss: 1.600429\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 747, steps: 1, D loss: 0.227394, acc:  60%, G loss: 1.649937\n",
      "Ep: 747, steps: 2, D loss: 0.248823, acc:  54%, G loss: 1.427171\n",
      "Ep: 747, steps: 3, D loss: 0.174391, acc:  81%, G loss: 1.871536\n",
      "Ep: 747, steps: 4, D loss: 0.186129, acc:  79%, G loss: 1.754239\n",
      "Ep: 747, steps: 5, D loss: 0.282522, acc:  47%, G loss: 1.618285\n",
      "Ep: 747, steps: 6, D loss: 0.246458, acc:  55%, G loss: 1.545044\n",
      "Ep: 747, steps: 7, D loss: 0.316241, acc:  32%, G loss: 1.461351\n",
      "Ep: 747, steps: 8, D loss: 0.227601, acc:  64%, G loss: 1.674767\n",
      "Ep: 747, steps: 9, D loss: 0.244452, acc:  56%, G loss: 1.604467\n",
      "Ep: 747, steps: 10, D loss: 0.177464, acc:  78%, G loss: 1.542912\n",
      "Ep: 747, steps: 11, D loss: 0.258776, acc:  48%, G loss: 1.762374\n",
      "Ep: 747, steps: 12, D loss: 0.279798, acc:  38%, G loss: 1.390901\n",
      "Ep: 747, steps: 13, D loss: 0.289809, acc:  34%, G loss: 1.411609\n",
      "Ep: 747, steps: 14, D loss: 0.270900, acc:  46%, G loss: 1.496217\n",
      "Saved Model\n",
      "Ep: 747, steps: 15, D loss: 0.264835, acc:  47%, G loss: 1.619583\n",
      "Ep: 747, steps: 16, D loss: 0.209051, acc:  70%, G loss: 1.540901\n",
      "Ep: 747, steps: 17, D loss: 0.242106, acc:  57%, G loss: 1.541004\n",
      "Ep: 747, steps: 18, D loss: 0.221821, acc:  64%, G loss: 1.582233\n",
      "Ep: 747, steps: 19, D loss: 0.183613, acc:  78%, G loss: 1.690602\n",
      "Ep: 747, steps: 20, D loss: 0.254151, acc:  47%, G loss: 1.571291\n",
      "Ep: 747, steps: 21, D loss: 0.202040, acc:  66%, G loss: 1.556072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 747, steps: 22, D loss: 0.218749, acc:  67%, G loss: 1.794043\n",
      "Ep: 747, steps: 23, D loss: 0.211839, acc:  68%, G loss: 1.562226\n",
      "Ep: 747, steps: 24, D loss: 0.252461, acc:  57%, G loss: 1.496464\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 748, steps: 1, D loss: 0.229215, acc:  59%, G loss: 1.643583\n",
      "Ep: 748, steps: 2, D loss: 0.257371, acc:  50%, G loss: 1.411022\n",
      "Ep: 748, steps: 3, D loss: 0.167216, acc:  85%, G loss: 1.874239\n",
      "Ep: 748, steps: 4, D loss: 0.175323, acc:  82%, G loss: 1.709344\n",
      "Ep: 748, steps: 5, D loss: 0.282555, acc:  49%, G loss: 1.627373\n",
      "Ep: 748, steps: 6, D loss: 0.239732, acc:  56%, G loss: 1.530718\n",
      "Ep: 748, steps: 7, D loss: 0.306981, acc:  34%, G loss: 1.433211\n",
      "Ep: 748, steps: 8, D loss: 0.227852, acc:  64%, G loss: 1.688669\n",
      "Ep: 748, steps: 9, D loss: 0.262769, acc:  49%, G loss: 1.577752\n",
      "Ep: 748, steps: 10, D loss: 0.170553, acc:  81%, G loss: 1.544471\n",
      "Ep: 748, steps: 11, D loss: 0.258753, acc:  50%, G loss: 1.746856\n",
      "Ep: 748, steps: 12, D loss: 0.293758, acc:  36%, G loss: 1.346994\n",
      "Ep: 748, steps: 13, D loss: 0.277764, acc:  38%, G loss: 1.428203\n",
      "Ep: 748, steps: 14, D loss: 0.264824, acc:  48%, G loss: 1.530302\n",
      "Ep: 748, steps: 15, D loss: 0.266201, acc:  46%, G loss: 1.594824\n",
      "Ep: 748, steps: 16, D loss: 0.252535, acc:  54%, G loss: 1.579561\n",
      "Ep: 748, steps: 17, D loss: 0.224189, acc:  63%, G loss: 1.521107\n",
      "Ep: 748, steps: 18, D loss: 0.247783, acc:  54%, G loss: 1.541504\n",
      "Ep: 748, steps: 19, D loss: 0.213390, acc:  67%, G loss: 1.571491\n",
      "Ep: 748, steps: 20, D loss: 0.183149, acc:  78%, G loss: 1.721935\n",
      "Ep: 748, steps: 21, D loss: 0.264773, acc:  41%, G loss: 1.569660\n",
      "Ep: 748, steps: 22, D loss: 0.212887, acc:  64%, G loss: 1.667481\n",
      "Ep: 748, steps: 23, D loss: 0.217751, acc:  66%, G loss: 1.811253\n",
      "Ep: 748, steps: 24, D loss: 0.211437, acc:  70%, G loss: 1.546633\n",
      "Ep: 748, steps: 25, D loss: 0.249001, acc:  57%, G loss: 1.589496\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 749, steps: 1, D loss: 0.234013, acc:  59%, G loss: 1.693976\n",
      "Ep: 749, steps: 2, D loss: 0.247078, acc:  54%, G loss: 1.517968\n",
      "Ep: 749, steps: 3, D loss: 0.174628, acc:  81%, G loss: 1.898694\n",
      "Ep: 749, steps: 4, D loss: 0.180191, acc:  80%, G loss: 1.683303\n",
      "Ep: 749, steps: 5, D loss: 0.284592, acc:  45%, G loss: 1.647142\n",
      "Ep: 749, steps: 6, D loss: 0.236407, acc:  57%, G loss: 1.546932\n",
      "Ep: 749, steps: 7, D loss: 0.309437, acc:  34%, G loss: 1.449277\n",
      "Ep: 749, steps: 8, D loss: 0.237434, acc:  61%, G loss: 1.734090\n",
      "Ep: 749, steps: 9, D loss: 0.246765, acc:  57%, G loss: 1.602300\n",
      "Ep: 749, steps: 10, D loss: 0.189768, acc:  74%, G loss: 1.601395\n",
      "Ep: 749, steps: 11, D loss: 0.254531, acc:  51%, G loss: 1.730502\n",
      "Ep: 749, steps: 12, D loss: 0.285396, acc:  38%, G loss: 1.384484\n",
      "Ep: 749, steps: 13, D loss: 0.294838, acc:  30%, G loss: 1.462615\n",
      "Ep: 749, steps: 14, D loss: 0.271442, acc:  45%, G loss: 1.560005\n",
      "Ep: 749, steps: 15, D loss: 0.263053, acc:  47%, G loss: 1.621289\n",
      "Ep: 749, steps: 16, D loss: 0.249839, acc:  53%, G loss: 1.606711\n",
      "Ep: 749, steps: 17, D loss: 0.210681, acc:  70%, G loss: 1.493523\n",
      "Ep: 749, steps: 18, D loss: 0.243569, acc:  57%, G loss: 1.564034\n",
      "Ep: 749, steps: 19, D loss: 0.223706, acc:  66%, G loss: 1.593337\n",
      "Ep: 749, steps: 20, D loss: 0.172643, acc:  81%, G loss: 1.761840\n",
      "Ep: 749, steps: 21, D loss: 0.254399, acc:  46%, G loss: 1.518500\n",
      "Ep: 749, steps: 22, D loss: 0.211190, acc:  65%, G loss: 1.502289\n",
      "Ep: 749, steps: 23, D loss: 0.211013, acc:  68%, G loss: 1.751673\n",
      "Ep: 749, steps: 24, D loss: 0.213434, acc:  69%, G loss: 1.520080\n",
      "Ep: 749, steps: 25, D loss: 0.257323, acc:  52%, G loss: 1.523703\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 750, steps: 1, D loss: 0.230256, acc:  60%, G loss: 1.570547\n",
      "Ep: 750, steps: 2, D loss: 0.257365, acc:  51%, G loss: 1.447660\n",
      "Ep: 750, steps: 3, D loss: 0.175207, acc:  81%, G loss: 1.961538\n",
      "Ep: 750, steps: 4, D loss: 0.184053, acc:  82%, G loss: 1.711424\n",
      "Ep: 750, steps: 5, D loss: 0.273793, acc:  51%, G loss: 1.605133\n",
      "Ep: 750, steps: 6, D loss: 0.239486, acc:  57%, G loss: 1.519777\n",
      "Ep: 750, steps: 7, D loss: 0.314435, acc:  31%, G loss: 1.565267\n",
      "Ep: 750, steps: 8, D loss: 0.234519, acc:  63%, G loss: 1.735893\n",
      "Ep: 750, steps: 9, D loss: 0.242611, acc:  58%, G loss: 1.580502\n",
      "Ep: 750, steps: 10, D loss: 0.172230, acc:  83%, G loss: 1.534950\n",
      "Ep: 750, steps: 11, D loss: 0.280925, acc:  44%, G loss: 1.702523\n",
      "Ep: 750, steps: 12, D loss: 0.293513, acc:  35%, G loss: 1.305210\n",
      "Saved Model\n",
      "Ep: 750, steps: 13, D loss: 0.283462, acc:  36%, G loss: 1.375502\n",
      "Ep: 750, steps: 14, D loss: 0.237634, acc:  58%, G loss: 1.583271\n",
      "Ep: 750, steps: 15, D loss: 0.256973, acc:  53%, G loss: 1.603490\n",
      "Ep: 750, steps: 16, D loss: 0.219976, acc:  66%, G loss: 1.476883\n",
      "Ep: 750, steps: 17, D loss: 0.227021, acc:  61%, G loss: 1.650662\n",
      "Ep: 750, steps: 18, D loss: 0.217653, acc:  66%, G loss: 1.587512\n",
      "Ep: 750, steps: 19, D loss: 0.189953, acc:  77%, G loss: 1.690734\n",
      "Ep: 750, steps: 20, D loss: 0.280738, acc:  34%, G loss: 1.460977\n",
      "Ep: 750, steps: 21, D loss: 0.229565, acc:  61%, G loss: 1.588116\n",
      "Ep: 750, steps: 22, D loss: 0.223354, acc:  64%, G loss: 1.778571\n",
      "Ep: 750, steps: 23, D loss: 0.209694, acc:  69%, G loss: 1.512339\n",
      "Ep: 750, steps: 24, D loss: 0.264268, acc:  53%, G loss: 1.596051\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 751, steps: 1, D loss: 0.219189, acc:  66%, G loss: 1.675160\n",
      "Ep: 751, steps: 2, D loss: 0.233339, acc:  59%, G loss: 1.480884\n",
      "Ep: 751, steps: 3, D loss: 0.163324, acc:  84%, G loss: 1.977513\n",
      "Ep: 751, steps: 4, D loss: 0.179571, acc:  80%, G loss: 1.701927\n",
      "Ep: 751, steps: 5, D loss: 0.294544, acc:  44%, G loss: 1.612424\n",
      "Ep: 751, steps: 6, D loss: 0.238800, acc:  55%, G loss: 1.509873\n",
      "Ep: 751, steps: 7, D loss: 0.296102, acc:  36%, G loss: 1.450415\n",
      "Ep: 751, steps: 8, D loss: 0.228613, acc:  63%, G loss: 1.720837\n",
      "Ep: 751, steps: 9, D loss: 0.239892, acc:  57%, G loss: 1.571466\n",
      "Ep: 751, steps: 10, D loss: 0.177658, acc:  81%, G loss: 1.544958\n",
      "Ep: 751, steps: 11, D loss: 0.271067, acc:  45%, G loss: 1.776714\n",
      "Ep: 751, steps: 12, D loss: 0.284328, acc:  36%, G loss: 1.376234\n",
      "Ep: 751, steps: 13, D loss: 0.290561, acc:  33%, G loss: 1.419487\n",
      "Ep: 751, steps: 14, D loss: 0.272266, acc:  45%, G loss: 1.500120\n",
      "Ep: 751, steps: 15, D loss: 0.267615, acc:  48%, G loss: 1.616259\n",
      "Ep: 751, steps: 16, D loss: 0.242713, acc:  56%, G loss: 1.593746\n",
      "Ep: 751, steps: 17, D loss: 0.225612, acc:  62%, G loss: 1.454594\n",
      "Ep: 751, steps: 18, D loss: 0.244652, acc:  57%, G loss: 1.619118\n",
      "Ep: 751, steps: 19, D loss: 0.219074, acc:  65%, G loss: 1.650396\n",
      "Ep: 751, steps: 20, D loss: 0.186613, acc:  77%, G loss: 1.702579\n",
      "Ep: 751, steps: 21, D loss: 0.257455, acc:  44%, G loss: 1.457898\n",
      "Ep: 751, steps: 22, D loss: 0.206396, acc:  66%, G loss: 1.539033\n",
      "Ep: 751, steps: 23, D loss: 0.214283, acc:  69%, G loss: 1.745305\n",
      "Ep: 751, steps: 24, D loss: 0.213096, acc:  69%, G loss: 1.498680\n",
      "Ep: 751, steps: 25, D loss: 0.251587, acc:  54%, G loss: 1.626519\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 752, steps: 1, D loss: 0.226616, acc:  61%, G loss: 1.696255\n",
      "Ep: 752, steps: 2, D loss: 0.246082, acc:  54%, G loss: 1.393406\n",
      "Ep: 752, steps: 3, D loss: 0.165815, acc:  84%, G loss: 1.884447\n",
      "Ep: 752, steps: 4, D loss: 0.185937, acc:  80%, G loss: 1.603597\n",
      "Ep: 752, steps: 5, D loss: 0.274538, acc:  51%, G loss: 1.607731\n",
      "Ep: 752, steps: 6, D loss: 0.239016, acc:  56%, G loss: 1.506861\n",
      "Ep: 752, steps: 7, D loss: 0.310211, acc:  34%, G loss: 1.496379\n",
      "Ep: 752, steps: 8, D loss: 0.230115, acc:  64%, G loss: 1.753606\n",
      "Ep: 752, steps: 9, D loss: 0.259520, acc:  50%, G loss: 1.576947\n",
      "Ep: 752, steps: 10, D loss: 0.188723, acc:  76%, G loss: 1.595033\n",
      "Ep: 752, steps: 11, D loss: 0.255297, acc:  50%, G loss: 1.714290\n",
      "Ep: 752, steps: 12, D loss: 0.295740, acc:  34%, G loss: 1.338545\n",
      "Ep: 752, steps: 13, D loss: 0.283485, acc:  36%, G loss: 1.416099\n",
      "Ep: 752, steps: 14, D loss: 0.274439, acc:  45%, G loss: 1.574217\n",
      "Ep: 752, steps: 15, D loss: 0.246428, acc:  53%, G loss: 1.583291\n",
      "Ep: 752, steps: 16, D loss: 0.247323, acc:  54%, G loss: 1.589037\n",
      "Ep: 752, steps: 17, D loss: 0.219457, acc:  68%, G loss: 1.472670\n",
      "Ep: 752, steps: 18, D loss: 0.237904, acc:  58%, G loss: 1.628644\n",
      "Ep: 752, steps: 19, D loss: 0.215885, acc:  66%, G loss: 1.626806\n",
      "Ep: 752, steps: 20, D loss: 0.189192, acc:  78%, G loss: 1.676452\n",
      "Ep: 752, steps: 21, D loss: 0.261255, acc:  42%, G loss: 1.439811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 752, steps: 22, D loss: 0.224764, acc:  64%, G loss: 1.601325\n",
      "Ep: 752, steps: 23, D loss: 0.223520, acc:  64%, G loss: 1.839134\n",
      "Ep: 752, steps: 24, D loss: 0.208944, acc:  71%, G loss: 1.495028\n",
      "Ep: 752, steps: 25, D loss: 0.256245, acc:  53%, G loss: 1.618046\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 753, steps: 1, D loss: 0.220891, acc:  66%, G loss: 1.646368\n",
      "Ep: 753, steps: 2, D loss: 0.277426, acc:  45%, G loss: 1.420633\n",
      "Ep: 753, steps: 3, D loss: 0.165261, acc:  83%, G loss: 1.875549\n",
      "Ep: 753, steps: 4, D loss: 0.189742, acc:  78%, G loss: 1.668779\n",
      "Ep: 753, steps: 5, D loss: 0.268045, acc:  50%, G loss: 1.694713\n",
      "Ep: 753, steps: 6, D loss: 0.246871, acc:  54%, G loss: 1.535015\n",
      "Ep: 753, steps: 7, D loss: 0.296529, acc:  38%, G loss: 1.416101\n",
      "Ep: 753, steps: 8, D loss: 0.232241, acc:  63%, G loss: 1.742525\n",
      "Ep: 753, steps: 9, D loss: 0.230979, acc:  62%, G loss: 1.570887\n",
      "Ep: 753, steps: 10, D loss: 0.187227, acc:  77%, G loss: 1.528827\n",
      "Saved Model\n",
      "Ep: 753, steps: 11, D loss: 0.270495, acc:  45%, G loss: 1.703472\n",
      "Ep: 753, steps: 12, D loss: 0.293259, acc:  33%, G loss: 1.325705\n",
      "Ep: 753, steps: 13, D loss: 0.279806, acc:  41%, G loss: 1.477634\n",
      "Ep: 753, steps: 14, D loss: 0.256464, acc:  50%, G loss: 1.603001\n",
      "Ep: 753, steps: 15, D loss: 0.244442, acc:  56%, G loss: 1.567028\n",
      "Ep: 753, steps: 16, D loss: 0.224463, acc:  66%, G loss: 1.488683\n",
      "Ep: 753, steps: 17, D loss: 0.236419, acc:  60%, G loss: 1.596422\n",
      "Ep: 753, steps: 18, D loss: 0.208032, acc:  67%, G loss: 1.592702\n",
      "Ep: 753, steps: 19, D loss: 0.176177, acc:  80%, G loss: 1.729775\n",
      "Ep: 753, steps: 20, D loss: 0.263498, acc:  40%, G loss: 1.413177\n",
      "Ep: 753, steps: 21, D loss: 0.217086, acc:  64%, G loss: 1.757767\n",
      "Ep: 753, steps: 22, D loss: 0.224265, acc:  65%, G loss: 1.828735\n",
      "Ep: 753, steps: 23, D loss: 0.210855, acc:  68%, G loss: 1.557792\n",
      "Ep: 753, steps: 24, D loss: 0.268491, acc:  52%, G loss: 1.555522\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 754, steps: 1, D loss: 0.218967, acc:  65%, G loss: 1.629031\n",
      "Ep: 754, steps: 2, D loss: 0.245101, acc:  56%, G loss: 1.437816\n",
      "Ep: 754, steps: 3, D loss: 0.166848, acc:  84%, G loss: 1.934504\n",
      "Ep: 754, steps: 4, D loss: 0.178018, acc:  82%, G loss: 1.644263\n",
      "Ep: 754, steps: 5, D loss: 0.263039, acc:  51%, G loss: 1.614044\n",
      "Ep: 754, steps: 6, D loss: 0.252657, acc:  55%, G loss: 1.513874\n",
      "Ep: 754, steps: 7, D loss: 0.320394, acc:  30%, G loss: 1.457933\n",
      "Ep: 754, steps: 8, D loss: 0.216003, acc:  67%, G loss: 1.705830\n",
      "Ep: 754, steps: 9, D loss: 0.240688, acc:  59%, G loss: 1.594376\n",
      "Ep: 754, steps: 10, D loss: 0.186754, acc:  78%, G loss: 1.577786\n",
      "Ep: 754, steps: 11, D loss: 0.262870, acc:  50%, G loss: 1.709210\n",
      "Ep: 754, steps: 12, D loss: 0.296897, acc:  33%, G loss: 1.387637\n",
      "Ep: 754, steps: 13, D loss: 0.287763, acc:  36%, G loss: 1.426300\n",
      "Ep: 754, steps: 14, D loss: 0.257836, acc:  52%, G loss: 1.590458\n",
      "Ep: 754, steps: 15, D loss: 0.270982, acc:  47%, G loss: 1.591308\n",
      "Ep: 754, steps: 16, D loss: 0.252430, acc:  54%, G loss: 1.604022\n",
      "Ep: 754, steps: 17, D loss: 0.228838, acc:  60%, G loss: 1.530736\n",
      "Ep: 754, steps: 18, D loss: 0.236171, acc:  58%, G loss: 1.626052\n",
      "Ep: 754, steps: 19, D loss: 0.212796, acc:  68%, G loss: 1.569791\n",
      "Ep: 754, steps: 20, D loss: 0.170611, acc:  81%, G loss: 1.635250\n",
      "Ep: 754, steps: 21, D loss: 0.261182, acc:  45%, G loss: 1.536292\n",
      "Ep: 754, steps: 22, D loss: 0.176499, acc:  79%, G loss: 1.513973\n",
      "Ep: 754, steps: 23, D loss: 0.221950, acc:  66%, G loss: 1.781270\n",
      "Ep: 754, steps: 24, D loss: 0.214201, acc:  67%, G loss: 1.495799\n",
      "Ep: 754, steps: 25, D loss: 0.245654, acc:  55%, G loss: 1.710643\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 755, steps: 1, D loss: 0.219866, acc:  65%, G loss: 1.613681\n",
      "Ep: 755, steps: 2, D loss: 0.247566, acc:  54%, G loss: 1.444761\n",
      "Ep: 755, steps: 3, D loss: 0.152209, acc:  88%, G loss: 1.952168\n",
      "Ep: 755, steps: 4, D loss: 0.164893, acc:  86%, G loss: 1.651903\n",
      "Ep: 755, steps: 5, D loss: 0.301342, acc:  44%, G loss: 1.569489\n",
      "Ep: 755, steps: 6, D loss: 0.246343, acc:  53%, G loss: 1.516056\n",
      "Ep: 755, steps: 7, D loss: 0.301646, acc:  35%, G loss: 1.686321\n",
      "Ep: 755, steps: 8, D loss: 0.223327, acc:  66%, G loss: 1.765621\n",
      "Ep: 755, steps: 9, D loss: 0.302418, acc:  44%, G loss: 1.601417\n",
      "Ep: 755, steps: 10, D loss: 0.219949, acc:  63%, G loss: 1.585072\n",
      "Ep: 755, steps: 11, D loss: 0.254456, acc:  49%, G loss: 1.754371\n",
      "Ep: 755, steps: 12, D loss: 0.301668, acc:  31%, G loss: 1.387763\n",
      "Ep: 755, steps: 13, D loss: 0.283798, acc:  36%, G loss: 1.411130\n",
      "Ep: 755, steps: 14, D loss: 0.257469, acc:  50%, G loss: 1.478641\n",
      "Ep: 755, steps: 15, D loss: 0.265414, acc:  46%, G loss: 1.605085\n",
      "Ep: 755, steps: 16, D loss: 0.240095, acc:  58%, G loss: 1.562490\n",
      "Ep: 755, steps: 17, D loss: 0.221768, acc:  66%, G loss: 1.509386\n",
      "Ep: 755, steps: 18, D loss: 0.226711, acc:  61%, G loss: 1.631155\n",
      "Ep: 755, steps: 19, D loss: 0.221174, acc:  64%, G loss: 1.621993\n",
      "Ep: 755, steps: 20, D loss: 0.210682, acc:  72%, G loss: 1.671317\n",
      "Ep: 755, steps: 21, D loss: 0.258124, acc:  44%, G loss: 1.412945\n",
      "Ep: 755, steps: 22, D loss: 0.216036, acc:  63%, G loss: 1.520200\n",
      "Ep: 755, steps: 23, D loss: 0.232641, acc:  60%, G loss: 1.744961\n",
      "Ep: 755, steps: 24, D loss: 0.210721, acc:  69%, G loss: 1.497141\n",
      "Ep: 755, steps: 25, D loss: 0.257190, acc:  54%, G loss: 1.475241\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 756, steps: 1, D loss: 0.230614, acc:  60%, G loss: 1.642555\n",
      "Ep: 756, steps: 2, D loss: 0.249851, acc:  53%, G loss: 1.419152\n",
      "Ep: 756, steps: 3, D loss: 0.178017, acc:  82%, G loss: 1.895466\n",
      "Ep: 756, steps: 4, D loss: 0.173943, acc:  83%, G loss: 1.632876\n",
      "Ep: 756, steps: 5, D loss: 0.278803, acc:  51%, G loss: 1.596029\n",
      "Ep: 756, steps: 6, D loss: 0.253062, acc:  53%, G loss: 1.488679\n",
      "Ep: 756, steps: 7, D loss: 0.303932, acc:  33%, G loss: 1.436692\n",
      "Ep: 756, steps: 8, D loss: 0.226150, acc:  61%, G loss: 1.694013\n",
      "Saved Model\n",
      "Ep: 756, steps: 9, D loss: 0.242991, acc:  57%, G loss: 1.582980\n",
      "Ep: 756, steps: 10, D loss: 0.261298, acc:  49%, G loss: 1.677855\n",
      "Ep: 756, steps: 11, D loss: 0.284622, acc:  38%, G loss: 1.323854\n",
      "Ep: 756, steps: 12, D loss: 0.282859, acc:  36%, G loss: 1.403844\n",
      "Ep: 756, steps: 13, D loss: 0.259886, acc:  50%, G loss: 1.535275\n",
      "Ep: 756, steps: 14, D loss: 0.266469, acc:  45%, G loss: 1.544467\n",
      "Ep: 756, steps: 15, D loss: 0.227115, acc:  64%, G loss: 1.578905\n",
      "Ep: 756, steps: 16, D loss: 0.221102, acc:  66%, G loss: 1.518542\n",
      "Ep: 756, steps: 17, D loss: 0.232710, acc:  58%, G loss: 1.623119\n",
      "Ep: 756, steps: 18, D loss: 0.225129, acc:  63%, G loss: 1.579232\n",
      "Ep: 756, steps: 19, D loss: 0.188786, acc:  76%, G loss: 1.662246\n",
      "Ep: 756, steps: 20, D loss: 0.239018, acc:  52%, G loss: 1.420388\n",
      "Ep: 756, steps: 21, D loss: 0.214772, acc:  63%, G loss: 1.523256\n",
      "Ep: 756, steps: 22, D loss: 0.223574, acc:  65%, G loss: 1.733473\n",
      "Ep: 756, steps: 23, D loss: 0.213132, acc:  67%, G loss: 1.521479\n",
      "Ep: 756, steps: 24, D loss: 0.262426, acc:  52%, G loss: 1.681799\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 757, steps: 1, D loss: 0.225641, acc:  63%, G loss: 1.695458\n",
      "Ep: 757, steps: 2, D loss: 0.241377, acc:  57%, G loss: 1.426643\n",
      "Ep: 757, steps: 3, D loss: 0.162823, acc:  83%, G loss: 1.849139\n",
      "Ep: 757, steps: 4, D loss: 0.176151, acc:  81%, G loss: 1.596141\n",
      "Ep: 757, steps: 5, D loss: 0.284064, acc:  49%, G loss: 1.677245\n",
      "Ep: 757, steps: 6, D loss: 0.252469, acc:  53%, G loss: 1.513704\n",
      "Ep: 757, steps: 7, D loss: 0.290506, acc:  37%, G loss: 1.458290\n",
      "Ep: 757, steps: 8, D loss: 0.232543, acc:  61%, G loss: 1.790252\n",
      "Ep: 757, steps: 9, D loss: 0.242137, acc:  58%, G loss: 1.572943\n",
      "Ep: 757, steps: 10, D loss: 0.190411, acc:  73%, G loss: 1.559388\n",
      "Ep: 757, steps: 11, D loss: 0.273984, acc:  44%, G loss: 1.735187\n",
      "Ep: 757, steps: 12, D loss: 0.293881, acc:  35%, G loss: 1.356784\n",
      "Ep: 757, steps: 13, D loss: 0.282097, acc:  36%, G loss: 1.370786\n",
      "Ep: 757, steps: 14, D loss: 0.268681, acc:  46%, G loss: 1.467183\n",
      "Ep: 757, steps: 15, D loss: 0.268855, acc:  46%, G loss: 1.540805\n",
      "Ep: 757, steps: 16, D loss: 0.244693, acc:  57%, G loss: 1.628544\n",
      "Ep: 757, steps: 17, D loss: 0.230760, acc:  62%, G loss: 1.607692\n",
      "Ep: 757, steps: 18, D loss: 0.237602, acc:  59%, G loss: 1.562765\n",
      "Ep: 757, steps: 19, D loss: 0.226354, acc:  64%, G loss: 1.605211\n",
      "Ep: 757, steps: 20, D loss: 0.176900, acc:  81%, G loss: 1.721223\n",
      "Ep: 757, steps: 21, D loss: 0.254821, acc:  45%, G loss: 1.609298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 757, steps: 22, D loss: 0.217062, acc:  64%, G loss: 1.749855\n",
      "Ep: 757, steps: 23, D loss: 0.207862, acc:  71%, G loss: 1.737402\n",
      "Ep: 757, steps: 24, D loss: 0.211844, acc:  68%, G loss: 1.521746\n",
      "Ep: 757, steps: 25, D loss: 0.231298, acc:  62%, G loss: 1.572261\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 758, steps: 1, D loss: 0.237169, acc:  58%, G loss: 1.682969\n",
      "Ep: 758, steps: 2, D loss: 0.246793, acc:  54%, G loss: 1.397128\n",
      "Ep: 758, steps: 3, D loss: 0.167612, acc:  85%, G loss: 1.837526\n",
      "Ep: 758, steps: 4, D loss: 0.188929, acc:  80%, G loss: 1.584211\n",
      "Ep: 758, steps: 5, D loss: 0.284657, acc:  48%, G loss: 1.595966\n",
      "Ep: 758, steps: 6, D loss: 0.255342, acc:  52%, G loss: 1.562159\n",
      "Ep: 758, steps: 7, D loss: 0.304319, acc:  33%, G loss: 1.508838\n",
      "Ep: 758, steps: 8, D loss: 0.220465, acc:  67%, G loss: 1.684574\n",
      "Ep: 758, steps: 9, D loss: 0.250156, acc:  55%, G loss: 1.566463\n",
      "Ep: 758, steps: 10, D loss: 0.172150, acc:  82%, G loss: 1.534636\n",
      "Ep: 758, steps: 11, D loss: 0.258326, acc:  49%, G loss: 1.756019\n",
      "Ep: 758, steps: 12, D loss: 0.295559, acc:  32%, G loss: 1.347502\n",
      "Ep: 758, steps: 13, D loss: 0.292948, acc:  32%, G loss: 1.426597\n",
      "Ep: 758, steps: 14, D loss: 0.261118, acc:  49%, G loss: 1.508137\n",
      "Ep: 758, steps: 15, D loss: 0.259733, acc:  49%, G loss: 1.582554\n",
      "Ep: 758, steps: 16, D loss: 0.249482, acc:  54%, G loss: 1.576367\n",
      "Ep: 758, steps: 17, D loss: 0.214441, acc:  71%, G loss: 1.499430\n",
      "Ep: 758, steps: 18, D loss: 0.234202, acc:  58%, G loss: 1.636146\n",
      "Ep: 758, steps: 19, D loss: 0.211585, acc:  68%, G loss: 1.621709\n",
      "Ep: 758, steps: 20, D loss: 0.187965, acc:  75%, G loss: 1.738972\n",
      "Ep: 758, steps: 21, D loss: 0.268064, acc:  40%, G loss: 1.487519\n",
      "Ep: 758, steps: 22, D loss: 0.214929, acc:  65%, G loss: 1.593287\n",
      "Ep: 758, steps: 23, D loss: 0.218712, acc:  66%, G loss: 1.769299\n",
      "Ep: 758, steps: 24, D loss: 0.217581, acc:  66%, G loss: 1.550915\n",
      "Ep: 758, steps: 25, D loss: 0.250262, acc:  55%, G loss: 1.556170\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 759, steps: 1, D loss: 0.233946, acc:  61%, G loss: 1.597997\n",
      "Ep: 759, steps: 2, D loss: 0.241048, acc:  57%, G loss: 1.433574\n",
      "Ep: 759, steps: 3, D loss: 0.179338, acc:  80%, G loss: 1.854551\n",
      "Ep: 759, steps: 4, D loss: 0.186905, acc:  80%, G loss: 1.579157\n",
      "Ep: 759, steps: 5, D loss: 0.264606, acc:  51%, G loss: 1.622557\n",
      "Ep: 759, steps: 6, D loss: 0.231921, acc:  57%, G loss: 1.518858\n",
      "Saved Model\n",
      "Ep: 759, steps: 7, D loss: 0.283313, acc:  41%, G loss: 1.781480\n",
      "Ep: 759, steps: 8, D loss: 0.231805, acc:  62%, G loss: 1.689001\n",
      "Ep: 759, steps: 9, D loss: 0.182590, acc:  79%, G loss: 1.556037\n",
      "Ep: 759, steps: 10, D loss: 0.246881, acc:  53%, G loss: 1.708248\n",
      "Ep: 759, steps: 11, D loss: 0.288334, acc:  37%, G loss: 1.392837\n",
      "Ep: 759, steps: 12, D loss: 0.281687, acc:  36%, G loss: 1.414904\n",
      "Ep: 759, steps: 13, D loss: 0.271789, acc:  46%, G loss: 1.548336\n",
      "Ep: 759, steps: 14, D loss: 0.266419, acc:  46%, G loss: 1.586060\n",
      "Ep: 759, steps: 15, D loss: 0.241975, acc:  59%, G loss: 1.564436\n",
      "Ep: 759, steps: 16, D loss: 0.220784, acc:  65%, G loss: 1.530636\n",
      "Ep: 759, steps: 17, D loss: 0.239003, acc:  58%, G loss: 1.647993\n",
      "Ep: 759, steps: 18, D loss: 0.216508, acc:  65%, G loss: 1.608908\n",
      "Ep: 759, steps: 19, D loss: 0.176405, acc:  81%, G loss: 1.738907\n",
      "Ep: 759, steps: 20, D loss: 0.259824, acc:  43%, G loss: 1.502755\n",
      "Ep: 759, steps: 21, D loss: 0.204636, acc:  67%, G loss: 1.592387\n",
      "Ep: 759, steps: 22, D loss: 0.214035, acc:  68%, G loss: 1.830444\n",
      "Ep: 759, steps: 23, D loss: 0.215033, acc:  67%, G loss: 1.493698\n",
      "Ep: 759, steps: 24, D loss: 0.238677, acc:  59%, G loss: 1.550308\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 760, steps: 1, D loss: 0.212975, acc:  66%, G loss: 1.650042\n",
      "Ep: 760, steps: 2, D loss: 0.250475, acc:  54%, G loss: 1.435995\n",
      "Ep: 760, steps: 3, D loss: 0.175948, acc:  81%, G loss: 1.898558\n",
      "Ep: 760, steps: 4, D loss: 0.180877, acc:  81%, G loss: 1.605937\n",
      "Ep: 760, steps: 5, D loss: 0.250725, acc:  58%, G loss: 1.550121\n",
      "Ep: 760, steps: 6, D loss: 0.244401, acc:  55%, G loss: 1.576804\n",
      "Ep: 760, steps: 7, D loss: 0.301985, acc:  34%, G loss: 1.495295\n",
      "Ep: 760, steps: 8, D loss: 0.246541, acc:  56%, G loss: 1.759907\n",
      "Ep: 760, steps: 9, D loss: 0.247577, acc:  55%, G loss: 1.620336\n",
      "Ep: 760, steps: 10, D loss: 0.181219, acc:  80%, G loss: 1.512747\n",
      "Ep: 760, steps: 11, D loss: 0.267143, acc:  48%, G loss: 1.710257\n",
      "Ep: 760, steps: 12, D loss: 0.304561, acc:  30%, G loss: 1.344808\n",
      "Ep: 760, steps: 13, D loss: 0.288513, acc:  35%, G loss: 1.454245\n",
      "Ep: 760, steps: 14, D loss: 0.273738, acc:  45%, G loss: 1.524257\n",
      "Ep: 760, steps: 15, D loss: 0.270801, acc:  43%, G loss: 1.604496\n",
      "Ep: 760, steps: 16, D loss: 0.240942, acc:  59%, G loss: 1.555819\n",
      "Ep: 760, steps: 17, D loss: 0.231898, acc:  61%, G loss: 1.471662\n",
      "Ep: 760, steps: 18, D loss: 0.232690, acc:  58%, G loss: 1.642688\n",
      "Ep: 760, steps: 19, D loss: 0.210900, acc:  68%, G loss: 1.585316\n",
      "Ep: 760, steps: 20, D loss: 0.183498, acc:  78%, G loss: 1.703090\n",
      "Ep: 760, steps: 21, D loss: 0.268594, acc:  39%, G loss: 1.567650\n",
      "Ep: 760, steps: 22, D loss: 0.224968, acc:  63%, G loss: 1.698963\n",
      "Ep: 760, steps: 23, D loss: 0.217576, acc:  67%, G loss: 1.771858\n",
      "Ep: 760, steps: 24, D loss: 0.208487, acc:  68%, G loss: 1.514296\n",
      "Ep: 760, steps: 25, D loss: 0.246329, acc:  58%, G loss: 1.520492\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 761, steps: 1, D loss: 0.239487, acc:  58%, G loss: 1.626221\n",
      "Ep: 761, steps: 2, D loss: 0.235104, acc:  58%, G loss: 1.425814\n",
      "Ep: 761, steps: 3, D loss: 0.172494, acc:  82%, G loss: 1.911336\n",
      "Ep: 761, steps: 4, D loss: 0.176150, acc:  84%, G loss: 1.619399\n",
      "Ep: 761, steps: 5, D loss: 0.280327, acc:  48%, G loss: 1.579153\n",
      "Ep: 761, steps: 6, D loss: 0.244235, acc:  54%, G loss: 1.543155\n",
      "Ep: 761, steps: 7, D loss: 0.296934, acc:  36%, G loss: 1.538266\n",
      "Ep: 761, steps: 8, D loss: 0.239734, acc:  59%, G loss: 1.719565\n",
      "Ep: 761, steps: 9, D loss: 0.234503, acc:  61%, G loss: 1.582949\n",
      "Ep: 761, steps: 10, D loss: 0.179450, acc:  79%, G loss: 1.530154\n",
      "Ep: 761, steps: 11, D loss: 0.259810, acc:  49%, G loss: 1.661074\n",
      "Ep: 761, steps: 12, D loss: 0.297881, acc:  32%, G loss: 1.386319\n",
      "Ep: 761, steps: 13, D loss: 0.276826, acc:  38%, G loss: 1.401993\n",
      "Ep: 761, steps: 14, D loss: 0.259524, acc:  50%, G loss: 1.482513\n",
      "Ep: 761, steps: 15, D loss: 0.279453, acc:  41%, G loss: 1.599743\n",
      "Ep: 761, steps: 16, D loss: 0.246140, acc:  55%, G loss: 1.566002\n",
      "Ep: 761, steps: 17, D loss: 0.222974, acc:  64%, G loss: 1.597456\n",
      "Ep: 761, steps: 18, D loss: 0.238749, acc:  58%, G loss: 1.611631\n",
      "Ep: 761, steps: 19, D loss: 0.222784, acc:  64%, G loss: 1.555511\n",
      "Ep: 761, steps: 20, D loss: 0.174613, acc:  82%, G loss: 1.723735\n",
      "Ep: 761, steps: 21, D loss: 0.257784, acc:  45%, G loss: 1.455914\n",
      "Ep: 761, steps: 22, D loss: 0.199779, acc:  67%, G loss: 1.532102\n",
      "Ep: 761, steps: 23, D loss: 0.227983, acc:  64%, G loss: 1.774439\n",
      "Ep: 761, steps: 24, D loss: 0.210634, acc:  67%, G loss: 1.523072\n",
      "Ep: 761, steps: 25, D loss: 0.267230, acc:  51%, G loss: 1.594076\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 762, steps: 1, D loss: 0.241602, acc:  58%, G loss: 1.721212\n",
      "Ep: 762, steps: 2, D loss: 0.262103, acc:  49%, G loss: 1.407285\n",
      "Ep: 762, steps: 3, D loss: 0.161752, acc:  84%, G loss: 1.896616\n",
      "Ep: 762, steps: 4, D loss: 0.181417, acc:  83%, G loss: 1.611669\n",
      "Saved Model\n",
      "Ep: 762, steps: 5, D loss: 0.290857, acc:  45%, G loss: 1.588903\n",
      "Ep: 762, steps: 6, D loss: 0.301640, acc:  30%, G loss: 1.572581\n",
      "Ep: 762, steps: 7, D loss: 0.221549, acc:  66%, G loss: 1.829745\n",
      "Ep: 762, steps: 8, D loss: 0.296644, acc:  46%, G loss: 1.565911\n",
      "Ep: 762, steps: 9, D loss: 0.183430, acc:  79%, G loss: 1.536340\n",
      "Ep: 762, steps: 10, D loss: 0.267787, acc:  45%, G loss: 1.656492\n",
      "Ep: 762, steps: 11, D loss: 0.286689, acc:  36%, G loss: 1.336214\n",
      "Ep: 762, steps: 12, D loss: 0.283860, acc:  36%, G loss: 1.390812\n",
      "Ep: 762, steps: 13, D loss: 0.275237, acc:  42%, G loss: 1.487506\n",
      "Ep: 762, steps: 14, D loss: 0.263934, acc:  48%, G loss: 1.568409\n",
      "Ep: 762, steps: 15, D loss: 0.247001, acc:  55%, G loss: 1.574318\n",
      "Ep: 762, steps: 16, D loss: 0.220859, acc:  68%, G loss: 1.458477\n",
      "Ep: 762, steps: 17, D loss: 0.243806, acc:  56%, G loss: 1.613438\n",
      "Ep: 762, steps: 18, D loss: 0.221833, acc:  65%, G loss: 1.518733\n",
      "Ep: 762, steps: 19, D loss: 0.193338, acc:  78%, G loss: 1.642188\n",
      "Ep: 762, steps: 20, D loss: 0.261384, acc:  41%, G loss: 1.429011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 762, steps: 21, D loss: 0.231848, acc:  58%, G loss: 1.584580\n",
      "Ep: 762, steps: 22, D loss: 0.220184, acc:  66%, G loss: 1.790736\n",
      "Ep: 762, steps: 23, D loss: 0.202174, acc:  73%, G loss: 1.483576\n",
      "Ep: 762, steps: 24, D loss: 0.242669, acc:  59%, G loss: 1.444999\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 763, steps: 1, D loss: 0.241017, acc:  55%, G loss: 1.651226\n",
      "Ep: 763, steps: 2, D loss: 0.254719, acc:  51%, G loss: 1.430663\n",
      "Ep: 763, steps: 3, D loss: 0.157815, acc:  86%, G loss: 1.913512\n",
      "Ep: 763, steps: 4, D loss: 0.170282, acc:  84%, G loss: 1.614084\n",
      "Ep: 763, steps: 5, D loss: 0.294574, acc:  42%, G loss: 1.562565\n",
      "Ep: 763, steps: 6, D loss: 0.256320, acc:  54%, G loss: 1.479691\n",
      "Ep: 763, steps: 7, D loss: 0.309836, acc:  32%, G loss: 1.432698\n",
      "Ep: 763, steps: 8, D loss: 0.241781, acc:  57%, G loss: 1.638092\n",
      "Ep: 763, steps: 9, D loss: 0.236899, acc:  59%, G loss: 1.586744\n",
      "Ep: 763, steps: 10, D loss: 0.181161, acc:  77%, G loss: 1.613807\n",
      "Ep: 763, steps: 11, D loss: 0.255738, acc:  50%, G loss: 1.690155\n",
      "Ep: 763, steps: 12, D loss: 0.295497, acc:  33%, G loss: 1.409722\n",
      "Ep: 763, steps: 13, D loss: 0.290253, acc:  35%, G loss: 1.404050\n",
      "Ep: 763, steps: 14, D loss: 0.261965, acc:  49%, G loss: 1.496962\n",
      "Ep: 763, steps: 15, D loss: 0.265070, acc:  45%, G loss: 1.588418\n",
      "Ep: 763, steps: 16, D loss: 0.241201, acc:  57%, G loss: 1.571798\n",
      "Ep: 763, steps: 17, D loss: 0.235198, acc:  61%, G loss: 1.455504\n",
      "Ep: 763, steps: 18, D loss: 0.234548, acc:  58%, G loss: 1.641954\n",
      "Ep: 763, steps: 19, D loss: 0.205778, acc:  70%, G loss: 1.582147\n",
      "Ep: 763, steps: 20, D loss: 0.182743, acc:  79%, G loss: 1.700562\n",
      "Ep: 763, steps: 21, D loss: 0.270249, acc:  38%, G loss: 1.484238\n",
      "Ep: 763, steps: 22, D loss: 0.200998, acc:  67%, G loss: 1.579201\n",
      "Ep: 763, steps: 23, D loss: 0.211132, acc:  68%, G loss: 1.773467\n",
      "Ep: 763, steps: 24, D loss: 0.222958, acc:  64%, G loss: 1.482136\n",
      "Ep: 763, steps: 25, D loss: 0.273306, acc:  48%, G loss: 1.573766\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 764, steps: 1, D loss: 0.225935, acc:  62%, G loss: 1.660563\n",
      "Ep: 764, steps: 2, D loss: 0.245867, acc:  54%, G loss: 1.488324\n",
      "Ep: 764, steps: 3, D loss: 0.179294, acc:  81%, G loss: 1.914050\n",
      "Ep: 764, steps: 4, D loss: 0.182527, acc:  81%, G loss: 1.604399\n",
      "Ep: 764, steps: 5, D loss: 0.280620, acc:  49%, G loss: 1.630798\n",
      "Ep: 764, steps: 6, D loss: 0.247939, acc:  53%, G loss: 1.623368\n",
      "Ep: 764, steps: 7, D loss: 0.315102, acc:  30%, G loss: 1.458131\n",
      "Ep: 764, steps: 8, D loss: 0.221063, acc:  67%, G loss: 1.793286\n",
      "Ep: 764, steps: 9, D loss: 0.236400, acc:  60%, G loss: 1.588861\n",
      "Ep: 764, steps: 10, D loss: 0.187603, acc:  75%, G loss: 1.552057\n",
      "Ep: 764, steps: 11, D loss: 0.263624, acc:  48%, G loss: 1.718907\n",
      "Ep: 764, steps: 12, D loss: 0.293137, acc:  33%, G loss: 1.353654\n",
      "Ep: 764, steps: 13, D loss: 0.295349, acc:  30%, G loss: 1.382816\n",
      "Ep: 764, steps: 14, D loss: 0.261838, acc:  48%, G loss: 1.511806\n",
      "Ep: 764, steps: 15, D loss: 0.257949, acc:  49%, G loss: 1.578585\n",
      "Ep: 764, steps: 16, D loss: 0.238437, acc:  60%, G loss: 1.593991\n",
      "Ep: 764, steps: 17, D loss: 0.227734, acc:  63%, G loss: 1.487719\n",
      "Ep: 764, steps: 18, D loss: 0.235660, acc:  59%, G loss: 1.589904\n",
      "Ep: 764, steps: 19, D loss: 0.208583, acc:  68%, G loss: 1.566930\n",
      "Ep: 764, steps: 20, D loss: 0.169445, acc:  83%, G loss: 1.593548\n",
      "Ep: 764, steps: 21, D loss: 0.265175, acc:  40%, G loss: 1.550355\n",
      "Ep: 764, steps: 22, D loss: 0.204988, acc:  68%, G loss: 1.674768\n",
      "Ep: 764, steps: 23, D loss: 0.218947, acc:  67%, G loss: 1.788724\n",
      "Ep: 764, steps: 24, D loss: 0.211386, acc:  69%, G loss: 1.610568\n",
      "Ep: 764, steps: 25, D loss: 0.240350, acc:  60%, G loss: 1.656551\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 765, steps: 1, D loss: 0.225103, acc:  62%, G loss: 1.651636\n",
      "Ep: 765, steps: 2, D loss: 0.244559, acc:  55%, G loss: 1.451878\n",
      "Saved Model\n",
      "Ep: 765, steps: 3, D loss: 0.170565, acc:  81%, G loss: 1.900010\n",
      "Ep: 765, steps: 4, D loss: 0.247084, acc:  59%, G loss: 1.673427\n",
      "Ep: 765, steps: 5, D loss: 0.243127, acc:  57%, G loss: 1.610018\n",
      "Ep: 765, steps: 6, D loss: 0.274258, acc:  41%, G loss: 1.519032\n",
      "Ep: 765, steps: 7, D loss: 0.223055, acc:  68%, G loss: 1.870579\n",
      "Ep: 765, steps: 8, D loss: 0.252983, acc:  53%, G loss: 1.563947\n",
      "Ep: 765, steps: 9, D loss: 0.185899, acc:  77%, G loss: 1.563362\n",
      "Ep: 765, steps: 10, D loss: 0.263013, acc:  49%, G loss: 1.742120\n",
      "Ep: 765, steps: 11, D loss: 0.301607, acc:  31%, G loss: 1.396760\n",
      "Ep: 765, steps: 12, D loss: 0.294321, acc:  30%, G loss: 1.460570\n",
      "Ep: 765, steps: 13, D loss: 0.262792, acc:  46%, G loss: 1.538579\n",
      "Ep: 765, steps: 14, D loss: 0.261677, acc:  46%, G loss: 1.568033\n",
      "Ep: 765, steps: 15, D loss: 0.254418, acc:  51%, G loss: 1.597542\n",
      "Ep: 765, steps: 16, D loss: 0.218201, acc:  67%, G loss: 1.501589\n",
      "Ep: 765, steps: 17, D loss: 0.237848, acc:  57%, G loss: 1.575212\n",
      "Ep: 765, steps: 18, D loss: 0.225064, acc:  63%, G loss: 1.591364\n",
      "Ep: 765, steps: 19, D loss: 0.185585, acc:  78%, G loss: 1.704682\n",
      "Ep: 765, steps: 20, D loss: 0.255633, acc:  44%, G loss: 1.490959\n",
      "Ep: 765, steps: 21, D loss: 0.220395, acc:  62%, G loss: 1.624642\n",
      "Ep: 765, steps: 22, D loss: 0.220814, acc:  65%, G loss: 1.769465\n",
      "Ep: 765, steps: 23, D loss: 0.211065, acc:  70%, G loss: 1.524902\n",
      "Ep: 765, steps: 24, D loss: 0.259749, acc:  55%, G loss: 1.490708\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 766, steps: 1, D loss: 0.217381, acc:  66%, G loss: 1.665107\n",
      "Ep: 766, steps: 2, D loss: 0.241855, acc:  57%, G loss: 1.473763\n",
      "Ep: 766, steps: 3, D loss: 0.176131, acc:  82%, G loss: 1.881717\n",
      "Ep: 766, steps: 4, D loss: 0.181960, acc:  83%, G loss: 1.645775\n",
      "Ep: 766, steps: 5, D loss: 0.276393, acc:  50%, G loss: 1.639655\n",
      "Ep: 766, steps: 6, D loss: 0.246556, acc:  54%, G loss: 1.516602\n",
      "Ep: 766, steps: 7, D loss: 0.325414, acc:  28%, G loss: 1.438366\n",
      "Ep: 766, steps: 8, D loss: 0.238756, acc:  60%, G loss: 1.735264\n",
      "Ep: 766, steps: 9, D loss: 0.245725, acc:  56%, G loss: 1.573449\n",
      "Ep: 766, steps: 10, D loss: 0.195862, acc:  72%, G loss: 1.600589\n",
      "Ep: 766, steps: 11, D loss: 0.267572, acc:  46%, G loss: 1.709334\n",
      "Ep: 766, steps: 12, D loss: 0.298616, acc:  33%, G loss: 1.414219\n",
      "Ep: 766, steps: 13, D loss: 0.282271, acc:  35%, G loss: 1.441320\n",
      "Ep: 766, steps: 14, D loss: 0.282068, acc:  38%, G loss: 1.523024\n",
      "Ep: 766, steps: 15, D loss: 0.251938, acc:  51%, G loss: 1.565488\n",
      "Ep: 766, steps: 16, D loss: 0.250272, acc:  53%, G loss: 1.578300\n",
      "Ep: 766, steps: 17, D loss: 0.218212, acc:  67%, G loss: 1.514121\n",
      "Ep: 766, steps: 18, D loss: 0.230279, acc:  61%, G loss: 1.615960\n",
      "Ep: 766, steps: 19, D loss: 0.213270, acc:  66%, G loss: 1.572003\n",
      "Ep: 766, steps: 20, D loss: 0.174578, acc:  80%, G loss: 1.679455\n",
      "Ep: 766, steps: 21, D loss: 0.248495, acc:  49%, G loss: 1.487457\n",
      "Ep: 766, steps: 22, D loss: 0.213095, acc:  65%, G loss: 1.659873\n",
      "Ep: 766, steps: 23, D loss: 0.224451, acc:  64%, G loss: 1.782086\n",
      "Ep: 766, steps: 24, D loss: 0.202779, acc:  70%, G loss: 1.554976\n",
      "Ep: 766, steps: 25, D loss: 0.251514, acc:  54%, G loss: 1.601872\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 767, steps: 1, D loss: 0.220547, acc:  65%, G loss: 1.648764\n",
      "Ep: 767, steps: 2, D loss: 0.256420, acc:  52%, G loss: 1.465408\n",
      "Ep: 767, steps: 3, D loss: 0.173055, acc:  82%, G loss: 1.908456\n",
      "Ep: 767, steps: 4, D loss: 0.186265, acc:  79%, G loss: 1.640693\n",
      "Ep: 767, steps: 5, D loss: 0.275160, acc:  49%, G loss: 1.622062\n",
      "Ep: 767, steps: 6, D loss: 0.248949, acc:  54%, G loss: 1.588849\n",
      "Ep: 767, steps: 7, D loss: 0.314858, acc:  29%, G loss: 1.505301\n",
      "Ep: 767, steps: 8, D loss: 0.224826, acc:  65%, G loss: 1.780081\n",
      "Ep: 767, steps: 9, D loss: 0.232419, acc:  62%, G loss: 1.565299\n",
      "Ep: 767, steps: 10, D loss: 0.186668, acc:  78%, G loss: 1.523382\n",
      "Ep: 767, steps: 11, D loss: 0.274132, acc:  45%, G loss: 1.850746\n",
      "Ep: 767, steps: 12, D loss: 0.277460, acc:  40%, G loss: 1.351506\n",
      "Ep: 767, steps: 13, D loss: 0.274621, acc:  40%, G loss: 1.467776\n",
      "Ep: 767, steps: 14, D loss: 0.277707, acc:  43%, G loss: 1.523993\n",
      "Ep: 767, steps: 15, D loss: 0.277606, acc:  43%, G loss: 1.523148\n",
      "Ep: 767, steps: 16, D loss: 0.236423, acc:  61%, G loss: 1.607950\n",
      "Ep: 767, steps: 17, D loss: 0.228781, acc:  62%, G loss: 1.520198\n",
      "Ep: 767, steps: 18, D loss: 0.235077, acc:  60%, G loss: 1.729395\n",
      "Ep: 767, steps: 19, D loss: 0.221004, acc:  66%, G loss: 1.602572\n",
      "Ep: 767, steps: 20, D loss: 0.202042, acc:  71%, G loss: 1.649256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 767, steps: 21, D loss: 0.266971, acc:  41%, G loss: 1.453154\n",
      "Ep: 767, steps: 22, D loss: 0.196438, acc:  68%, G loss: 1.551849\n",
      "Ep: 767, steps: 23, D loss: 0.220806, acc:  64%, G loss: 1.761619\n",
      "Ep: 767, steps: 24, D loss: 0.197117, acc:  76%, G loss: 1.558156\n",
      "Ep: 767, steps: 25, D loss: 0.245852, acc:  57%, G loss: 1.641233\n",
      "Data exhausted, Re Initialize\n",
      "Saved Model\n",
      "Ep: 768, steps: 1, D loss: 0.219809, acc:  65%, G loss: 1.606743\n",
      "Ep: 768, steps: 2, D loss: 0.166108, acc:  81%, G loss: 1.878506\n",
      "Ep: 768, steps: 3, D loss: 0.184493, acc:  77%, G loss: 1.655526\n",
      "Ep: 768, steps: 4, D loss: 0.268009, acc:  52%, G loss: 1.650345\n",
      "Ep: 768, steps: 5, D loss: 0.255635, acc:  52%, G loss: 1.532978\n",
      "Ep: 768, steps: 6, D loss: 0.293503, acc:  37%, G loss: 1.673873\n",
      "Ep: 768, steps: 7, D loss: 0.228290, acc:  64%, G loss: 1.807391\n",
      "Ep: 768, steps: 8, D loss: 0.248471, acc:  56%, G loss: 1.555607\n",
      "Ep: 768, steps: 9, D loss: 0.190004, acc:  73%, G loss: 1.550499\n",
      "Ep: 768, steps: 10, D loss: 0.268390, acc:  44%, G loss: 1.750511\n",
      "Ep: 768, steps: 11, D loss: 0.291575, acc:  34%, G loss: 1.362209\n",
      "Ep: 768, steps: 12, D loss: 0.282148, acc:  37%, G loss: 1.429767\n",
      "Ep: 768, steps: 13, D loss: 0.252296, acc:  54%, G loss: 1.465495\n",
      "Ep: 768, steps: 14, D loss: 0.282027, acc:  40%, G loss: 1.557560\n",
      "Ep: 768, steps: 15, D loss: 0.241511, acc:  58%, G loss: 1.580870\n",
      "Ep: 768, steps: 16, D loss: 0.227096, acc:  62%, G loss: 1.498519\n",
      "Ep: 768, steps: 17, D loss: 0.236222, acc:  56%, G loss: 1.674398\n",
      "Ep: 768, steps: 18, D loss: 0.211769, acc:  68%, G loss: 1.611129\n",
      "Ep: 768, steps: 19, D loss: 0.178551, acc:  80%, G loss: 1.772219\n",
      "Ep: 768, steps: 20, D loss: 0.254297, acc:  47%, G loss: 1.504021\n",
      "Ep: 768, steps: 21, D loss: 0.217288, acc:  64%, G loss: 1.671161\n",
      "Ep: 768, steps: 22, D loss: 0.232000, acc:  61%, G loss: 1.794809\n",
      "Ep: 768, steps: 23, D loss: 0.202585, acc:  73%, G loss: 1.474653\n",
      "Ep: 768, steps: 24, D loss: 0.248210, acc:  56%, G loss: 1.568597\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 769, steps: 1, D loss: 0.255832, acc:  51%, G loss: 1.631473\n",
      "Ep: 769, steps: 2, D loss: 0.260016, acc:  51%, G loss: 1.463052\n",
      "Ep: 769, steps: 3, D loss: 0.176336, acc:  81%, G loss: 1.900732\n",
      "Ep: 769, steps: 4, D loss: 0.180393, acc:  85%, G loss: 1.619198\n",
      "Ep: 769, steps: 5, D loss: 0.288097, acc:  45%, G loss: 1.607234\n",
      "Ep: 769, steps: 6, D loss: 0.233077, acc:  57%, G loss: 1.573101\n",
      "Ep: 769, steps: 7, D loss: 0.297642, acc:  34%, G loss: 1.482059\n",
      "Ep: 769, steps: 8, D loss: 0.232195, acc:  63%, G loss: 1.808589\n",
      "Ep: 769, steps: 9, D loss: 0.236710, acc:  61%, G loss: 1.532264\n",
      "Ep: 769, steps: 10, D loss: 0.179034, acc:  81%, G loss: 1.552004\n",
      "Ep: 769, steps: 11, D loss: 0.255925, acc:  49%, G loss: 1.707917\n",
      "Ep: 769, steps: 12, D loss: 0.278674, acc:  38%, G loss: 1.377759\n",
      "Ep: 769, steps: 13, D loss: 0.280006, acc:  36%, G loss: 1.460316\n",
      "Ep: 769, steps: 14, D loss: 0.262884, acc:  50%, G loss: 1.540377\n",
      "Ep: 769, steps: 15, D loss: 0.269016, acc:  44%, G loss: 1.568355\n",
      "Ep: 769, steps: 16, D loss: 0.249022, acc:  52%, G loss: 1.609518\n",
      "Ep: 769, steps: 17, D loss: 0.228234, acc:  61%, G loss: 1.523538\n",
      "Ep: 769, steps: 18, D loss: 0.248313, acc:  53%, G loss: 1.634332\n",
      "Ep: 769, steps: 19, D loss: 0.212115, acc:  68%, G loss: 1.602018\n",
      "Ep: 769, steps: 20, D loss: 0.184621, acc:  76%, G loss: 1.665199\n",
      "Ep: 769, steps: 21, D loss: 0.254442, acc:  45%, G loss: 1.422836\n",
      "Ep: 769, steps: 22, D loss: 0.214693, acc:  63%, G loss: 1.646877\n",
      "Ep: 769, steps: 23, D loss: 0.225390, acc:  64%, G loss: 1.836791\n",
      "Ep: 769, steps: 24, D loss: 0.193279, acc:  74%, G loss: 1.549330\n",
      "Ep: 769, steps: 25, D loss: 0.248734, acc:  57%, G loss: 1.557111\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 770, steps: 1, D loss: 0.215804, acc:  67%, G loss: 1.669014\n",
      "Ep: 770, steps: 2, D loss: 0.250496, acc:  55%, G loss: 1.430705\n",
      "Ep: 770, steps: 3, D loss: 0.167611, acc:  82%, G loss: 1.891633\n",
      "Ep: 770, steps: 4, D loss: 0.176322, acc:  83%, G loss: 1.695683\n",
      "Ep: 770, steps: 5, D loss: 0.285310, acc:  46%, G loss: 1.629393\n",
      "Ep: 770, steps: 6, D loss: 0.239040, acc:  55%, G loss: 1.626977\n",
      "Ep: 770, steps: 7, D loss: 0.304583, acc:  34%, G loss: 1.493059\n",
      "Ep: 770, steps: 8, D loss: 0.233995, acc:  64%, G loss: 1.807305\n",
      "Ep: 770, steps: 9, D loss: 0.242015, acc:  58%, G loss: 1.565106\n",
      "Ep: 770, steps: 10, D loss: 0.179521, acc:  78%, G loss: 1.542672\n",
      "Ep: 770, steps: 11, D loss: 0.257581, acc:  50%, G loss: 1.693219\n",
      "Ep: 770, steps: 12, D loss: 0.280668, acc:  39%, G loss: 1.332098\n",
      "Ep: 770, steps: 13, D loss: 0.283013, acc:  36%, G loss: 1.397996\n",
      "Ep: 770, steps: 14, D loss: 0.276260, acc:  44%, G loss: 1.478638\n",
      "Ep: 770, steps: 15, D loss: 0.260430, acc:  48%, G loss: 1.562317\n",
      "Ep: 770, steps: 16, D loss: 0.250574, acc:  54%, G loss: 1.649138\n",
      "Ep: 770, steps: 17, D loss: 0.227004, acc:  62%, G loss: 1.641612\n",
      "Ep: 770, steps: 18, D loss: 0.237434, acc:  59%, G loss: 1.638749\n",
      "Ep: 770, steps: 19, D loss: 0.221497, acc:  64%, G loss: 1.590198\n",
      "Ep: 770, steps: 20, D loss: 0.175654, acc:  81%, G loss: 1.702386\n",
      "Ep: 770, steps: 21, D loss: 0.264564, acc:  42%, G loss: 1.451663\n",
      "Ep: 770, steps: 22, D loss: 0.201219, acc:  69%, G loss: 1.704452\n",
      "Ep: 770, steps: 23, D loss: 0.227337, acc:  64%, G loss: 1.792081\n",
      "Saved Model\n",
      "Ep: 770, steps: 24, D loss: 0.210455, acc:  69%, G loss: 1.480093\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 771, steps: 1, D loss: 0.229174, acc:  62%, G loss: 1.668244\n",
      "Ep: 771, steps: 2, D loss: 0.244905, acc:  57%, G loss: 1.480444\n",
      "Ep: 771, steps: 3, D loss: 0.170366, acc:  82%, G loss: 1.897611\n",
      "Ep: 771, steps: 4, D loss: 0.170956, acc:  83%, G loss: 1.738803\n",
      "Ep: 771, steps: 5, D loss: 0.293908, acc:  45%, G loss: 1.593101\n",
      "Ep: 771, steps: 6, D loss: 0.254280, acc:  53%, G loss: 1.622504\n",
      "Ep: 771, steps: 7, D loss: 0.308755, acc:  30%, G loss: 1.611359\n",
      "Ep: 771, steps: 8, D loss: 0.227346, acc:  60%, G loss: 1.738625\n",
      "Ep: 771, steps: 9, D loss: 0.250676, acc:  56%, G loss: 1.556959\n",
      "Ep: 771, steps: 10, D loss: 0.188353, acc:  75%, G loss: 1.596446\n",
      "Ep: 771, steps: 11, D loss: 0.272407, acc:  44%, G loss: 1.637292\n",
      "Ep: 771, steps: 12, D loss: 0.284749, acc:  37%, G loss: 1.332381\n",
      "Ep: 771, steps: 13, D loss: 0.289948, acc:  31%, G loss: 1.383249\n",
      "Ep: 771, steps: 14, D loss: 0.279005, acc:  42%, G loss: 1.518682\n",
      "Ep: 771, steps: 15, D loss: 0.268041, acc:  42%, G loss: 1.528290\n",
      "Ep: 771, steps: 16, D loss: 0.239483, acc:  57%, G loss: 1.553130\n",
      "Ep: 771, steps: 17, D loss: 0.229740, acc:  63%, G loss: 1.486465\n",
      "Ep: 771, steps: 18, D loss: 0.239836, acc:  56%, G loss: 1.664412\n",
      "Ep: 771, steps: 19, D loss: 0.214405, acc:  67%, G loss: 1.564628\n",
      "Ep: 771, steps: 20, D loss: 0.178405, acc:  79%, G loss: 1.653269\n",
      "Ep: 771, steps: 21, D loss: 0.271061, acc:  38%, G loss: 1.472389\n",
      "Ep: 771, steps: 22, D loss: 0.213864, acc:  65%, G loss: 1.559641\n",
      "Ep: 771, steps: 23, D loss: 0.210970, acc:  69%, G loss: 1.762938\n",
      "Ep: 771, steps: 24, D loss: 0.214862, acc:  68%, G loss: 1.547344\n",
      "Ep: 771, steps: 25, D loss: 0.273476, acc:  49%, G loss: 1.590751\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 772, steps: 1, D loss: 0.217460, acc:  68%, G loss: 1.609349\n",
      "Ep: 772, steps: 2, D loss: 0.250342, acc:  55%, G loss: 1.397134\n",
      "Ep: 772, steps: 3, D loss: 0.162124, acc:  85%, G loss: 1.891112\n",
      "Ep: 772, steps: 4, D loss: 0.179161, acc:  83%, G loss: 1.653642\n",
      "Ep: 772, steps: 5, D loss: 0.262215, acc:  54%, G loss: 1.611648\n",
      "Ep: 772, steps: 6, D loss: 0.239918, acc:  57%, G loss: 1.676175\n",
      "Ep: 772, steps: 7, D loss: 0.306094, acc:  39%, G loss: 1.546455\n",
      "Ep: 772, steps: 8, D loss: 0.233953, acc:  61%, G loss: 1.758243\n",
      "Ep: 772, steps: 9, D loss: 0.248145, acc:  55%, G loss: 1.587430\n",
      "Ep: 772, steps: 10, D loss: 0.165007, acc:  84%, G loss: 1.596751\n",
      "Ep: 772, steps: 11, D loss: 0.271854, acc:  45%, G loss: 1.696948\n",
      "Ep: 772, steps: 12, D loss: 0.283250, acc:  38%, G loss: 1.381947\n",
      "Ep: 772, steps: 13, D loss: 0.288365, acc:  32%, G loss: 1.386818\n",
      "Ep: 772, steps: 14, D loss: 0.272124, acc:  44%, G loss: 1.474154\n",
      "Ep: 772, steps: 15, D loss: 0.270390, acc:  46%, G loss: 1.518885\n",
      "Ep: 772, steps: 16, D loss: 0.245872, acc:  54%, G loss: 1.613695\n",
      "Ep: 772, steps: 17, D loss: 0.229300, acc:  62%, G loss: 1.507212\n",
      "Ep: 772, steps: 18, D loss: 0.234332, acc:  59%, G loss: 1.635363\n",
      "Ep: 772, steps: 19, D loss: 0.225102, acc:  65%, G loss: 1.561936\n",
      "Ep: 772, steps: 20, D loss: 0.184610, acc:  77%, G loss: 1.682530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 772, steps: 21, D loss: 0.261196, acc:  42%, G loss: 1.455523\n",
      "Ep: 772, steps: 22, D loss: 0.230215, acc:  59%, G loss: 1.615057\n",
      "Ep: 772, steps: 23, D loss: 0.225290, acc:  65%, G loss: 1.847205\n",
      "Ep: 772, steps: 24, D loss: 0.206667, acc:  72%, G loss: 1.544699\n",
      "Ep: 772, steps: 25, D loss: 0.253273, acc:  52%, G loss: 1.540664\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 773, steps: 1, D loss: 0.219366, acc:  66%, G loss: 1.605704\n",
      "Ep: 773, steps: 2, D loss: 0.253265, acc:  52%, G loss: 1.423104\n",
      "Ep: 773, steps: 3, D loss: 0.171004, acc:  83%, G loss: 1.831022\n",
      "Ep: 773, steps: 4, D loss: 0.191059, acc:  77%, G loss: 1.677852\n",
      "Ep: 773, steps: 5, D loss: 0.285451, acc:  47%, G loss: 1.532633\n",
      "Ep: 773, steps: 6, D loss: 0.242298, acc:  55%, G loss: 1.540538\n",
      "Ep: 773, steps: 7, D loss: 0.308498, acc:  31%, G loss: 1.536079\n",
      "Ep: 773, steps: 8, D loss: 0.233478, acc:  62%, G loss: 1.732318\n",
      "Ep: 773, steps: 9, D loss: 0.253166, acc:  54%, G loss: 1.567602\n",
      "Ep: 773, steps: 10, D loss: 0.182597, acc:  77%, G loss: 1.643591\n",
      "Ep: 773, steps: 11, D loss: 0.264722, acc:  47%, G loss: 1.708849\n",
      "Ep: 773, steps: 12, D loss: 0.294443, acc:  34%, G loss: 1.304514\n",
      "Ep: 773, steps: 13, D loss: 0.285023, acc:  35%, G loss: 1.381080\n",
      "Ep: 773, steps: 14, D loss: 0.263906, acc:  49%, G loss: 1.534688\n",
      "Ep: 773, steps: 15, D loss: 0.260939, acc:  48%, G loss: 1.478405\n",
      "Ep: 773, steps: 16, D loss: 0.239264, acc:  59%, G loss: 1.589437\n",
      "Ep: 773, steps: 17, D loss: 0.225607, acc:  63%, G loss: 1.544431\n",
      "Ep: 773, steps: 18, D loss: 0.241524, acc:  55%, G loss: 1.667437\n",
      "Ep: 773, steps: 19, D loss: 0.219299, acc:  65%, G loss: 1.545078\n",
      "Ep: 773, steps: 20, D loss: 0.193383, acc:  77%, G loss: 1.625134\n",
      "Ep: 773, steps: 21, D loss: 0.268534, acc:  40%, G loss: 1.462179\n",
      "Saved Model\n",
      "Ep: 773, steps: 22, D loss: 0.221606, acc:  62%, G loss: 1.605708\n",
      "Ep: 773, steps: 23, D loss: 0.224496, acc:  64%, G loss: 1.484722\n",
      "Ep: 773, steps: 24, D loss: 0.248020, acc:  56%, G loss: 1.587067\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 774, steps: 1, D loss: 0.239158, acc:  60%, G loss: 1.580347\n",
      "Ep: 774, steps: 2, D loss: 0.236310, acc:  59%, G loss: 1.372545\n",
      "Ep: 774, steps: 3, D loss: 0.180950, acc:  80%, G loss: 1.874761\n",
      "Ep: 774, steps: 4, D loss: 0.194914, acc:  77%, G loss: 1.617970\n",
      "Ep: 774, steps: 5, D loss: 0.276193, acc:  52%, G loss: 1.592236\n",
      "Ep: 774, steps: 6, D loss: 0.252342, acc:  53%, G loss: 1.588326\n",
      "Ep: 774, steps: 7, D loss: 0.294129, acc:  35%, G loss: 1.442818\n",
      "Ep: 774, steps: 8, D loss: 0.215406, acc:  68%, G loss: 1.783211\n",
      "Ep: 774, steps: 9, D loss: 0.245147, acc:  57%, G loss: 1.579871\n",
      "Ep: 774, steps: 10, D loss: 0.183755, acc:  79%, G loss: 1.597182\n",
      "Ep: 774, steps: 11, D loss: 0.267659, acc:  45%, G loss: 1.646425\n",
      "Ep: 774, steps: 12, D loss: 0.290079, acc:  34%, G loss: 1.287710\n",
      "Ep: 774, steps: 13, D loss: 0.287926, acc:  31%, G loss: 1.382314\n",
      "Ep: 774, steps: 14, D loss: 0.267836, acc:  44%, G loss: 1.462582\n",
      "Ep: 774, steps: 15, D loss: 0.261450, acc:  45%, G loss: 1.529975\n",
      "Ep: 774, steps: 16, D loss: 0.256176, acc:  48%, G loss: 1.603666\n",
      "Ep: 774, steps: 17, D loss: 0.218860, acc:  66%, G loss: 1.574103\n",
      "Ep: 774, steps: 18, D loss: 0.231921, acc:  60%, G loss: 1.638089\n",
      "Ep: 774, steps: 19, D loss: 0.219890, acc:  64%, G loss: 1.547715\n",
      "Ep: 774, steps: 20, D loss: 0.179759, acc:  79%, G loss: 1.698276\n",
      "Ep: 774, steps: 21, D loss: 0.267509, acc:  40%, G loss: 1.668073\n",
      "Ep: 774, steps: 22, D loss: 0.201368, acc:  69%, G loss: 1.844789\n",
      "Ep: 774, steps: 23, D loss: 0.221725, acc:  67%, G loss: 1.787157\n",
      "Ep: 774, steps: 24, D loss: 0.206526, acc:  70%, G loss: 1.490393\n",
      "Ep: 774, steps: 25, D loss: 0.252579, acc:  53%, G loss: 1.497424\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 775, steps: 1, D loss: 0.221636, acc:  65%, G loss: 1.653967\n",
      "Ep: 775, steps: 2, D loss: 0.252529, acc:  54%, G loss: 1.425034\n",
      "Ep: 775, steps: 3, D loss: 0.160937, acc:  89%, G loss: 1.862137\n",
      "Ep: 775, steps: 4, D loss: 0.182477, acc:  80%, G loss: 1.674391\n",
      "Ep: 775, steps: 5, D loss: 0.279975, acc:  48%, G loss: 1.549158\n",
      "Ep: 775, steps: 6, D loss: 0.231412, acc:  56%, G loss: 1.533074\n",
      "Ep: 775, steps: 7, D loss: 0.292296, acc:  37%, G loss: 1.462607\n",
      "Ep: 775, steps: 8, D loss: 0.235849, acc:  60%, G loss: 1.623498\n",
      "Ep: 775, steps: 9, D loss: 0.244901, acc:  57%, G loss: 1.531533\n",
      "Ep: 775, steps: 10, D loss: 0.179351, acc:  80%, G loss: 1.553614\n",
      "Ep: 775, steps: 11, D loss: 0.271328, acc:  47%, G loss: 1.652481\n",
      "Ep: 775, steps: 12, D loss: 0.296603, acc:  32%, G loss: 1.311124\n",
      "Ep: 775, steps: 13, D loss: 0.289589, acc:  32%, G loss: 1.407320\n",
      "Ep: 775, steps: 14, D loss: 0.258364, acc:  50%, G loss: 1.494650\n",
      "Ep: 775, steps: 15, D loss: 0.272030, acc:  44%, G loss: 1.581159\n",
      "Ep: 775, steps: 16, D loss: 0.242270, acc:  58%, G loss: 1.550461\n",
      "Ep: 775, steps: 17, D loss: 0.228058, acc:  62%, G loss: 1.674426\n",
      "Ep: 775, steps: 18, D loss: 0.236979, acc:  57%, G loss: 1.736995\n",
      "Ep: 775, steps: 19, D loss: 0.230433, acc:  62%, G loss: 1.576624\n",
      "Ep: 775, steps: 20, D loss: 0.192024, acc:  76%, G loss: 1.704313\n",
      "Ep: 775, steps: 21, D loss: 0.247213, acc:  49%, G loss: 1.468536\n",
      "Ep: 775, steps: 22, D loss: 0.192823, acc:  71%, G loss: 1.521991\n",
      "Ep: 775, steps: 23, D loss: 0.212767, acc:  69%, G loss: 1.760196\n",
      "Ep: 775, steps: 24, D loss: 0.216961, acc:  66%, G loss: 1.492440\n",
      "Ep: 775, steps: 25, D loss: 0.247905, acc:  57%, G loss: 1.530985\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 776, steps: 1, D loss: 0.225543, acc:  63%, G loss: 1.654893\n",
      "Ep: 776, steps: 2, D loss: 0.238778, acc:  58%, G loss: 1.414840\n",
      "Ep: 776, steps: 3, D loss: 0.179917, acc:  79%, G loss: 1.841729\n",
      "Ep: 776, steps: 4, D loss: 0.181958, acc:  82%, G loss: 1.636995\n",
      "Ep: 776, steps: 5, D loss: 0.285817, acc:  46%, G loss: 1.572765\n",
      "Ep: 776, steps: 6, D loss: 0.237311, acc:  56%, G loss: 1.650895\n",
      "Ep: 776, steps: 7, D loss: 0.295734, acc:  36%, G loss: 1.578758\n",
      "Ep: 776, steps: 8, D loss: 0.231300, acc:  62%, G loss: 1.641663\n",
      "Ep: 776, steps: 9, D loss: 0.252048, acc:  54%, G loss: 1.601342\n",
      "Ep: 776, steps: 10, D loss: 0.202729, acc:  70%, G loss: 1.560814\n",
      "Ep: 776, steps: 11, D loss: 0.256846, acc:  51%, G loss: 1.661247\n",
      "Ep: 776, steps: 12, D loss: 0.279513, acc:  37%, G loss: 1.345454\n",
      "Ep: 776, steps: 13, D loss: 0.282978, acc:  35%, G loss: 1.358324\n",
      "Ep: 776, steps: 14, D loss: 0.273466, acc:  44%, G loss: 1.413214\n",
      "Ep: 776, steps: 15, D loss: 0.254027, acc:  50%, G loss: 1.542251\n",
      "Ep: 776, steps: 16, D loss: 0.248502, acc:  54%, G loss: 1.534611\n",
      "Ep: 776, steps: 17, D loss: 0.218253, acc:  70%, G loss: 1.617956\n",
      "Ep: 776, steps: 18, D loss: 0.230909, acc:  60%, G loss: 1.722489\n",
      "Ep: 776, steps: 19, D loss: 0.221642, acc:  65%, G loss: 1.573735\n",
      "Saved Model\n",
      "Ep: 776, steps: 20, D loss: 0.191777, acc:  76%, G loss: 1.630506\n",
      "Ep: 776, steps: 21, D loss: 0.201408, acc:  68%, G loss: 1.516774\n",
      "Ep: 776, steps: 22, D loss: 0.214562, acc:  68%, G loss: 1.800287\n",
      "Ep: 776, steps: 23, D loss: 0.199199, acc:  74%, G loss: 1.613248\n",
      "Ep: 776, steps: 24, D loss: 0.269973, acc:  50%, G loss: 1.516666\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 777, steps: 1, D loss: 0.210575, acc:  68%, G loss: 1.621220\n",
      "Ep: 777, steps: 2, D loss: 0.225571, acc:  64%, G loss: 1.424869\n",
      "Ep: 777, steps: 3, D loss: 0.169455, acc:  84%, G loss: 1.899698\n",
      "Ep: 777, steps: 4, D loss: 0.167562, acc:  84%, G loss: 1.681885\n",
      "Ep: 777, steps: 5, D loss: 0.296723, acc:  43%, G loss: 1.668764\n",
      "Ep: 777, steps: 6, D loss: 0.240247, acc:  56%, G loss: 1.571341\n",
      "Ep: 777, steps: 7, D loss: 0.305866, acc:  34%, G loss: 1.527682\n",
      "Ep: 777, steps: 8, D loss: 0.235285, acc:  59%, G loss: 1.740702\n",
      "Ep: 777, steps: 9, D loss: 0.243105, acc:  59%, G loss: 1.564597\n",
      "Ep: 777, steps: 10, D loss: 0.201560, acc:  71%, G loss: 1.533536\n",
      "Ep: 777, steps: 11, D loss: 0.253838, acc:  52%, G loss: 1.665290\n",
      "Ep: 777, steps: 12, D loss: 0.303553, acc:  30%, G loss: 1.327875\n",
      "Ep: 777, steps: 13, D loss: 0.293663, acc:  31%, G loss: 1.423720\n",
      "Ep: 777, steps: 14, D loss: 0.280300, acc:  41%, G loss: 1.525708\n",
      "Ep: 777, steps: 15, D loss: 0.258532, acc:  48%, G loss: 1.564390\n",
      "Ep: 777, steps: 16, D loss: 0.258240, acc:  50%, G loss: 1.582390\n",
      "Ep: 777, steps: 17, D loss: 0.216915, acc:  69%, G loss: 1.537501\n",
      "Ep: 777, steps: 18, D loss: 0.238224, acc:  59%, G loss: 1.640987\n",
      "Ep: 777, steps: 19, D loss: 0.208419, acc:  68%, G loss: 1.573552\n",
      "Ep: 777, steps: 20, D loss: 0.185454, acc:  79%, G loss: 1.632527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 777, steps: 21, D loss: 0.280296, acc:  34%, G loss: 1.489466\n",
      "Ep: 777, steps: 22, D loss: 0.207479, acc:  64%, G loss: 1.557237\n",
      "Ep: 777, steps: 23, D loss: 0.211418, acc:  71%, G loss: 1.792729\n",
      "Ep: 777, steps: 24, D loss: 0.216796, acc:  66%, G loss: 1.495967\n",
      "Ep: 777, steps: 25, D loss: 0.255155, acc:  53%, G loss: 1.614007\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 778, steps: 1, D loss: 0.224313, acc:  64%, G loss: 1.801774\n",
      "Ep: 778, steps: 2, D loss: 0.239985, acc:  57%, G loss: 1.416243\n",
      "Ep: 778, steps: 3, D loss: 0.174677, acc:  82%, G loss: 1.855957\n",
      "Ep: 778, steps: 4, D loss: 0.178534, acc:  82%, G loss: 1.671121\n",
      "Ep: 778, steps: 5, D loss: 0.273667, acc:  50%, G loss: 1.588641\n",
      "Ep: 778, steps: 6, D loss: 0.234556, acc:  57%, G loss: 1.593813\n",
      "Ep: 778, steps: 7, D loss: 0.322205, acc:  28%, G loss: 1.467734\n",
      "Ep: 778, steps: 8, D loss: 0.242719, acc:  58%, G loss: 1.686418\n",
      "Ep: 778, steps: 9, D loss: 0.250242, acc:  55%, G loss: 1.636695\n",
      "Ep: 778, steps: 10, D loss: 0.190194, acc:  74%, G loss: 1.588098\n",
      "Ep: 778, steps: 11, D loss: 0.256044, acc:  50%, G loss: 1.675954\n",
      "Ep: 778, steps: 12, D loss: 0.286817, acc:  35%, G loss: 1.330164\n",
      "Ep: 778, steps: 13, D loss: 0.294132, acc:  31%, G loss: 1.336676\n",
      "Ep: 778, steps: 14, D loss: 0.276056, acc:  43%, G loss: 1.459631\n",
      "Ep: 778, steps: 15, D loss: 0.267256, acc:  46%, G loss: 1.518523\n",
      "Ep: 778, steps: 16, D loss: 0.258158, acc:  49%, G loss: 1.563527\n",
      "Ep: 778, steps: 17, D loss: 0.224810, acc:  64%, G loss: 1.512392\n",
      "Ep: 778, steps: 18, D loss: 0.228382, acc:  61%, G loss: 1.682769\n",
      "Ep: 778, steps: 19, D loss: 0.215942, acc:  67%, G loss: 1.607388\n",
      "Ep: 778, steps: 20, D loss: 0.188467, acc:  78%, G loss: 1.680023\n",
      "Ep: 778, steps: 21, D loss: 0.261162, acc:  44%, G loss: 1.545371\n",
      "Ep: 778, steps: 22, D loss: 0.215579, acc:  65%, G loss: 1.778027\n",
      "Ep: 778, steps: 23, D loss: 0.225663, acc:  62%, G loss: 1.762400\n",
      "Ep: 778, steps: 24, D loss: 0.199765, acc:  74%, G loss: 1.551019\n",
      "Ep: 778, steps: 25, D loss: 0.267421, acc:  52%, G loss: 1.572585\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 779, steps: 1, D loss: 0.210244, acc:  67%, G loss: 1.755614\n",
      "Ep: 779, steps: 2, D loss: 0.244681, acc:  56%, G loss: 1.428087\n",
      "Ep: 779, steps: 3, D loss: 0.167530, acc:  85%, G loss: 1.911908\n",
      "Ep: 779, steps: 4, D loss: 0.178930, acc:  84%, G loss: 1.677057\n",
      "Ep: 779, steps: 5, D loss: 0.250873, acc:  57%, G loss: 1.642575\n",
      "Ep: 779, steps: 6, D loss: 0.235861, acc:  56%, G loss: 1.639634\n",
      "Ep: 779, steps: 7, D loss: 0.309384, acc:  31%, G loss: 1.421794\n",
      "Ep: 779, steps: 8, D loss: 0.228375, acc:  63%, G loss: 1.685062\n",
      "Ep: 779, steps: 9, D loss: 0.232329, acc:  61%, G loss: 1.599993\n",
      "Ep: 779, steps: 10, D loss: 0.172122, acc:  80%, G loss: 1.565489\n",
      "Ep: 779, steps: 11, D loss: 0.258897, acc:  50%, G loss: 1.721795\n",
      "Ep: 779, steps: 12, D loss: 0.295053, acc:  35%, G loss: 1.336193\n",
      "Ep: 779, steps: 13, D loss: 0.289993, acc:  31%, G loss: 1.367846\n",
      "Ep: 779, steps: 14, D loss: 0.277022, acc:  42%, G loss: 1.480746\n",
      "Ep: 779, steps: 15, D loss: 0.261187, acc:  49%, G loss: 1.486068\n",
      "Ep: 779, steps: 16, D loss: 0.243280, acc:  57%, G loss: 1.553218\n",
      "Ep: 779, steps: 17, D loss: 0.216709, acc:  68%, G loss: 1.566583\n",
      "Saved Model\n",
      "Ep: 779, steps: 18, D loss: 0.232253, acc:  60%, G loss: 1.674085\n",
      "Ep: 779, steps: 19, D loss: 0.199930, acc:  72%, G loss: 1.739294\n",
      "Ep: 779, steps: 20, D loss: 0.248515, acc:  49%, G loss: 1.450166\n",
      "Ep: 779, steps: 21, D loss: 0.185234, acc:  71%, G loss: 1.534108\n",
      "Ep: 779, steps: 22, D loss: 0.227656, acc:  61%, G loss: 1.776746\n",
      "Ep: 779, steps: 23, D loss: 0.208075, acc:  73%, G loss: 1.537267\n",
      "Ep: 779, steps: 24, D loss: 0.254023, acc:  55%, G loss: 1.624950\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 780, steps: 1, D loss: 0.230383, acc:  61%, G loss: 1.706271\n",
      "Ep: 780, steps: 2, D loss: 0.252616, acc:  53%, G loss: 1.452811\n",
      "Ep: 780, steps: 3, D loss: 0.169679, acc:  85%, G loss: 1.895425\n",
      "Ep: 780, steps: 4, D loss: 0.177026, acc:  82%, G loss: 1.692019\n",
      "Ep: 780, steps: 5, D loss: 0.275240, acc:  51%, G loss: 1.624177\n",
      "Ep: 780, steps: 6, D loss: 0.254625, acc:  53%, G loss: 1.615785\n",
      "Ep: 780, steps: 7, D loss: 0.284538, acc:  42%, G loss: 1.480945\n",
      "Ep: 780, steps: 8, D loss: 0.216218, acc:  68%, G loss: 1.683746\n",
      "Ep: 780, steps: 9, D loss: 0.255472, acc:  51%, G loss: 1.590500\n",
      "Ep: 780, steps: 10, D loss: 0.200859, acc:  72%, G loss: 1.497576\n",
      "Ep: 780, steps: 11, D loss: 0.271078, acc:  46%, G loss: 1.710552\n",
      "Ep: 780, steps: 12, D loss: 0.283697, acc:  39%, G loss: 1.317875\n",
      "Ep: 780, steps: 13, D loss: 0.279472, acc:  37%, G loss: 1.357517\n",
      "Ep: 780, steps: 14, D loss: 0.265005, acc:  48%, G loss: 1.515059\n",
      "Ep: 780, steps: 15, D loss: 0.279277, acc:  42%, G loss: 1.576441\n",
      "Ep: 780, steps: 16, D loss: 0.245839, acc:  56%, G loss: 1.578524\n",
      "Ep: 780, steps: 17, D loss: 0.227818, acc:  63%, G loss: 1.576680\n",
      "Ep: 780, steps: 18, D loss: 0.239054, acc:  56%, G loss: 1.659555\n",
      "Ep: 780, steps: 19, D loss: 0.222173, acc:  64%, G loss: 1.516711\n",
      "Ep: 780, steps: 20, D loss: 0.186584, acc:  76%, G loss: 1.651959\n",
      "Ep: 780, steps: 21, D loss: 0.258881, acc:  43%, G loss: 1.443071\n",
      "Ep: 780, steps: 22, D loss: 0.214084, acc:  65%, G loss: 1.535231\n",
      "Ep: 780, steps: 23, D loss: 0.223286, acc:  65%, G loss: 1.799254\n",
      "Ep: 780, steps: 24, D loss: 0.217409, acc:  64%, G loss: 1.534186\n",
      "Ep: 780, steps: 25, D loss: 0.243197, acc:  57%, G loss: 1.784648\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 781, steps: 1, D loss: 0.225789, acc:  62%, G loss: 1.757537\n",
      "Ep: 781, steps: 2, D loss: 0.239835, acc:  58%, G loss: 1.478339\n",
      "Ep: 781, steps: 3, D loss: 0.167629, acc:  83%, G loss: 1.899528\n",
      "Ep: 781, steps: 4, D loss: 0.176354, acc:  81%, G loss: 1.748740\n",
      "Ep: 781, steps: 5, D loss: 0.282238, acc:  48%, G loss: 1.747016\n",
      "Ep: 781, steps: 6, D loss: 0.236536, acc:  59%, G loss: 1.678457\n",
      "Ep: 781, steps: 7, D loss: 0.308253, acc:  32%, G loss: 1.487652\n",
      "Ep: 781, steps: 8, D loss: 0.226136, acc:  65%, G loss: 1.669319\n",
      "Ep: 781, steps: 9, D loss: 0.239801, acc:  59%, G loss: 1.564127\n",
      "Ep: 781, steps: 10, D loss: 0.204213, acc:  69%, G loss: 1.575127\n",
      "Ep: 781, steps: 11, D loss: 0.266469, acc:  46%, G loss: 1.674890\n",
      "Ep: 781, steps: 12, D loss: 0.289995, acc:  36%, G loss: 1.304771\n",
      "Ep: 781, steps: 13, D loss: 0.288967, acc:  33%, G loss: 1.396242\n",
      "Ep: 781, steps: 14, D loss: 0.266700, acc:  50%, G loss: 1.474795\n",
      "Ep: 781, steps: 15, D loss: 0.254984, acc:  49%, G loss: 1.550767\n",
      "Ep: 781, steps: 16, D loss: 0.235085, acc:  61%, G loss: 1.542052\n",
      "Ep: 781, steps: 17, D loss: 0.235357, acc:  61%, G loss: 1.543217\n",
      "Ep: 781, steps: 18, D loss: 0.238336, acc:  57%, G loss: 1.658597\n",
      "Ep: 781, steps: 19, D loss: 0.229209, acc:  61%, G loss: 1.532394\n",
      "Ep: 781, steps: 20, D loss: 0.174189, acc:  81%, G loss: 1.650129\n",
      "Ep: 781, steps: 21, D loss: 0.263246, acc:  41%, G loss: 1.435218\n",
      "Ep: 781, steps: 22, D loss: 0.210761, acc:  66%, G loss: 1.527672\n",
      "Ep: 781, steps: 23, D loss: 0.231282, acc:  63%, G loss: 1.727674\n",
      "Ep: 781, steps: 24, D loss: 0.219903, acc:  66%, G loss: 1.611678\n",
      "Ep: 781, steps: 25, D loss: 0.248947, acc:  54%, G loss: 1.582416\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 782, steps: 1, D loss: 0.230138, acc:  62%, G loss: 1.754312\n",
      "Ep: 782, steps: 2, D loss: 0.240593, acc:  57%, G loss: 1.505311\n",
      "Ep: 782, steps: 3, D loss: 0.160352, acc:  86%, G loss: 2.004515\n",
      "Ep: 782, steps: 4, D loss: 0.188120, acc:  79%, G loss: 1.776965\n",
      "Ep: 782, steps: 5, D loss: 0.271442, acc:  48%, G loss: 1.633815\n",
      "Ep: 782, steps: 6, D loss: 0.260424, acc:  50%, G loss: 1.602603\n",
      "Ep: 782, steps: 7, D loss: 0.313356, acc:  31%, G loss: 1.431757\n",
      "Ep: 782, steps: 8, D loss: 0.218655, acc:  67%, G loss: 1.654711\n",
      "Ep: 782, steps: 9, D loss: 0.237307, acc:  59%, G loss: 1.556453\n",
      "Ep: 782, steps: 10, D loss: 0.180203, acc:  76%, G loss: 1.546996\n",
      "Ep: 782, steps: 11, D loss: 0.251771, acc:  54%, G loss: 1.723152\n",
      "Ep: 782, steps: 12, D loss: 0.284707, acc:  37%, G loss: 1.309225\n",
      "Ep: 782, steps: 13, D loss: 0.289409, acc:  35%, G loss: 1.398828\n",
      "Ep: 782, steps: 14, D loss: 0.269189, acc:  44%, G loss: 1.500563\n",
      "Ep: 782, steps: 15, D loss: 0.278556, acc:  43%, G loss: 1.603991\n",
      "Saved Model\n",
      "Ep: 782, steps: 16, D loss: 0.252155, acc:  52%, G loss: 1.547045\n",
      "Ep: 782, steps: 17, D loss: 0.231310, acc:  61%, G loss: 1.605823\n",
      "Ep: 782, steps: 18, D loss: 0.228199, acc:  61%, G loss: 1.516413\n",
      "Ep: 782, steps: 19, D loss: 0.193271, acc:  72%, G loss: 1.622610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 782, steps: 20, D loss: 0.267112, acc:  39%, G loss: 1.519968\n",
      "Ep: 782, steps: 21, D loss: 0.195761, acc:  69%, G loss: 1.531994\n",
      "Ep: 782, steps: 22, D loss: 0.221604, acc:  66%, G loss: 1.713408\n",
      "Ep: 782, steps: 23, D loss: 0.210406, acc:  71%, G loss: 1.616899\n",
      "Ep: 782, steps: 24, D loss: 0.241496, acc:  57%, G loss: 1.542559\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 783, steps: 1, D loss: 0.226018, acc:  64%, G loss: 1.720688\n",
      "Ep: 783, steps: 2, D loss: 0.242362, acc:  57%, G loss: 1.479508\n",
      "Ep: 783, steps: 3, D loss: 0.155198, acc:  88%, G loss: 1.926828\n",
      "Ep: 783, steps: 4, D loss: 0.176054, acc:  83%, G loss: 1.709472\n",
      "Ep: 783, steps: 5, D loss: 0.280732, acc:  48%, G loss: 1.584575\n",
      "Ep: 783, steps: 6, D loss: 0.234899, acc:  55%, G loss: 1.546613\n",
      "Ep: 783, steps: 7, D loss: 0.311553, acc:  31%, G loss: 1.508693\n",
      "Ep: 783, steps: 8, D loss: 0.244096, acc:  56%, G loss: 1.730184\n",
      "Ep: 783, steps: 9, D loss: 0.249199, acc:  56%, G loss: 1.584902\n",
      "Ep: 783, steps: 10, D loss: 0.190286, acc:  72%, G loss: 1.558330\n",
      "Ep: 783, steps: 11, D loss: 0.267897, acc:  45%, G loss: 1.715027\n",
      "Ep: 783, steps: 12, D loss: 0.285292, acc:  36%, G loss: 1.343225\n",
      "Ep: 783, steps: 13, D loss: 0.273973, acc:  39%, G loss: 1.462692\n",
      "Ep: 783, steps: 14, D loss: 0.281312, acc:  42%, G loss: 1.482614\n",
      "Ep: 783, steps: 15, D loss: 0.243240, acc:  54%, G loss: 1.580698\n",
      "Ep: 783, steps: 16, D loss: 0.251542, acc:  52%, G loss: 1.563524\n",
      "Ep: 783, steps: 17, D loss: 0.234308, acc:  62%, G loss: 1.556381\n",
      "Ep: 783, steps: 18, D loss: 0.239571, acc:  57%, G loss: 1.640304\n",
      "Ep: 783, steps: 19, D loss: 0.220390, acc:  64%, G loss: 1.567047\n",
      "Ep: 783, steps: 20, D loss: 0.198758, acc:  77%, G loss: 1.620584\n",
      "Ep: 783, steps: 21, D loss: 0.273938, acc:  36%, G loss: 1.407837\n",
      "Ep: 783, steps: 22, D loss: 0.219654, acc:  62%, G loss: 1.629122\n",
      "Ep: 783, steps: 23, D loss: 0.215706, acc:  67%, G loss: 1.771644\n",
      "Ep: 783, steps: 24, D loss: 0.210342, acc:  69%, G loss: 1.458295\n",
      "Ep: 783, steps: 25, D loss: 0.245076, acc:  57%, G loss: 1.691931\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 784, steps: 1, D loss: 0.221753, acc:  65%, G loss: 1.647581\n",
      "Ep: 784, steps: 2, D loss: 0.239364, acc:  58%, G loss: 1.487192\n",
      "Ep: 784, steps: 3, D loss: 0.165203, acc:  84%, G loss: 1.942800\n",
      "Ep: 784, steps: 4, D loss: 0.177033, acc:  83%, G loss: 1.745004\n",
      "Ep: 784, steps: 5, D loss: 0.280932, acc:  49%, G loss: 1.683530\n",
      "Ep: 784, steps: 6, D loss: 0.232458, acc:  56%, G loss: 1.551791\n",
      "Ep: 784, steps: 7, D loss: 0.290951, acc:  38%, G loss: 1.441068\n",
      "Ep: 784, steps: 8, D loss: 0.221772, acc:  64%, G loss: 1.693517\n",
      "Ep: 784, steps: 9, D loss: 0.238435, acc:  60%, G loss: 1.584161\n",
      "Ep: 784, steps: 10, D loss: 0.192462, acc:  74%, G loss: 1.657830\n",
      "Ep: 784, steps: 11, D loss: 0.271815, acc:  44%, G loss: 1.823661\n",
      "Ep: 784, steps: 12, D loss: 0.282316, acc:  38%, G loss: 1.310451\n",
      "Ep: 784, steps: 13, D loss: 0.293313, acc:  33%, G loss: 1.399322\n",
      "Ep: 784, steps: 14, D loss: 0.273599, acc:  44%, G loss: 1.508425\n",
      "Ep: 784, steps: 15, D loss: 0.258895, acc:  50%, G loss: 1.578963\n",
      "Ep: 784, steps: 16, D loss: 0.248022, acc:  53%, G loss: 1.598065\n",
      "Ep: 784, steps: 17, D loss: 0.231899, acc:  61%, G loss: 1.557396\n",
      "Ep: 784, steps: 18, D loss: 0.229674, acc:  60%, G loss: 1.635013\n",
      "Ep: 784, steps: 19, D loss: 0.215793, acc:  66%, G loss: 1.568300\n",
      "Ep: 784, steps: 20, D loss: 0.175133, acc:  81%, G loss: 1.657863\n",
      "Ep: 784, steps: 21, D loss: 0.257069, acc:  45%, G loss: 1.440057\n",
      "Ep: 784, steps: 22, D loss: 0.197374, acc:  67%, G loss: 1.521557\n",
      "Ep: 784, steps: 23, D loss: 0.208542, acc:  70%, G loss: 1.779167\n",
      "Ep: 784, steps: 24, D loss: 0.212225, acc:  68%, G loss: 1.482707\n",
      "Ep: 784, steps: 25, D loss: 0.262349, acc:  51%, G loss: 1.557451\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 785, steps: 1, D loss: 0.234782, acc:  58%, G loss: 1.644188\n",
      "Ep: 785, steps: 2, D loss: 0.255413, acc:  52%, G loss: 1.589706\n",
      "Ep: 785, steps: 3, D loss: 0.174672, acc:  81%, G loss: 1.946822\n",
      "Ep: 785, steps: 4, D loss: 0.172733, acc:  83%, G loss: 1.754122\n",
      "Ep: 785, steps: 5, D loss: 0.273426, acc:  50%, G loss: 1.587163\n",
      "Ep: 785, steps: 6, D loss: 0.248019, acc:  53%, G loss: 1.549456\n",
      "Ep: 785, steps: 7, D loss: 0.319741, acc:  28%, G loss: 1.494174\n",
      "Ep: 785, steps: 8, D loss: 0.221924, acc:  65%, G loss: 1.675411\n",
      "Ep: 785, steps: 9, D loss: 0.242404, acc:  57%, G loss: 1.607088\n",
      "Ep: 785, steps: 10, D loss: 0.177952, acc:  79%, G loss: 1.517998\n",
      "Ep: 785, steps: 11, D loss: 0.250492, acc:  52%, G loss: 1.758795\n",
      "Ep: 785, steps: 12, D loss: 0.286377, acc:  35%, G loss: 1.366430\n",
      "Ep: 785, steps: 13, D loss: 0.287921, acc:  34%, G loss: 1.492406\n",
      "Saved Model\n",
      "Ep: 785, steps: 14, D loss: 0.256910, acc:  51%, G loss: 1.532062\n",
      "Ep: 785, steps: 15, D loss: 0.241029, acc:  56%, G loss: 1.546982\n",
      "Ep: 785, steps: 16, D loss: 0.233648, acc:  58%, G loss: 1.524371\n",
      "Ep: 785, steps: 17, D loss: 0.246419, acc:  56%, G loss: 1.633162\n",
      "Ep: 785, steps: 18, D loss: 0.226535, acc:  63%, G loss: 1.535090\n",
      "Ep: 785, steps: 19, D loss: 0.165097, acc:  83%, G loss: 1.689274\n",
      "Ep: 785, steps: 20, D loss: 0.273286, acc:  39%, G loss: 1.445431\n",
      "Ep: 785, steps: 21, D loss: 0.192934, acc:  67%, G loss: 1.508485\n",
      "Ep: 785, steps: 22, D loss: 0.234263, acc:  61%, G loss: 1.739610\n",
      "Ep: 785, steps: 23, D loss: 0.201223, acc:  73%, G loss: 1.547233\n",
      "Ep: 785, steps: 24, D loss: 0.245864, acc:  57%, G loss: 1.514370\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 786, steps: 1, D loss: 0.241834, acc:  57%, G loss: 1.716310\n",
      "Ep: 786, steps: 2, D loss: 0.249997, acc:  53%, G loss: 1.483889\n",
      "Ep: 786, steps: 3, D loss: 0.180318, acc:  79%, G loss: 1.823371\n",
      "Ep: 786, steps: 4, D loss: 0.174780, acc:  85%, G loss: 1.732388\n",
      "Ep: 786, steps: 5, D loss: 0.256716, acc:  56%, G loss: 1.673207\n",
      "Ep: 786, steps: 6, D loss: 0.242545, acc:  54%, G loss: 1.574715\n",
      "Ep: 786, steps: 7, D loss: 0.297494, acc:  38%, G loss: 1.460076\n",
      "Ep: 786, steps: 8, D loss: 0.234622, acc:  62%, G loss: 1.664287\n",
      "Ep: 786, steps: 9, D loss: 0.236160, acc:  58%, G loss: 1.604260\n",
      "Ep: 786, steps: 10, D loss: 0.185772, acc:  77%, G loss: 1.559952\n",
      "Ep: 786, steps: 11, D loss: 0.267630, acc:  47%, G loss: 1.710683\n",
      "Ep: 786, steps: 12, D loss: 0.291266, acc:  35%, G loss: 1.373225\n",
      "Ep: 786, steps: 13, D loss: 0.293257, acc:  32%, G loss: 1.423671\n",
      "Ep: 786, steps: 14, D loss: 0.263041, acc:  48%, G loss: 1.477846\n",
      "Ep: 786, steps: 15, D loss: 0.277678, acc:  38%, G loss: 1.535213\n",
      "Ep: 786, steps: 16, D loss: 0.259629, acc:  48%, G loss: 1.531570\n",
      "Ep: 786, steps: 17, D loss: 0.219901, acc:  68%, G loss: 1.531554\n",
      "Ep: 786, steps: 18, D loss: 0.230448, acc:  60%, G loss: 1.581769\n",
      "Ep: 786, steps: 19, D loss: 0.224319, acc:  63%, G loss: 1.539159\n",
      "Ep: 786, steps: 20, D loss: 0.200237, acc:  75%, G loss: 1.651360\n",
      "Ep: 786, steps: 21, D loss: 0.265778, acc:  38%, G loss: 1.396725\n",
      "Ep: 786, steps: 22, D loss: 0.184941, acc:  71%, G loss: 1.519639\n",
      "Ep: 786, steps: 23, D loss: 0.206189, acc:  72%, G loss: 1.792036\n",
      "Ep: 786, steps: 24, D loss: 0.196275, acc:  73%, G loss: 1.548996\n",
      "Ep: 786, steps: 25, D loss: 0.257685, acc:  53%, G loss: 1.562065\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 787, steps: 1, D loss: 0.218709, acc:  64%, G loss: 1.649921\n",
      "Ep: 787, steps: 2, D loss: 0.240433, acc:  58%, G loss: 1.569523\n",
      "Ep: 787, steps: 3, D loss: 0.169034, acc:  84%, G loss: 1.903072\n",
      "Ep: 787, steps: 4, D loss: 0.169512, acc:  85%, G loss: 1.761376\n",
      "Ep: 787, steps: 5, D loss: 0.270337, acc:  52%, G loss: 1.599375\n",
      "Ep: 787, steps: 6, D loss: 0.227875, acc:  58%, G loss: 1.534486\n",
      "Ep: 787, steps: 7, D loss: 0.307331, acc:  35%, G loss: 1.642402\n",
      "Ep: 787, steps: 8, D loss: 0.226719, acc:  64%, G loss: 1.749264\n",
      "Ep: 787, steps: 9, D loss: 0.236014, acc:  59%, G loss: 1.642382\n",
      "Ep: 787, steps: 10, D loss: 0.193599, acc:  73%, G loss: 1.543464\n",
      "Ep: 787, steps: 11, D loss: 0.277558, acc:  45%, G loss: 1.653622\n",
      "Ep: 787, steps: 12, D loss: 0.289612, acc:  38%, G loss: 1.358559\n",
      "Ep: 787, steps: 13, D loss: 0.295625, acc:  29%, G loss: 1.411097\n",
      "Ep: 787, steps: 14, D loss: 0.271738, acc:  45%, G loss: 1.442337\n",
      "Ep: 787, steps: 15, D loss: 0.265706, acc:  44%, G loss: 1.517186\n",
      "Ep: 787, steps: 16, D loss: 0.248880, acc:  54%, G loss: 1.584707\n",
      "Ep: 787, steps: 17, D loss: 0.223639, acc:  65%, G loss: 1.524189\n",
      "Ep: 787, steps: 18, D loss: 0.235204, acc:  59%, G loss: 1.625396\n",
      "Ep: 787, steps: 19, D loss: 0.221594, acc:  65%, G loss: 1.588451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 787, steps: 20, D loss: 0.185611, acc:  80%, G loss: 1.665840\n",
      "Ep: 787, steps: 21, D loss: 0.270781, acc:  38%, G loss: 1.555535\n",
      "Ep: 787, steps: 22, D loss: 0.202378, acc:  68%, G loss: 1.517553\n",
      "Ep: 787, steps: 23, D loss: 0.226249, acc:  63%, G loss: 1.746237\n",
      "Ep: 787, steps: 24, D loss: 0.212977, acc:  67%, G loss: 1.540075\n",
      "Ep: 787, steps: 25, D loss: 0.249575, acc:  54%, G loss: 1.559157\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 788, steps: 1, D loss: 0.233581, acc:  62%, G loss: 1.765482\n",
      "Ep: 788, steps: 2, D loss: 0.249326, acc:  53%, G loss: 1.438666\n",
      "Ep: 788, steps: 3, D loss: 0.175819, acc:  81%, G loss: 1.901134\n",
      "Ep: 788, steps: 4, D loss: 0.177263, acc:  82%, G loss: 1.672372\n",
      "Ep: 788, steps: 5, D loss: 0.275648, acc:  50%, G loss: 1.595931\n",
      "Ep: 788, steps: 6, D loss: 0.247001, acc:  54%, G loss: 1.531870\n",
      "Ep: 788, steps: 7, D loss: 0.314372, acc:  32%, G loss: 1.481144\n",
      "Ep: 788, steps: 8, D loss: 0.226890, acc:  64%, G loss: 1.712258\n",
      "Ep: 788, steps: 9, D loss: 0.232906, acc:  62%, G loss: 1.591480\n",
      "Ep: 788, steps: 10, D loss: 0.176044, acc:  81%, G loss: 1.542991\n",
      "Ep: 788, steps: 11, D loss: 0.267245, acc:  47%, G loss: 1.690635\n",
      "Saved Model\n",
      "Ep: 788, steps: 12, D loss: 0.292160, acc:  35%, G loss: 1.364925\n",
      "Ep: 788, steps: 13, D loss: 0.285938, acc:  39%, G loss: 1.443703\n",
      "Ep: 788, steps: 14, D loss: 0.257842, acc:  48%, G loss: 1.609216\n",
      "Ep: 788, steps: 15, D loss: 0.263956, acc:  46%, G loss: 1.550766\n",
      "Ep: 788, steps: 16, D loss: 0.233893, acc:  61%, G loss: 1.513590\n",
      "Ep: 788, steps: 17, D loss: 0.231074, acc:  61%, G loss: 1.608681\n",
      "Ep: 788, steps: 18, D loss: 0.207327, acc:  69%, G loss: 1.549441\n",
      "Ep: 788, steps: 19, D loss: 0.197871, acc:  75%, G loss: 1.665574\n",
      "Ep: 788, steps: 20, D loss: 0.264466, acc:  42%, G loss: 1.367988\n",
      "Ep: 788, steps: 21, D loss: 0.209889, acc:  65%, G loss: 1.575071\n",
      "Ep: 788, steps: 22, D loss: 0.216522, acc:  65%, G loss: 1.774041\n",
      "Ep: 788, steps: 23, D loss: 0.207483, acc:  69%, G loss: 1.505961\n",
      "Ep: 788, steps: 24, D loss: 0.239141, acc:  59%, G loss: 1.697351\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 789, steps: 1, D loss: 0.230797, acc:  61%, G loss: 1.649731\n",
      "Ep: 789, steps: 2, D loss: 0.239811, acc:  57%, G loss: 1.595846\n",
      "Ep: 789, steps: 3, D loss: 0.170426, acc:  81%, G loss: 1.875109\n",
      "Ep: 789, steps: 4, D loss: 0.169009, acc:  85%, G loss: 1.696300\n",
      "Ep: 789, steps: 5, D loss: 0.274992, acc:  50%, G loss: 1.645555\n",
      "Ep: 789, steps: 6, D loss: 0.255700, acc:  52%, G loss: 1.597976\n",
      "Ep: 789, steps: 7, D loss: 0.310954, acc:  30%, G loss: 1.407508\n",
      "Ep: 789, steps: 8, D loss: 0.212433, acc:  68%, G loss: 1.702128\n",
      "Ep: 789, steps: 9, D loss: 0.234390, acc:  61%, G loss: 1.596983\n",
      "Ep: 789, steps: 10, D loss: 0.193405, acc:  73%, G loss: 1.609913\n",
      "Ep: 789, steps: 11, D loss: 0.247089, acc:  55%, G loss: 1.675297\n",
      "Ep: 789, steps: 12, D loss: 0.290310, acc:  37%, G loss: 1.370902\n",
      "Ep: 789, steps: 13, D loss: 0.296227, acc:  31%, G loss: 1.341143\n",
      "Ep: 789, steps: 14, D loss: 0.260882, acc:  48%, G loss: 1.463833\n",
      "Ep: 789, steps: 15, D loss: 0.267580, acc:  45%, G loss: 1.526435\n",
      "Ep: 789, steps: 16, D loss: 0.256598, acc:  49%, G loss: 1.588321\n",
      "Ep: 789, steps: 17, D loss: 0.227888, acc:  62%, G loss: 1.571224\n",
      "Ep: 789, steps: 18, D loss: 0.240492, acc:  57%, G loss: 1.602442\n",
      "Ep: 789, steps: 19, D loss: 0.225179, acc:  62%, G loss: 1.572975\n",
      "Ep: 789, steps: 20, D loss: 0.196827, acc:  76%, G loss: 1.689537\n",
      "Ep: 789, steps: 21, D loss: 0.273130, acc:  38%, G loss: 1.426066\n",
      "Ep: 789, steps: 22, D loss: 0.184123, acc:  74%, G loss: 1.593096\n",
      "Ep: 789, steps: 23, D loss: 0.225528, acc:  64%, G loss: 1.730336\n",
      "Ep: 789, steps: 24, D loss: 0.207669, acc:  70%, G loss: 1.533038\n",
      "Ep: 789, steps: 25, D loss: 0.240336, acc:  57%, G loss: 1.555944\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 790, steps: 1, D loss: 0.234436, acc:  61%, G loss: 1.582593\n",
      "Ep: 790, steps: 2, D loss: 0.252951, acc:  53%, G loss: 1.462605\n",
      "Ep: 790, steps: 3, D loss: 0.180213, acc:  79%, G loss: 1.852888\n",
      "Ep: 790, steps: 4, D loss: 0.169919, acc:  87%, G loss: 1.694266\n",
      "Ep: 790, steps: 5, D loss: 0.282350, acc:  49%, G loss: 1.528059\n",
      "Ep: 790, steps: 6, D loss: 0.245932, acc:  54%, G loss: 1.543644\n",
      "Ep: 790, steps: 7, D loss: 0.301793, acc:  32%, G loss: 1.442823\n",
      "Ep: 790, steps: 8, D loss: 0.229972, acc:  60%, G loss: 1.663790\n",
      "Ep: 790, steps: 9, D loss: 0.250927, acc:  56%, G loss: 1.644736\n",
      "Ep: 790, steps: 10, D loss: 0.177497, acc:  80%, G loss: 1.510763\n",
      "Ep: 790, steps: 11, D loss: 0.274542, acc:  47%, G loss: 1.715943\n",
      "Ep: 790, steps: 12, D loss: 0.287408, acc:  34%, G loss: 1.373948\n",
      "Ep: 790, steps: 13, D loss: 0.282076, acc:  34%, G loss: 1.411347\n",
      "Ep: 790, steps: 14, D loss: 0.276480, acc:  44%, G loss: 1.480125\n",
      "Ep: 790, steps: 15, D loss: 0.265607, acc:  47%, G loss: 1.563541\n",
      "Ep: 790, steps: 16, D loss: 0.243252, acc:  54%, G loss: 1.543569\n",
      "Ep: 790, steps: 17, D loss: 0.222827, acc:  67%, G loss: 1.505540\n",
      "Ep: 790, steps: 18, D loss: 0.228135, acc:  62%, G loss: 1.594154\n",
      "Ep: 790, steps: 19, D loss: 0.206030, acc:  69%, G loss: 1.497862\n",
      "Ep: 790, steps: 20, D loss: 0.181544, acc:  79%, G loss: 1.641162\n",
      "Ep: 790, steps: 21, D loss: 0.269031, acc:  40%, G loss: 1.437677\n",
      "Ep: 790, steps: 22, D loss: 0.234614, acc:  58%, G loss: 1.508474\n",
      "Ep: 790, steps: 23, D loss: 0.227117, acc:  65%, G loss: 1.749939\n",
      "Ep: 790, steps: 24, D loss: 0.215914, acc:  68%, G loss: 1.612049\n",
      "Ep: 790, steps: 25, D loss: 0.247664, acc:  55%, G loss: 1.452364\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 791, steps: 1, D loss: 0.224307, acc:  65%, G loss: 1.681094\n",
      "Ep: 791, steps: 2, D loss: 0.239668, acc:  57%, G loss: 1.422572\n",
      "Ep: 791, steps: 3, D loss: 0.174026, acc:  81%, G loss: 1.902914\n",
      "Ep: 791, steps: 4, D loss: 0.183868, acc:  82%, G loss: 1.629606\n",
      "Ep: 791, steps: 5, D loss: 0.255206, acc:  56%, G loss: 1.603456\n",
      "Ep: 791, steps: 6, D loss: 0.246758, acc:  53%, G loss: 1.534714\n",
      "Ep: 791, steps: 7, D loss: 0.309171, acc:  32%, G loss: 1.410388\n",
      "Ep: 791, steps: 8, D loss: 0.221462, acc:  65%, G loss: 1.689137\n",
      "Ep: 791, steps: 9, D loss: 0.235469, acc:  61%, G loss: 1.547894\n",
      "Saved Model\n",
      "Ep: 791, steps: 10, D loss: 0.188137, acc:  74%, G loss: 1.485558\n",
      "Ep: 791, steps: 11, D loss: 0.264623, acc:  46%, G loss: 1.391207\n",
      "Ep: 791, steps: 12, D loss: 0.262630, acc:  46%, G loss: 1.458608\n",
      "Ep: 791, steps: 13, D loss: 0.259599, acc:  50%, G loss: 1.484078\n",
      "Ep: 791, steps: 14, D loss: 0.336335, acc:  28%, G loss: 1.361040\n",
      "Ep: 791, steps: 15, D loss: 0.256316, acc:  52%, G loss: 1.551214\n",
      "Ep: 791, steps: 16, D loss: 0.222386, acc:  65%, G loss: 1.488208\n",
      "Ep: 791, steps: 17, D loss: 0.253076, acc:  51%, G loss: 1.604205\n",
      "Ep: 791, steps: 18, D loss: 0.238501, acc:  59%, G loss: 1.507194\n",
      "Ep: 791, steps: 19, D loss: 0.184544, acc:  80%, G loss: 1.687320\n",
      "Ep: 791, steps: 20, D loss: 0.256694, acc:  44%, G loss: 1.445585\n",
      "Ep: 791, steps: 21, D loss: 0.206049, acc:  65%, G loss: 1.564447\n",
      "Ep: 791, steps: 22, D loss: 0.222638, acc:  63%, G loss: 1.775434\n",
      "Ep: 791, steps: 23, D loss: 0.202172, acc:  74%, G loss: 1.528504\n",
      "Ep: 791, steps: 24, D loss: 0.253125, acc:  55%, G loss: 1.608911\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 792, steps: 1, D loss: 0.230354, acc:  61%, G loss: 1.764666\n",
      "Ep: 792, steps: 2, D loss: 0.238498, acc:  57%, G loss: 1.473974\n",
      "Ep: 792, steps: 3, D loss: 0.166054, acc:  86%, G loss: 1.917526\n",
      "Ep: 792, steps: 4, D loss: 0.184108, acc:  80%, G loss: 1.732065\n",
      "Ep: 792, steps: 5, D loss: 0.291515, acc:  45%, G loss: 1.586320\n",
      "Ep: 792, steps: 6, D loss: 0.239398, acc:  54%, G loss: 1.569719\n",
      "Ep: 792, steps: 7, D loss: 0.299649, acc:  34%, G loss: 1.460258\n",
      "Ep: 792, steps: 8, D loss: 0.210392, acc:  67%, G loss: 1.705284\n",
      "Ep: 792, steps: 9, D loss: 0.240583, acc:  59%, G loss: 1.541999\n",
      "Ep: 792, steps: 10, D loss: 0.194251, acc:  74%, G loss: 1.484545\n",
      "Ep: 792, steps: 11, D loss: 0.265803, acc:  47%, G loss: 1.683785\n",
      "Ep: 792, steps: 12, D loss: 0.293418, acc:  36%, G loss: 1.362399\n",
      "Ep: 792, steps: 13, D loss: 0.296473, acc:  31%, G loss: 1.396188\n",
      "Ep: 792, steps: 14, D loss: 0.275660, acc:  43%, G loss: 1.476290\n",
      "Ep: 792, steps: 15, D loss: 0.260339, acc:  48%, G loss: 1.506037\n",
      "Ep: 792, steps: 16, D loss: 0.243010, acc:  55%, G loss: 1.542397\n",
      "Ep: 792, steps: 17, D loss: 0.224967, acc:  64%, G loss: 1.532481\n",
      "Ep: 792, steps: 18, D loss: 0.234952, acc:  57%, G loss: 1.606812\n",
      "Ep: 792, steps: 19, D loss: 0.223827, acc:  63%, G loss: 1.558115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 792, steps: 20, D loss: 0.187912, acc:  77%, G loss: 1.704156\n",
      "Ep: 792, steps: 21, D loss: 0.267125, acc:  41%, G loss: 1.424371\n",
      "Ep: 792, steps: 22, D loss: 0.211154, acc:  64%, G loss: 1.586487\n",
      "Ep: 792, steps: 23, D loss: 0.210356, acc:  69%, G loss: 1.781490\n",
      "Ep: 792, steps: 24, D loss: 0.211402, acc:  69%, G loss: 1.531270\n",
      "Ep: 792, steps: 25, D loss: 0.260925, acc:  51%, G loss: 1.527754\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 793, steps: 1, D loss: 0.226135, acc:  63%, G loss: 1.673539\n",
      "Ep: 793, steps: 2, D loss: 0.248445, acc:  54%, G loss: 1.485584\n",
      "Ep: 793, steps: 3, D loss: 0.177268, acc:  81%, G loss: 1.952864\n",
      "Ep: 793, steps: 4, D loss: 0.178515, acc:  81%, G loss: 1.708714\n",
      "Ep: 793, steps: 5, D loss: 0.283150, acc:  48%, G loss: 1.549284\n",
      "Ep: 793, steps: 6, D loss: 0.233468, acc:  57%, G loss: 1.561784\n",
      "Ep: 793, steps: 7, D loss: 0.303501, acc:  33%, G loss: 1.484449\n",
      "Ep: 793, steps: 8, D loss: 0.226104, acc:  64%, G loss: 1.678511\n",
      "Ep: 793, steps: 9, D loss: 0.243710, acc:  58%, G loss: 1.582258\n",
      "Ep: 793, steps: 10, D loss: 0.186753, acc:  77%, G loss: 1.489310\n",
      "Ep: 793, steps: 11, D loss: 0.256209, acc:  51%, G loss: 1.671259\n",
      "Ep: 793, steps: 12, D loss: 0.284144, acc:  38%, G loss: 1.349500\n",
      "Ep: 793, steps: 13, D loss: 0.283725, acc:  36%, G loss: 1.432391\n",
      "Ep: 793, steps: 14, D loss: 0.271697, acc:  46%, G loss: 1.500429\n",
      "Ep: 793, steps: 15, D loss: 0.279535, acc:  41%, G loss: 1.578060\n",
      "Ep: 793, steps: 16, D loss: 0.235595, acc:  59%, G loss: 1.552060\n",
      "Ep: 793, steps: 17, D loss: 0.230094, acc:  63%, G loss: 1.549153\n",
      "Ep: 793, steps: 18, D loss: 0.234738, acc:  59%, G loss: 1.625505\n",
      "Ep: 793, steps: 19, D loss: 0.209914, acc:  68%, G loss: 1.566354\n",
      "Ep: 793, steps: 20, D loss: 0.182100, acc:  80%, G loss: 1.647267\n",
      "Ep: 793, steps: 21, D loss: 0.271344, acc:  38%, G loss: 1.421423\n",
      "Ep: 793, steps: 22, D loss: 0.214262, acc:  65%, G loss: 1.527776\n",
      "Ep: 793, steps: 23, D loss: 0.207835, acc:  71%, G loss: 1.756973\n",
      "Ep: 793, steps: 24, D loss: 0.218435, acc:  65%, G loss: 1.568273\n",
      "Ep: 793, steps: 25, D loss: 0.260030, acc:  51%, G loss: 1.631553\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 794, steps: 1, D loss: 0.235678, acc:  59%, G loss: 1.609244\n",
      "Ep: 794, steps: 2, D loss: 0.236516, acc:  59%, G loss: 1.462993\n",
      "Ep: 794, steps: 3, D loss: 0.175133, acc:  81%, G loss: 1.874382\n",
      "Ep: 794, steps: 4, D loss: 0.179872, acc:  81%, G loss: 1.702697\n",
      "Ep: 794, steps: 5, D loss: 0.265503, acc:  53%, G loss: 1.617182\n",
      "Ep: 794, steps: 6, D loss: 0.237770, acc:  58%, G loss: 1.575138\n",
      "Ep: 794, steps: 7, D loss: 0.304983, acc:  35%, G loss: 1.402947\n",
      "Saved Model\n",
      "Ep: 794, steps: 8, D loss: 0.227609, acc:  63%, G loss: 1.696230\n",
      "Ep: 794, steps: 9, D loss: 0.192966, acc:  76%, G loss: 1.577116\n",
      "Ep: 794, steps: 10, D loss: 0.288654, acc:  40%, G loss: 1.559778\n",
      "Ep: 794, steps: 11, D loss: 0.270821, acc:  43%, G loss: 1.371875\n",
      "Ep: 794, steps: 12, D loss: 0.283333, acc:  36%, G loss: 1.445007\n",
      "Ep: 794, steps: 13, D loss: 0.250715, acc:  53%, G loss: 1.499995\n",
      "Ep: 794, steps: 14, D loss: 0.282667, acc:  38%, G loss: 1.504377\n",
      "Ep: 794, steps: 15, D loss: 0.257924, acc:  49%, G loss: 1.533717\n",
      "Ep: 794, steps: 16, D loss: 0.225174, acc:  64%, G loss: 1.481870\n",
      "Ep: 794, steps: 17, D loss: 0.227548, acc:  62%, G loss: 1.567217\n",
      "Ep: 794, steps: 18, D loss: 0.221156, acc:  66%, G loss: 1.511681\n",
      "Ep: 794, steps: 19, D loss: 0.188917, acc:  79%, G loss: 1.674806\n",
      "Ep: 794, steps: 20, D loss: 0.268420, acc:  39%, G loss: 1.686327\n",
      "Ep: 794, steps: 21, D loss: 0.194860, acc:  70%, G loss: 1.718158\n",
      "Ep: 794, steps: 22, D loss: 0.225563, acc:  63%, G loss: 1.797892\n",
      "Ep: 794, steps: 23, D loss: 0.221585, acc:  65%, G loss: 1.539152\n",
      "Ep: 794, steps: 24, D loss: 0.243477, acc:  56%, G loss: 1.680302\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 795, steps: 1, D loss: 0.228911, acc:  62%, G loss: 1.597158\n",
      "Ep: 795, steps: 2, D loss: 0.243052, acc:  55%, G loss: 1.433315\n",
      "Ep: 795, steps: 3, D loss: 0.184336, acc:  79%, G loss: 1.880297\n",
      "Ep: 795, steps: 4, D loss: 0.180409, acc:  81%, G loss: 1.673573\n",
      "Ep: 795, steps: 5, D loss: 0.287328, acc:  49%, G loss: 1.647565\n",
      "Ep: 795, steps: 6, D loss: 0.245305, acc:  55%, G loss: 1.597227\n",
      "Ep: 795, steps: 7, D loss: 0.296180, acc:  37%, G loss: 1.487078\n",
      "Ep: 795, steps: 8, D loss: 0.218636, acc:  66%, G loss: 1.697266\n",
      "Ep: 795, steps: 9, D loss: 0.263931, acc:  50%, G loss: 1.545222\n",
      "Ep: 795, steps: 10, D loss: 0.195772, acc:  73%, G loss: 1.494786\n",
      "Ep: 795, steps: 11, D loss: 0.264629, acc:  48%, G loss: 1.647937\n",
      "Ep: 795, steps: 12, D loss: 0.284448, acc:  37%, G loss: 1.341766\n",
      "Ep: 795, steps: 13, D loss: 0.290900, acc:  32%, G loss: 1.413701\n",
      "Ep: 795, steps: 14, D loss: 0.260516, acc:  49%, G loss: 1.443869\n",
      "Ep: 795, steps: 15, D loss: 0.262889, acc:  46%, G loss: 1.489256\n",
      "Ep: 795, steps: 16, D loss: 0.240195, acc:  57%, G loss: 1.568949\n",
      "Ep: 795, steps: 17, D loss: 0.232595, acc:  60%, G loss: 1.595420\n",
      "Ep: 795, steps: 18, D loss: 0.235973, acc:  58%, G loss: 1.610363\n",
      "Ep: 795, steps: 19, D loss: 0.227557, acc:  62%, G loss: 1.564959\n",
      "Ep: 795, steps: 20, D loss: 0.188313, acc:  78%, G loss: 1.661008\n",
      "Ep: 795, steps: 21, D loss: 0.257982, acc:  45%, G loss: 1.551435\n",
      "Ep: 795, steps: 22, D loss: 0.217638, acc:  63%, G loss: 1.654954\n",
      "Ep: 795, steps: 23, D loss: 0.218570, acc:  66%, G loss: 1.751158\n",
      "Ep: 795, steps: 24, D loss: 0.212347, acc:  68%, G loss: 1.548865\n",
      "Ep: 795, steps: 25, D loss: 0.251083, acc:  55%, G loss: 1.484660\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 796, steps: 1, D loss: 0.244101, acc:  56%, G loss: 1.645424\n",
      "Ep: 796, steps: 2, D loss: 0.244749, acc:  58%, G loss: 1.498138\n",
      "Ep: 796, steps: 3, D loss: 0.195622, acc:  73%, G loss: 1.931698\n",
      "Ep: 796, steps: 4, D loss: 0.179847, acc:  82%, G loss: 1.694467\n",
      "Ep: 796, steps: 5, D loss: 0.284574, acc:  45%, G loss: 1.579835\n",
      "Ep: 796, steps: 6, D loss: 0.241177, acc:  56%, G loss: 1.516173\n",
      "Ep: 796, steps: 7, D loss: 0.304191, acc:  33%, G loss: 1.530526\n",
      "Ep: 796, steps: 8, D loss: 0.218620, acc:  67%, G loss: 1.718258\n",
      "Ep: 796, steps: 9, D loss: 0.234273, acc:  61%, G loss: 1.564100\n",
      "Ep: 796, steps: 10, D loss: 0.177342, acc:  79%, G loss: 1.538940\n",
      "Ep: 796, steps: 11, D loss: 0.261236, acc:  48%, G loss: 1.665460\n",
      "Ep: 796, steps: 12, D loss: 0.285276, acc:  39%, G loss: 1.290180\n",
      "Ep: 796, steps: 13, D loss: 0.284280, acc:  36%, G loss: 1.411531\n",
      "Ep: 796, steps: 14, D loss: 0.266760, acc:  46%, G loss: 1.482064\n",
      "Ep: 796, steps: 15, D loss: 0.268326, acc:  45%, G loss: 1.491006\n",
      "Ep: 796, steps: 16, D loss: 0.241049, acc:  58%, G loss: 1.543684\n",
      "Ep: 796, steps: 17, D loss: 0.221674, acc:  66%, G loss: 1.480811\n",
      "Ep: 796, steps: 18, D loss: 0.245592, acc:  54%, G loss: 1.573288\n",
      "Ep: 796, steps: 19, D loss: 0.215157, acc:  67%, G loss: 1.546234\n",
      "Ep: 796, steps: 20, D loss: 0.194024, acc:  77%, G loss: 1.713248\n",
      "Ep: 796, steps: 21, D loss: 0.263281, acc:  43%, G loss: 1.403439\n",
      "Ep: 796, steps: 22, D loss: 0.181571, acc:  73%, G loss: 1.603916\n",
      "Ep: 796, steps: 23, D loss: 0.221283, acc:  63%, G loss: 1.860503\n",
      "Ep: 796, steps: 24, D loss: 0.208212, acc:  70%, G loss: 1.586741\n",
      "Ep: 796, steps: 25, D loss: 0.254086, acc:  56%, G loss: 1.673031\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 797, steps: 1, D loss: 0.223531, acc:  63%, G loss: 1.684474\n",
      "Ep: 797, steps: 2, D loss: 0.239378, acc:  58%, G loss: 1.500181\n",
      "Ep: 797, steps: 3, D loss: 0.165252, acc:  84%, G loss: 1.871913\n",
      "Ep: 797, steps: 4, D loss: 0.169189, acc:  84%, G loss: 1.740631\n",
      "Ep: 797, steps: 5, D loss: 0.271956, acc:  52%, G loss: 1.522340\n",
      "Saved Model\n",
      "Ep: 797, steps: 6, D loss: 0.238261, acc:  54%, G loss: 1.534354\n",
      "Ep: 797, steps: 7, D loss: 0.239641, acc:  56%, G loss: 1.623870\n",
      "Ep: 797, steps: 8, D loss: 0.199340, acc:  73%, G loss: 1.688829\n",
      "Ep: 797, steps: 9, D loss: 0.168396, acc:  81%, G loss: 1.597361\n",
      "Ep: 797, steps: 10, D loss: 0.267246, acc:  49%, G loss: 1.688733\n",
      "Ep: 797, steps: 11, D loss: 0.317975, acc:  30%, G loss: 1.329751\n",
      "Ep: 797, steps: 12, D loss: 0.302981, acc:  29%, G loss: 1.379538\n",
      "Ep: 797, steps: 13, D loss: 0.292175, acc:  38%, G loss: 1.455578\n",
      "Ep: 797, steps: 14, D loss: 0.250381, acc:  51%, G loss: 1.534966\n",
      "Ep: 797, steps: 15, D loss: 0.254834, acc:  50%, G loss: 1.559967\n",
      "Ep: 797, steps: 16, D loss: 0.223299, acc:  63%, G loss: 1.495072\n",
      "Ep: 797, steps: 17, D loss: 0.237716, acc:  57%, G loss: 1.559658\n",
      "Ep: 797, steps: 18, D loss: 0.203821, acc:  69%, G loss: 1.639549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 797, steps: 19, D loss: 0.186552, acc:  75%, G loss: 1.707014\n",
      "Ep: 797, steps: 20, D loss: 0.276687, acc:  36%, G loss: 1.449625\n",
      "Ep: 797, steps: 21, D loss: 0.196597, acc:  66%, G loss: 1.745422\n",
      "Ep: 797, steps: 22, D loss: 0.210297, acc:  70%, G loss: 1.844674\n",
      "Ep: 797, steps: 23, D loss: 0.222855, acc:  63%, G loss: 1.562041\n",
      "Ep: 797, steps: 24, D loss: 0.243096, acc:  56%, G loss: 1.675935\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 798, steps: 1, D loss: 0.238068, acc:  59%, G loss: 1.627345\n",
      "Ep: 798, steps: 2, D loss: 0.240075, acc:  58%, G loss: 1.399894\n",
      "Ep: 798, steps: 3, D loss: 0.183482, acc:  78%, G loss: 1.907999\n",
      "Ep: 798, steps: 4, D loss: 0.168255, acc:  86%, G loss: 1.659682\n",
      "Ep: 798, steps: 5, D loss: 0.263117, acc:  53%, G loss: 1.581727\n",
      "Ep: 798, steps: 6, D loss: 0.251357, acc:  52%, G loss: 1.540532\n",
      "Ep: 798, steps: 7, D loss: 0.327452, acc:  29%, G loss: 1.370654\n",
      "Ep: 798, steps: 8, D loss: 0.209526, acc:  70%, G loss: 1.663446\n",
      "Ep: 798, steps: 9, D loss: 0.248219, acc:  55%, G loss: 1.546750\n",
      "Ep: 798, steps: 10, D loss: 0.175739, acc:  81%, G loss: 1.531415\n",
      "Ep: 798, steps: 11, D loss: 0.259375, acc:  53%, G loss: 1.704649\n",
      "Ep: 798, steps: 12, D loss: 0.297602, acc:  32%, G loss: 1.344152\n",
      "Ep: 798, steps: 13, D loss: 0.284386, acc:  35%, G loss: 1.387250\n",
      "Ep: 798, steps: 14, D loss: 0.264672, acc:  49%, G loss: 1.444216\n",
      "Ep: 798, steps: 15, D loss: 0.284545, acc:  37%, G loss: 1.498538\n",
      "Ep: 798, steps: 16, D loss: 0.236273, acc:  60%, G loss: 1.586328\n",
      "Ep: 798, steps: 17, D loss: 0.234628, acc:  58%, G loss: 1.467514\n",
      "Ep: 798, steps: 18, D loss: 0.225191, acc:  62%, G loss: 1.577042\n",
      "Ep: 798, steps: 19, D loss: 0.218279, acc:  65%, G loss: 1.541983\n",
      "Ep: 798, steps: 20, D loss: 0.187430, acc:  76%, G loss: 1.689057\n",
      "Ep: 798, steps: 21, D loss: 0.267362, acc:  39%, G loss: 1.568778\n",
      "Ep: 798, steps: 22, D loss: 0.199162, acc:  68%, G loss: 1.674049\n",
      "Ep: 798, steps: 23, D loss: 0.217992, acc:  66%, G loss: 1.798924\n",
      "Ep: 798, steps: 24, D loss: 0.205069, acc:  70%, G loss: 1.624169\n",
      "Ep: 798, steps: 25, D loss: 0.237336, acc:  58%, G loss: 1.594153\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 799, steps: 1, D loss: 0.203771, acc:  73%, G loss: 1.614794\n",
      "Ep: 799, steps: 2, D loss: 0.249640, acc:  54%, G loss: 1.360052\n",
      "Ep: 799, steps: 3, D loss: 0.168243, acc:  82%, G loss: 1.843380\n",
      "Ep: 799, steps: 4, D loss: 0.168530, acc:  84%, G loss: 1.732010\n",
      "Ep: 799, steps: 5, D loss: 0.301352, acc:  45%, G loss: 1.528639\n",
      "Ep: 799, steps: 6, D loss: 0.257078, acc:  53%, G loss: 1.496250\n",
      "Ep: 799, steps: 7, D loss: 0.311029, acc:  30%, G loss: 1.504756\n",
      "Ep: 799, steps: 8, D loss: 0.223795, acc:  66%, G loss: 1.643460\n",
      "Ep: 799, steps: 9, D loss: 0.264693, acc:  51%, G loss: 1.574744\n",
      "Ep: 799, steps: 10, D loss: 0.190100, acc:  76%, G loss: 1.484849\n",
      "Ep: 799, steps: 11, D loss: 0.258132, acc:  48%, G loss: 1.644537\n",
      "Ep: 799, steps: 12, D loss: 0.279820, acc:  38%, G loss: 1.385700\n",
      "Ep: 799, steps: 13, D loss: 0.277096, acc:  38%, G loss: 1.389897\n",
      "Ep: 799, steps: 14, D loss: 0.272051, acc:  45%, G loss: 1.458340\n",
      "Ep: 799, steps: 15, D loss: 0.284548, acc:  40%, G loss: 1.537647\n",
      "Ep: 799, steps: 16, D loss: 0.255939, acc:  50%, G loss: 1.533202\n",
      "Ep: 799, steps: 17, D loss: 0.223189, acc:  66%, G loss: 1.472837\n",
      "Ep: 799, steps: 18, D loss: 0.238614, acc:  56%, G loss: 1.569796\n",
      "Ep: 799, steps: 19, D loss: 0.225056, acc:  64%, G loss: 1.506038\n",
      "Ep: 799, steps: 20, D loss: 0.183754, acc:  78%, G loss: 1.666990\n",
      "Ep: 799, steps: 21, D loss: 0.263149, acc:  42%, G loss: 1.398338\n",
      "Ep: 799, steps: 22, D loss: 0.204233, acc:  65%, G loss: 1.559200\n",
      "Ep: 799, steps: 23, D loss: 0.226816, acc:  63%, G loss: 1.824678\n",
      "Ep: 799, steps: 24, D loss: 0.211814, acc:  68%, G loss: 1.581749\n",
      "Ep: 799, steps: 25, D loss: 0.237647, acc:  58%, G loss: 1.552659\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 800, steps: 1, D loss: 0.217926, acc:  68%, G loss: 1.590729\n",
      "Ep: 800, steps: 2, D loss: 0.249051, acc:  54%, G loss: 1.375462\n",
      "Ep: 800, steps: 3, D loss: 0.162109, acc:  86%, G loss: 1.830323\n",
      "Saved Model\n",
      "Ep: 800, steps: 4, D loss: 0.173445, acc:  83%, G loss: 1.681726\n",
      "Ep: 800, steps: 5, D loss: 0.248325, acc:  54%, G loss: 1.580450\n",
      "Ep: 800, steps: 6, D loss: 0.377357, acc:  20%, G loss: 1.428146\n",
      "Ep: 800, steps: 7, D loss: 0.208760, acc:  71%, G loss: 1.666119\n",
      "Ep: 800, steps: 8, D loss: 0.251598, acc:  53%, G loss: 1.560728\n",
      "Ep: 800, steps: 9, D loss: 0.183585, acc:  76%, G loss: 1.433199\n",
      "Ep: 800, steps: 10, D loss: 0.274016, acc:  48%, G loss: 1.644890\n",
      "Ep: 800, steps: 11, D loss: 0.296627, acc:  36%, G loss: 1.341381\n",
      "Ep: 800, steps: 12, D loss: 0.303374, acc:  30%, G loss: 1.324190\n",
      "Ep: 800, steps: 13, D loss: 0.285288, acc:  38%, G loss: 1.441546\n",
      "Ep: 800, steps: 14, D loss: 0.255594, acc:  50%, G loss: 1.536005\n",
      "Ep: 800, steps: 15, D loss: 0.243999, acc:  56%, G loss: 1.566515\n",
      "Ep: 800, steps: 16, D loss: 0.210615, acc:  71%, G loss: 1.470732\n",
      "Ep: 800, steps: 17, D loss: 0.232038, acc:  60%, G loss: 1.550019\n",
      "Ep: 800, steps: 18, D loss: 0.215421, acc:  69%, G loss: 1.543383\n",
      "Ep: 800, steps: 19, D loss: 0.212733, acc:  72%, G loss: 1.619994\n",
      "Ep: 800, steps: 20, D loss: 0.275514, acc:  36%, G loss: 1.454404\n",
      "Ep: 800, steps: 21, D loss: 0.221355, acc:  63%, G loss: 1.621101\n",
      "Ep: 800, steps: 22, D loss: 0.216586, acc:  67%, G loss: 1.839021\n",
      "Ep: 800, steps: 23, D loss: 0.219655, acc:  66%, G loss: 1.508072\n",
      "Ep: 800, steps: 24, D loss: 0.249054, acc:  55%, G loss: 1.495653\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 801, steps: 1, D loss: 0.213536, acc:  67%, G loss: 1.574208\n",
      "Ep: 801, steps: 2, D loss: 0.234990, acc:  59%, G loss: 1.415408\n",
      "Ep: 801, steps: 3, D loss: 0.181356, acc:  77%, G loss: 1.821050\n",
      "Ep: 801, steps: 4, D loss: 0.179588, acc:  84%, G loss: 1.664866\n",
      "Ep: 801, steps: 5, D loss: 0.284617, acc:  48%, G loss: 1.578851\n",
      "Ep: 801, steps: 6, D loss: 0.225150, acc:  59%, G loss: 1.499635\n",
      "Ep: 801, steps: 7, D loss: 0.300169, acc:  34%, G loss: 1.458207\n",
      "Ep: 801, steps: 8, D loss: 0.235037, acc:  62%, G loss: 1.685815\n",
      "Ep: 801, steps: 9, D loss: 0.240739, acc:  59%, G loss: 1.570012\n",
      "Ep: 801, steps: 10, D loss: 0.188968, acc:  73%, G loss: 1.476029\n",
      "Ep: 801, steps: 11, D loss: 0.257774, acc:  49%, G loss: 1.629062\n",
      "Ep: 801, steps: 12, D loss: 0.287920, acc:  38%, G loss: 1.336123\n",
      "Ep: 801, steps: 13, D loss: 0.293743, acc:  31%, G loss: 1.419597\n",
      "Ep: 801, steps: 14, D loss: 0.276106, acc:  45%, G loss: 1.437837\n",
      "Ep: 801, steps: 15, D loss: 0.272530, acc:  43%, G loss: 1.525197\n",
      "Ep: 801, steps: 16, D loss: 0.237741, acc:  59%, G loss: 1.594602\n",
      "Ep: 801, steps: 17, D loss: 0.225071, acc:  66%, G loss: 1.492286\n",
      "Ep: 801, steps: 18, D loss: 0.230875, acc:  61%, G loss: 1.577108\n",
      "Ep: 801, steps: 19, D loss: 0.221205, acc:  64%, G loss: 1.508365\n",
      "Ep: 801, steps: 20, D loss: 0.179420, acc:  81%, G loss: 1.623272\n",
      "Ep: 801, steps: 21, D loss: 0.266747, acc:  41%, G loss: 1.404570\n",
      "Ep: 801, steps: 22, D loss: 0.211196, acc:  63%, G loss: 1.641505\n",
      "Ep: 801, steps: 23, D loss: 0.227448, acc:  63%, G loss: 1.777594\n",
      "Ep: 801, steps: 24, D loss: 0.211460, acc:  69%, G loss: 1.602530\n",
      "Ep: 801, steps: 25, D loss: 0.247775, acc:  55%, G loss: 1.438466\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 802, steps: 1, D loss: 0.234808, acc:  60%, G loss: 1.622276\n",
      "Ep: 802, steps: 2, D loss: 0.243902, acc:  56%, G loss: 1.426691\n",
      "Ep: 802, steps: 3, D loss: 0.172257, acc:  81%, G loss: 1.838462\n",
      "Ep: 802, steps: 4, D loss: 0.188621, acc:  78%, G loss: 1.638353\n",
      "Ep: 802, steps: 5, D loss: 0.283987, acc:  48%, G loss: 1.604076\n",
      "Ep: 802, steps: 6, D loss: 0.258650, acc:  51%, G loss: 1.524140\n",
      "Ep: 802, steps: 7, D loss: 0.294530, acc:  39%, G loss: 1.435145\n",
      "Ep: 802, steps: 8, D loss: 0.223323, acc:  65%, G loss: 1.717849\n",
      "Ep: 802, steps: 9, D loss: 0.240165, acc:  58%, G loss: 1.551279\n",
      "Ep: 802, steps: 10, D loss: 0.189576, acc:  73%, G loss: 1.504117\n",
      "Ep: 802, steps: 11, D loss: 0.268874, acc:  45%, G loss: 1.684315\n",
      "Ep: 802, steps: 12, D loss: 0.291379, acc:  36%, G loss: 1.359951\n",
      "Ep: 802, steps: 13, D loss: 0.285269, acc:  34%, G loss: 1.354785\n",
      "Ep: 802, steps: 14, D loss: 0.267132, acc:  44%, G loss: 1.472689\n",
      "Ep: 802, steps: 15, D loss: 0.251830, acc:  51%, G loss: 1.507999\n",
      "Ep: 802, steps: 16, D loss: 0.252614, acc:  52%, G loss: 1.558311\n",
      "Ep: 802, steps: 17, D loss: 0.229345, acc:  63%, G loss: 1.489824\n",
      "Ep: 802, steps: 18, D loss: 0.245756, acc:  55%, G loss: 1.571773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 802, steps: 19, D loss: 0.207181, acc:  68%, G loss: 1.571361\n",
      "Ep: 802, steps: 20, D loss: 0.178642, acc:  79%, G loss: 1.635834\n",
      "Ep: 802, steps: 21, D loss: 0.261269, acc:  42%, G loss: 1.411399\n",
      "Ep: 802, steps: 22, D loss: 0.210495, acc:  63%, G loss: 1.586164\n",
      "Ep: 802, steps: 23, D loss: 0.226767, acc:  62%, G loss: 1.854665\n",
      "Ep: 802, steps: 24, D loss: 0.225575, acc:  61%, G loss: 1.552586\n",
      "Ep: 802, steps: 25, D loss: 0.253107, acc:  54%, G loss: 1.600736\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 803, steps: 1, D loss: 0.212953, acc:  70%, G loss: 1.686390\n",
      "Saved Model\n",
      "Ep: 803, steps: 2, D loss: 0.223774, acc:  65%, G loss: 1.429988\n",
      "Ep: 803, steps: 3, D loss: 0.186223, acc:  80%, G loss: 1.644042\n",
      "Ep: 803, steps: 4, D loss: 0.266441, acc:  52%, G loss: 1.614490\n",
      "Ep: 803, steps: 5, D loss: 0.231372, acc:  57%, G loss: 1.505082\n",
      "Ep: 803, steps: 6, D loss: 0.310564, acc:  32%, G loss: 1.478748\n",
      "Ep: 803, steps: 7, D loss: 0.240045, acc:  60%, G loss: 1.649133\n",
      "Ep: 803, steps: 8, D loss: 0.239526, acc:  59%, G loss: 1.602823\n",
      "Ep: 803, steps: 9, D loss: 0.186740, acc:  77%, G loss: 1.576488\n",
      "Ep: 803, steps: 10, D loss: 0.263948, acc:  46%, G loss: 1.678103\n",
      "Ep: 803, steps: 11, D loss: 0.289511, acc:  34%, G loss: 1.300948\n",
      "Ep: 803, steps: 12, D loss: 0.289140, acc:  33%, G loss: 1.338779\n",
      "Ep: 803, steps: 13, D loss: 0.256813, acc:  49%, G loss: 1.426518\n",
      "Ep: 803, steps: 14, D loss: 0.258322, acc:  51%, G loss: 1.508546\n",
      "Ep: 803, steps: 15, D loss: 0.248083, acc:  54%, G loss: 1.530478\n",
      "Ep: 803, steps: 16, D loss: 0.226400, acc:  64%, G loss: 1.505182\n",
      "Ep: 803, steps: 17, D loss: 0.234228, acc:  59%, G loss: 1.617965\n",
      "Ep: 803, steps: 18, D loss: 0.222213, acc:  64%, G loss: 1.543613\n",
      "Ep: 803, steps: 19, D loss: 0.196088, acc:  77%, G loss: 1.605609\n",
      "Ep: 803, steps: 20, D loss: 0.269550, acc:  39%, G loss: 1.432725\n",
      "Ep: 803, steps: 21, D loss: 0.221136, acc:  60%, G loss: 1.518188\n",
      "Ep: 803, steps: 22, D loss: 0.214265, acc:  67%, G loss: 1.841394\n",
      "Ep: 803, steps: 23, D loss: 0.222567, acc:  65%, G loss: 1.545223\n",
      "Ep: 803, steps: 24, D loss: 0.248266, acc:  53%, G loss: 1.606241\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 804, steps: 1, D loss: 0.230217, acc:  61%, G loss: 1.772847\n",
      "Ep: 804, steps: 2, D loss: 0.241240, acc:  57%, G loss: 1.462241\n",
      "Ep: 804, steps: 3, D loss: 0.202881, acc:  75%, G loss: 1.898182\n",
      "Ep: 804, steps: 4, D loss: 0.173199, acc:  84%, G loss: 1.672900\n",
      "Ep: 804, steps: 5, D loss: 0.248583, acc:  57%, G loss: 1.606682\n",
      "Ep: 804, steps: 6, D loss: 0.250037, acc:  53%, G loss: 1.504014\n",
      "Ep: 804, steps: 7, D loss: 0.338876, acc:  26%, G loss: 1.465380\n",
      "Ep: 804, steps: 8, D loss: 0.229901, acc:  62%, G loss: 1.717172\n",
      "Ep: 804, steps: 9, D loss: 0.239922, acc:  59%, G loss: 1.574125\n",
      "Ep: 804, steps: 10, D loss: 0.182682, acc:  78%, G loss: 1.497007\n",
      "Ep: 804, steps: 11, D loss: 0.257616, acc:  50%, G loss: 1.680195\n",
      "Ep: 804, steps: 12, D loss: 0.276907, acc:  39%, G loss: 1.338811\n",
      "Ep: 804, steps: 13, D loss: 0.287841, acc:  32%, G loss: 1.389881\n",
      "Ep: 804, steps: 14, D loss: 0.266416, acc:  48%, G loss: 1.462706\n",
      "Ep: 804, steps: 15, D loss: 0.261283, acc:  47%, G loss: 1.534918\n",
      "Ep: 804, steps: 16, D loss: 0.247986, acc:  54%, G loss: 1.562917\n",
      "Ep: 804, steps: 17, D loss: 0.230161, acc:  62%, G loss: 1.479741\n",
      "Ep: 804, steps: 18, D loss: 0.229938, acc:  61%, G loss: 1.603398\n",
      "Ep: 804, steps: 19, D loss: 0.217362, acc:  66%, G loss: 1.530735\n",
      "Ep: 804, steps: 20, D loss: 0.194872, acc:  76%, G loss: 1.618492\n",
      "Ep: 804, steps: 21, D loss: 0.271206, acc:  39%, G loss: 1.399193\n",
      "Ep: 804, steps: 22, D loss: 0.210276, acc:  66%, G loss: 1.576074\n",
      "Ep: 804, steps: 23, D loss: 0.219248, acc:  64%, G loss: 1.772939\n",
      "Ep: 804, steps: 24, D loss: 0.211362, acc:  70%, G loss: 1.614442\n",
      "Ep: 804, steps: 25, D loss: 0.249402, acc:  54%, G loss: 1.579280\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 805, steps: 1, D loss: 0.224256, acc:  66%, G loss: 1.678257\n",
      "Ep: 805, steps: 2, D loss: 0.254660, acc:  52%, G loss: 1.448435\n",
      "Ep: 805, steps: 3, D loss: 0.193693, acc:  77%, G loss: 1.856202\n",
      "Ep: 805, steps: 4, D loss: 0.175160, acc:  85%, G loss: 1.652443\n",
      "Ep: 805, steps: 5, D loss: 0.280605, acc:  49%, G loss: 1.647451\n",
      "Ep: 805, steps: 6, D loss: 0.246483, acc:  55%, G loss: 1.540640\n",
      "Ep: 805, steps: 7, D loss: 0.294078, acc:  35%, G loss: 1.568954\n",
      "Ep: 805, steps: 8, D loss: 0.221906, acc:  67%, G loss: 1.783179\n",
      "Ep: 805, steps: 9, D loss: 0.254385, acc:  56%, G loss: 1.526517\n",
      "Ep: 805, steps: 10, D loss: 0.179377, acc:  80%, G loss: 1.506279\n",
      "Ep: 805, steps: 11, D loss: 0.272138, acc:  47%, G loss: 1.645904\n",
      "Ep: 805, steps: 12, D loss: 0.287968, acc:  36%, G loss: 1.291760\n",
      "Ep: 805, steps: 13, D loss: 0.290812, acc:  33%, G loss: 1.338712\n",
      "Ep: 805, steps: 14, D loss: 0.265947, acc:  46%, G loss: 1.488944\n",
      "Ep: 805, steps: 15, D loss: 0.267238, acc:  45%, G loss: 1.563425\n",
      "Ep: 805, steps: 16, D loss: 0.253272, acc:  50%, G loss: 1.591537\n",
      "Ep: 805, steps: 17, D loss: 0.223744, acc:  66%, G loss: 1.465405\n",
      "Ep: 805, steps: 18, D loss: 0.223457, acc:  63%, G loss: 1.562779\n",
      "Ep: 805, steps: 19, D loss: 0.212221, acc:  67%, G loss: 1.588510\n",
      "Ep: 805, steps: 20, D loss: 0.179158, acc:  79%, G loss: 1.625046\n",
      "Ep: 805, steps: 21, D loss: 0.264874, acc:  41%, G loss: 1.490440\n",
      "Ep: 805, steps: 22, D loss: 0.214438, acc:  63%, G loss: 1.554969\n",
      "Ep: 805, steps: 23, D loss: 0.229975, acc:  61%, G loss: 1.783475\n",
      "Ep: 805, steps: 24, D loss: 0.201856, acc:  72%, G loss: 1.613748\n",
      "Saved Model\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 805, steps: 25, D loss: 0.259060, acc:  51%, G loss: 1.687189\n",
      "Ep: 805, steps: 26, D loss: 0.246734, acc:  55%, G loss: 1.412798\n",
      "Ep: 805, steps: 27, D loss: 0.164872, acc:  84%, G loss: 1.896161\n",
      "Ep: 805, steps: 28, D loss: 0.188787, acc:  80%, G loss: 1.635349\n",
      "Ep: 805, steps: 29, D loss: 0.259248, acc:  54%, G loss: 1.568343\n",
      "Ep: 805, steps: 30, D loss: 0.255512, acc:  52%, G loss: 1.512809\n",
      "Ep: 805, steps: 31, D loss: 0.294567, acc:  35%, G loss: 1.620525\n",
      "Ep: 805, steps: 32, D loss: 0.231802, acc:  63%, G loss: 1.754020\n",
      "Ep: 805, steps: 33, D loss: 0.251623, acc:  54%, G loss: 1.569944\n",
      "Ep: 805, steps: 34, D loss: 0.192397, acc:  74%, G loss: 1.501514\n",
      "Ep: 805, steps: 35, D loss: 0.288183, acc:  42%, G loss: 1.711484\n",
      "Ep: 805, steps: 36, D loss: 0.285777, acc:  36%, G loss: 1.336618\n",
      "Ep: 805, steps: 37, D loss: 0.278716, acc:  37%, G loss: 1.395445\n",
      "Ep: 805, steps: 38, D loss: 0.267607, acc:  46%, G loss: 1.417872\n",
      "Ep: 805, steps: 39, D loss: 0.267918, acc:  45%, G loss: 1.578790\n",
      "Ep: 805, steps: 40, D loss: 0.254084, acc:  51%, G loss: 1.573120\n",
      "Ep: 805, steps: 41, D loss: 0.226282, acc:  64%, G loss: 1.516676\n",
      "Ep: 805, steps: 42, D loss: 0.239714, acc:  57%, G loss: 1.578186\n",
      "Ep: 805, steps: 43, D loss: 0.224565, acc:  64%, G loss: 1.546963\n",
      "Ep: 805, steps: 44, D loss: 0.192517, acc:  74%, G loss: 1.669161\n",
      "Ep: 805, steps: 45, D loss: 0.262100, acc:  42%, G loss: 1.412850\n",
      "Ep: 805, steps: 46, D loss: 0.210637, acc:  66%, G loss: 1.532709\n",
      "Ep: 805, steps: 47, D loss: 0.216358, acc:  66%, G loss: 1.787271\n",
      "Ep: 805, steps: 48, D loss: 0.207870, acc:  70%, G loss: 1.519815\n",
      "Ep: 805, steps: 49, D loss: 0.256579, acc:  54%, G loss: 1.591057\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 806, steps: 1, D loss: 0.244382, acc:  56%, G loss: 1.730089\n",
      "Ep: 806, steps: 2, D loss: 0.232068, acc:  60%, G loss: 1.433484\n",
      "Ep: 806, steps: 3, D loss: 0.179968, acc:  81%, G loss: 1.876104\n",
      "Ep: 806, steps: 4, D loss: 0.176268, acc:  84%, G loss: 1.682595\n",
      "Ep: 806, steps: 5, D loss: 0.270604, acc:  50%, G loss: 1.668770\n",
      "Ep: 806, steps: 6, D loss: 0.250222, acc:  53%, G loss: 1.529300\n",
      "Ep: 806, steps: 7, D loss: 0.301890, acc:  35%, G loss: 1.530360\n",
      "Ep: 806, steps: 8, D loss: 0.218722, acc:  68%, G loss: 1.775752\n",
      "Ep: 806, steps: 9, D loss: 0.236829, acc:  60%, G loss: 1.561934\n",
      "Ep: 806, steps: 10, D loss: 0.190218, acc:  74%, G loss: 1.526131\n",
      "Ep: 806, steps: 11, D loss: 0.256113, acc:  49%, G loss: 1.701628\n",
      "Ep: 806, steps: 12, D loss: 0.274523, acc:  40%, G loss: 1.394439\n",
      "Ep: 806, steps: 13, D loss: 0.275406, acc:  39%, G loss: 1.392467\n",
      "Ep: 806, steps: 14, D loss: 0.266100, acc:  47%, G loss: 1.499028\n",
      "Ep: 806, steps: 15, D loss: 0.270323, acc:  44%, G loss: 1.545618\n",
      "Ep: 806, steps: 16, D loss: 0.255191, acc:  51%, G loss: 1.552697\n",
      "Ep: 806, steps: 17, D loss: 0.246901, acc:  54%, G loss: 1.551391\n",
      "Ep: 806, steps: 18, D loss: 0.233250, acc:  60%, G loss: 1.611069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 806, steps: 19, D loss: 0.216577, acc:  66%, G loss: 1.557146\n",
      "Ep: 806, steps: 20, D loss: 0.175500, acc:  82%, G loss: 1.641665\n",
      "Ep: 806, steps: 21, D loss: 0.259863, acc:  43%, G loss: 1.435446\n",
      "Ep: 806, steps: 22, D loss: 0.211668, acc:  63%, G loss: 1.659855\n",
      "Ep: 806, steps: 23, D loss: 0.205567, acc:  72%, G loss: 1.831649\n",
      "Ep: 806, steps: 24, D loss: 0.196122, acc:  75%, G loss: 1.548195\n",
      "Ep: 806, steps: 25, D loss: 0.250712, acc:  55%, G loss: 1.684241\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 807, steps: 1, D loss: 0.243237, acc:  57%, G loss: 1.699516\n",
      "Ep: 807, steps: 2, D loss: 0.249507, acc:  53%, G loss: 1.485191\n",
      "Ep: 807, steps: 3, D loss: 0.180519, acc:  81%, G loss: 1.847667\n",
      "Ep: 807, steps: 4, D loss: 0.174914, acc:  84%, G loss: 1.636625\n",
      "Ep: 807, steps: 5, D loss: 0.272301, acc:  54%, G loss: 1.569057\n",
      "Ep: 807, steps: 6, D loss: 0.245412, acc:  52%, G loss: 1.523814\n",
      "Ep: 807, steps: 7, D loss: 0.301022, acc:  35%, G loss: 1.468928\n",
      "Ep: 807, steps: 8, D loss: 0.211959, acc:  68%, G loss: 1.734725\n",
      "Ep: 807, steps: 9, D loss: 0.248876, acc:  56%, G loss: 1.558927\n",
      "Ep: 807, steps: 10, D loss: 0.180191, acc:  80%, G loss: 1.529462\n",
      "Ep: 807, steps: 11, D loss: 0.277460, acc:  45%, G loss: 1.672883\n",
      "Ep: 807, steps: 12, D loss: 0.289973, acc:  33%, G loss: 1.377961\n",
      "Ep: 807, steps: 13, D loss: 0.288966, acc:  34%, G loss: 1.401273\n",
      "Ep: 807, steps: 14, D loss: 0.271893, acc:  42%, G loss: 1.451635\n",
      "Ep: 807, steps: 15, D loss: 0.264647, acc:  46%, G loss: 1.518000\n",
      "Ep: 807, steps: 16, D loss: 0.256009, acc:  49%, G loss: 1.532114\n",
      "Ep: 807, steps: 17, D loss: 0.220405, acc:  67%, G loss: 1.651962\n",
      "Ep: 807, steps: 18, D loss: 0.242683, acc:  55%, G loss: 1.621429\n",
      "Ep: 807, steps: 19, D loss: 0.220816, acc:  66%, G loss: 1.553660\n",
      "Ep: 807, steps: 20, D loss: 0.187988, acc:  77%, G loss: 1.659707\n",
      "Ep: 807, steps: 21, D loss: 0.266791, acc:  41%, G loss: 1.521148\n",
      "Saved Model\n",
      "Ep: 807, steps: 22, D loss: 0.205425, acc:  66%, G loss: 1.547617\n",
      "Ep: 807, steps: 23, D loss: 0.220717, acc:  64%, G loss: 1.535577\n",
      "Ep: 807, steps: 24, D loss: 0.253579, acc:  54%, G loss: 1.543813\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 808, steps: 1, D loss: 0.228061, acc:  63%, G loss: 1.679096\n",
      "Ep: 808, steps: 2, D loss: 0.243522, acc:  58%, G loss: 1.489719\n",
      "Ep: 808, steps: 3, D loss: 0.192534, acc:  76%, G loss: 1.826846\n",
      "Ep: 808, steps: 4, D loss: 0.193555, acc:  79%, G loss: 1.619887\n",
      "Ep: 808, steps: 5, D loss: 0.256754, acc:  55%, G loss: 1.567407\n",
      "Ep: 808, steps: 6, D loss: 0.239401, acc:  55%, G loss: 1.513331\n",
      "Ep: 808, steps: 7, D loss: 0.296977, acc:  35%, G loss: 1.497087\n",
      "Ep: 808, steps: 8, D loss: 0.224437, acc:  65%, G loss: 1.704960\n",
      "Ep: 808, steps: 9, D loss: 0.242124, acc:  57%, G loss: 1.568933\n",
      "Ep: 808, steps: 10, D loss: 0.191969, acc:  74%, G loss: 1.552464\n",
      "Ep: 808, steps: 11, D loss: 0.273420, acc:  44%, G loss: 1.700154\n",
      "Ep: 808, steps: 12, D loss: 0.281036, acc:  38%, G loss: 1.358294\n",
      "Ep: 808, steps: 13, D loss: 0.281989, acc:  35%, G loss: 1.395721\n",
      "Ep: 808, steps: 14, D loss: 0.265998, acc:  47%, G loss: 1.496303\n",
      "Ep: 808, steps: 15, D loss: 0.257905, acc:  47%, G loss: 1.536428\n",
      "Ep: 808, steps: 16, D loss: 0.235172, acc:  60%, G loss: 1.572841\n",
      "Ep: 808, steps: 17, D loss: 0.224510, acc:  63%, G loss: 1.490018\n",
      "Ep: 808, steps: 18, D loss: 0.230150, acc:  60%, G loss: 1.595554\n",
      "Ep: 808, steps: 19, D loss: 0.211618, acc:  68%, G loss: 1.556096\n",
      "Ep: 808, steps: 20, D loss: 0.185772, acc:  76%, G loss: 1.604993\n",
      "Ep: 808, steps: 21, D loss: 0.270423, acc:  42%, G loss: 1.470217\n",
      "Ep: 808, steps: 22, D loss: 0.207610, acc:  65%, G loss: 1.483669\n",
      "Ep: 808, steps: 23, D loss: 0.230800, acc:  61%, G loss: 1.758337\n",
      "Ep: 808, steps: 24, D loss: 0.207973, acc:  70%, G loss: 1.483795\n",
      "Ep: 808, steps: 25, D loss: 0.243145, acc:  58%, G loss: 1.486110\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 809, steps: 1, D loss: 0.232681, acc:  60%, G loss: 1.623084\n",
      "Ep: 809, steps: 2, D loss: 0.234542, acc:  58%, G loss: 1.430943\n",
      "Ep: 809, steps: 3, D loss: 0.193109, acc:  76%, G loss: 1.946566\n",
      "Ep: 809, steps: 4, D loss: 0.166737, acc:  85%, G loss: 1.738874\n",
      "Ep: 809, steps: 5, D loss: 0.271185, acc:  51%, G loss: 1.623488\n",
      "Ep: 809, steps: 6, D loss: 0.249963, acc:  53%, G loss: 1.499173\n",
      "Ep: 809, steps: 7, D loss: 0.312318, acc:  32%, G loss: 1.437971\n",
      "Ep: 809, steps: 8, D loss: 0.206550, acc:  70%, G loss: 1.669112\n",
      "Ep: 809, steps: 9, D loss: 0.247070, acc:  55%, G loss: 1.547088\n",
      "Ep: 809, steps: 10, D loss: 0.183415, acc:  79%, G loss: 1.499721\n",
      "Ep: 809, steps: 11, D loss: 0.269011, acc:  48%, G loss: 1.718136\n",
      "Ep: 809, steps: 12, D loss: 0.274673, acc:  40%, G loss: 1.433461\n",
      "Ep: 809, steps: 13, D loss: 0.277440, acc:  39%, G loss: 1.372306\n",
      "Ep: 809, steps: 14, D loss: 0.268603, acc:  45%, G loss: 1.469926\n",
      "Ep: 809, steps: 15, D loss: 0.264928, acc:  48%, G loss: 1.513757\n",
      "Ep: 809, steps: 16, D loss: 0.253931, acc:  51%, G loss: 1.582709\n",
      "Ep: 809, steps: 17, D loss: 0.231856, acc:  61%, G loss: 1.462909\n",
      "Ep: 809, steps: 18, D loss: 0.241303, acc:  56%, G loss: 1.572847\n",
      "Ep: 809, steps: 19, D loss: 0.227158, acc:  62%, G loss: 1.541261\n",
      "Ep: 809, steps: 20, D loss: 0.184066, acc:  80%, G loss: 1.649140\n",
      "Ep: 809, steps: 21, D loss: 0.256420, acc:  44%, G loss: 1.412183\n",
      "Ep: 809, steps: 22, D loss: 0.219503, acc:  62%, G loss: 1.600593\n",
      "Ep: 809, steps: 23, D loss: 0.217441, acc:  67%, G loss: 1.774151\n",
      "Ep: 809, steps: 24, D loss: 0.219863, acc:  66%, G loss: 1.533357\n",
      "Ep: 809, steps: 25, D loss: 0.239452, acc:  59%, G loss: 1.634773\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 810, steps: 1, D loss: 0.243509, acc:  57%, G loss: 1.695958\n",
      "Ep: 810, steps: 2, D loss: 0.234015, acc:  57%, G loss: 1.461342\n",
      "Ep: 810, steps: 3, D loss: 0.186152, acc:  77%, G loss: 1.860614\n",
      "Ep: 810, steps: 4, D loss: 0.185579, acc:  80%, G loss: 1.667677\n",
      "Ep: 810, steps: 5, D loss: 0.264180, acc:  54%, G loss: 1.573163\n",
      "Ep: 810, steps: 6, D loss: 0.243816, acc:  55%, G loss: 1.541127\n",
      "Ep: 810, steps: 7, D loss: 0.303943, acc:  32%, G loss: 1.482240\n",
      "Ep: 810, steps: 8, D loss: 0.217681, acc:  67%, G loss: 1.781892\n",
      "Ep: 810, steps: 9, D loss: 0.249395, acc:  55%, G loss: 1.597476\n",
      "Ep: 810, steps: 10, D loss: 0.191394, acc:  76%, G loss: 1.509559\n",
      "Ep: 810, steps: 11, D loss: 0.281498, acc:  42%, G loss: 1.703138\n",
      "Ep: 810, steps: 12, D loss: 0.274866, acc:  42%, G loss: 1.315649\n",
      "Ep: 810, steps: 13, D loss: 0.290570, acc:  32%, G loss: 1.396872\n",
      "Ep: 810, steps: 14, D loss: 0.271654, acc:  43%, G loss: 1.448437\n",
      "Ep: 810, steps: 15, D loss: 0.254867, acc:  49%, G loss: 1.497228\n",
      "Ep: 810, steps: 16, D loss: 0.255379, acc:  50%, G loss: 1.540565\n",
      "Ep: 810, steps: 17, D loss: 0.226873, acc:  64%, G loss: 1.466434\n",
      "Ep: 810, steps: 18, D loss: 0.241346, acc:  56%, G loss: 1.586837\n",
      "Ep: 810, steps: 19, D loss: 0.218157, acc:  66%, G loss: 1.556855\n",
      "Saved Model\n",
      "Ep: 810, steps: 20, D loss: 0.197075, acc:  75%, G loss: 1.669749\n",
      "Ep: 810, steps: 21, D loss: 0.204495, acc:  68%, G loss: 1.496399\n",
      "Ep: 810, steps: 22, D loss: 0.220481, acc:  66%, G loss: 1.718720\n",
      "Ep: 810, steps: 23, D loss: 0.210988, acc:  69%, G loss: 1.497127\n",
      "Ep: 810, steps: 24, D loss: 0.246208, acc:  57%, G loss: 1.625813\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 811, steps: 1, D loss: 0.217218, acc:  66%, G loss: 1.742694\n",
      "Ep: 811, steps: 2, D loss: 0.236287, acc:  60%, G loss: 1.497473\n",
      "Ep: 811, steps: 3, D loss: 0.182848, acc:  79%, G loss: 1.918203\n",
      "Ep: 811, steps: 4, D loss: 0.164804, acc:  86%, G loss: 1.752930\n",
      "Ep: 811, steps: 5, D loss: 0.275909, acc:  48%, G loss: 1.559215\n",
      "Ep: 811, steps: 6, D loss: 0.247363, acc:  53%, G loss: 1.527079\n",
      "Ep: 811, steps: 7, D loss: 0.295154, acc:  36%, G loss: 1.575930\n",
      "Ep: 811, steps: 8, D loss: 0.218810, acc:  64%, G loss: 1.844339\n",
      "Ep: 811, steps: 9, D loss: 0.285045, acc:  48%, G loss: 1.619031\n",
      "Ep: 811, steps: 10, D loss: 0.211487, acc:  66%, G loss: 1.656310\n",
      "Ep: 811, steps: 11, D loss: 0.276247, acc:  45%, G loss: 1.735432\n",
      "Ep: 811, steps: 12, D loss: 0.284996, acc:  37%, G loss: 1.349960\n",
      "Ep: 811, steps: 13, D loss: 0.284391, acc:  35%, G loss: 1.442263\n",
      "Ep: 811, steps: 14, D loss: 0.272101, acc:  44%, G loss: 1.444437\n",
      "Ep: 811, steps: 15, D loss: 0.255630, acc:  52%, G loss: 1.485443\n",
      "Ep: 811, steps: 16, D loss: 0.246229, acc:  54%, G loss: 1.507715\n",
      "Ep: 811, steps: 17, D loss: 0.226105, acc:  62%, G loss: 1.446738\n",
      "Ep: 811, steps: 18, D loss: 0.226614, acc:  63%, G loss: 1.593174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 811, steps: 19, D loss: 0.221729, acc:  65%, G loss: 1.532903\n",
      "Ep: 811, steps: 20, D loss: 0.187892, acc:  78%, G loss: 1.661834\n",
      "Ep: 811, steps: 21, D loss: 0.272820, acc:  39%, G loss: 1.364182\n",
      "Ep: 811, steps: 22, D loss: 0.225839, acc:  60%, G loss: 1.496234\n",
      "Ep: 811, steps: 23, D loss: 0.224739, acc:  64%, G loss: 1.718374\n",
      "Ep: 811, steps: 24, D loss: 0.223516, acc:  63%, G loss: 1.527890\n",
      "Ep: 811, steps: 25, D loss: 0.248346, acc:  56%, G loss: 1.511591\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 812, steps: 1, D loss: 0.229517, acc:  62%, G loss: 1.681688\n",
      "Ep: 812, steps: 2, D loss: 0.241007, acc:  56%, G loss: 1.490655\n",
      "Ep: 812, steps: 3, D loss: 0.186486, acc:  77%, G loss: 1.877772\n",
      "Ep: 812, steps: 4, D loss: 0.181914, acc:  82%, G loss: 1.676449\n",
      "Ep: 812, steps: 5, D loss: 0.269439, acc:  49%, G loss: 1.607704\n",
      "Ep: 812, steps: 6, D loss: 0.249947, acc:  52%, G loss: 1.497227\n",
      "Ep: 812, steps: 7, D loss: 0.306117, acc:  33%, G loss: 1.410412\n",
      "Ep: 812, steps: 8, D loss: 0.225693, acc:  64%, G loss: 1.736391\n",
      "Ep: 812, steps: 9, D loss: 0.226040, acc:  64%, G loss: 1.619767\n",
      "Ep: 812, steps: 10, D loss: 0.182362, acc:  79%, G loss: 1.577731\n",
      "Ep: 812, steps: 11, D loss: 0.262895, acc:  48%, G loss: 1.767489\n",
      "Ep: 812, steps: 12, D loss: 0.291829, acc:  34%, G loss: 1.384974\n",
      "Ep: 812, steps: 13, D loss: 0.283178, acc:  36%, G loss: 1.385990\n",
      "Ep: 812, steps: 14, D loss: 0.262401, acc:  48%, G loss: 1.466454\n",
      "Ep: 812, steps: 15, D loss: 0.275575, acc:  44%, G loss: 1.511782\n",
      "Ep: 812, steps: 16, D loss: 0.236037, acc:  58%, G loss: 1.562576\n",
      "Ep: 812, steps: 17, D loss: 0.225492, acc:  65%, G loss: 1.452145\n",
      "Ep: 812, steps: 18, D loss: 0.235091, acc:  59%, G loss: 1.581186\n",
      "Ep: 812, steps: 19, D loss: 0.228809, acc:  63%, G loss: 1.562399\n",
      "Ep: 812, steps: 20, D loss: 0.186919, acc:  76%, G loss: 1.674318\n",
      "Ep: 812, steps: 21, D loss: 0.272916, acc:  38%, G loss: 1.481321\n",
      "Ep: 812, steps: 22, D loss: 0.222985, acc:  60%, G loss: 1.586222\n",
      "Ep: 812, steps: 23, D loss: 0.217713, acc:  66%, G loss: 1.752884\n",
      "Ep: 812, steps: 24, D loss: 0.214804, acc:  67%, G loss: 1.532117\n",
      "Ep: 812, steps: 25, D loss: 0.249521, acc:  56%, G loss: 1.566185\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 813, steps: 1, D loss: 0.232943, acc:  60%, G loss: 1.744969\n",
      "Ep: 813, steps: 2, D loss: 0.238991, acc:  58%, G loss: 1.517785\n",
      "Ep: 813, steps: 3, D loss: 0.184376, acc:  78%, G loss: 1.921180\n",
      "Ep: 813, steps: 4, D loss: 0.187695, acc:  80%, G loss: 1.727514\n",
      "Ep: 813, steps: 5, D loss: 0.254551, acc:  54%, G loss: 1.620276\n",
      "Ep: 813, steps: 6, D loss: 0.245296, acc:  56%, G loss: 1.477042\n",
      "Ep: 813, steps: 7, D loss: 0.312456, acc:  30%, G loss: 1.439536\n",
      "Ep: 813, steps: 8, D loss: 0.213638, acc:  68%, G loss: 1.693004\n",
      "Ep: 813, steps: 9, D loss: 0.248340, acc:  55%, G loss: 1.580709\n",
      "Ep: 813, steps: 10, D loss: 0.177784, acc:  78%, G loss: 1.565313\n",
      "Ep: 813, steps: 11, D loss: 0.265580, acc:  48%, G loss: 1.692690\n",
      "Ep: 813, steps: 12, D loss: 0.293945, acc:  36%, G loss: 1.369217\n",
      "Ep: 813, steps: 13, D loss: 0.279246, acc:  38%, G loss: 1.576940\n",
      "Ep: 813, steps: 14, D loss: 0.266599, acc:  47%, G loss: 1.565072\n",
      "Ep: 813, steps: 15, D loss: 0.257032, acc:  49%, G loss: 1.510570\n",
      "Ep: 813, steps: 16, D loss: 0.248764, acc:  53%, G loss: 1.564962\n",
      "Saved Model\n",
      "Ep: 813, steps: 17, D loss: 0.225943, acc:  64%, G loss: 1.512232\n",
      "Ep: 813, steps: 18, D loss: 0.210544, acc:  67%, G loss: 1.592342\n",
      "Ep: 813, steps: 19, D loss: 0.179129, acc:  77%, G loss: 1.694682\n",
      "Ep: 813, steps: 20, D loss: 0.254300, acc:  46%, G loss: 1.489275\n",
      "Ep: 813, steps: 21, D loss: 0.196462, acc:  69%, G loss: 1.725289\n",
      "Ep: 813, steps: 22, D loss: 0.221625, acc:  64%, G loss: 1.765489\n",
      "Ep: 813, steps: 23, D loss: 0.204896, acc:  70%, G loss: 1.535034\n",
      "Ep: 813, steps: 24, D loss: 0.252191, acc:  55%, G loss: 1.545869\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 814, steps: 1, D loss: 0.245098, acc:  57%, G loss: 1.688886\n",
      "Ep: 814, steps: 2, D loss: 0.237392, acc:  58%, G loss: 1.437836\n",
      "Ep: 814, steps: 3, D loss: 0.179013, acc:  82%, G loss: 1.850498\n",
      "Ep: 814, steps: 4, D loss: 0.186001, acc:  77%, G loss: 1.720652\n",
      "Ep: 814, steps: 5, D loss: 0.285069, acc:  48%, G loss: 1.571273\n",
      "Ep: 814, steps: 6, D loss: 0.240482, acc:  55%, G loss: 1.499524\n",
      "Ep: 814, steps: 7, D loss: 0.306554, acc:  34%, G loss: 1.472208\n",
      "Ep: 814, steps: 8, D loss: 0.225211, acc:  66%, G loss: 1.757398\n",
      "Ep: 814, steps: 9, D loss: 0.245086, acc:  57%, G loss: 1.572177\n",
      "Ep: 814, steps: 10, D loss: 0.184101, acc:  78%, G loss: 1.536121\n",
      "Ep: 814, steps: 11, D loss: 0.278880, acc:  42%, G loss: 1.701839\n",
      "Ep: 814, steps: 12, D loss: 0.277576, acc:  39%, G loss: 1.368059\n",
      "Ep: 814, steps: 13, D loss: 0.285327, acc:  34%, G loss: 1.454205\n",
      "Ep: 814, steps: 14, D loss: 0.266849, acc:  45%, G loss: 1.479918\n",
      "Ep: 814, steps: 15, D loss: 0.267455, acc:  45%, G loss: 1.537488\n",
      "Ep: 814, steps: 16, D loss: 0.252417, acc:  52%, G loss: 1.584208\n",
      "Ep: 814, steps: 17, D loss: 0.226511, acc:  62%, G loss: 1.538719\n",
      "Ep: 814, steps: 18, D loss: 0.244165, acc:  56%, G loss: 1.503400\n",
      "Ep: 814, steps: 19, D loss: 0.221688, acc:  64%, G loss: 1.561420\n",
      "Ep: 814, steps: 20, D loss: 0.173801, acc:  83%, G loss: 1.712833\n",
      "Ep: 814, steps: 21, D loss: 0.258735, acc:  44%, G loss: 1.403075\n",
      "Ep: 814, steps: 22, D loss: 0.211474, acc:  66%, G loss: 1.500311\n",
      "Ep: 814, steps: 23, D loss: 0.230263, acc:  62%, G loss: 1.836847\n",
      "Ep: 814, steps: 24, D loss: 0.214262, acc:  68%, G loss: 1.538189\n",
      "Ep: 814, steps: 25, D loss: 0.242853, acc:  56%, G loss: 1.618321\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 815, steps: 1, D loss: 0.216872, acc:  66%, G loss: 1.716442\n",
      "Ep: 815, steps: 2, D loss: 0.237214, acc:  60%, G loss: 1.507146\n",
      "Ep: 815, steps: 3, D loss: 0.182968, acc:  80%, G loss: 1.881322\n",
      "Ep: 815, steps: 4, D loss: 0.171683, acc:  84%, G loss: 1.691079\n",
      "Ep: 815, steps: 5, D loss: 0.286129, acc:  45%, G loss: 1.655362\n",
      "Ep: 815, steps: 6, D loss: 0.253442, acc:  53%, G loss: 1.553747\n",
      "Ep: 815, steps: 7, D loss: 0.304291, acc:  36%, G loss: 1.515981\n",
      "Ep: 815, steps: 8, D loss: 0.213435, acc:  69%, G loss: 1.797538\n",
      "Ep: 815, steps: 9, D loss: 0.253371, acc:  56%, G loss: 1.581941\n",
      "Ep: 815, steps: 10, D loss: 0.184073, acc:  78%, G loss: 1.536700\n",
      "Ep: 815, steps: 11, D loss: 0.274381, acc:  43%, G loss: 1.640381\n",
      "Ep: 815, steps: 12, D loss: 0.274348, acc:  41%, G loss: 1.372628\n",
      "Ep: 815, steps: 13, D loss: 0.282317, acc:  36%, G loss: 1.384363\n",
      "Ep: 815, steps: 14, D loss: 0.265759, acc:  46%, G loss: 1.441393\n",
      "Ep: 815, steps: 15, D loss: 0.278477, acc:  41%, G loss: 1.542675\n",
      "Ep: 815, steps: 16, D loss: 0.247958, acc:  54%, G loss: 1.520244\n",
      "Ep: 815, steps: 17, D loss: 0.230907, acc:  60%, G loss: 1.513111\n",
      "Ep: 815, steps: 18, D loss: 0.257635, acc:  51%, G loss: 1.563459\n",
      "Ep: 815, steps: 19, D loss: 0.220119, acc:  65%, G loss: 1.528671\n",
      "Ep: 815, steps: 20, D loss: 0.183440, acc:  79%, G loss: 1.712842\n",
      "Ep: 815, steps: 21, D loss: 0.259345, acc:  42%, G loss: 1.461404\n",
      "Ep: 815, steps: 22, D loss: 0.205524, acc:  66%, G loss: 1.514062\n",
      "Ep: 815, steps: 23, D loss: 0.218494, acc:  66%, G loss: 1.728217\n",
      "Ep: 815, steps: 24, D loss: 0.210637, acc:  69%, G loss: 1.562224\n",
      "Ep: 815, steps: 25, D loss: 0.255374, acc:  52%, G loss: 1.494860\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 816, steps: 1, D loss: 0.226888, acc:  63%, G loss: 1.638576\n",
      "Ep: 816, steps: 2, D loss: 0.244453, acc:  57%, G loss: 1.453487\n",
      "Ep: 816, steps: 3, D loss: 0.194099, acc:  77%, G loss: 1.860343\n",
      "Ep: 816, steps: 4, D loss: 0.176467, acc:  84%, G loss: 1.714092\n",
      "Ep: 816, steps: 5, D loss: 0.273975, acc:  52%, G loss: 1.587839\n",
      "Ep: 816, steps: 6, D loss: 0.253878, acc:  52%, G loss: 1.481804\n",
      "Ep: 816, steps: 7, D loss: 0.298349, acc:  37%, G loss: 1.593834\n",
      "Ep: 816, steps: 8, D loss: 0.228136, acc:  62%, G loss: 1.713387\n",
      "Saved Model\n",
      "Ep: 816, steps: 9, D loss: 0.251992, acc:  56%, G loss: 1.633259\n",
      "Ep: 816, steps: 10, D loss: 0.276404, acc:  42%, G loss: 1.621032\n",
      "Ep: 816, steps: 11, D loss: 0.261510, acc:  45%, G loss: 1.354659\n",
      "Ep: 816, steps: 12, D loss: 0.275600, acc:  39%, G loss: 1.479816\n",
      "Ep: 816, steps: 13, D loss: 0.261929, acc:  48%, G loss: 1.457679\n",
      "Ep: 816, steps: 14, D loss: 0.263088, acc:  47%, G loss: 1.554051\n",
      "Ep: 816, steps: 15, D loss: 0.245128, acc:  55%, G loss: 1.570379\n",
      "Ep: 816, steps: 16, D loss: 0.240210, acc:  56%, G loss: 1.539740\n",
      "Ep: 816, steps: 17, D loss: 0.241474, acc:  55%, G loss: 1.623435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 816, steps: 18, D loss: 0.226452, acc:  64%, G loss: 1.559893\n",
      "Ep: 816, steps: 19, D loss: 0.185452, acc:  81%, G loss: 1.649949\n",
      "Ep: 816, steps: 20, D loss: 0.258687, acc:  44%, G loss: 1.407829\n",
      "Ep: 816, steps: 21, D loss: 0.246992, acc:  56%, G loss: 1.538416\n",
      "Ep: 816, steps: 22, D loss: 0.240067, acc:  56%, G loss: 1.705549\n",
      "Ep: 816, steps: 23, D loss: 0.218338, acc:  66%, G loss: 1.510534\n",
      "Ep: 816, steps: 24, D loss: 0.256631, acc:  55%, G loss: 1.662122\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 817, steps: 1, D loss: 0.230878, acc:  64%, G loss: 1.738914\n",
      "Ep: 817, steps: 2, D loss: 0.241961, acc:  56%, G loss: 1.511713\n",
      "Ep: 817, steps: 3, D loss: 0.190764, acc:  76%, G loss: 1.866020\n",
      "Ep: 817, steps: 4, D loss: 0.188238, acc:  79%, G loss: 1.687024\n",
      "Ep: 817, steps: 5, D loss: 0.254225, acc:  55%, G loss: 1.624088\n",
      "Ep: 817, steps: 6, D loss: 0.251603, acc:  53%, G loss: 1.500525\n",
      "Ep: 817, steps: 7, D loss: 0.293803, acc:  38%, G loss: 1.369002\n",
      "Ep: 817, steps: 8, D loss: 0.216937, acc:  67%, G loss: 1.661839\n",
      "Ep: 817, steps: 9, D loss: 0.241736, acc:  57%, G loss: 1.590299\n",
      "Ep: 817, steps: 10, D loss: 0.196412, acc:  73%, G loss: 1.502782\n",
      "Ep: 817, steps: 11, D loss: 0.279207, acc:  44%, G loss: 1.753616\n",
      "Ep: 817, steps: 12, D loss: 0.271430, acc:  40%, G loss: 1.343788\n",
      "Ep: 817, steps: 13, D loss: 0.285571, acc:  34%, G loss: 1.386720\n",
      "Ep: 817, steps: 14, D loss: 0.265921, acc:  46%, G loss: 1.457596\n",
      "Ep: 817, steps: 15, D loss: 0.266937, acc:  46%, G loss: 1.526709\n",
      "Ep: 817, steps: 16, D loss: 0.247560, acc:  53%, G loss: 1.514945\n",
      "Ep: 817, steps: 17, D loss: 0.231723, acc:  60%, G loss: 1.472186\n",
      "Ep: 817, steps: 18, D loss: 0.231346, acc:  60%, G loss: 1.577231\n",
      "Ep: 817, steps: 19, D loss: 0.219550, acc:  64%, G loss: 1.554411\n",
      "Ep: 817, steps: 20, D loss: 0.187898, acc:  78%, G loss: 1.666376\n",
      "Ep: 817, steps: 21, D loss: 0.257680, acc:  45%, G loss: 1.403407\n",
      "Ep: 817, steps: 22, D loss: 0.211309, acc:  65%, G loss: 1.577126\n",
      "Ep: 817, steps: 23, D loss: 0.216062, acc:  68%, G loss: 1.889865\n",
      "Ep: 817, steps: 24, D loss: 0.202531, acc:  71%, G loss: 1.528767\n",
      "Ep: 817, steps: 25, D loss: 0.246704, acc:  56%, G loss: 1.566761\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 818, steps: 1, D loss: 0.227691, acc:  63%, G loss: 1.665372\n",
      "Ep: 818, steps: 2, D loss: 0.246432, acc:  55%, G loss: 1.510251\n",
      "Ep: 818, steps: 3, D loss: 0.181876, acc:  80%, G loss: 1.901402\n",
      "Ep: 818, steps: 4, D loss: 0.175153, acc:  83%, G loss: 1.695575\n",
      "Ep: 818, steps: 5, D loss: 0.254644, acc:  58%, G loss: 1.588278\n",
      "Ep: 818, steps: 6, D loss: 0.236791, acc:  55%, G loss: 1.512849\n",
      "Ep: 818, steps: 7, D loss: 0.311821, acc:  33%, G loss: 1.491270\n",
      "Ep: 818, steps: 8, D loss: 0.216389, acc:  68%, G loss: 1.673568\n",
      "Ep: 818, steps: 9, D loss: 0.241322, acc:  57%, G loss: 1.582201\n",
      "Ep: 818, steps: 10, D loss: 0.181341, acc:  79%, G loss: 1.499186\n",
      "Ep: 818, steps: 11, D loss: 0.246266, acc:  54%, G loss: 1.718846\n",
      "Ep: 818, steps: 12, D loss: 0.301190, acc:  31%, G loss: 1.338472\n",
      "Ep: 818, steps: 13, D loss: 0.293305, acc:  33%, G loss: 1.418623\n",
      "Ep: 818, steps: 14, D loss: 0.263777, acc:  48%, G loss: 1.495983\n",
      "Ep: 818, steps: 15, D loss: 0.278287, acc:  43%, G loss: 1.519669\n",
      "Ep: 818, steps: 16, D loss: 0.252831, acc:  52%, G loss: 1.522642\n",
      "Ep: 818, steps: 17, D loss: 0.224200, acc:  64%, G loss: 1.502913\n",
      "Ep: 818, steps: 18, D loss: 0.237594, acc:  58%, G loss: 1.605185\n",
      "Ep: 818, steps: 19, D loss: 0.224784, acc:  63%, G loss: 1.551399\n",
      "Ep: 818, steps: 20, D loss: 0.189779, acc:  77%, G loss: 1.655939\n",
      "Ep: 818, steps: 21, D loss: 0.271363, acc:  37%, G loss: 1.449228\n",
      "Ep: 818, steps: 22, D loss: 0.231272, acc:  61%, G loss: 1.607567\n",
      "Ep: 818, steps: 23, D loss: 0.223604, acc:  65%, G loss: 1.793099\n",
      "Saved Model\n",
      "Ep: 818, steps: 24, D loss: 0.217283, acc:  67%, G loss: 1.578369\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 819, steps: 1, D loss: 0.226094, acc:  65%, G loss: 1.630552\n",
      "Ep: 819, steps: 2, D loss: 0.229670, acc:  62%, G loss: 1.526898\n",
      "Ep: 819, steps: 3, D loss: 0.175406, acc:  81%, G loss: 1.966190\n",
      "Ep: 819, steps: 4, D loss: 0.171036, acc:  86%, G loss: 1.771510\n",
      "Ep: 819, steps: 5, D loss: 0.294543, acc:  47%, G loss: 1.642712\n",
      "Ep: 819, steps: 6, D loss: 0.253862, acc:  53%, G loss: 1.512693\n",
      "Ep: 819, steps: 7, D loss: 0.318455, acc:  33%, G loss: 1.482106\n",
      "Ep: 819, steps: 8, D loss: 0.219916, acc:  66%, G loss: 1.680880\n",
      "Ep: 819, steps: 9, D loss: 0.244115, acc:  58%, G loss: 1.531079\n",
      "Ep: 819, steps: 10, D loss: 0.177718, acc:  78%, G loss: 1.548247\n",
      "Ep: 819, steps: 11, D loss: 0.256732, acc:  50%, G loss: 1.772987\n",
      "Ep: 819, steps: 12, D loss: 0.280046, acc:  40%, G loss: 1.394194\n",
      "Ep: 819, steps: 13, D loss: 0.277990, acc:  38%, G loss: 1.399176\n",
      "Ep: 819, steps: 14, D loss: 0.275964, acc:  41%, G loss: 1.440786\n",
      "Ep: 819, steps: 15, D loss: 0.264841, acc:  48%, G loss: 1.517180\n",
      "Ep: 819, steps: 16, D loss: 0.249901, acc:  52%, G loss: 1.572463\n",
      "Ep: 819, steps: 17, D loss: 0.231632, acc:  61%, G loss: 1.501046\n",
      "Ep: 819, steps: 18, D loss: 0.244669, acc:  55%, G loss: 1.539368\n",
      "Ep: 819, steps: 19, D loss: 0.217135, acc:  66%, G loss: 1.534932\n",
      "Ep: 819, steps: 20, D loss: 0.177438, acc:  79%, G loss: 1.669816\n",
      "Ep: 819, steps: 21, D loss: 0.259919, acc:  42%, G loss: 1.438988\n",
      "Ep: 819, steps: 22, D loss: 0.227819, acc:  60%, G loss: 1.654740\n",
      "Ep: 819, steps: 23, D loss: 0.213561, acc:  68%, G loss: 1.763083\n",
      "Ep: 819, steps: 24, D loss: 0.211692, acc:  68%, G loss: 1.539276\n",
      "Ep: 819, steps: 25, D loss: 0.271878, acc:  47%, G loss: 1.689827\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 820, steps: 1, D loss: 0.225653, acc:  64%, G loss: 1.677839\n",
      "Ep: 820, steps: 2, D loss: 0.232217, acc:  60%, G loss: 1.502008\n",
      "Ep: 820, steps: 3, D loss: 0.179540, acc:  82%, G loss: 1.919462\n",
      "Ep: 820, steps: 4, D loss: 0.193507, acc:  77%, G loss: 1.686438\n",
      "Ep: 820, steps: 5, D loss: 0.268102, acc:  53%, G loss: 1.566589\n",
      "Ep: 820, steps: 6, D loss: 0.230426, acc:  57%, G loss: 1.513970\n",
      "Ep: 820, steps: 7, D loss: 0.318192, acc:  31%, G loss: 1.484869\n",
      "Ep: 820, steps: 8, D loss: 0.216706, acc:  68%, G loss: 1.729067\n",
      "Ep: 820, steps: 9, D loss: 0.257768, acc:  54%, G loss: 1.576853\n",
      "Ep: 820, steps: 10, D loss: 0.192618, acc:  75%, G loss: 1.500123\n",
      "Ep: 820, steps: 11, D loss: 0.267090, acc:  47%, G loss: 1.745385\n",
      "Ep: 820, steps: 12, D loss: 0.275389, acc:  42%, G loss: 1.361633\n",
      "Ep: 820, steps: 13, D loss: 0.288138, acc:  33%, G loss: 1.419204\n",
      "Ep: 820, steps: 14, D loss: 0.261756, acc:  47%, G loss: 1.462767\n",
      "Ep: 820, steps: 15, D loss: 0.264580, acc:  47%, G loss: 1.485670\n",
      "Ep: 820, steps: 16, D loss: 0.255336, acc:  51%, G loss: 1.547591\n",
      "Ep: 820, steps: 17, D loss: 0.225361, acc:  63%, G loss: 1.497306\n",
      "Ep: 820, steps: 18, D loss: 0.237134, acc:  59%, G loss: 1.592055\n",
      "Ep: 820, steps: 19, D loss: 0.225320, acc:  64%, G loss: 1.514868\n",
      "Ep: 820, steps: 20, D loss: 0.190177, acc:  77%, G loss: 1.591764\n",
      "Ep: 820, steps: 21, D loss: 0.271910, acc:  39%, G loss: 1.543632\n",
      "Ep: 820, steps: 22, D loss: 0.205071, acc:  68%, G loss: 1.530409\n",
      "Ep: 820, steps: 23, D loss: 0.216726, acc:  68%, G loss: 1.790444\n",
      "Ep: 820, steps: 24, D loss: 0.210325, acc:  70%, G loss: 1.458179\n",
      "Ep: 820, steps: 25, D loss: 0.245512, acc:  55%, G loss: 1.526838\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 821, steps: 1, D loss: 0.237413, acc:  60%, G loss: 1.636384\n",
      "Ep: 821, steps: 2, D loss: 0.249773, acc:  53%, G loss: 1.465543\n",
      "Ep: 821, steps: 3, D loss: 0.186537, acc:  80%, G loss: 1.871432\n",
      "Ep: 821, steps: 4, D loss: 0.181248, acc:  82%, G loss: 1.685967\n",
      "Ep: 821, steps: 5, D loss: 0.282077, acc:  47%, G loss: 1.544356\n",
      "Ep: 821, steps: 6, D loss: 0.235481, acc:  54%, G loss: 1.493361\n",
      "Ep: 821, steps: 7, D loss: 0.317774, acc:  32%, G loss: 1.543975\n",
      "Ep: 821, steps: 8, D loss: 0.228466, acc:  63%, G loss: 1.714004\n",
      "Ep: 821, steps: 9, D loss: 0.282450, acc:  44%, G loss: 1.623945\n",
      "Ep: 821, steps: 10, D loss: 0.188184, acc:  78%, G loss: 1.537835\n",
      "Ep: 821, steps: 11, D loss: 0.248006, acc:  53%, G loss: 1.705382\n",
      "Ep: 821, steps: 12, D loss: 0.277138, acc:  40%, G loss: 1.363878\n",
      "Ep: 821, steps: 13, D loss: 0.280892, acc:  36%, G loss: 1.379967\n",
      "Ep: 821, steps: 14, D loss: 0.256519, acc:  51%, G loss: 1.492354\n",
      "Saved Model\n",
      "Ep: 821, steps: 15, D loss: 0.274876, acc:  47%, G loss: 1.544203\n",
      "Ep: 821, steps: 16, D loss: 0.224288, acc:  62%, G loss: 1.538439\n",
      "Ep: 821, steps: 17, D loss: 0.243453, acc:  55%, G loss: 1.601190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 821, steps: 18, D loss: 0.228078, acc:  62%, G loss: 1.550723\n",
      "Ep: 821, steps: 19, D loss: 0.189342, acc:  75%, G loss: 1.599728\n",
      "Ep: 821, steps: 20, D loss: 0.268743, acc:  40%, G loss: 1.497167\n",
      "Ep: 821, steps: 21, D loss: 0.214661, acc:  63%, G loss: 1.639909\n",
      "Ep: 821, steps: 22, D loss: 0.215845, acc:  67%, G loss: 1.737989\n",
      "Ep: 821, steps: 23, D loss: 0.219374, acc:  66%, G loss: 1.475814\n",
      "Ep: 821, steps: 24, D loss: 0.254140, acc:  53%, G loss: 1.474870\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 822, steps: 1, D loss: 0.222555, acc:  64%, G loss: 1.727718\n",
      "Ep: 822, steps: 2, D loss: 0.247052, acc:  55%, G loss: 1.529938\n",
      "Ep: 822, steps: 3, D loss: 0.202860, acc:  75%, G loss: 1.857432\n",
      "Ep: 822, steps: 4, D loss: 0.191234, acc:  78%, G loss: 1.674299\n",
      "Ep: 822, steps: 5, D loss: 0.285564, acc:  47%, G loss: 1.580270\n",
      "Ep: 822, steps: 6, D loss: 0.235471, acc:  55%, G loss: 1.469123\n",
      "Ep: 822, steps: 7, D loss: 0.313749, acc:  29%, G loss: 1.404095\n",
      "Ep: 822, steps: 8, D loss: 0.221653, acc:  67%, G loss: 1.704180\n",
      "Ep: 822, steps: 9, D loss: 0.241062, acc:  57%, G loss: 1.529957\n",
      "Ep: 822, steps: 10, D loss: 0.188442, acc:  78%, G loss: 1.556416\n",
      "Ep: 822, steps: 11, D loss: 0.272487, acc:  46%, G loss: 1.698506\n",
      "Ep: 822, steps: 12, D loss: 0.276874, acc:  38%, G loss: 1.342063\n",
      "Ep: 822, steps: 13, D loss: 0.281584, acc:  38%, G loss: 1.345622\n",
      "Ep: 822, steps: 14, D loss: 0.270889, acc:  43%, G loss: 1.407213\n",
      "Ep: 822, steps: 15, D loss: 0.262648, acc:  48%, G loss: 1.501945\n",
      "Ep: 822, steps: 16, D loss: 0.249052, acc:  55%, G loss: 1.542588\n",
      "Ep: 822, steps: 17, D loss: 0.220519, acc:  65%, G loss: 1.480086\n",
      "Ep: 822, steps: 18, D loss: 0.232290, acc:  60%, G loss: 1.570432\n",
      "Ep: 822, steps: 19, D loss: 0.217944, acc:  65%, G loss: 1.501908\n",
      "Ep: 822, steps: 20, D loss: 0.195315, acc:  74%, G loss: 1.686448\n",
      "Ep: 822, steps: 21, D loss: 0.276096, acc:  37%, G loss: 1.430594\n",
      "Ep: 822, steps: 22, D loss: 0.224609, acc:  64%, G loss: 1.601090\n",
      "Ep: 822, steps: 23, D loss: 0.229481, acc:  62%, G loss: 1.781576\n",
      "Ep: 822, steps: 24, D loss: 0.221825, acc:  65%, G loss: 1.475900\n",
      "Ep: 822, steps: 25, D loss: 0.258947, acc:  53%, G loss: 1.560598\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 823, steps: 1, D loss: 0.241089, acc:  58%, G loss: 1.635753\n",
      "Ep: 823, steps: 2, D loss: 0.251708, acc:  52%, G loss: 1.445968\n",
      "Ep: 823, steps: 3, D loss: 0.195375, acc:  76%, G loss: 1.863329\n",
      "Ep: 823, steps: 4, D loss: 0.192947, acc:  79%, G loss: 1.621020\n",
      "Ep: 823, steps: 5, D loss: 0.282075, acc:  47%, G loss: 1.600458\n",
      "Ep: 823, steps: 6, D loss: 0.244270, acc:  56%, G loss: 1.499634\n",
      "Ep: 823, steps: 7, D loss: 0.298880, acc:  36%, G loss: 1.435045\n",
      "Ep: 823, steps: 8, D loss: 0.210338, acc:  70%, G loss: 1.645839\n",
      "Ep: 823, steps: 9, D loss: 0.250271, acc:  54%, G loss: 1.576336\n",
      "Ep: 823, steps: 10, D loss: 0.177123, acc:  79%, G loss: 1.498116\n",
      "Ep: 823, steps: 11, D loss: 0.277649, acc:  44%, G loss: 1.761259\n",
      "Ep: 823, steps: 12, D loss: 0.272093, acc:  42%, G loss: 1.356547\n",
      "Ep: 823, steps: 13, D loss: 0.273854, acc:  41%, G loss: 1.372518\n",
      "Ep: 823, steps: 14, D loss: 0.270287, acc:  42%, G loss: 1.463461\n",
      "Ep: 823, steps: 15, D loss: 0.275343, acc:  44%, G loss: 1.508099\n",
      "Ep: 823, steps: 16, D loss: 0.243625, acc:  56%, G loss: 1.533821\n",
      "Ep: 823, steps: 17, D loss: 0.233204, acc:  60%, G loss: 1.408605\n",
      "Ep: 823, steps: 18, D loss: 0.239864, acc:  57%, G loss: 1.587546\n",
      "Ep: 823, steps: 19, D loss: 0.220063, acc:  64%, G loss: 1.489799\n",
      "Ep: 823, steps: 20, D loss: 0.190894, acc:  77%, G loss: 1.624298\n",
      "Ep: 823, steps: 21, D loss: 0.269163, acc:  38%, G loss: 1.516347\n",
      "Ep: 823, steps: 22, D loss: 0.212788, acc:  65%, G loss: 1.502693\n",
      "Ep: 823, steps: 23, D loss: 0.209529, acc:  69%, G loss: 1.752096\n",
      "Ep: 823, steps: 24, D loss: 0.207078, acc:  71%, G loss: 1.521904\n",
      "Ep: 823, steps: 25, D loss: 0.263124, acc:  51%, G loss: 1.516238\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 824, steps: 1, D loss: 0.222797, acc:  66%, G loss: 1.621961\n",
      "Ep: 824, steps: 2, D loss: 0.251195, acc:  54%, G loss: 1.473105\n",
      "Ep: 824, steps: 3, D loss: 0.196102, acc:  75%, G loss: 1.902511\n",
      "Ep: 824, steps: 4, D loss: 0.178994, acc:  83%, G loss: 1.679428\n",
      "Ep: 824, steps: 5, D loss: 0.276286, acc:  50%, G loss: 1.595915\n",
      "Ep: 824, steps: 6, D loss: 0.236221, acc:  55%, G loss: 1.545228\n",
      "Saved Model\n",
      "Ep: 824, steps: 7, D loss: 0.286226, acc:  39%, G loss: 1.569730\n",
      "Ep: 824, steps: 8, D loss: 0.235054, acc:  63%, G loss: 1.621421\n",
      "Ep: 824, steps: 9, D loss: 0.193814, acc:  73%, G loss: 1.544074\n",
      "Ep: 824, steps: 10, D loss: 0.248316, acc:  54%, G loss: 1.717681\n",
      "Ep: 824, steps: 11, D loss: 0.286775, acc:  37%, G loss: 1.342555\n",
      "Ep: 824, steps: 12, D loss: 0.281965, acc:  36%, G loss: 1.387303\n",
      "Ep: 824, steps: 13, D loss: 0.265678, acc:  47%, G loss: 1.460958\n",
      "Ep: 824, steps: 14, D loss: 0.260718, acc:  47%, G loss: 1.531463\n",
      "Ep: 824, steps: 15, D loss: 0.248241, acc:  53%, G loss: 1.544979\n",
      "Ep: 824, steps: 16, D loss: 0.234763, acc:  56%, G loss: 1.487160\n",
      "Ep: 824, steps: 17, D loss: 0.231038, acc:  62%, G loss: 1.607782\n",
      "Ep: 824, steps: 18, D loss: 0.230670, acc:  60%, G loss: 1.547987\n",
      "Ep: 824, steps: 19, D loss: 0.193643, acc:  75%, G loss: 1.618665\n",
      "Ep: 824, steps: 20, D loss: 0.265596, acc:  41%, G loss: 1.437828\n",
      "Ep: 824, steps: 21, D loss: 0.209865, acc:  65%, G loss: 1.558791\n",
      "Ep: 824, steps: 22, D loss: 0.222249, acc:  66%, G loss: 1.765288\n",
      "Ep: 824, steps: 23, D loss: 0.208950, acc:  70%, G loss: 1.503381\n",
      "Ep: 824, steps: 24, D loss: 0.237104, acc:  60%, G loss: 1.498504\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 825, steps: 1, D loss: 0.226563, acc:  62%, G loss: 1.714260\n",
      "Ep: 825, steps: 2, D loss: 0.249376, acc:  54%, G loss: 1.487735\n",
      "Ep: 825, steps: 3, D loss: 0.197300, acc:  76%, G loss: 1.821139\n",
      "Ep: 825, steps: 4, D loss: 0.175806, acc:  85%, G loss: 1.666572\n",
      "Ep: 825, steps: 5, D loss: 0.263939, acc:  53%, G loss: 1.530458\n",
      "Ep: 825, steps: 6, D loss: 0.236074, acc:  56%, G loss: 1.481469\n",
      "Ep: 825, steps: 7, D loss: 0.311925, acc:  32%, G loss: 1.509499\n",
      "Ep: 825, steps: 8, D loss: 0.230279, acc:  62%, G loss: 1.685642\n",
      "Ep: 825, steps: 9, D loss: 0.262628, acc:  52%, G loss: 1.601147\n",
      "Ep: 825, steps: 10, D loss: 0.180783, acc:  77%, G loss: 1.492477\n",
      "Ep: 825, steps: 11, D loss: 0.269979, acc:  46%, G loss: 1.658521\n",
      "Ep: 825, steps: 12, D loss: 0.285651, acc:  35%, G loss: 1.350421\n",
      "Ep: 825, steps: 13, D loss: 0.278492, acc:  38%, G loss: 1.417106\n",
      "Ep: 825, steps: 14, D loss: 0.266761, acc:  47%, G loss: 1.529886\n",
      "Ep: 825, steps: 15, D loss: 0.266320, acc:  46%, G loss: 1.498349\n",
      "Ep: 825, steps: 16, D loss: 0.261267, acc:  48%, G loss: 1.560803\n",
      "Ep: 825, steps: 17, D loss: 0.240774, acc:  55%, G loss: 1.491312\n",
      "Ep: 825, steps: 18, D loss: 0.244747, acc:  54%, G loss: 1.554226\n",
      "Ep: 825, steps: 19, D loss: 0.232562, acc:  62%, G loss: 1.516391\n",
      "Ep: 825, steps: 20, D loss: 0.189114, acc:  78%, G loss: 1.584677\n",
      "Ep: 825, steps: 21, D loss: 0.264944, acc:  41%, G loss: 1.456744\n",
      "Ep: 825, steps: 22, D loss: 0.216904, acc:  64%, G loss: 1.495384\n",
      "Ep: 825, steps: 23, D loss: 0.233085, acc:  60%, G loss: 1.748895\n",
      "Ep: 825, steps: 24, D loss: 0.208692, acc:  71%, G loss: 1.476199\n",
      "Ep: 825, steps: 25, D loss: 0.245771, acc:  56%, G loss: 1.669850\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 826, steps: 1, D loss: 0.216598, acc:  67%, G loss: 1.609610\n",
      "Ep: 826, steps: 2, D loss: 0.240226, acc:  58%, G loss: 1.422595\n",
      "Ep: 826, steps: 3, D loss: 0.193205, acc:  76%, G loss: 1.851714\n",
      "Ep: 826, steps: 4, D loss: 0.176029, acc:  85%, G loss: 1.649570\n",
      "Ep: 826, steps: 5, D loss: 0.280705, acc:  50%, G loss: 1.629284\n",
      "Ep: 826, steps: 6, D loss: 0.240431, acc:  54%, G loss: 1.501807\n",
      "Ep: 826, steps: 7, D loss: 0.311787, acc:  32%, G loss: 1.391348\n",
      "Ep: 826, steps: 8, D loss: 0.221577, acc:  65%, G loss: 1.664372\n",
      "Ep: 826, steps: 9, D loss: 0.243643, acc:  58%, G loss: 1.606610\n",
      "Ep: 826, steps: 10, D loss: 0.176596, acc:  80%, G loss: 1.531162\n",
      "Ep: 826, steps: 11, D loss: 0.264113, acc:  47%, G loss: 1.631551\n",
      "Ep: 826, steps: 12, D loss: 0.285534, acc:  38%, G loss: 1.349212\n",
      "Ep: 826, steps: 13, D loss: 0.290656, acc:  34%, G loss: 1.398524\n",
      "Ep: 826, steps: 14, D loss: 0.257702, acc:  51%, G loss: 1.453701\n",
      "Ep: 826, steps: 15, D loss: 0.275111, acc:  43%, G loss: 1.521541\n",
      "Ep: 826, steps: 16, D loss: 0.249895, acc:  53%, G loss: 1.582564\n",
      "Ep: 826, steps: 17, D loss: 0.224194, acc:  64%, G loss: 1.502888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 826, steps: 18, D loss: 0.236159, acc:  59%, G loss: 1.583939\n",
      "Ep: 826, steps: 19, D loss: 0.210048, acc:  70%, G loss: 1.555057\n",
      "Ep: 826, steps: 20, D loss: 0.190792, acc:  77%, G loss: 1.656888\n",
      "Ep: 826, steps: 21, D loss: 0.271275, acc:  38%, G loss: 1.457822\n",
      "Saved Model\n",
      "Ep: 826, steps: 22, D loss: 0.208793, acc:  66%, G loss: 1.547807\n",
      "Ep: 826, steps: 23, D loss: 0.217884, acc:  65%, G loss: 1.485199\n",
      "Ep: 826, steps: 24, D loss: 0.245607, acc:  56%, G loss: 1.514719\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 827, steps: 1, D loss: 0.255986, acc:  53%, G loss: 1.591203\n",
      "Ep: 827, steps: 2, D loss: 0.236712, acc:  58%, G loss: 1.465024\n",
      "Ep: 827, steps: 3, D loss: 0.194822, acc:  74%, G loss: 1.786227\n",
      "Ep: 827, steps: 4, D loss: 0.196528, acc:  78%, G loss: 1.611392\n",
      "Ep: 827, steps: 5, D loss: 0.245532, acc:  58%, G loss: 1.642294\n",
      "Ep: 827, steps: 6, D loss: 0.236273, acc:  56%, G loss: 1.505283\n",
      "Ep: 827, steps: 7, D loss: 0.293513, acc:  38%, G loss: 1.479456\n",
      "Ep: 827, steps: 8, D loss: 0.225095, acc:  66%, G loss: 1.618650\n",
      "Ep: 827, steps: 9, D loss: 0.240938, acc:  59%, G loss: 1.544887\n",
      "Ep: 827, steps: 10, D loss: 0.175878, acc:  82%, G loss: 1.529311\n",
      "Ep: 827, steps: 11, D loss: 0.263394, acc:  46%, G loss: 1.701988\n",
      "Ep: 827, steps: 12, D loss: 0.282793, acc:  36%, G loss: 1.303542\n",
      "Ep: 827, steps: 13, D loss: 0.275918, acc:  39%, G loss: 1.403444\n",
      "Ep: 827, steps: 14, D loss: 0.268862, acc:  45%, G loss: 1.463368\n",
      "Ep: 827, steps: 15, D loss: 0.277174, acc:  41%, G loss: 1.513415\n",
      "Ep: 827, steps: 16, D loss: 0.251797, acc:  54%, G loss: 1.563384\n",
      "Ep: 827, steps: 17, D loss: 0.233325, acc:  61%, G loss: 1.484699\n",
      "Ep: 827, steps: 18, D loss: 0.224790, acc:  65%, G loss: 1.577186\n",
      "Ep: 827, steps: 19, D loss: 0.215315, acc:  66%, G loss: 1.551716\n",
      "Ep: 827, steps: 20, D loss: 0.182467, acc:  77%, G loss: 1.652730\n",
      "Ep: 827, steps: 21, D loss: 0.255360, acc:  47%, G loss: 1.436471\n",
      "Ep: 827, steps: 22, D loss: 0.226980, acc:  59%, G loss: 1.564359\n",
      "Ep: 827, steps: 23, D loss: 0.231409, acc:  59%, G loss: 1.806722\n",
      "Ep: 827, steps: 24, D loss: 0.214581, acc:  66%, G loss: 1.526303\n",
      "Ep: 827, steps: 25, D loss: 0.241899, acc:  56%, G loss: 1.559945\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 828, steps: 1, D loss: 0.226908, acc:  64%, G loss: 1.739588\n",
      "Ep: 828, steps: 2, D loss: 0.255499, acc:  52%, G loss: 1.520045\n",
      "Ep: 828, steps: 3, D loss: 0.202501, acc:  73%, G loss: 1.821079\n",
      "Ep: 828, steps: 4, D loss: 0.197059, acc:  78%, G loss: 1.655510\n",
      "Ep: 828, steps: 5, D loss: 0.255088, acc:  58%, G loss: 1.570007\n",
      "Ep: 828, steps: 6, D loss: 0.244466, acc:  55%, G loss: 1.416782\n",
      "Ep: 828, steps: 7, D loss: 0.311470, acc:  30%, G loss: 1.410701\n",
      "Ep: 828, steps: 8, D loss: 0.220191, acc:  66%, G loss: 1.721848\n",
      "Ep: 828, steps: 9, D loss: 0.251340, acc:  54%, G loss: 1.552030\n",
      "Ep: 828, steps: 10, D loss: 0.180095, acc:  80%, G loss: 1.642424\n",
      "Ep: 828, steps: 11, D loss: 0.273770, acc:  45%, G loss: 1.671753\n",
      "Ep: 828, steps: 12, D loss: 0.269600, acc:  41%, G loss: 1.386875\n",
      "Ep: 828, steps: 13, D loss: 0.278795, acc:  38%, G loss: 1.355404\n",
      "Ep: 828, steps: 14, D loss: 0.252520, acc:  50%, G loss: 1.456678\n",
      "Ep: 828, steps: 15, D loss: 0.279416, acc:  46%, G loss: 1.435967\n",
      "Ep: 828, steps: 16, D loss: 0.250905, acc:  53%, G loss: 1.584473\n",
      "Ep: 828, steps: 17, D loss: 0.225608, acc:  64%, G loss: 1.524513\n",
      "Ep: 828, steps: 18, D loss: 0.235476, acc:  58%, G loss: 1.566939\n",
      "Ep: 828, steps: 19, D loss: 0.226980, acc:  62%, G loss: 1.526543\n",
      "Ep: 828, steps: 20, D loss: 0.185462, acc:  77%, G loss: 1.697458\n",
      "Ep: 828, steps: 21, D loss: 0.265007, acc:  41%, G loss: 1.439938\n",
      "Ep: 828, steps: 22, D loss: 0.203805, acc:  67%, G loss: 1.539589\n",
      "Ep: 828, steps: 23, D loss: 0.224807, acc:  64%, G loss: 1.769410\n",
      "Ep: 828, steps: 24, D loss: 0.209228, acc:  69%, G loss: 1.533673\n",
      "Ep: 828, steps: 25, D loss: 0.251824, acc:  54%, G loss: 1.517715\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 829, steps: 1, D loss: 0.223690, acc:  63%, G loss: 1.674775\n",
      "Ep: 829, steps: 2, D loss: 0.230486, acc:  61%, G loss: 1.457360\n",
      "Ep: 829, steps: 3, D loss: 0.182281, acc:  79%, G loss: 1.828550\n",
      "Ep: 829, steps: 4, D loss: 0.177935, acc:  83%, G loss: 1.735748\n",
      "Ep: 829, steps: 5, D loss: 0.260230, acc:  53%, G loss: 1.586191\n",
      "Ep: 829, steps: 6, D loss: 0.253752, acc:  54%, G loss: 1.513580\n",
      "Ep: 829, steps: 7, D loss: 0.298274, acc:  36%, G loss: 1.512058\n",
      "Saved Model\n",
      "Ep: 829, steps: 8, D loss: 0.223690, acc:  64%, G loss: 1.675466\n",
      "Ep: 829, steps: 9, D loss: 0.211287, acc:  67%, G loss: 1.503533\n",
      "Ep: 829, steps: 10, D loss: 0.296221, acc:  37%, G loss: 1.614137\n",
      "Ep: 829, steps: 11, D loss: 0.262285, acc:  47%, G loss: 1.395842\n",
      "Ep: 829, steps: 12, D loss: 0.274111, acc:  40%, G loss: 1.424385\n",
      "Ep: 829, steps: 13, D loss: 0.253431, acc:  52%, G loss: 1.558121\n",
      "Ep: 829, steps: 14, D loss: 0.276320, acc:  44%, G loss: 1.496711\n",
      "Ep: 829, steps: 15, D loss: 0.241852, acc:  57%, G loss: 1.560519\n",
      "Ep: 829, steps: 16, D loss: 0.198808, acc:  74%, G loss: 1.555293\n",
      "Ep: 829, steps: 17, D loss: 0.237928, acc:  58%, G loss: 1.542849\n",
      "Ep: 829, steps: 18, D loss: 0.230736, acc:  62%, G loss: 1.528317\n",
      "Ep: 829, steps: 19, D loss: 0.166724, acc:  81%, G loss: 1.664389\n",
      "Ep: 829, steps: 20, D loss: 0.267295, acc:  40%, G loss: 1.430632\n",
      "Ep: 829, steps: 21, D loss: 0.208000, acc:  64%, G loss: 1.638639\n",
      "Ep: 829, steps: 22, D loss: 0.233312, acc:  59%, G loss: 1.765049\n",
      "Ep: 829, steps: 23, D loss: 0.206430, acc:  69%, G loss: 1.499487\n",
      "Ep: 829, steps: 24, D loss: 0.261026, acc:  52%, G loss: 1.445850\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 830, steps: 1, D loss: 0.215037, acc:  68%, G loss: 1.595653\n",
      "Ep: 830, steps: 2, D loss: 0.245584, acc:  54%, G loss: 1.413743\n",
      "Ep: 830, steps: 3, D loss: 0.183573, acc:  80%, G loss: 1.836619\n",
      "Ep: 830, steps: 4, D loss: 0.191894, acc:  79%, G loss: 1.606961\n",
      "Ep: 830, steps: 5, D loss: 0.258162, acc:  58%, G loss: 1.557111\n",
      "Ep: 830, steps: 6, D loss: 0.246797, acc:  53%, G loss: 1.499146\n",
      "Ep: 830, steps: 7, D loss: 0.314929, acc:  33%, G loss: 1.422850\n",
      "Ep: 830, steps: 8, D loss: 0.220024, acc:  66%, G loss: 1.590950\n",
      "Ep: 830, steps: 9, D loss: 0.245763, acc:  57%, G loss: 1.554885\n",
      "Ep: 830, steps: 10, D loss: 0.197781, acc:  72%, G loss: 1.586370\n",
      "Ep: 830, steps: 11, D loss: 0.259697, acc:  48%, G loss: 1.707884\n",
      "Ep: 830, steps: 12, D loss: 0.279034, acc:  40%, G loss: 1.370509\n",
      "Ep: 830, steps: 13, D loss: 0.288227, acc:  36%, G loss: 1.406622\n",
      "Ep: 830, steps: 14, D loss: 0.268147, acc:  43%, G loss: 1.534823\n",
      "Ep: 830, steps: 15, D loss: 0.261177, acc:  49%, G loss: 1.523104\n",
      "Ep: 830, steps: 16, D loss: 0.244221, acc:  56%, G loss: 1.550937\n",
      "Ep: 830, steps: 17, D loss: 0.233120, acc:  60%, G loss: 1.439119\n",
      "Ep: 830, steps: 18, D loss: 0.231908, acc:  59%, G loss: 1.560759\n",
      "Ep: 830, steps: 19, D loss: 0.213812, acc:  68%, G loss: 1.528052\n",
      "Ep: 830, steps: 20, D loss: 0.199620, acc:  72%, G loss: 1.623186\n",
      "Ep: 830, steps: 21, D loss: 0.258729, acc:  41%, G loss: 1.418283\n",
      "Ep: 830, steps: 22, D loss: 0.212168, acc:  63%, G loss: 1.522569\n",
      "Ep: 830, steps: 23, D loss: 0.229177, acc:  63%, G loss: 1.818118\n",
      "Ep: 830, steps: 24, D loss: 0.208162, acc:  70%, G loss: 1.459270\n",
      "Ep: 830, steps: 25, D loss: 0.254324, acc:  55%, G loss: 1.456519\n",
      "Data exhausted, Re Initialize\n",
      "Ep: 831, steps: 1, D loss: 0.224463, acc:  63%, G loss: 1.620451\n",
      "Ep: 831, steps: 2, D loss: 0.250122, acc:  53%, G loss: 1.454519\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
